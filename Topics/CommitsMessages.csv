Dominant Topic,Topics,System,message
Data conversion,Data conversion,ztd-jni,"Fix all -Xcheck=jni warnings

Also add tests to validate that we produce the same
bytestream as the zstd binary at different levels (1,3,6,9)/"
,,ztd-jni,Zstd v0.4.4/
,,ztd-jni,Rebase it on top of zstd v0.2.1/
Restructuring the code,"Compression tasks, Restructuring the code",ztd-jni,"Reorder the static methods, All compression first then all decompression
then the rest, inputs checking + utility methods"
Data conversion,Data conversion,ztd-jni,"Fix the 32bit compatibility

We need to double cast the jlongs (always 64 bits)
through size_t (equals to pointer size) in order to
be able to cast it then as a pointer. Else we get
compiler warnings./"
,,ztd-jni,Upgrade to Zstd-0.4.0/
Compression tasks,Compression tasks,ztd-jni,Zstd 0.3 - add compression using HC variant/
Performance management,Performance management,ztd-jni,Concurrency fix/
,,ztd-jni,Zstd-0.4.1 and performance test in streaming mode/
,,ztd-jni,fix buffer sizes/
Performance management,Performance management,ztd-jni,"Use JNI_ABORT on releasing the source buffer critical

It is more clear that the source is not modified/"
Compression tasks,Compression tasks,ztd-jni,"Add support for InputStream.read()

FilterInputStream forwards read() directly to the underlying stream,
bypassing Zstd entirely.
compression decompression
Extend tests to exercise this for the first byte read in a few test
cases./"
,,ztd-jni,Add decompression support for the legacy v04 format/
API Management,API Management,ztd-jni,Move to the new streaming API/
,,ztd-jni,adding support for legacy dictionary trainer
,,ztd-jni,"Align the JNI names with the new streaming API, Move to the new streaming API, Use the ZBUFF based streaming compression"
,,ztd-jni,Zstd-0.7.4/
API Management,API Management,ztd-jni,"Finish the move to the new Zstd streaming API

On the decoding size we have gained ability to decode
multiple frames but we have lost:

- support for legacy formats bu it will be restored with
  zstd-1.0.0

- support for decoding un-finished streams, we have to
  figure out how it will play out with the multi-frame
  support./"
Compression tasks,Compression tasks,ztd-jni,"Add Java wrappers and C implementations of compress/decompress using direct ByteBuffer
compression decompression
The wrappers work a bit differently than the ones that operate on byte[] due to the different semantics of ByteBuffer.  I've tried to make this implementation operate on the position(), capacity(), and limit() properties of the buffers in the idiomatic way./"
API Management,"API Management, Compression tasks",ztd-jni,Align the JNI names with the new streaming API/
,,ztd-jni,"Use the ZBUFF based streaming compression

It simplifies the whole interaction as the zstd is taking
care of the buffers, blocks and allignments so we don't have
to know anything about its internals./"
Compression tasks,Compression tasks,ztd-jni,Zstd-1.1.0 update/
,,ztd-jni,"Make the continuous decompression a property if ZstdInputStream/Zstd streaming decompression done

Ignore the read size hints and read all the available
input up to ZSTD_DStreamInSize. This will handle when
when read hint = 1 that is ambigous (could mean read 1
byte or don't read, we have more data in the output
buffer.

This will also handle the skippable frames corner case
that was not handled correctly previously./"
,,ztd-jni,"Cleanup the streaming decompression

We still have to fix the first read size magic number./"
,,ztd-jni,skip in multiples of recommendedDOutSize/
,,ztd-jni,"WIP streaming decompression

It fixes the case where the requested reads are not
multiple of the compression block size./"
,,ztd-jni,Simplifications and initial implementation of available()/
,,ztd-jni,"Finish the move to the new Zstd streaming API

On the decoding size we have gained ability to decode
multiple frames but we have lost:
compression decompression
- support for legacy formats bu it will be restored with
  zstd-1.0.0
dictionary
- support for decoding un-finished streams, we have to
  figure out how it will play out with the multi-frame
  support./"
Compression tasks,Compression tasks,ztd-jni,"Add Java wrappers and C implementations of compress/decompress using direct ByteBuffer
dictionary
The wrappers work a bit differently than the ones that operate on byte[] due to the different semantics of ByteBuffer.  I've tried to make this implementation operate on the position(), capacity(), and limit() properties of the buffers in the idiomatic way./inputs checking + utility methods./"
Compression tasks,Compression tasks,ztd-jni,"Reorder the static methods

All compression first then all decompression then the rest/"
API Management,API Management,ztd-jni,"Zstd-1.1.2   ,Move to the new streaming API   ,  "
API Management,API Management,ztd-jni,"Zstd-0.7.4   ,Move to the new streaming API   ,  "
,,ztd-jni,"Zstd-1.2.0   ,  "
Feature migration,Feature migration,ztd-jni,"Zstd-1.1.4   ,  "
Feature migration,Feature migration,ztd-jni,"Upgrade to Zstd-1.1.3   ,"
,,ztd-jni,"  Zstd-1.1.4   ,  "
Feature migration,Feature migration,ztd-jni,"Add dictionary support.   ,"
,,ztd-jni,"Zstd-0.7.0   ,  "
Compression tasks,Compression tasks,ztd-jni,Add compress/decompress for direct ByteBuffers with dicts/
Feature migration,Feature migration,ztd-jni,Import Zstd-1.3.0/
,,ztd-jni,Zstd-1.2.0/
Feature migration,Feature migration,ztd-jni,configure token session
Compression tasks,Compression tasks,ztd-jni,"More work on the dictionary support

Also adding tests for byte[] and direct buffers compression and
decompression using byte[] or pre-compiled dictionaries/"
API Management,API Management,ztd-jni,"Fallback to the system's provided libzstd-jni.so

In this way we can cover systems with incompatible libc implementations
and systems that are not covered by the maven's distributed jar.

Also rename the shared library to libzstd-jni.so in order not to
conflict with the zstd's libzstd.so/"
Feature migration,Feature migration,ztd-jni,Import Zstd-1.3.2/
Feature migration,Feature migration,ztd-jni,Import zstd-1.3.3/
Feature migration,Feature migration,ztd-jni,"Zstd-0.8.0   ,  "
Feature migration,Feature migration,ztd-jni,"Import Zstd-1.3.4   ,  "
,,ztd-jni,"Import zstd-1.3.3   ,  "
Feature migration,Feature migration,ztd-jni,Import Zstd-1.3.4/
Feature migration,Feature migration,ztd-jni,Add support for legacy dictionary trainer/
Data conversion,Data conversion,ztd-jni,"Add support for byte array dicts in the streams

It adds .withDict(byte[] dict) method to all Input/Output stream
implementation.

Next steps: tests, CDict support also/"
Performance management,Performance management,ztd-jni,Small optimization of DictTrainer from direct buffers/
API Management,API Management,ztd-jni,"Revert ""Simplify Native.load() by using Java 7 NIO2 API""

This reverts commit 8dfb4f93586b16d64d10caadd44320cadbe2be96./"
,,ztd-jni,Clean up temporary native library on Linux. Does not work on Windows/
Feature migration,Feature migration,ztd-jni,Import v1.3.6/
Performance management,Performance management,ztd-jni,"Synchronize all state-changing methods

Addresses #82/"
Feature migration,Feature migration,ztd-jni,Import v1.3.8/
Compression tasks,Compression tasks,ztd-jni,Add the ability to specify checksum flag in static compress methods./
Compression tasks,Compression tasks,ztd-jni,"Reorder the static methods

All compression first then all decompression then the rest   ,  inputs checking + utility methods.   ,  "
,,ztd-jni,"Add Java wrappers and C implementations of compress/decompress using direct ByteBuffer blocks

The wrappers work a bit differently than the ones that operate on byte[] due to the different semantics of ByteBuffer.  I've tried to make this implementation operate on the position(), capacity(), and limit() properties of the buffers in the idiomatic way.   ,  Add compress/decompress for direct ByteBuffers with dicts   ,  More work on the dictionary support

Also adding tests for byte[] and direct buffers compression and
decompression using byte[] or pre-compiled dictionaries   ,  "
API Management,"API Management, Compression tasks",ztd-jni,"expose faster API to allow re-using of dictionaries   ,  "
,,ztd-jni,"More work on the dictionary support

Also adding tests for byte[] and direct buffers compression and
decompression using byte[] or pre-compiled dictionaries   ,  "
Compression tasks,Compression tasks,ztd-jni,"Implemented DirectBuffer streaming compressor   ,  "
API Management,"API Management, Compression tasks",ztd-jni,"Add support for InputStream.read()

FilterInputStream forwards read() directly to the underlying stream,
bypassing Zstd entirely.
external library
Extend tests to exercise this for the first byte read in a few test
cases.   ,  "
,,ztd-jni,"Simplifications and initial implementation of available()   ,  "
,,ztd-jni,"WIP streaming decompression
compression decompression
It fixes the case where the requested reads are not
multiple of the compression block size.   ,  "
,,ztd-jni,"Add decompression support for the legacy v04 format   ,  "
,,ztd-jni,"Cleanup the streaming decompression

We still have to fix the first read size magic number.   ,  "
,,ztd-jni,"Finish the move to the new Zstd streaming API

This will also handle the skippable frames corner case
that was not handled correctly previously.   ,  "
API Management,"API Management, Compression tasks",ztd-jni,"Move to the new streaming API   ,  "
,,ztd-jni,"Don't close frames on flush if there is nothing written   ,  "
,,ztd-jni,"Align the JNI names with the new streaming API   ,  "
,,ztd-jni,"Concurrency fix   ,  Use the ZBUFF based streaming compression
external library
It simplifies the whole interaction as the zstd is taking
care of the buffers, blocks and allignments so we don't have
to know anything about its internals.   ,  Make ""flush"" close the frame if ""syncFlush"" is required   ,  "
Performance management,Performance management,ztd-jni,"Use JNI_ABORT on releasing the source buffer critical

It is more clear that the source is not modified   ,  "
,,ztd-jni,"Finish the move to the new Zstd streaming API

On the decoding size we have gained ability to decode
multiple frames but we have lost:

- support for legacy formats bu it will be restored with
  zstd-1.0.0

- support for decoding un-finished streams, we have to
  figure out how it will play out with the multi-frame
  support.   ,  Fix all -Xcheck=jni warnings

Also add tests to validate that we produce the same
bytestream as the zstd binary at different levels (1,3,6,9)   ,  Zstd v0.4.4   ,  "
Performance management,Performance management,ztd-jni,"Use JNI_ABORT on releasing the source buffer critical

It is more clear that the source is not modified   ,  Zstd v0.4.4   ,  Small simplification, "
,,ztd-jni,"memory performance impprovement

We can construct the cursors on the stack and
preserve only the current position in the object   ,  "
Compression tasks,Compression tasks,ztd-jni,"Add Java wrappers and C implementations of compress/decompress using direct ByteBuffer
compression
The wrappers work a bit differently than the ones that operate on byte[] due to the different semantics of ByteBuffer.  I've tried to make this implementation operate on the position(), capacity(), and limit() properties of the buffers in the idiomatic way.   ,  "
Feature migration,Feature migration,ztd-jni,"Add support for legacy format v0.4   ,  Upgrade to Zstd-1.1.3   ,  "
Compression tasks,Compression tasks,ztd-jni,"Upgrade to Zstd-1.1.3   ,  "
,,ztd-jni,"Add compression decompression support for the legacy v04 format   ,  "
Feature migration,Feature migration,ztd-jni,"Add support for legacy dictionary trainer   ,  "
Performance management,Performance management,ztd-jni,"Increase the coverage a bit   ,  "
Performance management,Performance management,ztd-jni,"Small optimization of DictTrainer from direct buffers   ,  "
Performance management,Performance management,ztd-jni,"Make read(buf, offset, len) read at least 1 byte or return -1

This is the default behaviour for InputStream and it's better to follow
it for consistency.

Follow up of the discussion in #75   ,  "
Performance management,Performance management,ztd-jni,"Clean up temporary native library on Linux. Does not work on Windows   ,  "
Memory Management,Memory Management,vlc-android,"libvlcjni: rework exceptions throwing from jni

Throw OutOfMemoryError instead of IllegalStateException for allocation errors.   ,  "
exception Management,exception Management,vlc-android,rework exceptions throwing from jni
Restructuring the code,Restructuring the code,vlc-android,"LibVLC: Helper functions + documentation   ,  "
,,vlc-android,"LibVLC: Implement VideoHelper   ,  "
,,vlc-android,"Refactor video size into ScaleType enum   ,  "
Feature migration,Feature migration,vlc-android,"Integrate DisplayManagaer to LibVLC   ,  "
Memory Management,Memory Management,vlc-android,OutOfMemoryError
API Management,API Management,vlc-android,"Medialibrary: Add playlist & genre callbacks   ,  "
,,vlc-android,"Real track count for Albums and Playlist

Regular count could be out of date   ,  "
,,vlc-android,"Medialibrary: Pagination for audio requests   ,  "
,,vlc-android,"Medialibrary: Check for nullPtr queries   ,  "
,,vlc-android,"Add Medialibrary Folder API   ,  "
Restructuring the code,Restructuring the code,vlc-android,"Medialibrary callbacks refactoring   ,  "
Restructuring the code,Restructuring the code,vlc-android,"Add a callback for ext devices (un)mount   ,  "
Restructuring the code,Restructuring the code,vlc-android,"LibVLC: MediaPlayer: add record() method   ,  "
Restructuring the code,Restructuring the code,vlc-android,"LibVLC: Implement record event   ,  "
Restructuring the code,Restructuring the code,vlc-android,refactor common methods
exception Management,exception Management,vlc-android,Fix UninitializedPropertyAccessException
API Management,API Management,vlc-android,Update API
API Management,API Management,vlc-android,Bump medialibrary version
Restructuring the code,Restructuring the code,vlc-android,Refactor BrowserModel: add a delegate to perform the path operations
Feature migration,Feature migration,vlc-android,Migrate app theme settings
Restructuring the code,Restructuring the code,vlc-android,Refactor empty and loading views in a custom view
API Management,API Management,vlc-android,Medialibrary: folder & video groups equals methods
API Management,API Management,vlc-android,Add new parameter
API Management,API Management,vlc-android,Update libraries versions
API Management,API Management,vlc-android,Video refactor: actions re-implementation
API Management,API Management,vlc-android,Match new API
Restructuring the code,Restructuring the code,vlc-android,Ensure Medialibrary is started only once
API Management,API Management,vlc-android,Video card reorganisation
Restructuring the code,Restructuring the code,vlc-android,Start refactoring video fragments
API Management,API Management,vlc-android,Update libraries dependencies
API Management,API Management,vlc-android,Medialibrary: Match new API changes
Restructuring the code,Restructuring the code,vlc-android,Use a dedicated version number for migrations and code cleanup
Restructuring the code,Restructuring the code,vlc-android,PlaylistManager: code factorization
Restructuring the code,Restructuring the code,vlc-android,"Code cleaning, replace runnables by coroutine call"
Memory Management,Memory Management,Rocksdb,"Fix for 2PC causing WAL to grow too large

Summary:
Consider the following single column family scenario:
prepare in log A
commit in log B
*WAL is too large, flush all CFs to releast log A* memory
*CFA is on log B so we do not see CFA is depending on log A so no flush is requested*
Memory leak
To fix this we must also consider the log containing the prepare section when determining what log a CF is dependent on.
Closes https://github.com/facebook/rocksdb//1768

Differential Revision: D4403265

ed By: reidHoruff

fbshipit-source-id: ce800ff   ,  "
Threads Management,Threads Management,Rocksdb,"Fix CompactFiles() bug when used with CompactionFilter using SuperVersion

Summary:
GetAndRefSuperVersion() should not be called again in the same thread before ReturnAndCleanupSuperVersion() is called.

We solve this issue in the same way Iterator is solving it, but using GetReferencedSuperVersion()

This was discovered in https://github.com/facebook/mysql-5.6/issues/427 by alxyang
Closes https://github.com/facebook/rocksdb//1803


fbshipit-source-id: 5e54322   ,  "
Restructuring the code,Restructuring the code,Rocksdb,"Disable IngestExternalFile in ReadOnly mode

Summary:
Refactor to disable IngestExternalFile() in read only mode
Closes https://github.com/facebook/rocksdb//1781

Differential Revision: D4439179

ed By: IslamAbdelRahman

fbshipit-source-id: b7e46e7   ,  "
Memory Management,Memory Management,Rocksdb,"Flush job should release reference current version if sync log failed memory

Summary:
Fix the bug when sync log fail, FlushJob::Run() will not be execute and
reference to cfd->current() will not be release.
Closes https://github.com/facebook/rocksdb//1792

Differential Revision: D4441316

ed By: yiwu-arbug

fbshipit-source-id: 5523e28   ,  "
Memory Management,Memory Management,Rocksdb,"Change DB::GetApproximateSizes for more flexibility needed for MyRocks

Summary:
Added an option to GetApproximateSizes to exclude file stats, as MyRocks has those counted exactly and we need only stats from memtables.
Closes https://github.com/facebook/rocksdb//1787

Differential Revision: D4441111

ed By: IslamAbdelRahman

fbshipit-source-id: c11f4c3   ,  "
,,Rocksdb,"Add test DBTest2.GetRaceFlush which can expose a data race bug

Summary:
A current data race issue in Get() and Flush() can cause a Get() to return wrong results when a flush happened in the middle. Disable the test for now.
Closes https://github.com/facebook/rocksdb//1813

Differential Revision: D4472310

ed By: siying

fbshipit-source-id: 5755ebd   ,  "
,,Rocksdb,"Range deletions unsupported in tailing iterator

Summary:
change the iterator status to NotSupported as soon as a range tombstone
is encountered by a ForwardIterator.
Closes https://github.com/facebook/rocksdb//1593

Differential Revision: D4246294

ed By: ajkr

fbshipit-source-id: aef9f49   ,  "
,,Rocksdb,"Test merge op covered by range deletion in memtable

Summary:
It's a test case for #1797. Also got rid of kTypeDeletion in the conditional since we treat it the same as kTypeRangeDeletion.
Closes https://github.com/facebook/rocksdb//1800

Differential Revision: D4451300

ed By: ajkr

fbshipit-source-id: b39dda1   ,  "
,,Rocksdb,"Fix DeleteRange including sentinels in output files

Summary:
when writing RangeDelAggregator::AddToBuilder, I forgot that there are sentinel tombstones in the middle of the interval map since gaps between real tombstones are represented with sentinels.

blame: #1614
Closes https://github.com/facebook/rocksdb//1804

Differential Revision: D4460426

ed By: ajkr

fbshipit-source-id: 69444b5   ,  "
,,Rocksdb,"NewLRUCache() to pick number of shard bits based on capacity if not given

Summary:
If the users use the NewLRUCache() without passing in the number of shard bits, instead of using hard-coded 6, we'll determine it based on capacity.
Closes https://github.com/facebook/rocksdb//1584

Differential Revision: D4242517

ed By: siying

fbshipit-source-id: 86b0f18   ,  "
,,Rocksdb,"Cleaner default options using C++11 in-class init

Summary:
C++11 in-class initialization is cleaner and makes it the default more explicit to our users and more visible.
Use it for ColumnFamilyOptions and DBOptions
Closes https://github.com/facebook/rocksdb//1822

Differential Revision: D4490473

ed By: IslamAbdelRahman

fbshipit-source-id: c493a87   ,  "
Threads Management,Threads Management,Rocksdb,"Move ThreadLocal implementation into .cc

Summary: Closes https://github.com/facebook/rocksdb//1829

Differential Revision: D4502314

ed By: siying

fbshipit-source-id: f46fac1   ,  "
,,Rocksdb,"Generalize Env registration framework

Summary:
The Env registration framework supports registering client Envs and selecting which one to instantiate according to a text field. This enabled things like adding the -env_uri argument to db_bench, so the same binary could be reused with different Envs just by changing CLI config.

Now this problem has come up again in a non-Env context, as I want to instantiate a client Statistics implementation from db_bench, which is configured entirely via text parameters. Also, in the future we may wish to use it for deserializing client objects when loading OPTIONS file.

This diff generalizes the Env registration logic to work with arbitrary types.

- Generalized registration and instantiation code by templating them
- The entire implementation is in a header file as that's Google style guide's recommendation for template definitions
- Pattern match with std::regex_match rather than checking prefix, which was the previous behavior
- Rename functions/files to be non-Env-specific
Closes https://github.com/facebook/rocksdb//1776

Differential Revision: D4421933

ed By: ajkr

fbshipit-source-id: 34647d1   ,  "
,,Rocksdb,"Add path to WritableFileWriter. (#4039)

Summary:
We want to sample the file I/O issued by RocksDB and report the function calls. This requires us to include the file paths otherwise it's hard to tell what has been going on.
 Request resolved: https://github.com/facebook/rocksdb//4039

Differential Revision: D8670178

ed By: riversand963

fbshipit-source-id: 97ee806d1c583a2983e28e213ee764dc6ac28f7a   ,  "
,,Rocksdb,"Support pragma once in all header files and cleanup some warnings (#4339)

Summary:
As you know, almost all compilers support ""pragma once"" keyword instead of using include guards. To be keep consistency between header files, all header files are edited.

Besides this, try to fix some warnings about loss of data.
 Request resolved: https://github.com/facebook/rocksdb//4339

Differential Revision: D9654990

ed By: ajkr

fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848   ,  "
,,Rocksdb,"Add path to WritableFileWriter. (#4039)

Summary:
In RocksDB, for a given SST file, all data blocks are compressed with the same dictionary. When we compress a block using the dictionary's raw bytes, the compression library first has to digest the dictionary to get it into a usable form. This digestion work is redundant and ideally should be done once per file.

ZSTD offers APIs for the caller to create and reuse a digested dictionary object (`ZSTD_CDict`). In this PR, we call `ZSTD_createCDict` once per file to digest the raw bytes. Then we use `ZSTD_compress_usingCDict` to compress each data block using the pre-digested dictionary. Once the file's created `ZSTD_freeCDict` releases the resources held by the digested dictionary.

There are a couple other changes included in this PR:

- Changed the parameter object for (un)compression functions from `CompressionContext`/`UncompressionContext` to `CompressionInfo`/`UncompressionInfo`. This avoids the previous pattern, where `CompressionContext`/`UncompressionContext` had to be mutated before calling a (un)compression function depending on whether dictionary should be used. I felt that mutation was error-prone so eliminated it.
- Added support for digested uncompression dictionaries (`ZSTD_DDict`) as well. However, this PR does not support reusing them across uncompression calls for the same file. That work is deferred to a later PR when we will store the `ZSTD_DDict` objects in block cache.
dictionary compression
Differential Revision: D9257078

ed By: ajkr

fbshipit-source-id: 21b8cb6bbdd48e459f1c62343780ab66c0a64438   ,  "
Compression tasks,"Compression tasks, threads management",Rocksdb,"Digest ZSTD compression dictionary once per SST file (#4251)

Summary:
In RocksDB, for a given SST file, all data blocks are compressed with the same dictionary. When we compress a block using the dictionary's raw bytes, the compression library first has to digest the dictionary to get it into a usable form. This digestion work is redundant and ideally should be done once per file.

ZSTD offers APIs for the caller to create and reuse a digested dictionary object (`ZSTD_CDict`). In this PR, we call `ZSTD_createCDict` once per file to digest the raw bytes. Then we use `ZSTD_compress_usingCDict` to compress each data block using the pre-digested dictionary. Once the file's created `ZSTD_freeCDict` releases the resources held by the digested dictionary.

There are a couple other changes included in this PR:

- Changed the parameter object for (un)compression functions from `CompressionContext`/`UncompressionContext` to `CompressionInfo`/`UncompressionInfo`. This avoids the previous pattern, where `CompressionContext`/`UncompressionContext` had to be mutated before calling a (un)compression function depending on whether dictionary should be used. I felt that mutation was error-prone so eliminated it.
- Added support for digested uncompression dictionaries (`ZSTD_DDict`) as well. However, this PR does not support reusing them across uncompression calls for the same file. That work is deferred to a later PR when we will store the `ZSTD_DDict` objects in block cache.
 Request resolved: https://github.com/facebook/rocksdb//4251
compression dictionary
Differential Revision: D9257078

ed By: ajkr

fbshipit-source-id: 21b8cb6bbdd48e459f1c62343780ab66c0a64438   ,  Revert ""Digest ZSTD compression dictionary once per SST file (#4251)"" (#4347)
  ,  "
,,Rocksdb,"Invoke OnTableFileCreated for empty SSTs (#4307)

Summary:
The API comment on `OnTableFileCreationStarted` (https://github.com/facebook/rocksdb/blob/b6280d01f9f9c4305c536dfb804775fce3956280/include/rocksdb/listener.h#L331-L333) led users to believe a call to `OnTableFileCreationStarted` will always be matched with a call to `OnTableFileCreated`. However, we were skipping the `OnTableFileCreated` call in one case: no error happens but also no file is generated since there's no data.

This PR adds the call to `OnTableFileCreated` for that case. The filename will be ""(nil)"" and the size will be zero.

Differential Revision: D9485201

ed By: ajkr

fbshipit-source-id: 2f077ec7913f128487aae2624c69a50762394df6   ,  "
Compression tasks,"Thread management, Compression tasks",Rocksdb,"Revert ""Digest ZSTD compression dictionary once per SST file (#4251)"" (#4347)

Summary:
When reading an expired key using `Get(..., std::string* value)` API, BlobDB first read the index entry and decode expiration from it. In this case, although BlobDB reset the PinnableSlice, the index entry is stored in user provided string `value`. The value will be returned as a garbage value, despite status being NotFound. Fixing it by use a different PinnableSlice to read the index entry.
 Request resolved: https://github.com/facebook/rocksdb//4321

Differential Revision: D9519042

ed By: yiwu-arbug

fbshipit-source-id: f054c951a1fa98265228be94f931904ed7056677   ,  "
,,Rocksdb,"BlobDB: Implement DisableFileDeletions (#4314)


Summary:
In RocksDB, for a given SST file, all data blocks are compressed with the same dictionary. When we compress a block using the dictionary's raw bytes, the compression library first has to digest the dictionary to get it into a usable form. This digestion work is redundant and ideally should be done once per file.
Differential Revision: D9654990
 compression decompression
ed By: ajkr

fbshipit-source-id: c2cf3d2d03a599847684bed81378c401920ca848   ,  "
Memory Management,Memory Management,Rocksdb,"Move MemoryAllocator option from Cache to BlockBasedTableOptions (#4676)

Summary:
Per offline discussion with siying, `MemoryAllocator` and `Cache` should be decouple. The idea is that memory allocator handles memory allocation, while cache handle cache policy.

It is normal that external cache libraries pack couple the two components for better optimization. If we want to integrate with such library in the future, we can make a wrapper of the library implementing both `Cache` and `MemoryAllocator` interface.
 Request resolved: https://github.com/facebook/rocksdb//4676

Differential Revision: D13047662

ed By: yiwu-arbug

fbshipit-source-id: cd42e246d80ab600b4de47d073f7d2db308ce6dd   ,  "
,,Rocksdb,"Revert ""apply ReadOptions.iterate_upper_bound to transaction iterator… (#4705)

Summary:
… (#4656)""

This reverts commit b76398a82bde58bfcfa3ed5ba3dbfb6168c241de.

Will add test coverage for iterate_upper_bound before re-commit b76398
 Request resolved: https://github.com/facebook/rocksdb//4705

Differential Revision: D13148592

ed By: miasantreble

fbshipit-source-id: 4d1ce0bfd9f7a5359a7688bd780eb06a66f45b1f   ,  "
Threads Management,Threads Management,Rocksdb,"Lock free MultiGet (#4754)

Summary:
Avoid locking the DB mutex in order to reference SuperVersions. Instead, we get the thread local cached SuperVersion for each column family in the list. It depends on finding a sequence number that overlaps with all the open memtables. We start with the latest published sequence number, and if any of the memtables is sealed before we can get all the SuperVersions, the process is repeated. After a few times, give up and lock the DB mutex.


Differential Revision: D13363550

ed By: anand1976

fbshipit-source-id: 6243e8de7dbd9c8bb490a8eca385da0c855b1dd4   ,  "
Compression tasks,Compression Tasks,Rocksdb,"Cache dictionary used for decompressing data blocks (#4881)

Summary:
- If block cache disabled or not used for meta-blocks, `BlockBasedTableReader::Rep::uncompression_dict` owns the `UncompressionDict`. It is preloaded during `PrefetchIndexAndFilterBlocks`.
- If block cache is enabled and used for meta-blocks, block cache owns the `UncompressionDict`, which holds dictionary and digested dictionary when needed. It is never prefetched though there is a TODO for this in the code. The cache key is simply the compression dictionary block handle. compression dcompression
- New stats for compression dictionary accesses in block cache: ""BLOCK_CACHE_COMPRESSION_DICT_*"" and ""compression_dict_block_read_count""
 Request resolved: https://github.com/facebook/rocksdb//4881

Differential Revision: D13663801

ed By: ajkr

fbshipit-source-id: bdcc54044e180855cdcc57639b493b0e016c9a3f   ,  "
,,Rocksdb,"Fix ticker stat for number files closed (#4703)

Summary:
We haven't been populating `NO_FILE_CLOSES` since v1.5.8 even though it was never marked as deprecated. Start populating it again. Conveniently `DeleteTableReader` has an unused `void*` argument that we can use...

Blame: 63f216ee0a2a6e28f9dfe24913d134d3a7fa3aca

Closes #4700.
 Request resolved: https://github.com/facebook/rocksdb//4703

Differential Revision: D13146769

ed By: ajkr

fbshipit-source-id: ad8d6fb0493e701f60a165a3bca1787d255be008   ,  "
Memory Management,Memory Management,Rocksdb,"Disallow customized hash function in DynamicBloom (#4915)

Summary:
I didn't find where customized hash function is used in DynamicBloom. This can only reduce performance. Remove it.
 Request resolved: https://github.com/facebook/rocksdb//4915

Differential Revision: D13794452

ed By: siying

fbshipit-source-id: e38669b11e01444d2d782da11c7decabbd851819   ,  "
Threads Management,Threads Management,Rocksdb,"Refactor atomic flush result installation to MANIFEST (#4791)

Summary:
as titled.
Since different bg flush threads can flush different sets of column families
(due to column family creation and drop), we decide not to let one thread
perform atomic flush result installation for other threads. Bg flush threads
will install their atomic flush results sequentially to MANIFEST, using
a conditional variable, i.e. atomic_flush_install_cv_ to coordinate.
 Request resolved: https://github.com/facebook/rocksdb//4791

Differential Revision: D13498930

ed By: riversand963

fbshipit-source-id: dd7482fc41f4bd22dad1e1ef7d4764ef424688d7   ,  "
,,Rocksdb,"PlainTable should avoid copying Get() results from immortal source. (#4924)

Summary:
https://github.com/facebook/rocksdb//4053 avoids memcopy for Get() results if files are immortable
(read-only DB, max_open_files=-1) and the file is ammaped. The same optimization is being applied to PlainTable
here.
 Request resolved: https://github.com/facebook/rocksdb//4924

Differential Revision: D13827749

ed By: siying

fbshipit-source-id: 1f2cbfc530b40ce08ccd53f95f6e78de4d1c2f96   ,  "
,,Rocksdb,"Remove v1 RangeDelAggregator (#4778)

Summary:
Now that v2 is fully functional, the v1 aggregator is removed.
The v2 aggregator has been renamed.
 Request resolved: https://github.com/facebook/rocksdb//4778

Differential Revision: D13495930

ed By: abhimadan

fbshipit-source-id: 9d69500a60a283e79b6c4fa938fc68a8aa4d40d6   ,  "
,,Rocksdb,"Modify FragmentedRangeTombstoneList member layout (#4632)

Summary:
Rather than storing a `vector<RangeTombstone>`, we now store a
`vector<RangeTombstoneStack>` and a `vector<SequenceNumber>`. A
`RangeTombstoneStack` contains the start and end keys of a range tombstone
fragment, and indices into the seqnum vector to indicate which sequence
numbers the fragment is located at. The diagram below illustrates an


This format allows binary searching the tombstone list to use less key
comparisons, which helps in cases where there are many overlapping
tombstones. Also, this format makes it easier to add DBIter-like
semantics to `FragmentedRangeTombstoneIterator` in the future.
 Request resolved: https://github.com/facebook/rocksdb//4632

Differential Revision: D13053103

ed By: abhimadan

fbshipit-source-id: e8220cc712fcf5be4d602913bb23ace8ea5f8ef0   ,  "
Performance management,Performance management,Rocksdb,"Preload some files even if options.max_open_files (#3340)

Summary:
Choose to preload some files if options.max_open_files != -1. This can slightly narrow the gap of performance between options.max_open_files is -1 and a large number. To avoid a significant regression to DB reopen speed if options.max_open_files != -1. Limit the files to preload in DB open time to 16.
 Request resolved: https://github.com/facebook/rocksdb//3340

Differential Revision: D6686945

ed By: siying

fbshipit-source-id: 8ec11bbdb46e3d0cdee7b6ad5897a09c5a07869f   ,  "
,,Rocksdb,"Add a placeholder in manifest indicating ignorable record (#4960)

Summary:
We want to reserve some right that some extra information added manifest
in the future can be forward compatible by previous versions. Now we create a
place holder for that. A bit in tag is added to indicate that a field can be
safely ignored.
 Request resolved: https://github.com/facebook/rocksdb//4960

Differential Revision: D14000484

ed By: siying

fbshipit-source-id: cbf5bad3f9d5ec798f789806f244d1c20d3b66d6   ,  "
,,Rocksdb,"Move MemoryAllocator option from Cache to BlockBasedTableOptions (#4676)

Summary:
Per offline discussion with siying, `MemoryAllocator` and `Cache` should be decouple. The idea is that memory allocator handles memory allocation, while cache handle cache policy.

It is normal that external cache libraries pack couple the two components for better optimization. If we want to integrate with such library in the future, we can make a wrapper of the library implementing both `Cache` and `MemoryAllocator` interface.
 Request resolved: https://github.com/facebook/rocksdb//4676

Differential Revision: D13047662

ed By: yiwu-arbug

fbshipit-source-id: cd42e246d80ab600b4de47d073f7d2db308ce6dd   ,  Revert ""Move MemoryAllocator option from Cache to BlockBasedTableOpti… (#4697)
  ,  "
Performance management,Performance Managament,Rocksdb,"Add a new CPU time counter to compaction report (#4889)

Summary:
Measure CPU time consumed for a compaction and report it in the stats report
Enable NowCPUNanos() to work for MacOS
 Request resolved: https://github.com/facebook/rocksdb//4889

Differential Revision: D13701276

ed By: zinoale

fbshipit-source-id: 5024e5bbccd4dd10fd90d947870237f436445055   ,  "
,,Rocksdb,"Move MemoryAllocator option from Cache to BlockBasedTableOptions (#4676)

Summary:
Per offline discussion with siying, `MemoryAllocator` and `Cache` should be decouple. The idea is that memory allocator handles memory allocation, while cache handle cache policy.

It is normal that external cache libraries pack couple the two components for better optimization. If we want to integrate with such library in the future, we can make a wrapper of the library implementing both `Cache` and `MemoryAllocator` interface.
 Request resolved: https://github.com/facebook/rocksdb//4676

Differential Revision: D13047662

ed By: yiwu-arbug

fbshipit-source-id: cd42e246d80ab600b4de47d073f7d2db308ce6dd   ,  Revert ""Move MemoryAllocator option from Cache to BlockBasedTableOpti… (#4697)
 ,  "
,,Rocksdb,"Fix compatibility of public ticker stats (#4701)

Summary:
Currently, `Statistics` can record tick by `recordTick()` whose second parameter is an `uint64_t`.
That means tick can only increase.
If we want to reduce tick, we have to work around like `RecordTick(statistics_, NO_ITERATORS, uint64_t(-1));`.
That's kind of a hack.

So, this PR divide `NO_ITERATORS` into two counters `NO_ITERATOR_CREATED` and `NO_ITERATOR_DELETE`, making the counters increase only.

Fixes #3013 .
 Request resolved: https://github.com/facebook/rocksdb//4498

  ,  "
,,Rocksdb,"Extend Transaction::GetForUpdate with do_validate (#4680)

Summary:
Transaction::GetForUpdate is extended with a do_validate parameter with default value of true. If false it skips validating the snapshot (if there is any) before doing the read. After the read it also returns the latest value (expects the ReadOptions::snapshot to be nullptr). This allows RocksDB applications to use GetForUpdate similarly to how InnoDB does. Similarly ::Merge, ::Put, ::Delete, and ::SingleDelete are extended with assume_exclusive_tracked with default value of false. It true it indicates that call is assumed to be after a ::GetForUpdate(do_validate=false).
The Java APIs are accordingly updated.
 Request resolved: https://github.com/facebook/rocksdb//4680

  ,  "
,,Rocksdb,"Divide `NO_ITERATORS` into two counters `NO_ITERATOR_CREATED` and `NO_ITERATOR_DELETE` (#4498)

Summary:
Currently, `Statistics` can record tick by `recordTick()` whose second parameter is an `uint64_t`.
That means tick can only increase.
If we want to reduce tick, we have to work around like `RecordTick(statistics_, NO_ITERATORS, uint64_t(-1));`.
That's kind of a hack.

So, this PR divide `NO_ITERATORS` into two counters `NO_ITERATOR_CREATED` and `NO_ITERATOR_DELETE`, making the counters increase only.

Fixes #3013 .
 Request resolved: https://github.com/facebook/rocksdb//4498
  ,  "
,,Rocksdb,"Revert ""apply ReadOptions.iterate_upper_bound to transaction iterator… (#4705)

Summary:
Currently transaction iterator does not apply `ReadOptions.iterate_upper_bound` when iterating. This PR attempts to fix the problem by having `BaseDeltaIterator` enforcing the upper bound check when iterator state is changed.
 Request resolved: https://github.com/facebook/rocksdb//4656

  ,  "
,,Rocksdb,"Remove redundant member var and set options (#4631)

Summary:
In the past, both `DBImpl::atomic_flush_` and
`DBImpl::immutable_db_options_.atomic_flush` exist. However, we fail to set
`immutable_db_options_.atomic_flush`, but use `DBImpl::atomic_flush_` which is
set correctly. This does not lead to incorrect behavior, but is a duplicate of
information.

Since `immutable_db_options_` is always there and has `atomic_flush`, we should
use it as source of truth and remove `DBImpl::atomic_flush_`.
  ,  "
Memory Management,"Memory Management,Compression tasks, restructring the code",Rocksdb,"Fix BlockBasedTable not always using memory allocator if available (#4678)

Summary:
Fix block based table reader not using memory_allocator when allocating index blocks and compression dictionary blocks.
 Request resolved: https://github.com/facebook/rocksdb//4678
Memory leak solved
Differential Revision: D13054594

ed By: yiwu-arbug

fbshipit-source-id: 379f25bcc665395662511c4f873f4b7b55104ce2   ,  Remove two variables from BlockContents class and don't use class Block for compressed block (#4650)
  ,  "
Memory Management,"Memory Management, Compression tasks, restructring the code",Rocksdb,"Remove two variables from BlockContents class and don't use class Block for compressed block (#4650)

Summary:
Fix block based table reader not using memory_allocator when allocating index blocks and compression dictionary blocks.
 Request resolved: https://github.com/facebook/rocksdb//4678
Memory leak solved
Differential Revision: D13054594

ed By: yiwu-arbug

fbshipit-source-id: 379f25bcc665395662511c4f873f4b7b55104ce2   ,  "
Memory Management,"Memory Management, Compression tasks, restructring the code",Rocksdb,"Remove two variables from BlockContents class and don't use class Block for compressed block (#4650)

Summary:
- If block cache disabled or not used for meta-blocks, `BlockBasedTableReader::Rep::uncompression_dict` owns the `UncompressionDict`. It is preloaded during `PrefetchIndexAndFilterBlocks`.
- If block cache is enabled and used for meta-blocks, block cache owns the `UncompressionDict`, which holds dictionary and digested dictionary when needed. It is never prefetched though there is a TODO for this in the code. The cache key is simply the compression dictionary block handle.
- New stats for compression dictionary accesses in block cache: ""BLOCK_CACHE_COMPRESSION_DICT_*"" and ""compression_dict_block_read_count""
 Request resolved: https://github.com/facebook/rocksdb//4881
compression dictionary
Differential Revision: D13663801

ed By: ajkr

fbshipit-source-id: bdcc54044e180855cdcc57639b493b0e016c9a3f   ,  Fix BlockBasedTable not always using memory allocator if available (#4678)
  ,  "
,,Rocksdb,"Add SstFileReader to read sst files (#4717)

Summary:
A user friendly sst file reader is useful when we want to access sst
files outside of RocksDB. For example, we can generate an sst file
with SstFileWriter and send it to other places, then use SstFileReader
to read the file and process the entries in other ways.

Also rename the original SstFileReader to SstFileDumper because of
name conflict, and seems SstFileDumper is more appropriate for tools.

 ,  "
Restructuring the code,"Memory Management, Compression tasks, Restructuring the code",Rocksdb,"Cache dictionary used for decompressing data blocks (#4881)

Summary:
Per offline discussion with siying, `MemoryAllocator` and `Cache` should be decouple. The idea is that memory allocator handles memory allocation, while cache handle cache policy.

It is normal that external cache libraries pack couple the two components for better optimization. If we want to integrate with such library in the future, we can make a wrapper of the library implementing both `Cache` and `MemoryAllocator` interface. Memory leak solved
 Request resolved: https://github.com/facebook/rocksdb//4676

Differential Revision: D13047662

ed By: yiwu-arbug

fbshipit-source-id: cd42e246d80ab600b4de47d073f7d2db308ce6dd   ,  "
Data Conversion, Data Conversion,Rocksdb,"Fix the build error caused by the dynamic array (#4918)

Summary:
In the MixGraph benchmark of db_bench #4788 , the char array is initialized with an argument from user's input, which can cause build error on some platforms. Also, the msg char array size can be potentially smaller than the printed data, which should be extended from 100 to 256.

Tested with make check.
 Request resolved: https://github.com/facebook/rocksdb//4918

Differential Revision: D13844298

ed By: sagar0

fbshipit-source-id: 33c4809c5c4438f0a9f7b289d3f42e20c545bbab   ,  Generate mixed workload with Get, Put, Seek in db_bench (#4788)

 ,  "
Restructuring the code ,Restructuring the code,Rocksdb,"Refine db_stress params for atomic flush (#4781)

Summary:
Separate flag for enabling option from flag for enabling dedicated atomic stress test. I have found setting the former without setting the latter can detect different problems.
 Request resolved: https://github.com/facebook/rocksdb//4781

Differential Revision: D13463211
  ,  "
API Management, API Usage,Rocksdb,"tools: use provided options instead of the default (#4839)

Summary:
The current implementation hardcode the default options in different
places, which makes it impossible to support other environments  API(like
encrypted environment).
 Request resolved: https://github.com/facebook/rocksdb//4839

   ,  "
Restructuring the code,"Memory Management, Compression tasks, Restructuring the code",Rocksdb,"Cache dictionary used for decompressing data blocks (#4881)

Summary:
This is essentially a re-submission of #4251 with a few improvements:

- Split `CompressionDict` into two separate classes: `CompressionDict` and `UncompressionDict`
- Eliminated `Init` functions. Instead do all initialization work in constructors.
- Added test case for parallel DB open, which is the scenario where #4251 failed under TSAN.
 Request resolved: https://github.com/facebook/rocksdb//4849

   ,  "
Restructuring the code,Restructuring the code,Rocksdb,"Always delete Blob DB files in the background (#4928)

Summary:
Blob DB files are not tracked by the SFM, so they currently don't get
deleted in the background. Force them to be deleted in background so
rate limiting can be applied
 Request resolved: https://github.com/facebook/rocksdb//4928
  ,  "
Restructuring the code,Restructuring the code,Rocksdb,"Deleting Blob files also goes through SstFileManager (#4904)

Summary:
Right now, deleting blob files is not rate limited, even if SstFileManger is specified.
On the other hand, rate limiting blob deletion is not supported. With this change, Blob file
deletion will go through SstFileManager too.
 Request resolved: https://github.com/facebook/rocksdb//4904
   ,  "
,,Rocksdb,"WritePrepared: fix issue with snapshot released during compaction (#4858)

Summary:
Compaction iterator keep a copy of list of live snapshots at the beginning of compaction, and then query snapshot checker to verify if values of a sequence number is visible to these snapshots. However when the snapshot is released in the middle of compaction, the snapshot checker implementation (i.e. WritePreparedSnapshotChecker) may remove info with the snapshot and may report incorrect result, which lead to values being compacted out when it shouldn't. This patch conservatively keep the values if snapshot checker determines that the snapshots is released.
 Request resolved: https://github.com/facebook/rocksdb//4858
  ,  "
Threads management,Threads management,Rocksdb,"WritePrepared: fix ValidateSnapshot with long-running txn (#4961)

Summary:
ValidateSnapshot checks if another txn has committed a value to about-to-be-locked key since a particular snapshot. It applies an optimization of looking into only the memtable if snapshot seq is larger than the earliest seq in the memtables. With a long-running txn in WritePrepared, the prepared value might be flushed out to the disk and yet it commits after the snapshot, which breaks this optimization. The patch fixes that by disabling this optimization when the min_uncomitted seq at the time the snapshot was taken is lower than earliest seq in the memtables.
 Request resolved: https://github.com/facebook/rocksdb//4961

Differential Revision: D14009947

ed By: maysamyabandeh

fbshipit-source-id: 1d11679950326f7c4094b433e6b821b729f08850   ,  "
API Usage,API Usage,Rocksdb,"WritePrepared: Report released snapshots in IsInSnapshot (#4856)

Summary:
Previously IsInSnapshot assumed that the snapshot is valid at the time that the function is called. However there are cases where that might not be valid. Example is background compactions where the compaction algorithm operates with a list of snapshots some of which might be released by the time they are being passed to IsInSnapshot. The patch make two changes to enable the caller to tell difference: i) any live snapshot below max is added to max_committed_seq_, which allows IsInSnapshot to confidently tell whether the passed snapshot is invalid if it below max, ii) extends IsInSnapshot API with a ""released"" variable that is set true when IsInSnapshot find no such snapshot below max and also find no other way to give a certain return value. In such cases the return value is true but the caller should also check the ""released"" boolean after the call.
In short here is the changes in the API:
i) If the snapshot is valid, no change is required.
ii) If the snapshot might be invalid, a reference to ""released"" boolean must be passed to IsInSnapshot.
ii-a) If snapshot is above max, IsInSnapshot can figure the return valid using the commit cache.
ii-b) otherwise if snapshot is in old_commit_map_, IsInSnapshot can use that to tell if value was visible to the snapshot.
ii-c) otherwise it sets ""released"" to true and returns true as well.
 Request resolved: https://github.com/facebook/rocksdb//4856
  ,  "
,,Rocksdb,"WritePrepared: Fix double snapshot release issue (#4727)

Summary:
Currently the garbage collection of items in old_commit_map_ was done upon ::ReleaseSnapshot. The assumption behind this method was that the sequence number of snapshots are unique, which is incorrect. In the very rare cases that two consecutive snapshot have the same sequence number this could lead the release of the first snapshot affect the old_commit_map_ that is necessary to service the reads of the second snapshot. The bug would be triggered only if i) two snapshot have the same seq, ii) both of them are very old (older than the last ~4m transactions), and iii) there is commit entry overlapping with the snapshot seq number.
It is fixed by doing the cleanup of old_commit_map_ in UpdateSnapshot: the new list of snapshots are compared with the old one and the missing sequence numbers are concluded released. If two snapshots have the same seq number, after the release of one of them, the seq number still appears in the snapshot least and thus not cleaned up prematurely.
 Request resolved: https://github.com/facebook/rocksdb//4727

  ,  "
,,Rocksdb,"WritePrepared: commit of delayed prepared entries (#4894)

Summary:
Here is the order of ops in a commit: 1) update commit cache 2) publish seq, 3) RemovePrepared. In case of a delayed prepared, there will be a gap between when the commit is visible to snapshots until delayed_prepared_ is cleaned up. To tell apart this case from a delayed uncommitted txn from, the commit entry of a delayed prepared is also stored in delayed_prepared_commits_, which is updated before publishing the commit.
Also logic in GetSnapshotInternal that ensures that each new snapshot is always larger than max_evicted_seq_ is updated to check against the upcoming value of max_evicted_seq_ rather than its current one. This is because AdvanceMaxEvictedSeq gets the list of snapshots lower than the new max, before updating max_evicted_seq_.
 Request resolved: https://github.com/facebook/rocksdb//4894
  
  ,  "
,,Rocksdb,"Extend Transaction::GetForUpdate with do_validate (#4680)

Summary:
Transaction::GetForUpdate is extended with a do_validate parameter with default value of true. If false it skips validating the snapshot (if there is any) before doing the read. After the read it also returns the latest value (expects the ReadOptions::snapshot to be nullptr). This allows RocksDB applications to use GetForUpdate similarly to how InnoDB does. Similarly ::Merge, ::Put, ::Delete, and ::SingleDelete are extended with assume_exclusive_tracked with default value of false. It true it indicates that call is assumed to be after a ::GetForUpdate(do_validate=false).
The Java APIs are accordingly updated.
 Request resolved: https://github.com/facebook/rocksdb//4680

Differential Revision: D13068508

ed By: maysamyabandeh

fbshipit-source-id: f0b59db28f7f6a078b60844d902057140765e67d    ,  "
,,Rocksdb,"WritePrepared: Report released snapshots in IsInSnapshot (#4856)

Summary:
Previously IsInSnapshot assumed that the snapshot is valid at the time that the function is called. However there are cases where that might not be valid. Example is background compactions where the compaction algorithm operates with a list of snapshots some of which might be released by the time they are being passed to IsInSnapshot. The patch make two changes to enable the caller to tell difference: i) any live snapshot below max is added to max_committed_seq_, which allows IsInSnapshot to confidently tell whether the passed snapshot is invalid if it below max, ii) extends IsInSnapshot API with a ""released"" variable that is set true when IsInSnapshot find no such snapshot below max and also find no other way to give a certain return value. In such cases the return value is true but the caller should also check the ""released"" boolean after the call.
In short here is the changes in the API:
i) If the snapshot is valid, no change is required.
ii) If the snapshot might be invalid, a reference to ""released"" boolean must be passed to IsInSnapshot.
ii-a) If snapshot is above max, IsInSnapshot can figure the return valid using the commit cache.
ii-b) otherwise if snapshot is in old_commit_map_, IsInSnapshot can use that to tell if value was visible to the snapshot.
ii-c) otherwise it sets ""released"" to true and returns true as well.
  ,  "
,,Rocksdb,"WritePrepared: fix two versions in compaction see different status for released snapshots (#4890)

Summary:
Fix how CompactionIterator::findEarliestVisibleSnapshots handles released snapshot. It fixing the two scenarios:

Scenario 1:
key1 has two values v1 and v2. There're two snapshots s1 and s2 taken after v1 and v2 are committed. Right after compaction output v2, s1 is released. Now findEarliestVisibleSnapshot may see s1 being released, and return the next snapshot, which is s2. That's larger than v2's earliest visible snapshot, which was s1.
The fix: the only place we check against last snapshot and current key snapshot is when we decide whether to compact out a value if it is hidden by a later value. In the check if we see current snapshot is even larger than last snapshot, we know last snapshot is released, and we are safe to compact out current key.

Scenario 2:
key1 has two values v1 and v2. there are two snapshots s1 and s2 taken after v1 and v2 are committed. During compaction before we process the key, s1 is released. When compaction process v2, snapshot checker may return kSnapshotReleased, and the earliest visible snapshot for v2 become s2. When compaction process v1, snapshot checker may return kIsInSnapshot (for WritePrepared transaction, it could be because v1 is still in commit cache). The result will become inconsistent here.
The fix: remember the set of released snapshots ever reported by snapshot checker, and ignore them when finding result for findEarliestVisibleSnapshot.
 Request resolved: https://github.com/facebook/rocksdb//4890
 ,  "
Memory Management,"THREAD MANAGEMENT, MEMORY MANAGENT",Rocksdb,"Reorder DBIter fields to reduce memory usage (#5078)

Summary:
The patch reorders DBIter fields to put 1-byte fields together and let the compiler optimize the memory usage by using less 64-bit allocations for bools and enums.

This might have a negative side effect of putting the variables that are accessed together into different cache lines and hence increasing the cache misses. Not sure what benchmark would verify that thought. I ran simple, single-threaded seekrandom benchmarks but the variance in the results is too much to be conclusive.

Differential Revision: D14562676

ed By: maysamyabandeh

fbshipit-source-id: 2284655d46e079b6e9a860e94be5defb6f482167   ,  "
,,Rocksdb,"Apply modernize-use-override (2nd iteration)

Summary:
Use C++11’s override and remove virtual where applicable.
Change are automatically generated.

Reviewed By: Orvid

Differential Revision: D14090024

fbshipit-source-id: 1e9432e87d2657e1ff0028e15370a85d1739ba2a   ,  "
API Usage,API Usage,Rocksdb,"add GetStatsHistory to retrieve stats snapshots (#4748)

Summary:
This PR adds public `GetStatsHistory` API to retrieve stats history in the form of an std map. The key of the map is the timestamp in microseconds when the stats snapshot is taken, the value is another std map from stats name to stats value (stored in std string). Two DBOptions are introduced: `stats_persist_period_sec` (default 10 minutes) controls the intervals between two snapshots are taken; `max_stats_history_count` (default 10) controls the max number of history snapshots to keep in memory. RocksDB will stop collecting stats snapshots if `stats_persist_period_sec` is set to 0.

(This PR is the in-memory part

Differential Revision: D13961471

ed By: miasantreble

fbshipit-source-id: ac836d401ecb84ea92216bf9966f969dedf4ad04   ,  "
Compression tasks,"Compression tasks, memory management",Rocksdb,"Temporarily Disable DBTest2.PresetCompressionDict (#5003)

Summary:
DBTest2.PresetCompressionDict is flaky. Temparily disable it for now.
 Request resolved: https://github.com/facebook/rocksdb//5003

 ,  "
,,Rocksdb,"Atomic ingest (#4895)

Summary:
Make file ingestion atomic.

 as title.
Ingesting external SST files into multiple column families should be atomic. If
a crash occurs and db reopens, either all column families have successfully
ingested the files before the crash, or non of the ingestions have any effect
on the state of the db.

Also add unit tests for atomic ingestion.

Note that the unit test here does not cover the case of incomplete atomic group
in the MANIFEST, which is covered in VersionSetTest already.
 Request resolved: https://github.com/facebook/rocksdb//4895

Differential Revision: D13718245

ed By: riversand963

fbshipit-source-id: 7df97cc483af73ad44dd6993008f99b083852198   ,  "
,,Rocksdb,"add whole key bloom filter support in memtables (#4985)

Summary:
MyRocks calls `GetForUpdate` on `INSERT`, for unique key check, and in almost all cases GetForUpdate returns empty result. For such cases, whole key bloom filter is helpful.
 Request resolved: https://github.com/facebook/rocksdb//4985

Differential Revision: D14118257

ed By: miasantreble

fbshipit-source-id: d35cb7109c62fd5ad541a26968e3a3e16d3e85ea   ,  "
,,Rocksdb,"WritePrepared: optimize read path by avoiding virtual (#5018)

Summary:
The read path includes a callback function, ReadCallback, which would eventually calls IsInSnapshot to figure if a particular seq is in the reading snapshot or not. This callback is virtual, which adds the cost of multiple virtual function call to each read. The first few checks in IsInSnapshot, however, are quite trivial and take care of majority of the cases. The patch moves those to a non-virtual function in the the parent class, ReadCallback, to lower the virtual callback cost.
 Request resolved: https://github.com/facebook/rocksdb//5018

Differential Revision: D14226562

ed By: maysamyabandeh

fbshipit-source-id: 6feed5b34f3b082e52092c5ef143e29b49c46b44   ,  "
,,Rocksdb,"Apply modernize-use-override (3)

Summary:
Use C++11’s override and remove virtual where applicable.
Change are automatically generated.


Differential Revision: D14131816

fbshipit-source-id: f20e7f7cecf2e699d70f5fa036f72c0e3f59b50e   ,  "
,,Rocksdb,"Add missing functionality to RocksJava (#4833)

Summary:
This is my latest round of changes to add missing items to RocksJava. More to come in future PRs.
 Request resolved: https://github.com/facebook/rocksdb//4833

Differential Revision: D14152266

ed By: sagar0

fbshipit-source-id: d6cff67e26da06c131491b5cf6911a8cd0db0775   ,  "
Performance management,Performance management,Rocksdb,"fix NowNanos overflow (#5062)

Summary:
The original implementation of WinEnvIO::NowNanos() has a constant data overflow

The fix uses pre-computed nano_seconds_per_period_ to present the nano seconds per performance counter period, in the case if nano::den is divisible by perf_counter_frequency_. Otherwise it falls back to use high_resolution_clock.
siying ajkr
 Request resolved: https://github.com/facebook/rocksdb//5062

Differential Revision: D14426842

ed By: anand1976

fbshipit-source-id: 127f1daf423dd4b30edd0dcf8ea0466f468bec12   ,  "
,,Rocksdb,"Enhance transaction_test_util with delays (#4970)

Summary:
Enhance ::Insert and ::Verify test functions to add artificial delay between prepare and commit, and take snapshot and reads respectively.  A future PR will make use of these to improve stress tests to test against long-running transactions as well as long-running backup jobs. Also randomly sets set_snapshot to false for inserters to skip setting the snapshot in the initialization phase and let the snapshot be taken later explicitly.
 Request resolved: https://github.com/facebook/rocksdb//4970

Differential Revision: D14031342

ed By: maysamyabandeh

fbshipit-source-id: b52b453751f0b25b81b23c48892bc1d152464cab   ,  "
,,Rocksdb,"BlobDB::Open() should put all existing trash files to delete scheduler (#5103)

Summary:
Right now, BlobDB::Open() fails to put all trash files to delete scheduler,
which causes some trash files permanently untracked.
 Request resolved: https://github.com/facebook/rocksdb//5103

Differential Revision: D14606095

ed By: siying

fbshipit-source-id: 41a9437a2948abb235c0ed85f9a04612d0e50183   ,  "
Threads Management,Threads Management,Rocksdb,"WritePrepared: Improve stress tests with slow threads (#4974)

Summary:
The transaction stress tests, stress a high concurrency scenario. In WritePrepared/WriteUnPrepared we need to also stress the scenarios where an inserting/reading transaction is very slow. This would stress the corner cases that the caching is not sufficient and other slower data structures are engaged. To emulate such cases we make use of slow inserter/verifier threads and also reduce the size of cache data structures.
 Request resolved: https://github.com/facebook/rocksdb//4974

Differential Revision: D14143070

ed By: maysamyabandeh

fbshipit-source-id: 81eb674678faf9fae0f654cd60ebcc74e26aeee7  ,  "
,"Compression, restructring the code",Rocksdb,"WritePrepared: handle adding prepare before max_evicted_seq_ (#5025)

Summary:
The patch fixes an improbable race condition between AddPrepared from one write queue and AdvanceMaxEvictedSeq from another queue. In this scenario AddPrepared finds prepare_seq lower than max and adding to PrepareHeap as usual while AdvanceMaxEvictedSeq has finished checking PrepareHeap against the future max. Thus when AdvanceMaxEvictedSeq finishes off by updating the max_evicted_seq_, PrepareHeap ends up with a prepared_seq lower than it which breaks the PrepareHeap contract. The fix is that in AddPrepared we check against the future_max_evicted_seq_ instead, which is update before AdvanceMaxEvictedSeq acquire prepare_mutex_ and looks into PrepareHeap.
A unit test added to test for the failure scenario. The code is also refactored a bit to remove the duplicate code between AdvanceMaxEvictedSeq and AddPrepared.
 Request resolved: https://github.com/facebook/rocksdb//5025

Differential Revision: D14249028

ed By: maysamyabandeh

fbshipit-source-id: 072ea56663f40359662c05fafa6ac524417b0622   ,  "
,"Compression, restructring the code",Rocksdb,"WritePrepared: commit only from the 2nd queue (#5014)

Summary:
When two_write_queues is enabled we call ::AddPrepared only from the main queue, which writes to both WAL and memtable, and call ::AddCommitted from the 2nd queue, which writes only to WAL. This simplifies the logic by avoiding concurrency between AddPrepared and also between AddCommitted. The patch fixes one case that did not conform with the rule above. This would allow future refactoring. For example AdvaneMaxEvictedSeq, which is invoked by AddCommitted, can be simplified by assuming lack of concurrent calls to it.
 Request resolved: https://github.com/facebook/rocksdb//5014
  ,  "
,"Compression, restructring the code",Rocksdb,"WritePrepared: handle adding prepare before max_evicted_seq_ (#5025)

Summary:
The patch fixes an improbable race condition between AddPrepared from one write queue and AdvanceMaxEvictedSeq from another queue. In this scenario AddPrepared finds prepare_seq lower than max and adding to PrepareHeap as usual while AdvanceMaxEvictedSeq has finished checking PrepareHeap against the future max. Thus when AdvanceMaxEvictedSeq finishes off by updating the max_evicted_seq_, PrepareHeap ends up with a prepared_seq lower than it which breaks the PrepareHeap contract. The fix is that in AddPrepared we check against the future_max_evicted_seq_ instead, which is update before AdvanceMaxEvictedSeq acquire prepare_mutex_ and looks into PrepareHeap.
A unit test added to test for the failure scenario. The code is also refactored a bit to remove the duplicate code between AdvanceMaxEvictedSeq and AddPrepared.
 Request resolved: https://github.com/facebook/rocksdb//5025

Differential Revision: D14249028

ed By: maysamyabandeh

fbshipit-source-id: 072ea56663f40359662c05fafa6ac524417b0622 ,  "
,,Rocksdb,"WritePrepared: optimize read path by avoiding virtual (#5018)

Summary:
The read path includes a callback function, ReadCallback, which would eventually calls IsInSnapshot to figure if a particular seq is in the reading snapshot or not. This callback is virtual, which adds the cost of multiple virtual function call to each read. The first few checks in IsInSnapshot, however, are quite trivial and take care of majority of the cases. The patch moves those to a non-virtual function in the the parent class, ReadCallback, to lower the virtual callback cost.
 Request resolved: https://github.com/facebook/rocksdb//5018

    ,  "
Threads Management,Threads Management,Rocksdb,"FIFO Compaction with TTL

Summary:
Introducing FIFO compactions with TTL.

FIFO compaction is based on size only which makes it tricky to enable in production as use cases can have organic growth. A user requested an option to drop files based on the time of their creation instead of the total size.

To address that request:
- Added a new TTL option to FIFO compaction options.
- Updated FIFO compaction score to take TTL into consideration.
- Added a new table property, creation_time, to keep track of when the SST file is created.
- Creation_time is set as below:
  - On Flush: Set to the time of flush.
  - On Compaction: Set to the max creation_time of all the files involved in the compaction.
  - On Repair and Recovery: Set to the time of repair/recovery.
  - Old files created prior to this code change will have a creation_time of 0.
- FIFO compaction with TTL is enabled when ttl > 0. All files older than ttl will be deleted during compaction. i.e. `if (file.creation_time < (current_time - ttl)) then delete(file)`. This will enable cases where you might want to delete all files older than, say, 1 day.
- FIFO compaction will fall back to the prior way of deleting files based on size if:
  - the creation_time of all files involved in compaction is 0.
  - the total size (of all SST files combined) does not drop below `compaction_options_fifo.max_table_files_size` even if the files older than ttl are deleted.
 ,  "
Threads Management,Threads Management,Rocksdb,"using ThreadLocalPtr to hide ROCKSDB_SUPPORT_THREAD_LOCAL from public…

Summary:
… headers

https://github.com/facebook/rocksdb//2199 should not reference RocksDB-specific macros (like ROCKSDB_SUPPORT_THREAD_LOCAL in this case) to public headers, `iostats_context.h` and `perf_context.h`. We shouldn't do that because users have to provide these compiler flags when building their binary with RocksDB.

We should hide the thread local global variable inside our implementation and just expose a function api to retrieve these variables. It may break some users for now but good for long term.

make check -j64
Closes https://github.com/facebook/rocksdb//2380

   ,  "
,,Rocksdb,"Unit Tests for sync, range sync and file close failures

Summary: Closes https://github.com/facebook/rocksdb//2454

Differential Revision: D5255320

ed By: siying

fbshipit-source-id: 0080830fa8eb5da6de25e17ba68aee91018c7913   ,  "
,,Rocksdb," Make ""make analyze"" happy

Summary:
""make analyze"" is reporting some errors. It's complicated to look but it seems to me that they are all false positive. Anyway, I think cleaning them up is a good idea. Some of the changes are hacky but I don't know a better way.
Closes https://github.com/facebook/rocksdb//2508

 ,  "
Threads Management,Threads Management,Rocksdb,"Fix DBWriteTest::ReturnSequenceNumberMultiThreaded data race

Summary:
rocksdb::Random is not thread-safe. Have one Random for each thread instead.
Closes https://github.com/facebook/rocksdb//2400

Differential Revision: D5173919

ed By: yiwu-arbug

fbshipit-source-id: 1a99c7b877f3893eb22355af49e321bcad4e53e6   ,  "
Network Management,Network Management,Rocksdb,"Call RateLimiter for compaction reads

Summary:
Allow users to rate limit background work based on read bytes, written bytes, or sum of read and written bytes. Support these by changing the RateLimiter API, so no additional options were needed.
Closes https://github.com/facebook/rocksdb//2433

Differential Revision: D5216946

ed By: ajkr

fbshipit-source-id: aec57a8357dbb4bfde2003261094d786d94f724e   ,  "
Network Management,Network Management,Rocksdb,"Encryption at rest support

Summary:
This PR adds support for encrypting data stored by RocksDB when written to disk.

It adds an `EncryptedEnv` override of the `Env` class with matching overrides for sequential&random access files.
The encryption itself is done through a configurable `EncryptionProvider`. This class creates is asked to create `BlockAccessCipherStream` for a file. This is where the actual encryption/decryption is being done.
Currently there is a Counter mode implementation of `BlockAccessCipherStream` with a `ROT13` block cipher (NOTE the `ROT13` is for demo purposes only!!).

The Counter operation mode uses an initial counter & random initialization vector (IV).
Both are created randomly for each file and stored in a 4K (default size) block that is prefixed to that file. The `EncryptedEnv` implementation is such that clients of the `Env` class do not see this prefix (nor data, nor in filesize).
The largest part of the prefix block is also encrypted, and there is room left for implementation specific settings/values/keys in there.

To test the encryption, the `DBTestBase` class has been extended to consider a new environment variable called `ENCRYPTED_ENV`. If set, the test will setup a encrypted instance of the `Env` class to use for all tests.
Typically you would run it like this:

```
ENCRYPTED_ENV=1 make check_some
```

There is also an added test that checks that some data inserted into the database is or is not ""visible"" on disk. With `ENCRYPTED_ENV` active it must not find plain text strings, with `ENCRYPTED_ENV` unset, it must find the plain text strings.
Closes https://github.com/facebook/rocksdb//2424

Differential Revision: D5322178

ed By: sdwilsh

fbshipit-source-id: 253b0a9c2c498cc98f580df7f2623cbf7678a27f   ,  "
,,Rocksdb,"Optimize for serial commits in 2PC

Summary:
Throughput: 46k tps in our sysbench settings (filling the details later)

The idea is to have the simplest change that gives us a reasonable boost
in 2PC throughput.

Major design changes:
1. The WAL file internal buffer is not flushed after each write. Instead
it is flushed before critical operations (WAL copy via fs) or when
FlushWAL is called by MySQL. Flushing the WAL buffer is also protected
via mutex_.
2. Use two sequence numbers: last seq, and last seq for write. Last seq
is the last visible sequence number for reads. Last seq for write is the
next sequence number that should be used to write to WAL/memtable. This
allows to have a memtable write be in parallel to WAL writes.
3. BatchGroup is not used for writes. This means that we can have
parallel writers which changes a major assumption in the code base. To
accommodate for that i) allow only 1 WriteImpl that intends to write to
memtable via mem_mutex_--which is fine since in 2PC almost all of the memtable writes
come via group commit phase which is serial anyway, ii) make all the
parts in the code base that assumed to be the only writer (via
EnterUnbatched) to also acquire mem_mutex_, iii) stat updates are
protected via a stat_mutex_.

Note: the first commit has the approach figured out but is not clean.
Submitting the PR anyway to get the early feedback on the approach. If
we are ok with the approach I will go ahead with this updates:
0) Rebase with Yi's pipelining changes
1) Currently batching is disabled by default to make sure that it will be
consistent with all unit tests. Will make this optional via a config.
2) A couple of unit tests are disabled. They need to be updated with the
serial commit of 2PC taken into account.
3) Replacing BatchGroup with mem_mutex_ got a bit ugly as it requires
releasing mutex_ beforehand (the same way EnterUnbatched does). This
needs to be cleaned up.
Closes https://github.com/facebook/rocksdb//2345

Differential Revision: D5210732

ed By: maysamyabandeh

fbshipit-source-id: 78653bd95a35cd1e831e555e0e57bdfd695355a4   ,  "
Data conversion,Data conversion,Rocksdb,"Fix db_write_test clang/windows build failure

Summary:
Fix db_write_test clang/windows build failure. Explicitly cast size_t (unsigned long) to uint32_t (unsigned int).
Closes https://github.com/facebook/rocksdb//2407

Differential Revision: D5182995

ed By: yiwu-arbug

fbshipit-source-id: aba225a9fccb12d5bfbdc2cd6efc11040706a9d2   ,     ,  "
Memory Management,Memory Management,Rocksdb,"Support ingest file when range deletions exist

Summary:
Previously we returned NotSupported when ingesting files into a database containing any range deletions. This diff adds the support.

- Flush if any memtable contains range deletions overlapping the to-be-ingested file
- Place to-be-ingested file before any level that contains range deletions overlapping it.
- Added support for `Version` to return iterators over range deletions in a given level. Previously, we piggybacked getting range deletions onto `Version`'s `Get()` / `AddIterator()` functions by passing them a `RangeDelAggregator*`. But file ingestion needs to get iterators over range deletions, not populate an aggregator (since the aggregator does collapsing and doesn't expose the actual ranges).
Closes https://github.com/facebook/rocksdb//2370

Differential Revision: D5127648

ed By: ajkr

fbshipit-source-id: 816faeb9708adfa5287962bafdde717db56e3f1a    ,  "
,,Rocksdb,"Introduce OnBackgroundError callback

Summary:
Some users want to prevent rocksdb from entering read-only mode in certain error cases. This diff gives them a callback, `OnBackgroundError`, that they can use to achieve it.

- call `OnBackgroundError` every time we consider setting `bg_error_`. Use its result to assign `bg_error_` but not to change the function's return status.
- classified calls using `BackgroundErrorReason` to give the callback some info about where the error happened
- renamed `ParanoidCheck` to something more specific so we can provide a clear `BackgroundErrorReason`
- unit tests for the most common cases: flush or compaction errors
Closes https://github.com/facebook/rocksdb//2477

Differential Revision: D5300190

ed By: ajkr

fbshipit-source-id: a0ea4564249719b83428e3f4c6ca2c49e366e9b3   ,  "
,,Rocksdb,"Support ingest file when range deletions exist

Summary:
Previously we returned NotSupported when ingesting files into a database containing any range deletions. This diff adds the support.

- Flush if any memtable contains range deletions overlapping the to-be-ingested file
- Place to-be-ingested file before any level that contains range deletions overlapping it.
- Added support for `Version` to return iterators over range deletions in a given level. Previously, we piggybacked getting range deletions onto `Version`'s `Get()` / `AddIterator()` functions by passing them a `RangeDelAggregator*`. But file ingestion needs to get iterators over range deletions, not populate an aggregator (since the aggregator does collapsing and doesn't expose the actual ranges).
Closes https://github.com/facebook/rocksdb//2370

Differential Revision: D5127648

ed By: ajkr

fbshipit-source-id: 816faeb9708adfa5287962bafdde717db56e3f1a   ,  "
Memory Management,Memory Management,Rocksdb,"db: avoid `#include`ing malloc and jemalloc simultaneously

Summary:
This fixes a compilation failure on Linux when the system libc is not
glibc. jemalloc's configure script incorrectly assumes that glibc is
always used on Linux systems, producing glibc-style signatures; when
the system libc is e.g. musl, the following error is observed

This works around the issue by rearranging the sources such that
jemalloc's headers are never in the same scope as the system's malloc
header. The jemalloc issue has been reported as well, see:
https://github.com/jemalloc/jemalloc/issues/778.

cc tschottdorf
Closes https://github.com/facebook/rocksdb//2188

Differential Revision: D5163048

ed By: siying

fbshipit-source-id: c553125458892def175c1be5682b0330d80b2a0d   ,  "
Memory Management,Memory Management,Rocksdb,"Improve write buffer manager (and allow the size to be tracked in block cache)

Summary:
Improve write buffer manager in several ways:
1. Size is tracked when arena block is allocated, rather than every allocation, so that it can better track actual memory usage and the tracking overhead is slightly lower.
2. We start to trigger memtable flush when 7/8 of the memory cap hits, instead of 100%, and make 100% much harder to hit.
3. Allow a cache object to be passed into buffer manager and the size allocated by memtable can be costed there. This can help users have one single memory cap across block cache and memtable.
Closes https://github.com/facebook/rocksdb//2350

Differential Revision: D5110648

ed By: siying

fbshipit-source-id: b4238113094bf22574001e446b5d88523ba00017   ,  "
Memory Management,Memory Management,Rocksdb,"fix coredump for release nullptr

Summary:
Coredump will be triggered when ingest external sst file after delete range.
ref https://github.com/facebook/rocksdb/issues/2398
Closes https://github.com/facebook/rocksdb//2463

Differential Revision: D5275599

ed By: ajkr

fbshipit-source-id: 0828dbc062ea8c74e913877cd63494fd3478a30d  ,    ,  "
,,Rocksdb,"Improve the error message for I/O related errors.

Summary:
Force people to write something other than file name while returning status for IOError.
Closes https://github.com/facebook/rocksdb//2493

Differential Revision: D5321309

ed By: siying

fbshipit-source-id: 38bcf6c19e80831cd3e300a047e975cbb131d822   ,  "
,,Rocksdb,"Fix mock_env.cc uninitialized variable

Summary:
Mingw is complaining about uninitialized variable in mock_env.cc. e.g. https://travis-ci.org/facebook/rocksdb/jobs/240132276
The fix is to initialize the variable.
Closes https://github.com/facebook/rocksdb//2428

Differential Revision: D5211306

ed By: yiwu-arbug

fbshipit-source-id: ee02bf0327dcea8590a2aa087f0176fecaf8621c   ,  "
Performance management,Performance management,Rocksdb,"fixed wrong type for ""allow_compaction"" parameter

Summary:
should be boolean, not uint64_t
MSVC complains about it during compilation with error `include\rocksdb\advanced_options.h(77): warning C4800: 'uint64_t': forcing value to bool 'true' or 'false' (performance warning)`
Closes https://github.com/facebook/rocksdb//2487

Differential Revision: D5310685

ed By: siying

fbshipit-source-id: 719a33b3dba4f711aa72e3f229013c188015dc86   ,  "
,,Rocksdb,"Sample number of reads per SST file

Summary:
We estimate number of reads per SST files, by updating the counter per file in sampled read requests. This information can later be used to trigger compactions to improve read performacne.
Closes https://github.com/facebook/rocksdb//2417

Differential Revision: D5193528

ed By: siying

fbshipit-source-id: b4241c5ad0eaf444b61afb53f8e6290d9f5da2df   ,  "
,,Rocksdb,"WriteOptions.low_pri which can throttle low pri writes if needed

Summary:
If ReadOptions.low_pri=true and compaction is behind, the write will either return immediate or be slowed down based on ReadOptions.no_slowdown.
Closes https://github.com/facebook/rocksdb//2369

Differential Revision: D5127619

ed By: siying

fbshipit-source-id: d30e1cff515890af0eff32dfb869d2e4c9545eb0   ,  "
,,Rocksdb,"default implementation for InRange

Summary:
it's confusing to implementors of prefix extractor to implement an unused function
Closes https://github.com/facebook/rocksdb//2460

Differential Revision: D5267408

ed By: ajkr

fbshipit-source-id: 2f1fe3131efc978f6098ae7a80e52bc7a0b13571   ,  "
,,Rocksdb,"Histogram of number of merge operands

Summary:
Add a histogram in statistics to help users understand how many merge operands they merge.
Closes https://github.com/facebook/rocksdb//2373

Differential Revision: D5139983

ed By: siying

fbshipit-source-id: 61b9ba8ca83f358530a4833d68f0103b56a0e182   ,  "
API Management,API Management,Rocksdb,"Java APIs for put, merge and delete in file ingestion

Summary:
Adding SSTFileWriter's newly introduced put, merge and delete apis to the Java api. The C++ APIs were first introduced in #2361.

Add is deprecated in favor of Put.
Merge is especially needed to support streaming for Cassandra-on-RocksDB work in https://issues.apache.org/jira/browse/CASSANDRA-13476.
Closes https://github.com/facebook/rocksdb//2392

Differential Revision: D5165091

ed By: sagar0

fbshipit-source-id: 6f0ad396a7cbd2e27ca63e702584784dd72acaab   ,  "
,,Rocksdb,"Allow ignoring unknown options when loading options from a file

Summary:
Added a flag, `ignore_unknown_options`, to skip unknown options when loading an options file (using `LoadLatestOptions`/`LoadOptionsFromFile`) or while verifying options (using `CheckOptionsCompatibility`). This will help in downgrading the db to an older version.

Also added `--ignore_unknown_options` flag to ldb
   ,  "
Threads Management,Threads Management,Rocksdb,"Implement ReopenWritibaleFile on Windows and other fixes

Summary:
Make default impl return NoSupported so the db_blob
  tests exist in a meaningful manner.
  Replace std::thread to port::Thread
Closes https://github.com/facebook/rocksdb//2465

Differential Revision: D5275563

ed By: yiwu-arbug

fbshipit-source-id: cedf1a18a2c05e20d768c1308b3f3224dbd70ab6   ,  "
Threads Management,Threads Management,Rocksdb,"Dedup release

Summary:
cc tamird sagar0
Closes https://github.com/facebook/rocksdb//2325

Differential Revision: D5098302

ed By: sagar0

fbshipit-source-id: 297c5506b5d9b2ed1d7719c8caf0b96cffe503b8  ,  "
,,Rocksdb,"Make direct I/O write use incremental buffer

Summary:
Currently for direct I/O, the large maximum buffer is always allocated. This will be wasteful if users flush the data in much smaller chunks. This diff fix this by changing the behavior of incremental buffer works. When we enlarge buffer, we try to copy the existing data in the buffer to the enlarged buffer, rather than flush the buffer first. This can make sure that no extra I/O is introduced because of buffer enlargement.
Closes https://github.com/facebook/rocksdb//2403

Differential Revision: D5178403

ed By: siying

fbshipit-source-id: a8fe1e7304bdb8cab2973340022fe80ff83449fd   ,  "
Restructuring the code,"Restructuring the code, compression tasks",Rocksdb,"Fix blob db compression bug

Summary:
`CompressBlock()` will return the uncompressed slice (i.e. `Slice(value_unc)`) if compression ratio is not good enough. This is undesired. We need to always assign the compressed slice to `value`.
Closes https://github.com/facebook/rocksdb//2447

Differential Revision: D5244682

ed By: yiwu-arbug

fbshipit-source-id: 6989dd8852c9622822ba9acec9beea02007dff09   ,  "
,,Rocksdb,"Fix clang errors by asserting the precondition

Summary:
USE_CLANG=1 make -j32 analyze
The two errors would disappear after the assertion.
Closes https://github.com/facebook/rocksdb//2416

Differential Revision: D5193526

ed By: maysamyabandeh

fbshipit-source-id: 16a21f18f68023f862764dd3ab9e00ca60b0eefa   ,  "
Database Management,Database Management,Rocksdb,"write exact sequence number for each put in write batch

Summary:
At the beginning of write batch write, grab the latest sequence from base db and assume sequence number will increment by 1 for each put and delete in the database, and store the exact sequence number with each put. This is assuming we are the only writer to increment sequence number (no external file ingestion, etc) and there should be no holes in the sequence number.

Also having some minor naming changes.
Closes https://github.com/facebook/rocksdb//2402
 ,  "
Database Management,Database Management,Rocksdb,"Add a Close() method to DB to return status when closing a db

Summary:
Currently, the only way to close an open DB is to destroy the DB store
object. There is no way for the caller to know the status. In one
instance, the destructor encountered an error due to failure to
close a log file on HDFS. In order to prevent silent failures, we add
DB::Close() that calls CloseImpl() which must be implemented by its
descendants. db persistence
The main failure point in the destructor is closing the log file. This
patch also adds a Close() entry point to Logger in order to get status.
When DBOptions::info_log is allocated and owned by the DBImpl, it is
explicitly closed by DBImpl::CloseImpl() database.
Closes https://github.com/facebook/rocksdb//3348

Differential Revision: D6698158

ed By: anand1976

fbshipit-source-id: 9468e2892553eb09c4c41b8723f590c0dbd8ab7d   ,  "
Memory Management,Memory Management,Rocksdb,"Fix Flush() keep waiting after flush finish

Summary:
Flush() call could be waiting indefinitely if min_write_buffer_number_to_merge is used. Consider the sequence:
1. User call Flush() with flush_options.wait = true
2. The manual flush started in the background
3. New memtable become immutable because of writes. The new memtable will not trigger flush if min_write_buffer_number_to_merge is not reached.
4. The manual flush finish.

Because of the new memtable created at step 3 not being flush, previous logic of WaitForFlushMemTable() keep waiting, despite the memtables it intent to flush has been flushed.

Here instead of checking if there are any more memtables to flush, WaitForFlushMemTable() also check the id of the earliest memtable. If the id is larger than that of latest memtable at the time flush was initiated, to avoid memory leak it means all the memtable at the time of flush start has all been flush.
Closes https://github.com/facebook/rocksdb//3378

Differential Revision: D6746789

ed By: yiwu-arbug

fbshipit-source-id: 35e698f71c7f90b06337a93e6825f4ea3b619bfa   ,  "
Database management,Database management,Rocksdb,"WritePrepared Txn: Fix DBIterator and add test

Summary:
In DBIter, Prev() calls FindValueForCurrentKey() to search the current value backward from the database. If it finds that there are too many stale value being skipped, it falls back to FindValueForCurrentKeyUsingSeek() db, seeking directly to the key with snapshot sequence. After introducing read_callback, however, the key it seeks to might not be visible, according to read_callback. It thus needs to keep searching forward until the first visible value.
Closes https://github.com/facebook/rocksdb//3382

Differential Revision: D6756148

ed By: yiwu-arbug

fbshipit-source-id: 064e39b1eec5e083af1c10142600f26d1d2697be   ,  "
Database management,Database management,Rocksdb,"fix DBTest.AutomaticConflictsWithManualCompaction

Summary:
After af92d4ad112f192693f6017f24f9ae1b00e1f053, only exclusive manual compaction can have conflict. updated the conflict-checking test database case accordingly. But we missed the point that exclusive manual compaction can only conflict with automatic compactions scheduled after it, since it waits on pending automatic compactions before it begins running.

This PR updates the test case to ensure the automatic compactions are scheduled after the manual compaction starts but before it finishes, thus ensuring a conflict. I also cleaned up the test case to use less space as I saw it cause out-of-space error on travis.
Closes https://github.com/facebook/rocksdb//3375
  ,  "
Restructuring the code,Restructuring the code,Rocksdb,"Reduce heavy hitter for Get operation

Summary:
This PR addresses the following heavy hitters in `Get` operation by moving calls to `StatisticsImpl::recordTick` from `BlockBasedTable` to `Version::Get`
 ,  "
Data conversion,DataConversion,Rocksdb,"add WriteBatch::WriteBatch(std::string&&)

Summary:
to save a string copy for some use cases.

The change is pretty straightforward, please feel free to let me know if you want to suggest any tests for it.
Closes https://github.com/facebook/rocksdb//3349
  ,  "
Memory Management,Memory Management,Rocksdb,"Add a histogram stat for memtable flush

Summary:
Add a new histogram stat called rocksdb.db.flush.micros for memtable
flush
Avoid memory leak  ,  "
Data Conversion,Data Conversion,Rocksdb,"Make Universal compaction options dynamic cast

Summary:
Let me know if more test coverage is needed
Closes https://github.com/facebook/rocksdb//3213
 ,  "
Memory Management,Memory Management,Rocksdb,"Consider an increase to buffer size when reading option file, from 4K to 8K.

Summary:  If the buffer size is increased to 8192 then 1 system call read is used to read the contents.

  As I think the buffer size is just used for reading `OPTIONS` files, and I thought it likely that `OPTIONS` files have increased in size (as more options are added), I thought I would suggest an increase.
  ,  "
Compression tasks,Compression Tasks,Rocksdb,"Add a BlockBasedTableOption to turn off index block compression.

Summary:
Add a new bool option index_uncompressed in BlockBasedTableOptions.
Closes https://github.com/facebook/rocksdb//3303

Differential Revision: D6686161

ed By: anand1976

fbshipit-source-id: 748b46993d48a01e5f89b6bd3e41f06a59ec6054   ,  "
API Management,"Memory Management, API Usage",Rocksdb,"Disable onboard cache for compaction output

Summary:
FILE_FLAG_WRITE_THROUGH is for disabling device on-board cache in windows API, which should be disabled if user doesn't need system cache.
There was a perf issue related with this external library, we found during memtable flush, the high percentile latency jumps significantly. During profiling, we found those high latency (P99.9) read requests got queue-jumped by write requests from memtable flush and takes 80ms or even more time to wait, even when SSD overall IO throughput is relatively low.

After enabling FILE_FLAG_WRITE_THROUGH, we rerun the test found high percentile latency drops a lot without observable impact on writes.

 other rocksdb instances have manually triggered memtable flush operations (memtable is tiny), creating a lot of randomized the small file writes operations during test.
  ,  "
Restructuring the code,Restructuring the code,Rocksdb,"Refactor ReadBlockContents()

Summary:
Divide ReadBlockContents() to multiple sub-functions. Maintaining the input and intermediate data in a new class BlockFetcher.
I hope in general it makes the code easier to maintain.
Another motivation to do it is to clearly divide the logic before file reading and after file reading. The refactor will help us evaluate how can we make I/O async in the future.
Closes https://github.com/facebook/rocksdb//3244

Differential Revision: D6520983

ed By: siying

fbshipit-source-id: 338d90bc0338472d46be7a7682028dc9114b12e9   ,  "
performance management,performance management,Rocksdb,"Split HarnessTest_Randomized to avoid timeout

Summary:
Split HarnessTest_Randomized to two tests toi avoid timeout
Closes https://github.com/facebook/rocksdb//3424

Differential Revision: D6826006

ed By: maysamyabandeh

fbshipit-source-id: 59c9a11c7da092206effce6e4fa3792f9c66bef2   ,  "
API Management,"API Usage,DataBase Management",Rocksdb,"WritePrepared Txn: make db_stress transactional

Summary:
Add ""--use_txn"" option to use transactional API in db_stress, default being WRITE_PREPARED policy, which is the main intention of modifying db_stress. It also extend the existing snapshots to verify that before releasing a snapshot a read from it returns the same value as before.
Closes https://github.com/facebook/rocksdb//3243

Differential Revision: D6556912

ed By: maysamyabandeh

fbshipit-source-id: 1ae31465be362d44bd06e635e2e9e49a1da11268   ,  "
Data conversion,"Data conversion, Memory Management",Rocksdb,"fix backup meta-file buffer overrun

Summary:
- check most times after calling snprintf that the buffer didn't fill up. Invalid parser. Previously we'd proceed and use `buf_size - len` as the length in subsequent calls, which underflowed as those are unsigned size_t.
- replace some memcpys with snprintf for consistency
Closes https://github.com/facebook/rocksdb//3255

Differential Revision: D6541464

ed By: ajkr

fbshipit-source-id: 8610ea6a24f38e0a37c6d17bc65b7c712da6d932   ,  "
Database management,Database management,Rocksdb,"Blob DB: dump blob_db_options.min_blob_size

Summary:
min_blob_size was missing from BlobDBOptions::Dump.
Closes https://github.com/facebook/rocksdb//3400
persistence database
Differential Revision: D6781525

ed By: yiwu-arbug

fbshipit-source-id: 40d9b391578d7f8c91bd89f4ce2eda5064864c25   ,  "
database management,database management,Rocksdb,"Blob DB: avoid having a separate read of checksum

Summary:
Previously on a blob db read, we are making a read of the blob value, and then make another read to get CRC checksum. I'm combining the two read into one.
database
readrandom db_bench with 1G database with base db size ,  "
Restructuring the code,Restructuring the code,Rocksdb,"BlobDB: refactor DB open logic

Summary:
Refactor BlobDB open logic. List of changes:

Major:
* On reopen, mark blob files found as immutable, do not use them for writing new keys. Refactored methods
* Not to scan the whole file to find file footer. Instead just seek to the end of the file and try to read footer.

Minor:
* Move most of the real logic from blob_db.cc to blob_db_impl.cc.
* Not to hold shared_ptr of event listeners in global maps in blob_db.cc
* Some changes to rfactor BlobFile interface.
* Improve logging and error handling.
Closes https://github.com/facebook/rocksdb//3246

Differential Revision: D6526147

ed By: yiwu-arbug

fbshipit-source-id: 9dc4cdd63359a2f9b696af817086949da8d06952   ,  "
database management,database management,Rocksdb,"Blob DB: fix crash when DB full but no candidate file to evict

Summary:
When blob_files is empty, std::min_element will return blobfiles.end(), which cannot be dereference. Fixing it.
Closes https://github.com/facebook/rocksdb//3387

Differential Revision: D6764927

ed By: yiwu-arbug

fbshipit-source-id: 86f78700132be95760d35ac63480dfd3a8bbe17a   ,  "
Restructuring the code,Restructuring the code,Rocksdb,"BlobDB: Remove the need to get sequence number per write

Summary:
Refactor change. Previously we store sequence number range of each blob files, and use the sequence number range to check if the file can be possibly visible by a snapshot. But it adds complexity to the code, since the sequence number is only available after a write. (The current implementation get sequence number by calling GetLatestSequenceNumber(), which is wrong.) With the patch, we are not storing sequence number range, and check if snapshot_sequence < obsolete_sequence to decide if the file is visible by a snapshot (previously we check if first_sequence <= snapshot_sequence < obsolete_sequence).
Closes https://github.com/facebook/rocksdb//3274

Differential Revision: D6571497

ed By: yiwu-arbug

fbshipit-source-id: ca06479dc1fcd8782f6525b62b7762cd47d61909   ,  "
,,Rocksdb,"WritePrepared Txn: GC old_commit_map_

Summary:
Garbage collect entries from old_commit_map_ when the corresponding snapshots are released.
Closes https://github.com/facebook/rocksdb//3247


fbshipit-source-id: 15d1566d85d4ac07036bc0dc47418f6c3228d4bf   ,  "
,,Rocksdb,"WritePrepared Txn: non-2pc write in one round

Summary:
Currently non-2pc writes do the 2nd dummy write to actually commit the transaction. This was necessary to ensure that publishing the commit sequence number will be done only from one queue (the queue that does not write to memtable). This is however not necessary when we have only one write queue, which is actually the setup that would be used by non-2pc writes. This patch eliminates the 2nd write when two_write_queues are disabled by updating the commit map in the 1st write.
Closes https://github.com/facebook/rocksdb//3277


fbshipit-source-id: 8ab458f7ca506905962f9166026b2ec81e749c46   ,  "
,,Rocksdb," WritePrepared Txn: address some pending TODOs

Summary:
This patch addresses a couple of minor TODOs for WritePrepared Txn such as double checking some assert statements at runtime as well, skip extra AddPrepared in non-2pc transactions, and safety check for infinite loops.
Closes https://github.com/facebook/rocksdb//3302

   ,  "
,,Rocksdb,"Refresh snapshot list during long compactions (#5099)

Summary:
Part of compaction cpu goes to processing snapshot list, the larger the list the bigger the overhead. Although the lifetime of most of the snapshots is much shorter than the lifetime of compactions, the compaction conservatively operates on the list of snapshots that it initially obtained. This patch allows the snapshot list to be updated via a callback if the compaction is taking long. This should let the compaction to continue more efficiently with much smaller snapshot list.
 Request resolved: https://github.com/facebook/rocksdb//5099



fbshipit-source-id: 7649f56c3b6b2fb334962048150142a3bf9c1a12   ,  "
Performance management,Performance management,Rocksdb,"DBIter to use IteratorWrapper for inner iterator (#5214)

Summary:
It's hard to get DBIter to directly use InternalIterator::NextAndGetResult() because the code change would be complicated. Instead, use IteratorWrapper, where Next() is already using NextAndGetResult(). Performance number is hard to measure because it is small and ther is variation. I run readseq many times, and there seems to be 1% gain.
 Request resolved: https://github.com/facebook/rocksdb//5214

fbshipit-source-id: 17af1965c409c2fe90cd85037fbd2c5a1364f82a   ,  "
,,Rocksdb,"DBIter::Next() can skip user key checking if previous entry's seqnum is 0 (#5244)

Summary:
Right now, DBIter::Next() always checks whether an entry is for the same user key as the previous entry to see whether the key should be hidden to the user. However, if previous entry's sequence number is 0, the check is not needed because 0 is the oldest possible sequence number.

We could extend it from seqnum 0 case to simply prev_seqno >= current_seqno. However, it is less robust with bug or unexpected situations, while the gain is relatively low. We can always extend it later when needed.

In a readseq benchmark with full formed LSM-tree, number of key comparisons called is reduced from 2.981 to 2.165. readseq against a fully compacted DB, no key comparison is called. Performance in this benchmark didn't show obvious improvement, which is expected because key comparisons only takes small percentage of CPU. But it may show up to be more effective if users have an expensive customized comparator.
 Request resolved: https://github.com/facebook/rocksdb//5244


fbshipit-source-id: b7e1ef3ec4fa928cba509683d2b3246e35d270d9   ,  "
,,Rocksdb,"Merging iterator to avoid child iterator reseek for some cases (#5286)

Summary:
When reseek happens in merging iterator, reseeking a child iterator can be avoided if:
(1) the iterator represents imutable data
(2) reseek() to a larger key than the current key
(3) the current key of the child iterator is larger than the seek key
because it is guaranteed that the result will fall into the same position.

This optimization will be useful for use cases where users keep seeking to keys nearby in ascending order.
 Request resolved: https://github.com/facebook/rocksdb//5286



fbshipit-source-id: 35f79ffd5ce3609146faa8cd55f2bfd733502f83   ,  "
Threads Management,Threads Management,Rocksdb,"Unordered Writes (#5218)

Summary:
Performing unordered writes in rocksdb when unordered_write option is set to true. When enabled the writes to memtable are done without joining any write thread. This offers much higher write throughput since the upcoming writes would not have to wait for the slowest memtable write to finish. The tradeoff is that the writes visible to a snapshot might change over time. If the application cannot tolerate that, it should implement its own mechanisms to work around that. Using TransactionDB with WRITE_PREPARED write policy is one way to achieve that. Doing so increases the max throughput by 2.2x without however compromising the snapshot guarantees.
The patch is prepared based on an original by siying
Existing unit tests are extended to include unordered_write option.


- WRITER_PREPARED with unordered_write disable concurrency control: 185.3 MB/s MB/s (2.35x)

Limitations:
- The feature is not yet extended to `max_successive_merges` > 0. The feature is also incompatible with `enable_pipelined_write` = true as well as with `allow_concurrent_memtable_write` = false.
 Request resolved: https://github.com/facebook/rocksdb//5218

Differential Revision: D15219029

ed By: maysamyabandeh

fbshipit-source-id: 38f2abc4af8780148c6128acdba2b3227bc81759   ,  "
Threads Management,Threads Management,Rocksdb,"Fix a flaky test with test sync point (#5310)

Summary:
If DB is opened with `avoid_unnecessary_blocking_io` being true, then `~ColumnFamilyHandleImpl` enqueues a purge request and schedules a background thread to perform the deletion. Without test sync point, whether the SST file is purged or not at a later point in time is not deterministic. If the SST does not exist, it will cause an assertion failure.

How to reproduce:
```
$git checkout 6492430eaf1a13730eec81321528558cbf486c96
$make -j20 deletefile_test
$gtest-parallel --repeat 1000 --worker 16 ./deletefile_test --gtest_filter=DeleteFileTest.BackgroundPurgeCFDropTest
```
The test may fail a few times.
With changes made in this PR, repeat the above commands, and the test should not fail.


fbshipit-source-id: c4308d5f8da83472c893bf7f8ceed347fbfa850f   ,  "
,,Rocksdb,"Provide an option so that SST ingestion won't fall back to copy after hard linking fails (#5333)

Summary:
RocksDB always tries to perform a hard link operation on the external SST file to ingest. This operation can fail if the external SST resides on a different device/FS, or the underlying FS does not support hard link. Currently RocksDB assumes that if the link fails, the user is willing to perform file copy, which is not true according to the post. This commit provides an option named  'failed_move_fall_back_to_copy' for users to choose which behavior they want.
 Request resolved: https://github.com/facebook/rocksdb//5333


fbshipit-source-id: f3626e13f845db4f7ed970a53ec8a2b1f0d62214   ,  "
,,Rocksdb,"Revert snap_refresh_nanos feature (#5269)

Summary:
Our daily stress tests are failing after this feature. Reverting temporarily until we figure the reason for test failures.
 Request resolved: https://github.com/facebook/rocksdb//5269


fbshipit-source-id: e4002b99690a97df30d4b4b58bf0f61e9591bc6e   ,  "
,,Rocksdb,"Refresh snapshot list during long compactions (#5099)

Summary:
Part of compaction cpu goes to processing snapshot list, the larger the list the bigger the overhead. Although the lifetime of most of the snapshots is much shorter than the lifetime of compactions, the compaction conservatively operates on the list of snapshots that it initially obtained. This patch allows the snapshot list to be updated via a callback if the compaction is taking long. This should let the compaction to continue more efficiently with much smaller snapshot list.
 Request resolved: https://github.com/facebook/rocksdb//5099

Differential Revision: D15086710

ed By: maysamyabandeh

fbshipit-source-id: 7649f56c3b6b2fb334962048150142a3bf9c1a12   ,  "
,,Rocksdb,"Merging iterator to avoid child iterator reseek for some cases (#5286)

Summary:
When reseek happens in merging iterator, reseeking a child iterator can be avoided if:
(1) the iterator represents imutable data
(2) reseek() to a larger key than the current key
(3) the current key of the child iterator is larger than the seek key
because it is guaranteed that the result will fall into the same position.

This optimization will be useful for use cases where users keep seeking to keys nearby in ascending order.
 Request resolved: https://github.com/facebook/rocksdb//5286

Differential Revision: D15283635

ed By: siying

fbshipit-source-id: 35f79ffd5ce3609146faa8cd55f2bfd733502f83   ,  "
,,Rocksdb,"Make RocksDB secondary instance respect atomic groups in version edits. (#5411)

Summary:
With this commit, RocksDB secondary instance respects atomic groups in version edits.
 Request resolved: https://github.com/facebook/rocksdb//5411

Differential Revision: D15617512

ed By: HaoyuHuang

fbshipit-source-id: 913f4ede391d772dcaf5649e3cd2099fa292d120   ,  "
,"Compression, restructring the code",Rocksdb,"refactor SavePoints (#5192)

Summary:
Savepoints are assumed to be used in a stack-wise fashion (only
the top element should be used), so they were stored by `WriteBatch`
in a member variable `save_points` using an std::stack.

Conceptually this is fine, but the implementation had a few issues:
- the `save_points_` instance variable was a plain pointer to a heap-
  allocated `SavePoints` struct. The destructor of `WriteBatch` simply
  deletes this pointer. However, the copy constructor of WriteBatch
  just copied that pointer, meaning that copying a WriteBatch with
  active savepoints will very likely have crashed before. Now a proper
  copy of the savepoints is made in the copy constructor, and not just
  a copy of the pointer
- `save_points_` was an std::stack, which defaults to `std::deque` for
  the underlying container. A deque is a bit over the top here, as we
  only need access to the most recent savepoint (i.e. stack.top()) but
  never any elements at the front. std::deque is rather expensive to
  initialize in common environments. For example, the STL implementation
  shipped with GNU g++ will perform a heap allocation of more than 500
  bytes to create an empty deque object. Although the `save_points_`
  container is created lazily by RocksDB, moving from a deque to a plain
  `std::vector` is much more memory-efficient. So `save_points_` is now
  a vector.
- `save_points_` was changed from a plain pointer to an `std::unique_ptr`,
  making ownership more explicit.
 Request resolved: https://github.com/facebook/rocksdb//5192

Differential Revision: D15024074

ed By: maysamyabandeh

fbshipit-source-id: 5b128786d3789cde94e46465c9e91badd07a25d7   ,  "
Compression tasks,Compression tasks,Rocksdb,"Add support for loading dynamic libraries into the RocksDB environment (#5281)

Summary:
This change adds a Dynamic Library class to the RocksDB Env.  Dynamic libraries are populated via the  Env::LoadLibrary method. API

The addition of dynamic library support allows for a few different features to be developed:
1.  The compression code can be changed to use dynamic library support.  This would allow RocksDB to determine at run-time what compression packages were installed.  This change would eliminate the need to make sure the build-time and run-time environment had the same library set.  It would also simplify some of the Java build issues (where it attempts to build and include various packages inside the RocksDB jars).

2.  Along with other features (to be provided in a subsequent PR), this change would allow code/configurations to be added to RocksDB at run-time.  For example, the build system includes code for building an ""rados"" environment and adding ""Cassandra"" features.  Instead of these extensions being built into the base RocksDB code, these extensions could be loaded at run-time as required/appropriate, either by configuration or explicitly.

We intend to push out other changes in support of the extending RocksDB at run-time via configurations.
 Request resolved: https://github.com/facebook/rocksdb//5281


fbshipit-source-id: 452cd4f54511c0bceee18f6d9d919aae9fd25fef   ,  "
,,Rocksdb,"Auto roll logger to enforce options.keep_log_file_num immediately after a new file is created (#5370)

Summary:
Right now, with auto roll logger, options.keep_log_file_num enforcement is triggered by events like DB reopen or full obsolete scan happens. In the mean time, the size and number of log files can grow without a limit. We put a stronger enforcement to the option, so that the number of log files can always under control.
 Request resolved: https://github.com/facebook/rocksdb//5370

Differential Revision: D15570413

ed By: siying

fbshipit-source-id: 0916c3c4d42ab8fdd29389ee7fd7e1557b03176e   ,  "
,,Rocksdb,"Optionally wait on bytes_per_sync to smooth I/O (#5183)

Summary:
The existing implementation does not guarantee bytes reach disk every `bytes_per_sync` when writing SST files, or every `wal_bytes_per_sync` when writing WALs. This can cause confusing behavior for users who enable this feature to avoid large syncs during flush and compaction, but then end up hitting them anyways.

My understanding of the existing behavior is we used `sync_file_range` with `SYNC_FILE_RANGE_WRITE` to submit ranges for async writeback, such that we could continue processing the next range of bytes while that I/O is happening. I believe we can preserve that benefit while also limiting how far the processing can get ahead of the I/O, which prevents huge syncs from happening when the file finishes.

Consider this `sync_file_range` usage: `sync_file_range(fd_, 0, static_cast<off_t>(offset + nbytes), SYNC_FILE_RANGE_WAIT_BEFORE | SYNC_FILE_RANGE_WRITE)`. Expanding the range to start at 0 and adding the `SYNC_FILE_RANGE_WAIT_BEFORE` flag causes any pending writeback (like from a previous call to `sync_file_range`) to finish before it proceeds to submit the latest `nbytes` for writeback. The latest `nbytes` are still written back asynchronously, unless processing exceeds I/O speed, in which case the following `sync_file_range` will need to wait on it.

There is a second change in this PR to use `fdatasync` when `sync_file_range` is unavailable (determined statically) or has some known problem with the underlying filesystem (determined dynamically).

The above two changes only apply when the user enables a new option, `strict_bytes_per_sync`.
 Request resolved: https://github.com/facebook/rocksdb//5183

Differential Revision: D14953553

ed By: siying

fbshipit-source-id: 445c3862e019fb7b470f9c7f314fc231b62706e9   ,  "
,,Rocksdb,"Add BlockBasedTableOptions::index_shortening (#5174)

Summary:
Introduce BlockBasedTableOptions::index_shortening to give users control on which key shortening techniques to be used in building index blocks. Before this patch, both separators and successor keys where shortened in indexes. With this patch, the default is set to kShortenSeparators to only shorten the separators. Since each index block has many separators and only one successor (last key), the change should not have negative impact on index block size. However it should prevent many unnecessary block loads where due to approximation introduced by shorted successor, seek would land us to the previous block and then fix it by moving to the next one.
 Request resolved: https://github.com/facebook/rocksdb//5174

Differential Revision: D14884185

ed By: al13n321

fbshipit-source-id: 1b08bc8c03edcf09b6b8c16e9a7eea08ad4dd534   ,  "
,,Rocksdb,"Make GetEntryFromCache a member function. (#5394)

Summary:
The commit makes GetEntryFromCache become a member function. It also makes all its callers become member functions.
 Request resolved: https://github.com/facebook/rocksdb//5394

Differential Revision: D15579222

ed By: HaoyuHuang

fbshipit-source-id: 07509c42ee9022dcded54950012bd3bd562aa1ae   ,  "
,,Rocksdb,"Move some memory related files from util/ to memory/ (#5382)

Summary:
Move arena, allocator, and memory tools under util to a separate memory/ directory.
 Request resolved: https://github.com/facebook/rocksdb//5382

Differential Revision: D15564655

ed By: siying

fbshipit-source-id: 9cd6b5d0d3d52b39606e19221fa154596e5852a5   ,  "
Data Conversion,Data Conversion,Rocksdb,"fix implicit conversion error reported by clang check (#5277)

Summary:
fix the following clang check errors
```
tools/db_stress.cc:3609:30: error: implicit conversion loses integer precision: 'std::vector::size_type' (aka 'unsigned long') to 'int' [-Werror,-Wshorten-64-to-32]
    int num_keys = rand_keys.size();
        ~~~~~~~~   ~~~~~~~~~~^~~~~~
tools/db_stress.cc:3888:30: error: implicit conversion loses integer precision: 'std::vector::size_type' (aka 'unsigned long') to 'int' [-Werror,-Wshorten-64-to-32]
    int num_keys = rand_keys.size();
        ~~~~~~~~   ~~~~~~~~~~^~~~~~
2 errors generated.

Differential Revision: D15196620

ed By: miasantreble

fbshipit-source-id: d56b1420d4a9f1df875fc52877a5fbb342bc7cae   ,  "
,,Rocksdb,"multiget: fix memory issues due to vector auto resizing (#5279)

Summary:
This PR fixes three memory issues found by ASAN
* in db_stress, the key vector for MultiGet is created using `emplace_back` which could potentially invalidates references to the underlying storage (vector<string>) due to auto resizing. Fix by calling reserve in advance.
* Similar issue in construction of GetContext autovector in version_set.cc
* In multiget_context.h use T[] specialization for unique_ptr that holds a char array
 Request resolved: https://github.com/facebook/rocksdb//5279

Differential Revision: D15202893

ed By: miasantreble

fbshipit-source-id: 14cc2cda0ed64d29f2a1e264a6bfdaa4294ee75d   ,  "
,,Rocksdb,"Add option to use MultiGet in db_stress (#5264)

Summary:
The new option will pick a batch size randomly in the range 1-64. It will then space the keys in the batch by random intervals.
 Request resolved: https://github.com/facebook/rocksdb//5264

Differential Revision: D15175522

ed By: anand1976

fbshipit-source-id: c16baa69d0f1ff4cf53c55c813ddd82c8aeb58fc   ,  "
,,Rocksdb,"WritePrepared: disableWAL in commit without prepare (#5327)

Summary:
When committing a transaction without prepare, WritePrepared simply writes the batch to db and add the commit entry to CommitCache. When two_write_queues=true, following the rule of committing only from 2nd write queue, the first write, writes the batch and the only thing the 2nd write does is to write the commit entry to CommitCache. Currently the write batch in 2nd write is set to an empty LogData entry, while the write to the WAL could simply be entirely disabled.
 Request resolved: https://github.com/facebook/rocksdb//5327

Differential Revision: D15424546

ed By: maysamyabandeh

fbshipit-source-id: 3d9ea3922d5196984c584d62a3ed57e1f7ca7b9f   ,  "
Memory Management,Memory Management,Rocksdb,"Add support for timestamp in Get/Put (#5079)

Summary:
It's useful to be able to (optionally) associate key-value pairs with user-provided timestamps. This PR is an early effort towards this goal and continues the work of facebook#4942. A suite of new unit tests exist in DBBasicTestWithTimestampWithParam. Support for timestamp requires the user to provide timestamp as a slice in `ReadOptions` and `WriteOptions`. All timestamps of the same database must share the same length, format, etc. The format of the timestamp is the same throughout the same database, and the user is responsible for providing a comparator function (Comparator) to order the <key, timestamp> tuples. Once created, the format and length of the timestamp cannot change (at least for now).

$TEST_TMPDIR=/dev/shm ./db_bench -benchmarks=fillseq,readrandom -num=1000000
$TEST_TMPDIR=/dev/shm ./db_bench -benchmarks=fillrandom -num=1000000
```
Repeat for 6 times for both versions.

Results are as follows:
```
|        | readrandom | fillrandom |
| master | 16.77 MB/s | 47.05 MB/s |
| PR5079 | 16.44 MB/s | 47.03 MB/s |
```
 Request resolved: https://github.com/facebook/rocksdb//5079

Differential Revision: D15132946

ed By: riversand963

fbshipit-source-id: 833a0d657eac21182f0f206c910a6438154c742c   ,  "
,,Rocksdb,"Add a MultiRead() method to Env (#5311)

Summary:
Define the Env:: MultiRead() method to allow callers to request multiple block reads in one shot. The underlying Env implementation can parallelize it if it chooses to in order to reduce the overall IO latency.
 Request resolved: https://github.com/facebook/rocksdb//5311

Differential Revision: D15502172

ed By: anand1976

fbshipit-source-id: 2b228269c2e11b5f54694d6b2bb3119c8a8ce2b9   ,  "
Performance management,Performance management,Rocksdb,"Refactor the handling of cache related counters and statistics (#5408)

Summary:
The patch cleans up the handling of cache hit/miss/insertion related
performance counters, get context counters, and statistics by
eliminating some code duplication and factoring out the affected logic
into separate methods. In addition, it makes the semantics of cache hit
metrics more consistent by changing the code so that accessing a
partition of partitioned indexes/filters through a pinned reference no
longer counts as a cache hit.
 Request resolved: https://github.com/facebook/rocksdb//5408

Differential Revision: D15610883

ed By: ltamasi

fbshipit-source-id: ee749c18965077aca971d8f8bee8b24ed8fa76f1   ,  "
,,Rocksdb,"Revert to checking the upper bound on a per-key basis in BlockBasedTableIterator (#5428)

Summary:
PR #5111 reduced the number of key comparisons when iterating with
upper/lower bounds; however, this caused a regression for MyRocks.
Reverting to the previous behavior in BlockBasedTableIterator as a hotfix.
 Request resolved: https://github.com/facebook/rocksdb//5428

Differential Revision: D15721038

ed By: ltamasi

fbshipit-source-id: 5450106442f1763bccd17f6cfd648697f2ae8b6c   ,  "
,,Rocksdb,"Add block cache tracer. (#5410)

Summary:
This PR adds a help class block cache tracer to read/write block cache accesses. It uses the trace reader/writer to perform this task.
 Request resolved: https://github.com/facebook/rocksdb//5410

Differential Revision: D15612843

ed By: HaoyuHuang

fbshipit-source-id: f30fd1e1524355ca87db5d533a5c086728b141ea   ,  "
Compression tasks,Compression Tasks,Rocksdb,"Speed up Snappy uncompression, new Logger interface.

- Removed one copy of an uncompressed block contents changing
  the signature of Snappy_Uncompress() so it uncompresses into a
  flat array instead of a std::string. compression decompression
        
  Speeds up readrandom ~10%.

- Instead of a combination of Env/WritableFile, we now have a
  Logger interface that can be easily overridden applications
  that want to supply their own logging.

- Separated out the gcc and Sun Studio parts of atomic_pointer.h
  so we can use 'asm', 'volatile' keywords for Sun Studio.




git-svn-id: https://leveldb.googlecode.com/svn/trunk@39 62dab493-f737-651d-591e-8d6aee1b9529/"
Threads Management,Threads Management,Rocksdb,"Bugfix for issue 33; reduce lock contention in Get(), parallel benchmarks.

- Fix for issue 33 (non-null-terminated result from
  leveldb_property_value())

- Support for running multiple instances of a benchmark in parallel.

- Reduce lock contention on Get():
  (1) Do not hold the lock while searching memtables.
  (2) Shard block and table caches 16-ways.

  Benchmark for evaluating this change:
  $ db_bench --benchmarks=fillseq1,readrandom --threads=$n
  (fillseq1 is a small hack to make sure fillseq runs once regardless
  of number of threads specified on the command line).



git-svn-id: https://leveldb.googlecode.com/svn/trunk@49 62dab493-f737-651d-591e-8d6aee1b9529/"
Compression Tasks,Compression Tasks,Rocksdb,"Another change is to only use class
BlockContents for compressed block, and narrow the class Block to only be used for uncompressed
blocks, in compressed block cache"
,,Rocksdb,"Bugfixes for iterator and documentation.

- Fix bug in Iterator::Prev where it would return the wrong key.
  Fixes issues 29 and 30.

- Added a tweak to testharness to allow running just some tests.

- Fixing two minor documentation errors based on issues 28 and 25.

- Cleanup; fix namespaces of export-to-C code.
  Also fix one ""const char*"" vs ""char*"" mismatch.



git-svn-id: https://leveldb.googlecode.com/svn/trunk@48 62dab493-f737-651d-591e-8d6aee1b9529/"
Compression tasks,"compression tasks, resutructuring the code",Rocksdb,"Speed up Snappy uncompression, new Logger interface.

- Removed one copy of an uncompressed block contents changing
  the signature of Snappy_Uncompress() so it uncompresses into a
  flat array instead of a std::string.
        
  Speeds up readrandom ~10%.

- Instead of a combination of Env/WritableFile, we now have a
  Logger interface that can be easily overridden applications
  that want to supply their own logging.

- Separated out the gcc and Sun Studio parts of atomic_pointer.h
  so we can use 'asm', 'volatile' keywords for Sun Studio.




git-svn-id: https://leveldb.googlecode.com/svn/trunk@39 62dab493-f737-651d-591e-8d6aee1b9529/Small tweaks and bugfixes for Issue 18 and 19."
,,Rocksdb,"Slight tweak to the no-overlap optimization: only push to
level 2 to reduce the amount of wasted space when the same
small key range is being repeatedly overwritten.

Fix for Issue 18: Avoid failure on Windows by avoiding
deletion of lock file until the end of DestroyDB().

Fix for Issue 19: Disregard sequence numbers when checking for 
overlap in sstable ranges. This fixes issue 19: when writing 
the same key over and over again, we would generate a sequence 
of sstables that were never merged together since their sequence
numbers were disjoint.

Don't ignore map/unmap error checks.

Miscellaneous fixes for small problems Sanjay found while diagnosing
issue/9 and issue/16 (corruption_testr failures).
- log::Reader reports the record type when it finds an unexpected type.
- log::Reader no longer reports an error when it encounters an expected
  zero record regardless of the setting of the ""checksum"" flag.
- Added a missing forward declaration.
- Documented a side-effects of larger write buffer sizes
  (longer recovery time).



git-svn-id: https://leveldb.googlecode.com/svn/trunk@37 62dab493-f737-651d-591e-8d6aee1b9529/"
Memory Management,"Memory Management, compression tasks, resutructuring the code",Rocksdb,"Speed up Snappy uncompression, new Logger interface.

- Removed one copy of an uncompressed block contents changing
  the signature of Snappy_Uncompress() so it uncompresses into a
  flat array instead of a std::string.
        
  Speeds up readrandom ~10%.

- Instead of a combination of Env/WritableFile, we now have a
  Logger interface that can be easily overridden applications
  that want to supply their own logging.

- Separated out the gcc and Sun Studio parts of atomic_pointer.h
  so we can use 'asm', 'volatile' keywords for Sun Studio.




git-svn-id: https://leveldb.googlecode.com/svn/trunk@39 62dab493-f737-651d-591e-8d6aee1b9529/Sun Studio support, and fix for test related memory fixes.

- LevelDB patch for Sun Studio
  Based on a patch submitted by Theo Schlossnagle - thanks!
  This fixes Issue 17.

- Fix a couple of test related memory leaks.



git-svn-id: https://leveldb.googlecode.com/svn/trunk@38 62dab493-f737-651d-591e-8d6aee1b9529/"
Performance management,Performance management,Rocksdb,"A number of smaller fixes and performance improvements:

- Implemented Get() directly instead of building on top of a full
  merging iterator stack.  This speeds up the ""readrandom"" benchmark
  by up to 15-30%.

- Fixed an opensource compilation problem.
  Added --db=<name> flag to control where the database is placed.

- Automatically compact a file when we have done enough
  overlapping seeks to that file.

- Fixed a performance bug where we would read from at least one
  file in a level even if none of the files overlapped the key
  being read.

- Makefile fix for Mac OSX installations that have XCode 4 without XCode 3.

- Unified the two occurrences of binary search in a file-list
  into one routine.

- Found and fixed a bug where we would unnecessarily search the
  last file when looking for a key larger than all data in the
  level.

- A fix to avoid the need for trivial move compactions and
  therefore gets rid of two out of five syncs in ""fillseq"".

- Removed the MANIFEST file write when switching to a new
  memtable/log-file for a 10-20% improvement on fill speed on ext4.

- Adding a SNAPPY setting in the Makefile for folks who have
  Snappy installed. Snappy compresses values and speeds up writes.



git-svn-id: https://leveldb.googlecode.com/svn/trunk@32 62dab493-f737-651d-591e-8d6aee1b9529/"
,,Rocksdb,"Speed up Snappy uncompression, new Logger interface.

- Removed one copy of an uncompressed block contents changing
  the signature of Snappy_Uncompress() so it uncompresses into a
  flat array instead of a std::string.
        
  Speeds up readrandom ~10%.

- Instead of a combination of Env/WritableFile, we now have a
  Logger interface that can be easily overridden applications
  that want to supply their own logging.

- Separated out the gcc and Sun Studio parts of atomic_pointer.h
  so we can use 'asm', 'volatile' keywords for Sun Studio.




git-svn-id: https://leveldb.googlecode.com/svn/trunk@39 62dab493-f737-651d-591e-8d6aee1b9529/"
,,Rocksdb,"Small tweaks and bugfixes for Issue 18 and 19.

Slight tweak to the no-overlap optimization: only push to
level 2 to reduce the amount of wasted space when the same
small key range is being repeatedly overwritten.

Fix for Issue 18: Avoid failure on Windows by avoiding
deletion of lock file until the end of DestroyDB().

Fix for Issue 19: Disregard sequence numbers when checking for 
overlap in sstable ranges. This fixes issue 19: when writing 
the same key over and over again, we would generate a sequence 
of sstables that were never merged together since their sequence
numbers were disjoint.

Don't ignore map/unmap error checks.

Miscellaneous fixes for small problems Sanjay found while diagnosing
issue/9 and issue/16 (corruption_testr failures).
- log::Reader reports the record type when it finds an unexpected type.
- log::Reader no longer reports an error when it encounters an expected
  zero record regardless of the setting of the ""checksum"" flag.
- Added a missing forward declaration.
- Documented a side-effects of larger write buffer sizes
  (longer recovery time).



git-svn-id: https://leveldb.googlecode.com/svn/trunk@37 62dab493-f737-651d-591e-8d6aee1b9529/"
,,Rocksdb,"sync with upstream @21706995

Fixed race condition reported by Dave Smit (dizzyd@dizzyd,com)
on the leveldb mailing list.  We were not signalling
waiters after a trivial move from level-0.  The result was
that in some cases (hard to reproduce), a write would get
stuck forever waiting for the number of level-0 files to drop
below its hard limit.

The new code is simpler: there is just one condition variable
instead of two, and the condition variable is signalled after
every piece of background work finishes.  Also, all compaction
work (including for manual compactions) is done in the
background thread, and therefore we can remove the
""compacting_"" variable.



git-svn-id: https://leveldb.googlecode.com/svn/trunk@31 62dab493-f737-651d-591e-8d6aee1b9529/"
Threads Management,"Threads Management, Compression tasks",Rocksdb,"Bugfix for issue 33; reduce lock contention in Get(), parallel benchmarks.

- Fix for issue 33 (non-null-terminated result from
  leveldb_property_value())

- Support for running multiple instances of a benchmark in parallel.

- Reduce lock contention on Get():
  (1) Do not hold the lock while searching memtables.
  (2) Shard block and table caches 16-ways.

  Benchmark for evaluating this change:
  $ db_bench --benchmarks=fillseq1,readrandom --threads=$n
  (fillseq1 is a small hack to make sure fillseq runs once regardless
  of number of threads specified on the command line).



git-svn-id: https://leveldb.googlecode.com/svn/trunk@49 62dab493-f737-651d-591e-8d6aee1b9529/Speed up Snappy uncompression, "
,,Rocksdb,"new Logger interface.

- Removed one copy of an uncompressed block contents changing
  the signature of Snappy_Uncompress() so it uncompresses into a
  flat array instead of a std::string.
        
  Speeds up readrandom ~10%.

- Instead of a combination of Env/WritableFile, we now have a
  Logger interface that can be easily overridden applications
  that want to supply their own logging.

- Separated out the gcc and Sun Studio parts of atomic_pointer.h
  so we can use 'asm', 'volatile' keywords for Sun Studio.




git-svn-id: https://leveldb.googlecode.com/svn/trunk@39 62dab493-f737-651d-591e-8d6aee1b9529/"
,,Rocksdb,"Sun Studio support, and fix for test related memory fixes.

- LevelDB patch for Sun Studio
  Based on a patch submitted by Theo Schlossnagle - thanks!
  This fixes Issue 17.

- Fix a couple of test related memory leaks.



git-svn-id: https://leveldb.googlecode.com/svn/trunk@38 62dab493-f737-651d-591e-8d6aee1b9529/"
Performance management,Performance management,Rocksdb,"@23023120

git-svn-id: https://leveldb.googlecode.com/svn/trunk@47 62dab493-f737-651d-591e-8d6aee1b9529/Small tweaks and bugfixes for Issue 18 and 19.

Slight tweak to the no-overlap optimization: only push to
level 2 to reduce the amount of wasted s"
,,Rocksdb,"A number of fixes:

- Replace raw slice comparison with a call to user comparator.
  Added test for custom comparators.

- Fix end of namespace comments.

- Fixed bug in picking inputs for a level-0 compaction.

  When finding overlapping files, the covered range may expand
  as files are added to the input set.  We now correctly expand
  the range when this happens instead of continuing to use the
  old range.  For example, suppose L0 contains files with the
  following ranges:

  and the initial compaction target is F3.  We used to search
  for range f..j which yielded {F2,F3}.  However we now expand
  the range as soon as another file is added.  In this case,
  when F2 is added, we expand the range to c..j and restart the
  search.  That picks up file F1 as well.

  This change fixes a bug related to deleted keys showing up
  incorrectly after a compaction as described in Issue 44.

(Sync with upstream @25072954)/"
,,Rocksdb,"Pass system's CFLAGS, remove exit time destructor, sstable bug fix.

- Pass system's values of CFLAGS,LDFLAGS.
  Don't override OPT if it's already set.
  Original patch by Alessio Treglia <alessio@debian.org>:
  http://code.google.com/p/leveldb/issues/detail?id=27#c6

- Remove 1 exit time destructor from leveldb.
  See http://crbug.com/101600

- Fix problem where sstable building code would pass an
  internal key to the user comparator.

(Sync with uptream at 25436817.)/"
,,Rocksdb,"A number of bugfixes:

- Added DB::CompactRange() method.

  Changed manual compaction code so it breaks up compactions of
  big ranges into smaller compactions.

  Changed the code that pushes the output of memtable compactions
  to higher levels to obey the grandparent constraint: i.e., we
  must never have a single file in level L that overlaps too
  much data in level L+1 (to avoid very expensive L-1 compactions).

  Added code to pretty-print internal keys.

- Fixed bug where we would not detect overlap with files in
  level-0 because we were incorrectly using binary search
  on an array of files with overlapping ranges.

  Added ""leveldb.sstables"" property that can be used to dump
  all of the sstables and ranges that make up the db state.

- Removing post_write_snapshot support.  Email to leveldb mailing
  list brought up no users, just confusion from one person about
  what it meant.

- Fixing static_cast char to unsigned on BIG_ENDIAN platforms.

  Fixes	Issue 35 and Issue 36.

- Comment clarification to address leveldb Issue 37.

- Change license in posix_logger.h to match other files.

- A build problem where uint32 was used instead of uint32_t.

Sync with upstream @24408625/"
Threads Management,Threads Management,Rocksdb,"A number of bugfixes:

- Added DB::CompactRange() method.

  Changed manual compaction code so it breaks up compactions of
  big ranges into smaller compactions.

  Changed the code that pushes the output of memtable compactions
  to higher levels to obey the grandparent constraint: i.e., we
  must never have a single file in level L that overlaps too
  much data in level L+1 (to avoid very expensive L-1 compactions).

  Added code to pretty-print internal keys.

- Fixed bug where we would not detect overlap with files in
  level-0 because we were incorrectly using binary search
  on an array of files with overlapping ranges.

  Added ""leveldb.sstables"" property that can be used to dump
  all of the sstables and ranges that make up the db state.

- Removing post_write_snapshot support.  Email to leveldb mailing
  list brought up no users, just confusion from one person about
  what it meant.

- Fixing static_cast char to unsigned on BIG_ENDIAN platforms.

  Fixes	Issue 35 and Issue 36.

- Comment clarification to address leveldb Issue 37.

- Change license in posix_logger.h to match other files.

- A build problem where uint32 was used instead of uint32_t.

Sync with upstream @24408625/Bugfixes: for Get(), don't hold mutex while writing log.

- Fix bug in Get: when it triggers a compaction, it could sometimes
  mark the compaction with the wrong level (if there was a gap
  in the set of levels examined for the Get).

- Do not hold mutex while writing to the log file or to the
  MANIFEST file.

  Added a new benchmark that runs a writer thread concurrently with
  reader threads.



- Fixed race in optimized Get.  It should have been using the
  pinned memtables, not the current memtables.



git-svn-id: https://leveldb.googlecode.com/svn/trunk@50 62dab493-f737-651d-591e-8d6aee1b9529/"
Threads Management,Threads Management,Rocksdb,"A number of fixes:

- Replace raw slice comparison with a call to user comparator.
  Added test for custom comparators.

- Fix end of namespace comments.

- Fixed bug in picking inputs for a level-0 compaction.

  When finding overlapping files, the covered range may expand
  as files are added to the input set.  We now correctly expand
  the range when this happens instead of continuing to use the
  old range.  For example, suppose L0 contains files with the
  following ranges:

  and the initial compaction target is F3.  We used to search
  for range f..j which yielded {F2,F3}.  However we now expand
  the range as soon as another file is added.  In this case,
  when F2 is added, we expand the range to c..j and restart the
  search.  That picks up file F1 as well.

  This change fixes a bug related to deleted keys showing up
  incorrectly after a compaction as described in Issue 44.

(Sync with upstream @25072954)/A number of bugfixes:

- Added DB::CompactRange() method.

  Changed manual compaction code so it breaks up compactions of
  big ranges into smaller compactions.

  Changed the code that pushes the output of memtable compactions
  to higher levels to obey the grandparent constraint: i.e., we
  must never have a single file in level L that overlaps too
  much data in level L+1 (to avoid very expensive L-1 compactions).

  Added code to pretty-print internal keys.

- Fixed bug where we would not detect overlap with files in
  level-0 because we were incorrectly using binary search
  on an array of files with overlapping ranges.

  Added ""leveldb.sstables"" property that can be used to dump
  all of the sstables and ranges that make up the db state.

- Removing post_write_snapshot support.  Email to leveldb mailing
  list brought up no users, just confusion from one person about
  what it meant.

- Fixing static_cast char to unsigned on BIG_ENDIAN platforms.

  Fixes	Issue 35 and Issue 36.

- Comment clarification to address leveldb Issue 37.

- Change license in posix_logger.h to match other files.

- A build problem where uint32 was used instead of uint32_t.

Sync with upstream @24408625/Bugfixes: for Get(), don't hold mutex while writing log.

- Fix bug in Get: when it triggers a compaction, it could sometimes
  mark the compaction with the wrong level (if there was a gap
  in the set of levels examined for the Get).

- Do not hold mutex while writing to the log file or to the
  MANIFEST file.

  Added a new benchmark that runs a writer thread concurrently with
  reader threads.

  Percentiles
  ------------------------------
  micros/op: avg  median 99   99.9  99.99  99.999 max
  ------------------------------------------------------
  before:    42   38     110  225   32000  42000  48000
  after:     24   20     55   65    130    1100   7000

- Fixed race in optimized Get.  It should have been using the
  pinned memtables, not the current memtables.



git-svn-id: https://leveldb.googlecode.com/svn/trunk@50 62dab493-f737-651d-591e-8d6aee1b9529/"
Threads Management,Threads Management,Rocksdb,"Pass system's CFLAGS, remove exit time destructor, sstable bug fix.

- Pass system's values of CFLAGS,LDFLAGS.
  Don't override OPT if it's already set.
  Original patch by Alessio Treglia <alessio@debian.org>:
  http://code.google.com/p/leveldb/issues/detail?id=27#c6

- Remove 1 exit time destructor from leveldb.
  See http://crbug.com/101600

- Fix problem where sstable building code would pass an
  internal key to the user comparator.

(Sync with uptream at 25436817.)/A number of fixes:

- Replace raw slice comparison with a call to user comparator.
  Added test for custom comparators.

- Fix end of namespace comments.

- Fixed bug in picking inputs for a level-0 compaction.

  When finding overlapping files, the covered range may expand
  as files are added to the input set.  We now correctly expand
  the range when this happens instead of continuing to use the
  old range.  For example, suppose L0 contains files with the
  following ranges:

  and the initial compaction target is F3.  We used to search
  for range f..j which yielded {F2,F3}.  However we now expand
  the range as soon as another file is added.  In this case,
  when F2 is added, we expand the range to c..j and restart the
  search.  That picks up file F1 as well.

  This change fixes a bug related to deleted keys showing up
  incorrectly after a compaction as described in Issue 44.

(Sync with upstream @25072954)/A number of bugfixes:

- Added DB::CompactRange() method.

  Changed manual compaction code so it breaks up compactions of
  big ranges into smaller compactions.

  Changed the code that pushes the output of memtable compactions
  to higher levels to obey the grandparent constraint: i.e., we
  must never have a single file in level L that overlaps too
  much data in level L+1 (to avoid very expensive L-1 compactions).

  Added code to pretty-print internal keys.

- Fixed bug where we would not detect overlap with files in
  level-0 because we were incorrectly using binary search
  on an array of files with overlapping ranges.

  Added ""leveldb.sstables"" property that can be used to dump
  all of the sstables and ranges that make up the db state.

- Removing post_write_snapshot support.  Email to leveldb mailing
  list brought up no users, just confusion from one person about
  what it meant.

- Fixing static_cast char to unsigned on BIG_ENDIAN platforms.

  Fixes	Issue 35 and Issue 36.

- Comment clarification to address leveldb Issue 37.

- Change license in posix_logger.h to match other files.

- A build problem where uint32 was used instead of uint32_t.

Sync with upstream @24408625/Bugfixes: for Get(), don't hold mutex while writing log.

- Fix bug in Get: when it triggers a compaction, it could sometimes
  mark the compaction with the wrong level (if there was a gap
  in the set of levels examined for the Get).

- Do not hold mutex while writing to the log file or to the
  MANIFEST file.

  Added a new benchmark that runs a writer thread concurrently with
  reader threads.

- Fixed race in optimized Get.  It should have been using the
  pinned memtables, not the current memtables.



git-svn-id: https://leveldb.googlecode.com/svn/trunk@50 62dab493-f737-651d-591e-8d6aee1b9529/"
,,Rocksdb,fixed issues 66 (leaking files on disk error)  and 68 (no sync of CURRENT file)/
,,Rocksdb,"Added bloom filter support.

In particular, we add a new FilterPolicy class.  An instance
of this class can be supplied in Options when opening a
database.  If supplied, the instance is used to generate
summaries of keys (e.g., a bloom filter) which are placed in
sstables.  These summaries are consulted by DB::Get() so we
can avoid reading sstable blocks that are guaranteed to not
contain the key we are looking for.

This change provides one implementation of FilterPolicy
based on bloom filters.

Other changes:
- Updated version number to 1.4.
- Some build tweaks.
- C binding for CompactRange.
- A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom.
- Minor .gitignore update./"
,,Rocksdb,use mmap on 64-bit machines to speed-up reads; small build fixes/
,,Rocksdb,build shared libraries; updated version to 1.3; add Status accessors/
,,Rocksdb,"Added bloom filter support.

In particular, we add a new FilterPolicy class.  An instance
of this class can be supplied in Options when opening a
database.  If supplied, the instance is used to generate
summaries of keys (e.g., a bloom filter) which are placed in
sstables.  These summaries are consulted by DB::Get() so we
can avoid reading sstable blocks that are guaranteed to not
contain the key we are looking for.

This change provides one implementation of FilterPolicy
based on bloom filters.

Other changes:
- Updated version number to 1.4.
- Some build tweaks.
- C binding for CompactRange.
- A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom.
- Minor .gitignore update./use mmap on 64-bit machines to speed-up reads; small build fixes/"
Threads Management,Threads Management,Rocksdb,"Added bloom filter support.

In particular, we add a new FilterPolicy class.  An instance
of this class can be supplied in Options when opening a
database.  If supplied, the instance is used to generate
summaries of keys (e.g., a bloom filter) which are placed in
sstables.  These summaries are consulted by DB::Get() so we
can avoid reading sstable blocks that are guaranteed to not
contain the key we are looking for.

This change provides one implementation of FilterPolicy
based on bloom filters.

Other changes:
- Updated version number to 1.4.
- Some build tweaks.
- C binding for CompactRange.
- A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom.
- Minor .gitignore update./fix LOCK file deletion to prevent crash on windows/added group commit; drastically speeds up mult-threaded synchronous write workloads/"
Threads Management,Threads Management,Rocksdb,added group commit; drastically speeds up mult-threaded synchronous write workloads/
,,Rocksdb,"Added bloom filter support.

In particular, we add a new FilterPolicy class.  An instance
of this class can be supplied in Options when opening a
database.  If supplied, the instance is used to generate
summaries of keys (e.g., a bloom filter) which are placed in
sstables.  These summaries are consulted by DB::Get() so we
can avoid reading sstable blocks that are guaranteed to not
contain the key we are looking for.

This change provides one implementation of FilterPolicy
based on bloom filters.

Other changes:
- Updated version number to 1.4.
- Some build tweaks.
- C binding for CompactRange.
- A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom.
- Minor .gitignore update./fixed issues 66 (leaking files on disk error)  and 68 (no sync of CURRENT file)/"
,,Rocksdb,"Added bloom filter support.

In particular, we add a new FilterPolicy class.  An instance
of this class can be supplied in Options when opening a
database.  If supplied, the instance is used to generate
summaries of keys (e.g., a bloom filter) which are placed in
sstables.  These summaries are consulted by DB::Get() so we
can avoid reading sstable blocks that are guaranteed to not
contain the key we are looking for.

This change provides one implementation of FilterPolicy
based on bloom filters.

Other changes:
- Updated version number to 1.4.
- Some build tweaks.
- C binding for CompactRange.
- A few more benchmarks: deleteseq, deleterandom, readmissing, seekrandom.
- Minor .gitignore update./avoid very large compactions; fix build on Linux/"
,,Rocksdb,"bits_per_key is already configurable. It defines how many bloom bits will be used for every key in the database.

My change in this patch is to make the Hash code that is used for blooms to be confgurable. In fact,
one can specify a modified HashCode that inspects only parts of the Key to generate the Hash (used
by booms).

Test Plan: none

Differential Revision: https://reviews.facebook.net/D4059/"
,,Rocksdb,"Add support to specify the number of shards for the Block cache. By default, the block cache is sharded into 16 parts.

Summary:
Task ID: #

Blame Rev:

Test Plan: Revert Plan:

Differential Revision: https://reviews.facebook.net/D3273/"
,,Rocksdb,"Make some variables configurable for each db instance

Summary:
Make configurable 'targetFileSize', 'targetFileSizeMultiplier',
'maxBytesForLevelBase', 'maxBytesForLevelMultiplier',
'expandedCompactionFactor', 'maxGrandParentOverlapFactor'


Differential Revision: https://reviews.facebook.net/D3801/"
,,Rocksdb,"Support --bufferedio=[0,1] from db_bench. If bufferedio = 0, then the read code path clears the OS page cache after the IO is completed. The default remains as bufferedio=1

Summary:
Task ID: #

Blame Rev:

Test Plan: Revert Plan:

Differential Revision: https://reviews.facebook.net/D3429/"
Restructuring the code,"Restructuring the code, Compression Tasks, DataBase Management",Rocksdb,"add zlib compression

Summary: add zlib compression

Test Plan: Will add more testcases

some variables configurable for each db instance

Summary:
Make configurable 'targetFileSize', 'targetFileSizeMultiplier',
'maxBytesForLevelBase', 'maxBytesForLevelMultiplier',
'expandedCompactionFactor', 'maxGrandParentOverlapFactor'

Differential Revision: https://reviews.facebook.net/D3801/"
Restructuring the code,"Restructuring the code, Compression Tasks, DataBase Management",Rocksdb,"add zlib compression

Summary: add zlib compression

Test Plan: Will add more testcases


Differential Revision: https://reviews.facebook.net/D3873/"
Restructuring the code,"Restructuring the code, Compression Tasks, DataBase Management",Rocksdb,"add bzip2 compression

Summary: add bzip2 compression

Test Plan: testcases in table_test


Differential Revision: https://reviews.facebook.net/D3909/"
,,Rocksdb,"Introduce a new option disableDataSync for opening the database. If this is set to true, then the data written to newly created data files are not sycned to disk, instead depend on the OS to flush dirty data to stable storage. This option is good for bulk

Test Plan:
manual tests

Task ID: #

Blame Rev:

Differential Revision: https://reviews.facebook.net/D4515/"
,,Rocksdb,"add flush interface to DB

Summary: as subject. The flush will flush everything in the db.

Test Plan: new test in db_test.cc


Differential Revision: https://reviews.facebook.net/D4029/"
,,Rocksdb,"add disable WAL option

Summary: add disable WAL option

Test Plan: new testcase in db_test.cc


Differential Revision: https://reviews.facebook.net/D4011/"
,,Rocksdb,"Make some variables configurable for each db instance

Summary:
Make configurable 'targetFileSize', 'targetFileSizeMultiplier',
'maxBytesForLevelBase', 'maxBytesForLevelMultiplier',
'expandedCompactionFactor', 'maxGrandParentOverlapFactor'

Test Plan: N/A

Differential Revision: https://reviews.facebook.net/D3801/"
,,Rocksdb,"Print log message when we are throttling writes.

Summary:
Added option --writes=xxx to specify the number of keys that we want to overwrite in the benchmark.

Task ID: #

Blame Rev:

Test Plan: Revert Plan:

Reviewers: adsharma

CC: sc

Differential Revision: https://reviews.facebook.net/D3465/"
,,Rocksdb,"Make Leveldb save data into HDFS files. You have to set USE_HDFS in your environment variable to compile leveldb with HDFS support.

Test Plan: Run benchmark.

Differential Revision: https://reviews.facebook.net/D3549/Print log message when we are throttling writes.

Summary:
Added option --writes=xxx to specify the number of keys that we want to overwrite in the benchmark.

Task ID: #

Blame Rev:

Test Plan: Revert Plan:

Reviewers: adsharma

CC: sc

Differential Revision: https://reviews.facebook.net/D3465/"
,,Rocksdb,"Fix table-cache size bug, gather table-cache statistics and  prevent readahead done by fs. Summary:

Summary:
The db_bench test was not using the specified value for the max-file-open. Fixed.

The fs readhead is switched off.

Gather statistics about the table cache and print it out at the end of the tets run.

Test Plan: Revert Plan:

Reviewers: adsharma, sc

Reviewed By: adsharma

Differential Revision: https://reviews.facebook.net/D3441/"
,,Rocksdb,"Ability to make the benchmark issue a large number of IOs. This is helpful to populate many gigabytes of data for benchmarking at scale.

Summary:
Task ID: #

Blame Rev:

Test Plan: Revert Plan:

Differential Revision: https://reviews.facebook.net/D3333/"
,,Rocksdb,"Ability to switch on checksum verification from benchmark.

Summary:
Task ID: #

Blame Rev:

Test Plan: Revert Plan:

Differential Revision: https://reviews.facebook.net/D3309/"
,,Rocksdb,"Support arcdiff.

Summary:
Task ID: #

Blame Rev:

Test Plan: Revert Plan:

Differential Revision: https://reviews.facebook.net/D3105/"
,,Rocksdb,"add flush interface to DB

Summary: as subject. The flush will flush everything in the db.

Test Plan: new test in db_test.cc

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D4029/"
,,Rocksdb,"Make some variables configurable for each db instance

Summary:
Make configurable 'targetFileSize', 'targetFileSizeMultiplier',
'maxBytesForLevelBase', 'maxBytesForLevelMultiplier',
'expandedCompactionFactor', 'maxGrandParentOverlapFactor'

Test Plan: N/A

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D3801/"
,,Rocksdb,"add disable WAL option

Summary: add disable WAL option

Test Plan: new testcase in db_test.cc

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D4011/"
Compression tasks,Compression Tasks,Rocksdb,"fix compile warning

Summary: as subject

Test Plan: compile

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D3957/"
,,Rocksdb,"add bzip2 compression

Summary: add bzip2 compression

Test Plan: testcases in table_test

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D3909/"
,,Rocksdb,"add zlib compression

Summary: add zlib compression

Test Plan: Will add more testcases

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D3873/"
,,Rocksdb,"Fix all warnings generated by -Wall option to the compiler.

Summary:
The default compilation process now uses ""-Wall"" to compile.
Fix all compilation error generated by gcc.

Test Plan: make all check

Reviewers: heyongqiang, emayanke, sheki

Reviewed By: heyongqiang

CC: MarkCallaghan

Differential Revision: https://reviews.facebook.net/D6525/"
,,Rocksdb,"Enable LevelDb to create a new log file if current log file is too large.

Summary: Enable LevelDb to create a new log file if current log file is too large.

Test Plan:
Write a script and manually check the generated info LOG.

Task ID: 1803577

Blame Rev:

Reviewers: dhruba, heyongqiang

Reviewed By: heyongqiang

CC: zshao

Differential Revision: https://reviews.facebook.net/D6003/"
,,Rocksdb,"Clean up compiler warnings generated by -Wall option.

Summary:
Clean up compiler warnings generated by -Wall option.
make clean all OPT=-Wall

This is a pre-requisite before making a new release.

Test Plan: compile and run unit tests

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5019/"
,,Rocksdb,"merge 1.5

Summary:

as subject

Test Plan:

db_test table_test

Reviewers: dhruba/"
,,Rocksdb,"Print the block cache size in the LOG.

Summary: Print the block cache size in the LOG.

Test Plan: run db_bench and look at LOG. This is helpful while I was debugging one use-case.

Reviewers: heyongqiang, MarkCallaghan

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5739/"
,,Rocksdb,"The sharding of the block cache is limited to 2*20 pieces.

Summary:
The numbers of shards that the block cache is divided into is
configurable. However, if the user specifies that he/she wants
the block cache to be divided into more than 2**20 pieces, then
the system will rey to allocate a huge array of that size) that
could fail.

It is better to limit the sharding of the block cache to an
upper bound. The default sharding is 16 shards (i.e. 2**4)
and the maximum is now 2 million shards (i.e. 2**20).

Also, fixed a bug with the LRUCache where the numShardBits
should be a private member of the LRUCache object rather than
a static variable.

Test Plan:
run db_bench with --cache_numshardbits=64.

Task ID: #

Blame Rev:

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5013/"
Performance Management,"Performance management, compression tasks",Rocksdb,"Merge branch 'master' into performance

Conflicts:
	db/db_impl.cc
	db/version_set.cc
	util/options.cc/Fix all warnings generated by -Wall option to the compiler.

Summary:
The default compilation process now uses ""-Wall"" to compile.
Fix all compilation error generated by gcc.

Test Plan: make all check

Reviewers: heyongqiang, emayanke, sheki

Reviewed By: heyongqiang

CC: MarkCallaghan

Differential Revision: https://reviews.facebook.net/D6525/"
,,Rocksdb,"Ability to invoke application hook for every key during compaction.

Summary:
There are certain use-cases where the application intends to
delete older keys aftre they have expired a certian time period.
One option for those applications is to periodically scan the
entire database and delete appropriate keys.

A better way is to allow the application to hook into the
compaction process. This patch allows the application to set
a method callback for every key that is being compacted. If
this method returns true, then the key is not preserved in
the output of the compaction.

Test Plan:
This is mostly to preview the proposed new public api.
Since it is a public api, please do due diligence on reviewing it.

I will be writing test cases for this api in mynext version of
this patch.

Reviewers: MarkCallaghan, heyongqiang

Reviewed By: heyongqiang

CC: sheki, adsharma

Differential Revision: https://reviews.facebook.net/D6285/"
,,Rocksdb,"Make compression options configurable. These include window-bits, level and strategy for ZlibCompression

Summary: Leveldb currently uses windowBits=-14 while using zlib compression.(It was earlier 15). This makes the setting configurable. Related changes here: https://reviews.facebook.net/D6105

Test Plan: make all check

Reviewers: dhruba, MarkCallaghan, sheki, heyongqiang

Differential Revision: https://reviews.facebook.net/D6393/"
,,Rocksdb,"Add two more options: disable block cache and make table cache shard number configuable

Summary:

as subject

Test Plan:

run db_bench and db_test

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D6111/"
,,Rocksdb,"Allow having different compression algorithms on different levels.

Summary:
The leveldb API is enhanced to support different compression algorithms at
different levels.

This adds the option min_level_to_compress to db_bench that specifies
the minimum level for which compression should be done when
compression is enabled. This can be used to disable compression for levels
0 and 1 which are likely to suffer from stalls because of the CPU load
for memtable flushes and (L0,L1) compaction.  Level 0 is special as it
gets frequent memtable flushes. Level 1 is special as it frequently
gets all:all file compactions between it and level 0. But all other levels
could be the same. For any level N where N > 1, the rate of sequential
IO for that level should be the same. The last level is the
exception because it might not be full and because files from it are
not read to compact with the next larger level.

The same amount of time will be spent doing compaction at any
level N excluding N=0, 1 or the last level. By this standard all
of those levels should use the same compression. The difference is that
the loss (using more disk space) from a faster compression algorithm
is less significant for N=2 than for N=3. So we might be willing to
trade disk space for faster write rates with no compression
for L0 and L1, snappy for L2, zlib for L3. Using a faster compression
algorithm for the mid levels also allows us to reclaim some cpu
without trading off much loss in disk space overhead.

Also note that little is to be gained by compressing levels 0 and 1. For
a 4-level tree they account for 10% of the data. For a 5-level tree they
account for 1% of the data.

With compression enabled:
* memtable flush rate is ~18MB/second
* (L0,L1) compaction rate is ~30MB/second

With compression enabled but min_level_to_compress=2
* memtable flush rate is ~320MB/second
* (L0,L1) compaction rate is ~560MB/second

This practicaly takes the same code from https://reviews.facebook.net/D6225
but makes the leveldb api more general purpose with a few additional
lines of code.

Test Plan: make check

Differential Revision: https://reviews.facebook.net/D6261/"
,,Rocksdb,"db_bench was not correctly initializing the value for delete_obsolete_files_period_micros option.

Summary:
The parameter delete_obsolete_files_period_micros controls the
periodicity of deleting obsolete files. db_bench was reading in
this parameter intoa local variable called 'l' but was incorrectly
using another local variable called 'n' while setting it in the
db.options data structure.
This patch also logs the value of delete_obsolete_files_period_micros
in the LOG file at db startup time.

I am hoping that this will improve the overall write throughput drastically.

Test Plan: run db_bench

Reviewers: MarkCallaghan, heyongqiang

Reviewed By: MarkCallaghan

Differential Revision: https://reviews.facebook.net/D6099/"
,,Rocksdb,"The deletion of obsolete files should not occur very frequently.

Summary:
The method DeleteObsolete files is a very costly methind, especially
when the number of files in a system is large. It makes a list of
all live-files and then scans the directory to compute the diff.
By default, this method is executed after every compaction run.

This patch makes it such that DeleteObsolete files is never
invoked twice within a configured period.

Test Plan: run all unit tests

Reviewers: heyongqiang, MarkCallaghan

Reviewed By: MarkCallaghan

Differential Revision: https://reviews.facebook.net/D6045/"
,,Rocksdb,"add an option to disable seek compaction

Summary:
as subject. This diff should be good for benchmarking.

will send another diff to make it better in the case the seek compaction is enable.
In that coming diff, will not count a seek if the bloomfilter filters.

Test Plan: build

Reviewers: dhruba, MarkCallaghan

Reviewed By: MarkCallaghan

Differential Revision: https://reviews.facebook.net/D5481/"
,,Rocksdb,"put log in a seperate dir

Summary: added a new option db_log_dir, which points the log dir. Inside that dir, in order to make log names unique, the log file name is prefixed with the leveldb data dir absolute path.

Test Plan: db_test

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D5205/"
,,Rocksdb,"Introduce a new method Env->Fsync() that issues fsync (instead of fdatasync).

Summary:
Introduce a new method Env->Fsync() that issues fsync (instead of fdatasync).
This is needed for data durability when running on ext3 filesystems.
Added options to the benchmark db_bench to generate performance numbers
with either fsync or fdatasync enabled.

Cleaned up Makefile to build leveldb_shell only when building the thrift
leveldb server.

Test Plan: build and run benchmark

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D4911/"
,,Rocksdb,"Log the open-options to the LOG.

Summary: Log the open-options to the LOG. Use options_ instead of options because SanitizeOptions could modify the max_file_open limit.

Test Plan: num db_bench

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D4833/"
,,Rocksdb,"adding a scribe logger in leveldb to log leveldb deploy stats

Summary:
as subject.

A new log is written to scribe via thrift client when a new db is opened and when there is
a compaction.

a new option var scribe_log_db_stats is added.

Test Plan: manually checked using command ""ptail -time 0 leveldb_deploy_stats""

Reviewers: dhruba

Differential Revision: https://reviews.facebook.net/D4659/"
Threads Management,Threads Management,Rocksdb,"This is the mega-patch multi-threaded compaction
published in https://reviews.facebook.net/D5997.

Summary:
This patch allows compaction to occur in multiple background threads
concurrently.

If a manual compaction is issued, the system falls back to a
single-compaction-thread model. This is done to ensure correctess
and simplicity of code. When the manual compaction is finished,
the system resumes its concurrent-compaction mode automatically.

The updates to the manifest are done via group-commit approach.

Test Plan: run db_bench/"
,,Rocksdb,"An configurable option to write data using write instead of mmap.

Summary:
We have seen that reading data via the pread call (instead of
mmap) is much faster on Linux 2.6.x kernels. This patch makes
an equivalent option to switch off mmaps for the write path
as well.

db_bench --mmap_write=0 will use write() instead of mmap() to
write data to a file.

This change is backward compatible, the default
option is to continue using mmap for writing to a file.

Test Plan: ""make check all""

Differential Revision: https://reviews.facebook.net/D5781/"
,,Rocksdb,"The BackupAPI should also list the length of the manifest file.

Summary:
The GetLiveFiles() api lists the set of sst files and the current
MANIFEST file. But the database continues to append new data to the
MANIFEST file even when the application is backing it up to the
backup location. This means that the database-version that is
stored in the MANIFEST FILE in the backup location
does not correspond to the sst files returned by GetLiveFiles.

This API adds a new parameter to GetLiveFiles. This new parmeter
returns the current size of the MANIFEST file.

Test Plan: Unit test attached.

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5631/"
,,Rocksdb,"Allow a configurable number of background threads.

Summary:
The background threads are necessary for compaction.
For slower storage, it might be necessary to have more than
one compaction thread per DB. This patch allows creating
a configurable number of worker threads.
The default reamins at 1 (to maintain backward compatibility).

Test Plan:
run all unit tests. changes to db-bench coming in
a separate patch.

Reviewers: heyongqiang

Reviewed By: heyongqiang

CC: MarkCallaghan

Differential Revision: https://reviews.facebook.net/D5559/"
,,Rocksdb,"Ability to switch off filesystem read-aheads

Summary:
Ability to switch off filesystem read-aheads. This change is
backward-compatible: the default setting is to allow file
system read-aheads.

Test Plan: run benchmarks

Reviewers: heyongqiang, adsharma

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5391/"
,,Rocksdb,"Do not cache readahead-pages in the OS cache.

Summary:
When posix_fadvise(offset, offset) is usedm it frees up only those
pages in that specified range. But the filesystem could have done some
read-aheads and those get cached in the OS cache.

Do not cache readahead-pages in the OS cache.

Test Plan: run db_bench benchmark.

Reviewers: vamsi, heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5379/"
,,Rocksdb,"Fix compiler warnings. Use uint64_t instead of uint.

Summary: Fix compiler warnings. Use uint64_t instead of uint.

Test Plan: build using -Wall

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5355/"
,,Rocksdb,"Unit test corruption_test do not compile.

Summary: Unit test corruption_test do not compile.

Test Plan: run unit tests

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D4797/"
Threads Management,Threads Management,Rocksdb,"stat's collection in leveldb

Summary:
Prototype stat's collection. Diff is a good estimate of what
the final code will look like.
A few assumptions :
  * Used a global static instance of the statistics object. Plan to pass
  it to each internal function. Static allows metrics only at app
  level.
  * In the Ticker's do not do any locking. Depend on the mutex at each
   function of LevelDB. If we ever remove the mutex, we should change
   here too. The other option is use atomic objects anyways as there
   won't be any contention as they will be always acquired only by one
   thread.
  * The counters are dumb, increment through lifecycle. Plan to use ods
    etc to get last5min stat etc.

Test Plan:
made changes in db_bench
Ran ./db_bench --statistics=1 --num=10000 --cache_size=5000
This will print the cache hit/miss stats.

Reviewers: dhruba, heyongqiang

Differential Revision: https://reviews.facebook.net/D6441/"
,,Rocksdb,"Trigger read compaction only if seeks to storage are incurred.

Summary:
In the current code, a Get() call can trigger compaction if it has to look at more than one file. This causes unnecessary compaction because looking at more than one file is a penalty only if the file is not yet in the cache. Also, th current code counts these files before the bloom filter check is applied.

This patch counts a 'seek' only if the file fails the bloom filter
check and has to read in data block(s) from the storage.

This patch also counts a 'seek' if a file is not present in the file-cache, because opening a file means that its index blocks need to be read into cache.

Test Plan: unit test attached. I will probably add one more unti tests.

Reviewers: heyongqiang

Reviewed By: heyongqiang

CC: MarkCallaghan

Differential Revision: https://reviews.facebook.net/D5709/"
Threads Management,Threads Management,Rocksdb,"This is the mega-patch multi-threaded compaction
published in https://reviews.facebook.net/D5997.

Summary:
This patch allows compaction to occur in multiple background threads
concurrently.

If a manual compaction is issued, the system falls back to a
single-compaction-thread model. This is done to ensure correctess
and simplicity of code. When the manual compaction is finished,
the system resumes its concurrent-compaction mode automatically.

The updates to the manifest are done via group-commit approach.

Test Plan: run db_bench/Fix compiler warnings. Use uint64_t instead of uint.

Summary: Fix compiler warnings. Use uint64_t instead of uint.

Test Plan: build using -Wall

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5355/"
Restructuring the code,"Restructuring the code, Thread management",Rocksdb,"Move filesize-based-sorting to outside the Mutex

Summary:
When a new version is created, we sort all the files at every
level based on their size. This is necessary because we want
to compact the largest file first. The sorting takes quite a
bit of CPU.

Moved the sorting code to be outside the mutex. Also, the
earlier code was sorting files at all levels but we do not
need to sort the highest-number level because those files
are never the cause of any compaction. To reduce sorting
costs, we sort only the first few files in each level
because it is likely that those are the only files in that
level that will be picked for compaction.

At steady state, I have seen that this patch increase
throughout from 1500 writes/sec to 1700 writes/sec at the
end of a 72 hour run. The cpu saving by not sorting the
last level was not distinctive in this test run because
there were only 100K files in the highest numbered level.
I expect the cpu saving to be significant when the number of
files is much higher.

This is mostly an early preview and not ready for rigorous review.

With this patch, the writs/sec is now bottlenecked not by the sorting code but by GetOverlappingInputs. I am working on a patch to optimize GetOverlappingInputs.

Test Plan: make check

Reviewers: MarkCallaghan, heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D6411/"
,,Rocksdb,"Merge branch 'master' into performance


Summary: as subject

Test Plan: run db_bench readrandom

Reviewers: dhruba

Reviewed By: dhruba

CC: MarkCallaghan, emayanke, sheki

Differential Revision: https://reviews.facebook.net/D6495/"
,,Rocksdb,"Flush Data at object destruction if disableWal is used.

Summary:
Added a conditional flush in ~DBImpl to flush.
There is still a chance of writes not being persisted if there is a
crash (not a clean shutdown) before the DBImpl instance is destroyed.

Test Plan: modified db_test to meet the new expectations.

Reviewers: dhruba, heyongqiang

Differential Revision: https://reviews.facebook.net/D6519/"
,,Rocksdb,"Use timer to measure sleep rather than assume it is 1000 usecs

Summary:
This makes the stall timers in MakeRoomForWrite more accurate by timing
the sleeps. From looking at the logs the real sleep times are usually
about 2000 usecs each when SleepForMicros(1000) is called. The modified LOG messages are:
2012/10/29-12:06:33.271984 2b3cc872f700 delaying write 13 usecs for level0_slowdown_writes_trigger
2012/10/29-12:06:34.688939 2b3cc872f700 delaying write 1728 usecs for rate limits with max score 3.83

Task ID: #

Blame Rev:

Test Plan:
run db_bench, look at DB/LOG

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D6297/"
,,Rocksdb,"Add more rates to db_bench output

Summary:
Adds the ""MB/sec in"" and ""MB/sec out"" to this line:
Amplification: 1.7 rate, 0.01 GB in, 0.02 GB out, 8.24 MB/sec in, 13.75 MB/sec out

Changes all values to be reported per interval and since test start for this line:
... thread 0: (10000,60000) ops and (19155.6,27307.5) ops/second in (0.522041,2.197198) seconds

Task ID: #

Blame Rev:

Test Plan:
run db_bench

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D6291/"
,,Rocksdb,"Adds DB::GetNextCompaction and then uses that for rate limiting db_bench

Summary:
Adds a method that returns the score for the next level that most
needs compaction. That method is then used by db_bench to rate limit threads.
Threads are put to sleep at the end of each stats interval until the score
is less than the limit. The limit is set via the --rate_limit=$double option.
The specified value must be > 1.0. Also adds the option --stats_per_interval
to enable additional metrics reported every stats interval.

Task ID: #

Blame Rev:

Test Plan:
run db_bench

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D6243/"
,,Rocksdb,"Normalize compaction stats by time in compaction

Summary:
I used server uptime to compute per-level IO throughput rates. I
intended to use time spent doing compaction at that level. This fixes that.

Task ID: #

Blame Rev:

Test Plan:
run db_bench, look at results

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D6237/"
,,Rocksdb,"Improve statistics

Summary:
This adds more statistics to be reported by GetProperty(""leveldb.stats"").
The new stats include time spent waiting on stalls in MakeRoomForWrite.
This also includes the total amplification rate where that is:
    (#bytes of sequential IO during compaction) / (#bytes from Put)
This also includes a lot more data for the per-level compaction report.
* Rn(MB) - MB read from level N during compaction between levels N and N+1
* Rnp1(MB) - MB read from level N+1 during compaction between levels N and N+1
* Wnew(MB) - new data written to the level during compaction
* Amplify - ( Write(MB) + Rnp1(MB) ) / Rn(MB)
* Rn - files read from level N during compaction between levels N and N+1
* Rnp1 - files read from level N+1 during compaction between levels N and N+1
* Wnp1 - files written to level N+1 during compaction between levels N and N+1
* NewW - new files written to level N+1 during compaction
* Count - number of compactions done for this level

This is the new output from DB::GetProperty(""leveldb.stats""). The old output stopped at Write(MB)

                               Compactions
Level  Files Size(MB) Time(sec) Read(MB) Write(MB)  Rn(MB) Rnp1(MB) Wnew(MB) Amplify Read(MB/s) Write(MB/s)   Rn Rnp1 Wnp1 NewW Count
-------------------------------------------------------------------------------------------------------------------------------------
  0        3        6        33        0       576       0        0      576    -1.0       0.0         1.3     0    0    0    0   290
  1      127      242       351     5316      5314     570     4747      567    17.0      12.1        12.1   287 2399 2685  286    32
  2      161      328        54      822       824     326      496      328     4.0       1.9         1.9   160  251  411  160   161
Amplification: 22.3 rate, 0.56 GB in, 12.55 GB out
Uptime(secs): 439.8
Stalls(secs): 206.938 level0_slowdown, 0.000 level0_numfiles, 24.129 memtable_compaction

Task ID: #

Blame Rev:

Test Plan:
run db_bench

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -
(cherry picked from commit ecdeead38f86cc02e754d0032600742c4f02fec8)

Reviewers: dhruba

Differential Revision: https://reviews.facebook.net/D6153/"
,,Rocksdb,"Delete files outside the mutex.

Summary:
The compaction process deletes a large number of files. This takes
quite a bit of time and is best done outside the mutex lock.

Test Plan: make check

Differential Revision: https://reviews.facebook.net/D6123/"
,,Rocksdb,"Merge branch 'master' into performance

Conflicts:
	db/db_impl.cc
	db/db_impl.h/Delete files outside the mutex.

Summary:
The compaction process deletes a large number of files. This takes
quite a bit of time and is best done outside the mutex lock.

Test Plan:

Reviewers:

CC:

Task ID: #

Blame Rev:/This is the mega-patch multi-threaded compaction
published in https://reviews.facebook.net/D5997.

Summary:
This patch allows compaction to occur in multiple background threads
concurrently.

If a manual compaction is issued, the system falls back to a
single-compaction-thread model. This is done to ensure correctess
and simplicity of code. When the manual compaction is finished,
the system resumes its concurrent-compaction mode automatically.

The updates to the manifest are done via group-commit approach.

Test Plan: run db_bench/The deletion of obsolete files should not occur very frequently.

Summary:
The method DeleteObsolete files is a very costly methind, especially
when the number of files in a system is large. It makes a list of
all live-files and then scans the directory to compute the diff.
By default, this method is executed after every compaction run.

This patch makes it such that DeleteObsolete files is never
invoked twice within a configured period.

Test Plan: run all unit tests

Reviewers: heyongqiang, MarkCallaghan

Reviewed By: MarkCallaghan

Differential Revision: https://reviews.facebook.net/D6045/"
,,Rocksdb,"If ReadCompaction is switched off, then it is better to not even submit background compaction jobs.

Summary:
If ReadCompaction is switched off, then it is better to not even
submit background compaction jobs. I see about 3% increase in
read-throughput on a pure memory database.

Test Plan: run db_bench

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5673/"
,,Rocksdb,"Print out the compile version in the LOG.

Summary: Print out the compile version in the LOG.

Test Plan: run dbbench and verify LOG

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5529/"
,,Rocksdb,"Ability to take a file-lvel snapshot from leveldb.

Summary:
A set of apis that allows an application to backup data from the
leveldb database based on a set of files.

Test Plan: unint test attached. more coming soon.

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5439/"
,,Rocksdb,"put log in a seperate dir

Summary: added a new option db_log_dir, which points the log dir. Inside that dir, in order to make log names unique, the log file name is prefixed with the leveldb data dir absolute path.

Test Plan: db_test

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D5205/merge 1.5

Summary:

as subject

Test Plan:

db_test table_test

Reviewers: dhruba/Do not spin in a tight loop attempting compactions if there is a compaction error

Summary: as subject. ported the change from google code leveldb 1.5

Test Plan: run db_test

Reviewers: dhruba

Differential Revision: https://reviews.facebook.net/D4839/fix db_test error with scribe logger turned on

Summary: as subject

Test Plan: db_test

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D4929/"
,,Rocksdb,"add more logs

Summary:

as subject

Test Plan:db_test

Reviewers: dhruba/Log the open-options to the LOG.

Summary: Log the open-options to the LOG. Use options_ instead of options because SanitizeOptions could modify the max_file_open limit.

Test Plan: num db_bench

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D4833/"
,,Rocksdb,"Fixed unit test c_test by initializing logger=NULL.

Summary:
Fixed unit test c_test by initializing logger=NULL.

Removed ""atomic"" from last_log_ts so that unit tests do not require C11 compiler.
Anyway, last_log_ts is mostly used for logging, so it is ok if it is loosely
accurate.

Test Plan: run c_test

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D4803/"
,,Rocksdb,"add compaction log Summary:

Summary:
add compaction summary to log

log looks like:

2012/08/17-18:18:32.557334 7fdcaa2bb700 Compaction summary: Base level 0, input file:[11 9 7 ],[]

Test Plan: tested via db_test

Reviewers: dhruba

Differential Revision: https://reviews.facebook.net/D4749/"
,,Rocksdb,"use ts as suffix for LOG.old files

Summary: as subject and only maintain 10 log files.

Test Plan: new test in db_test

Reviewers: dhruba

Differential Revision: https://reviews.facebook.net/D4731/"
Data Conversion,"Data Conversion, Thread management",Rocksdb,"The db_bench utility was broken in 1.5.4.fb because of a signed-unsigned comparision.

Summary:
The db_bench utility was broken in 1.5.4.fb because of a
signed-unsigned comparision.

The static variable FLAGS_min_level_to_compress was recently
changed from int to 'unsigned in' but it is initilized to a
nagative value -1.

The segfault is of this type:
Program received signal SIGSEGV, Segmentation fault.
Open (this=0x7fffffffdee0) at db/db_bench.cc:939
939	db/db_bench.cc: No such file or directory.
(gdb) where

Test Plan: run db_bench with no options.

Reviewers: heyongqiang

Reviewed By: heyongqiang

CC: MarkCallaghan, emayanke, sheki

Differential Revision: https://reviews.facebook.net/D6663/"
,,Rocksdb,"Metrics: record compaction drop's and bloom filter effectiveness

Summary: Record BloomFliter hits and drop off reasons during compaction.

Test Plan: Unit tests work.

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D6591/"
,,Rocksdb,"disable size compaction in ldb reduce_levels and added compression and file size parameter to it

Summary:
disable size compaction in ldb reduce_levels, this will avoid compactions rather than the manual comapction,

added --compression=none|snappy|zlib|bzip2 and --file_size= per-file size to ldb reduce_levels command

Test Plan: run ldb

Reviewers: dhruba, MarkCallaghan

Reviewed By: dhruba

CC: sheki, emayanke

Differential Revision: https://reviews.facebook.net/D6597/"
,,Rocksdb,"Add a readonly db

Summary: as subject

Test Plan: run db_bench readrandom

Reviewers: dhruba

Reviewed By: dhruba

CC: MarkCallaghan, emayanke, sheki

Differential Revision: https://reviews.facebook.net/D6495/"
,,Rocksdb,"Merge branch 'master' into performance

Conflicts:
	db/db_bench.cc
	util/options.cc/Allow having different compression algorithms on different levels.

Summary:
The leveldb API is enhanced to support different compression algorithms at
different levels.

This adds the option min_level_to_compress to db_bench that specifies
the minimum level for which compression should be done when
compression is enabled. This can be used to disable compression for levels
0 and 1 which are likely to suffer from stalls because of the CPU load
for memtable flushes and (L0,L1) compaction.  Level 0 is special as it
gets frequent memtable flushes. Level 1 is special as it frequently
gets all:all file compactions between it and level 0. But all other levels
could be the same. For any level N where N > 1, the rate of sequential
IO for that level should be the same. The last level is the
exception because it might not be full and because files from it are
not read to compact with the next larger level.

The same amount of time will be spent doing compaction at any
level N excluding N=0, 1 or the last level. By this standard all
of those levels should use the same compression. The difference is that
the loss (using more disk space) from a faster compression algorithm
is less significant for N=2 than for N=3. So we might be willing to
trade disk space for faster write rates with no compression
for L0 and L1, snappy for L2, zlib for L3. Using a faster compression
algorithm for the mid levels also allows us to reclaim some cpu
without trading off much loss in disk space overhead.

Also note that little is to be gained by compressing levels 0 and 1. For
a 4-level tree they account for 10% of the data. For a 5-level tree they
account for 1% of the data.

With compression enabled:
* memtable flush rate is ~18MB/second
* (L0,L1) compaction rate is ~30MB/second

With compression enabled but min_level_to_compress=2
* memtable flush rate is ~320MB/second
* (L0,L1) compaction rate is ~560MB/second

This practicaly takes the same code from https://reviews.facebook.net/D6225
but makes the leveldb api more general purpose with a few additional
lines of code.

Test Plan: make check

Differential Revision: https://reviews.facebook.net/D6261/"
,,Rocksdb,"This is the mega-patch multi-threaded compaction
published in https://reviews.facebook.net/D5997.

Summary:
This patch allows compaction to occur in multiple background threads
concurrently.

If a manual compaction is issued, the system falls back to a
single-compaction-thread model. This is done to ensure correctess
and simplicity of code. When the manual compaction is finished,
the system resumes its concurrent-compaction mode automatically.

The updates to the manifest are done via group-commit approach.

Test Plan: run db_bench/The deletion of obsolete files should not occur very frequently.

Summary:
The method DeleteObsolete files is a very costly methind, especially
when the number of files in a system is large. It makes a list of
all live-files and then scans the directory to compute the diff.
By default, this method is executed after every compaction run.

This patch makes it such that DeleteObsolete files is never
invoked twice within a configured period.

Test Plan: run all unit tests

Reviewers: heyongqiang, MarkCallaghan

Reviewed By: MarkCallaghan

Differential Revision: https://reviews.facebook.net/D6045/"
,,Rocksdb,"Enhance db_bench to allow setting the number of levels in a database.

Summary: Enhance db_bench to allow setting the number of levels in a database.

Test Plan: run db_bench and look at LOG

Reviewers: heyongqiang, MarkCallaghan

Reviewed By: MarkCallaghan

CC: MarkCallaghan

Differential Revision: https://reviews.facebook.net/D6027/"
,,Rocksdb,"Add --stats_interval option to db_bench

Summary:
The option is zero by default and in that case reporting is unchanged.
By unchanged, the interval at which stats are reported is scaled after each
report and newline is not issued after each report so one line is rewritten.
When non-zero it specifies the constant interval (in operations) at which
statistics are reported and the stats include the rate per interval. This
makes it easier to determine whether QPS changes over the duration of the test.

Task ID: #

Blame Rev:

Test Plan:
run db_bench

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: dhruba

Reviewed By: dhruba

CC: heyongqiang

Differential Revision: https://reviews.facebook.net/D5817/"
,,Rocksdb,"Fix the bounds check for the --readwritepercent option

Summary:
see above

Task ID: #

Blame Rev:

Test Plan:
run db_bench with invalid value for option

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: dhruba

Reviewed By: dhruba

CC: heyongqiang

Differential Revision: https://reviews.facebook.net/D5823/"
,,Rocksdb,"add a global var leveldb::useMmapRead to enable mmap Summary:

Summary:
as subject. this can be used for benchmarking.
If we want it for some cases, we can do more changes to make this part of the option.

Test Plan: db_test

Reviewers: dhruba

CC: MarkCallaghan

Differential Revision: https://reviews.facebook.net/D5451/"
,,Rocksdb,"scan a long for FLAGS_cache_size to fix a compiler warning

Summary:
FLAGS_cache_size is a long, no need to scan %lld into a size_t
for it (which generates a compiler warning)

Test Plan: run db_bench

Reviewers: dhruba, heyongqiang

Reviewed By: heyongqiang

CC: heyongqiang

Differential Revision: https://reviews.facebook.net/D5427/"
,,Rocksdb,"Add --compression_type=X option with valid values: snappy (default) none bzip2 zlib

Summary:
This adds an option to db_bench to specify the compression algorithm to
use for LevelDB

Test Plan: ran db_bench

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D5421/"
,,Rocksdb,"Enable db_bench to specify block size.

Summary: Enable db_bench to specify block size.

Test Plan: compile and run

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5373/"
,,Rocksdb,"The ReadnRandomWriteRandom was always looping FLAGS_num of times.

Summary: If none of reads or writes are specified by user, then pick the FLAGS_NUM as the number of iterations in the ReadRandomWriteRandom test. If either reads or writes are defined, then use their maximum.

Test Plan: run benchmark

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5217/"
,,Rocksdb,"Benchmark with both reads and writes at the same time.

Summary:
This patch enables the db_bench benchmark to issue both random reads and random writes at the same time. This options can be trigged via
./db_bench --benchmarks=readrandomwriterandom

The default percetage of reads is 90.

One can change the percentage of reads by specifying the --readwritepercent.
./db_bench --benchmarks=readrandomwriterandom=50

This is a feature request from Jeffro asking for leveldb performance with a 90:10 read:write ratio.

Test Plan: run on test machine.

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5067/"
,,Rocksdb,"merge 1.5

Summary:

as subject

Test Plan:

db_test table_test

Reviewers: dhruba/Introduce a new method Env->Fsync() that issues fsync (instead of fdatasync).

Summary:
Introduce a new method Env->Fsync() that issues fsync (instead of fdatasync).
This is needed for data durability when running on ext3 filesystems.
Added options to the benchmark db_bench to generate performance numbers
with either fsync or fdatasync enabled.

Cleaned up Makefile to build leveldb_shell only when building the thrift
leveldb server.

Test Plan: build and run benchmark

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D4911/"
,,Rocksdb,"regression for trigger compaction logic

Summary: as subject

Test Plan: manually run db_bench confirmed

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D4809/"
,,Rocksdb,"add disable wal to db_bench

Summary:
as subject.

fillrandom   :       3.696 micros/op 270530 ops/sec;   29.9 MB/s

Test Plan: db_bench

Reviewers: dhruba

Differential Revision: https://reviews.facebook.net/D4767/"
,,Rocksdb,"add more options to db_ben

Summary: as subject

Test Plan: run db_bench with new options

Reviewers: dhruba

Differential Revision: https://reviews.facebook.net/D4677/"
,,Rocksdb,"fix filename_test

Summary: as subject

Test Plan: run filename_test

Reviewers: dhruba

Differential Revision: https://reviews.facebook.net/D4965/"
Threads Management,Threads Management,Rocksdb,"Add a readonly db

Summary: as subject

Test Plan: run db_bench readrandom

Reviewers: dhruba

Reviewed By: dhruba

CC: MarkCallaghan, emayanke, sheki

Differential Revision: https://reviews.facebook.net/D6495/Adds DB::GetNextCompaction and then uses that for rate limiting db_bench

Summary:
Adds a method that returns the score for the next level that most
needs compaction. That method is then used by db_bench to rate limit threads.
Threads are put to sleep at the end of each stats interval until the score
is less than the limit. The limit is set via the --rate_limit=$double option.
The specified value must be > 1.0. Also adds the option --stats_per_interval
to enable additional metrics reported every stats interval.

Task ID: #

Blame Rev:

Test Plan:
run db_bench

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D6243/Delete files outside the mutex.

Summary:
The compaction process deletes a large number of files. This takes
quite a bit of time and is best done outside the mutex lock.

Test Plan:

Reviewers:

CC:

Task ID: #

Blame Rev:/This is the mega-patch multi-threaded compaction
published in https://reviews.facebook.net/D5997.

Summary:
This patch allows compaction to occur in multiple background threads
concurrently.

If a manual compaction is issued, the system falls back to a
single-compaction-thread model. This is done to ensure correctess
and simplicity of code. When the manual compaction is finished,
the system resumes its concurrent-compaction mode automatically.

The updates to the manifest are done via group-commit approach.

Test Plan: run db_bench/The deletion of obsolete files should not occur very frequently.

Summary:
The method DeleteObsolete files is a very costly methind, especially
when the number of files in a system is large. It makes a list of
all live-files and then scans the directory to compute the diff.
By default, this method is executed after every compaction run.

This patch makes it such that DeleteObsolete files is never
invoked twice within a configured period.

Test Plan: run all unit tests

Reviewers: heyongqiang, MarkCallaghan

Reviewed By: MarkCallaghan

Differential Revision: https://reviews.facebook.net/D6045/The BackupAPI should also list the length of the manifest file.

Summary:
The GetLiveFiles() api lists the set of sst files and the current
MANIFEST file. But the database continues to append new data to the
MANIFEST file even when the application is backing it up to the
backup location. This means that the database-version that is
stored in the MANIFEST FILE in the backup location
does not correspond to the sst files returned by GetLiveFiles.

This API adds a new parameter to GetLiveFiles. This new parmeter
returns the current size of the MANIFEST file.

Test Plan: Unit test attached.

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5631/Ability to take a file-lvel snapshot from leveldb.

Summary:
A set of apis that allows an application to backup data from the
leveldb database based on a set of files.

Test Plan: unint test attached. more coming soon.

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5439/Do not spin in a tight loop attempting compactions if there is a compaction error

Summary: as subject. ported the change from google code leveldb 1.5

Test Plan: run db_test

Reviewers: dhruba

Differential Revision: https://reviews.facebook.net/D4839/adding a scribe logger in leveldb to log leveldb deploy stats

Summary:
as subject.

A new log is written to scribe via thrift client when a new db is opened and when there is
a compaction.

a new option var scribe_log_db_stats is added.

Test Plan: manually checked using command ""ptail -time 0 leveldb_deploy_stats""

Reviewers: dhruba

Differential Revision: https://reviews.facebook.net/D4659/"
Threads Management,Threads Management,Rocksdb,"Move filesize-based-sorting to outside the Mutex

Summary:
When a new version is created, we sort all the files at every
level based on their size. This is necessary because we want
to compact the largest file first. The sorting takes quite a
bit of CPU.

Moved the sorting code to be outside the mutex. Also, the
earlier code was sorting files at all levels but we do not
need to sort the highest-number level because those files
are never the cause of any compaction. To reduce sorting
costs, we sort only the first few files in each level
because it is likely that those are the only files in that
level that will be picked for compaction.

At steady state, I have seen that this patch increase
throughout from 1500 writes/sec to 1700 writes/sec at the
end of a 72 hour run. The cpu saving by not sorting the
last level was not distinctive in this test run because
there were only 100K files in the highest numbered level.
I expect the cpu saving to be significant when the number of
files is much higher.

This is mostly an early preview and not ready for rigorous review.

With this patch, the writs/sec is now bottlenecked not by the sorting code but by GetOverlappingInputs. I am working on a patch to optimize GetOverlappingInputs.

Test Plan: make check

Reviewers: MarkCallaghan, heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D6411/Fixed compilation error in previous merge.

Summary:
Fixed compilation error in previous merge.

Test Plan:

Reviewers:

CC:

Task ID: #

Blame Rev:/The method GetOverlappingInputs should use binary search.

Summary:
The method Version::GetOverlappingInputs used a sequential search
to map a kay-range to a set of files. But the files are arranged
in ascending order of key, so a biary search is more effective.

This patch implements Version::GetOverlappingInputsBinarySearch
that finds one file that corresponds to the specified key range
and then iterates backwards and forwards to find all overlapping
files.

This patch is critical for making compactions efficient, especially
when there are thousands of files in a single level.

I measured that 1000 iterations of TEST_MaxNextLevelOverlappingBytes
takes 16000 microseconds without this patch. With this patch, the
same method takes about 4600 microseconds.

Test Plan: Almost all unit tests in db_test uses this method to lookup keys.

Reviewers: heyongqiang

Reviewed By: heyongqiang

CC: MarkCallaghan, emayanke, sheki

Differential Revision: https://reviews.facebook.net/D6465/Add a tool to change number of levels

Summary: as subject.

Test Plan: manually test it, will add a testcase

Reviewers: dhruba, MarkCallaghan

Differential Revision: https://reviews.facebook.net/D6345/Merge branch 'master' into performance

Conflicts:
	db/db_bench.cc
	util/options.cc/add ""seek_compaction"" to log for better debug Summary:

Summary: as subject

Test Plan: compile

Reviewers: dhruba

Reviewed By: dhruba

CC: MarkCallaghan

Differential Revision: https://reviews.facebook.net/D6117/This is the mega-patch multi-threaded compaction
published in https://reviews.facebook.net/D5997.

Summary:
This patch allows compaction to occur in multiple background threads
concurrently.

If a manual compaction is issued, the system falls back to a
single-compaction-thread model. This is done to ensure correctess
and simplicity of code. When the manual compaction is finished,
the system resumes its concurrent-compaction mode automatically.

The updates to the manifest are done via group-commit approach.

Test Plan: run db_bench/The BackupAPI should also list the length of the manifest file.

Summary:
The GetLiveFiles() api lists the set of sst files and the current
MANIFEST file. But the database continues to append new data to the
MANIFEST file even when the application is backing it up to the
backup location. This means that the database-version that is
stored in the MANIFEST FILE in the backup location
does not correspond to the sst files returned by GetLiveFiles.

This API adds a new parameter to GetLiveFiles. This new parmeter
returns the current size of the MANIFEST file.

Test Plan: Unit test attached.

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5631/Clean up compiler warnings generated by -Wall option.

Summary:
Clean up compiler warnings generated by -Wall option.
make clean all OPT=-Wall

This is a pre-requisite before making a new release.

Test Plan: compile and run unit tests

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5019/merge 1.5

Summary:

as subject

Test Plan:

db_test table_test

Reviewers: dhruba/Utility to dump manifest contents.

Summary:
./manifest_dump --file=/tmp/dbbench/MANIFEST-000002

Output looks like

Test Plan: run on test directory created by dbbench

Reviewers: heyongqiang

Reviewed By: heyongqiang

CC: hustliubo

Differential Revision: https://reviews.facebook.net/D4743/"
Compiler Management,"Compiler Management,Thread management",Rocksdb,"Flush Data at object destruction if disableWal is used.

Summary:
Added a conditional flush in ~DBImpl to flush.
There is still a chance of writes not being persisted if there is a
crash (not a clean shutdown) before the DBImpl instance is destroyed.

Test Plan: modified db_test to meet the new expectations.

Reviewers: dhruba, heyongqiang

Differential Revision: https://reviews.facebook.net/D6519/Fix all warnings generated by -Wall option to the compiler.

Summary:
The default compilation process now uses ""-Wall"" to compile.
Fix all compilation error generated by gcc.

Test Plan: make all check

Reviewers: heyongqiang, emayanke, sheki

Reviewed By: heyongqiang

CC: MarkCallaghan

Differential Revision: https://reviews.facebook.net/D6525/Ability to invoke application hook for every key during compaction.

Summary:
There are certain use-cases where the application intends to
delete older keys aftre they have expired a certian time period.
One option for those applications is to periodically scan the
entire database and delete appropriate keys.

A better way is to allow the application to hook into the
compaction process. This patch allows the application to set
a method callback for every key that is being compacted. If
this method returns true, then the key is not preserved in
the output of the compaction.

Test Plan:
This is mostly to preview the proposed new public api.
Since it is a public api, please do due diligence on reviewing it.

I will be writing test cases for this api in mynext version of
this patch.

Reviewers: MarkCallaghan, heyongqiang

Reviewed By: heyongqiang

CC: sheki, adsharma

Differential Revision: https://reviews.facebook.net/D6285/Make compression options configurable. These include window-bits, level and strategy for ZlibCompression

Summary: Leveldb currently uses windowBits=-14 while using zlib compression.(It was earlier 15). This makes the setting configurable. Related changes here: https://reviews.facebook.net/D6105

Test Plan: make all check

Reviewers: dhruba, MarkCallaghan, sheki, heyongqiang

Differential Revision: https://reviews.facebook.net/D6393/fix test failure

Summary: as subject

Test Plan: db_test

Reviewers: dhruba, MarkCallaghan

Reviewed By: MarkCallaghan

Differential Revision: https://reviews.facebook.net/D6309/add a test case to make sure chaning num_levels will fail Summary:

Summary: as subject

Test Plan: db_test

Reviewers: dhruba, MarkCallaghan

Reviewed By: MarkCallaghan

Differential Revision: https://reviews.facebook.net/D6303/Allow having different compression algorithms on different levels.

Summary:
The leveldb API is enhanced to support different compression algorithms at
different levels.

This adds the option min_level_to_compress to db_bench that specifies
the minimum level for which compression should be done when
compression is enabled. This can be used to disable compression for levels
0 and 1 which are likely to suffer from stalls because of the CPU load
for memtable flushes and (L0,L1) compaction.  Level 0 is special as it
gets frequent memtable flushes. Level 1 is special as it frequently
gets all:all file compactions between it and level 0. But all other levels
could be the same. For any level N where N > 1, the rate of sequential
IO for that level should be the same. The last level is the
exception because it might not be full and because files from it are
not read to compact with the next larger level.

The same amount of time will be spent doing compaction at any
level N excluding N=0, 1 or the last level. By this standard all
of those levels should use the same compression. The difference is that
the loss (using more disk space) from a faster compression algorithm
is less significant for N=2 than for N=3. So we might be willing to
trade disk space for faster write rates with no compression
for L0 and L1, snappy for L2, zlib for L3. Using a faster compression
algorithm for the mid levels also allows us to reclaim some cpu
without trading off much loss in disk space overhead.

Also note that little is to be gained by compressing levels 0 and 1. For
a 4-level tree they account for 10% of the data. For a 5-level tree they
account for 1% of the data.

With compression enabled:
* memtable flush rate is ~18MB/second
* (L0,L1) compaction rate is ~30MB/second

With compression enabled but min_level_to_compress=2
* memtable flush rate is ~320MB/second
* (L0,L1) compaction rate is ~560MB/second

This practicaly takes the same code from https://reviews.facebook.net/D6225
but makes the leveldb api more general purpose with a few additional
lines of code.

Test Plan: make check

Differential Revision: https://reviews.facebook.net/D6261/Fix unit test failure caused by delaying deleting obsolete files.

Summary:
A previous commit 4c107587ed47af84633f8c61f65516a504d6cd98 introduced
the idea that some version updates might not delete obsolete files.
This means that if a unit test blindly counts the number of files
in the db directory it might not represent the true state of the database.

Use GetLiveFiles() insteads to count the number of live files in the database.

Test Plan:
make check/Trigger read compaction only if seeks to storage are incurred.

Summary:
In the current code, a Get() call can trigger compaction if it has to look at more than one file. This causes unnecessary compaction because looking at more than one file is a penalty only if the file is not yet in the cache. Also, th current code counts these files before the bloom filter check is applied.

This patch counts a 'seek' only if the file fails the bloom filter
check and has to read in data block(s) from the storage.

This patch also counts a 'seek' if a file is not present in the file-cache, because opening a file means that its index blocks need to be read into cache.

Test Plan: unit test attached. I will probably add one more unti tests.

Reviewers: heyongqiang

Reviewed By: heyongqiang

CC: MarkCallaghan

Differential Revision: https://reviews.facebook.net/D5709/The BackupAPI should also list the length of the manifest file.

Summary:
The GetLiveFiles() api lists the set of sst files and the current
MANIFEST file. But the database continues to append new data to the
MANIFEST file even when the application is backing it up to the
backup location. This means that the database-version that is
stored in the MANIFEST FILE in the backup location
does not correspond to the sst files returned by GetLiveFiles.

This API adds a new parameter to GetLiveFiles. This new parmeter
returns the current size of the MANIFEST file.

Test Plan: Unit test attached.

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5631/Ability to take a file-lvel snapshot from leveldb.

Summary:
A set of apis that allows an application to backup data from the
leveldb database based on a set of files.

Test Plan: unint test attached. more coming soon.

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5439/put log in a seperate dir

Summary: added a new option db_log_dir, which points the log dir. Inside that dir, in order to make log names unique, the log file name is prefixed with the leveldb data dir absolute path.

Test Plan: db_test

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D5205/merge 1.5

Summary:

as subject

Test Plan:

db_test table_test

Reviewers: dhruba/Do not spin in a tight loop attempting compactions if there is a compaction error

Summary: as subject. ported the change from google code leveldb 1.5

Test Plan: run db_test

Reviewers: dhruba

Differential Revision: https://reviews.facebook.net/D4839/Prevent concurrent multiple opens of leveldb database.

Summary:
The fcntl call cannot detect lock conflicts when invoked multiple times
from the same thread.
Use a static lockedFile Set to record the paths that are locked.
A lockfile request checks to see if htis filename already exists in
lockedFiles, if so, then it triggers an error. Otherwise, it inserts
the filename in the lockedFiles Set.
A unlock file request verifies that the filename is in the lockedFiles
set and removes it from lockedFiles set.

Test Plan: unit test attached

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D4755/use ts as suffix for LOG.old files

Summary: as subject and only maintain 10 log files.

Test Plan: new test in db_test

Reviewers: dhruba

Differential Revision: https://reviews.facebook.net/D4731/"
,"Memory Management, API Management,Thread management",Rocksdb,"Move filesize-based-sorting to outside the Mutex

Summary:
When a new version is created, we sort all the files at every
level based on their size. This is necessary because we want
to compact the largest file first. The sorting takes quite a
bit of CPU.

Moved the sorting code to be outside the mutex. Also, the
earlier code was sorting files at all levels but we do not
need to sort the highest-number level because those files
are never the cause of any compaction. To reduce sorting
costs, we sort only the first few files in each level
because it is likely that those are the only files in that
level that will be picked for compaction.

At steady state, I have seen that this patch increase
throughout from 1500 writes/sec to 1700 writes/sec at the
end of a 72 hour run. The cpu saving by not sorting the
last level was not distinctive in this test run because
there were only 100K files in the highest numbered level.
I expect the cpu saving to be significant when the number of
files is much higher.

This is mostly an early preview and not ready for rigorous review.

With this patch, the writs/sec is now bottlenecked not by the sorting code but by GetOverlappingInputs. I am working on a patch to optimize GetOverlappingInputs.

Test Plan: make check

Reviewers: MarkCallaghan, heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D6411/Merge branch 'master' into performance

Conflicts:
	db/db_impl.cc
	db/version_set.cc
	util/options.cc/Avoid doing a exhaustive search when looking for overlapping files.

Summary:
The Version::GetOverlappingInputs() is called multiple times in
the compaction code path. Eack invocation does a binary search
for overlapping files in the specified key range.
This patch remembers the offset of an overlapped file when
GetOverlappingInputs() is called the first time within
a compaction run. Suceeding calls to GetOverlappingInputs()
uses the remembered index to avoid the binary search.

I measured that 1000 iterations of GetOverlappingInputs
takes around 4500 microseconds without this patch. If I use
this patch with the hint on every invocation, then 1000
iterations take about 3900 microsecond.

Test Plan: make check OPT=-g

Reviewers: heyongqiang

Reviewed By: heyongqiang

CC: MarkCallaghan, emayanke, sheki

Differential Revision: https://reviews.facebook.net/D6513/The method GetOverlappingInputs should use binary search.

Summary:
The method Version::GetOverlappingInputs used a sequential search
to map a kay-range to a set of files. But the files are arranged
in ascending order of key, so a biary search is more effective.

This patch implements Version::GetOverlappingInputsBinarySearch
that finds one file that corresponds to the specified key range
and then iterates backwards and forwards to find all overlapping
files.

This patch is critical for making compactions efficient, especially
when there are thousands of files in a single level.

I measured that 1000 iterations of TEST_MaxNextLevelOverlappingBytes
takes 16000 microseconds without this patch. With this patch, the
same method takes about 4600 microseconds.

Test Plan: Almost all unit tests in db_test uses this method to lookup keys.

Reviewers: heyongqiang

Reviewed By: heyongqiang

CC: MarkCallaghan, emayanke, sheki

Differential Revision: https://reviews.facebook.net/D6465/fix complie error

Summary:

as subject

Test Plan:n/a/Add a tool to change number of levels

Summary: as subject.

Test Plan: manually test it, will add a testcase

Reviewers: dhruba, MarkCallaghan

Differential Revision: https://reviews.facebook.net/D6345/Greedy algorithm for picking files to compact.

Summary:
It is best if we pick the largest file to compact in a level.
This reduces the write amplification factor for compactions.
Each level has an auxiliary data structure called files_by_size_
that sorts all files by their size. This data structure is
updated when a new version is created.

Test Plan: make check

Differential Revision: https://reviews.facebook.net/D6195/Merge branch 'master' into performance

Conflicts:
	db/db_impl.cc
	db/db_impl.h
	db/version_set.cc/add ""seek_compaction"" to log for better debug Summary:

Summary: as subject

Test Plan: compile

Reviewers: dhruba

Reviewed By: dhruba

CC: MarkCallaghan

Differential Revision: https://reviews.facebook.net/D6117/This is the mega-patch multi-threaded compaction
published in https://reviews.facebook.net/D5997.

Summary:
This patch allows compaction to occur in multiple background threads
concurrently.

If a manual compaction is issued, the system falls back to a
single-compaction-thread model. This is done to ensure correctess
and simplicity of code. When the manual compaction is finished,
the system resumes its concurrent-compaction mode automatically.

The updates to the manifest are done via group-commit approach.

Test Plan: run db_bench/If ReadCompaction is switched off, then it is better to not even submit background compaction jobs.

Summary:
If ReadCompaction is switched off, then it is better to not even
submit background compaction jobs. I see about 3% increase in
read-throughput on a pure memory database.

Test Plan: run db_bench

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5673/The BackupAPI should also list the length of the manifest file.

Summary:
The GetLiveFiles() api lists the set of sst files and the current
MANIFEST file. But the database continues to append new data to the
MANIFEST file even when the application is backing it up to the
backup location. This means that the database-version that is
stored in the MANIFEST FILE in the backup location
does not correspond to the sst files returned by GetLiveFiles.

This API adds a new parameter to GetLiveFiles. This new parmeter
returns the current size of the MANIFEST file.

Test Plan: Unit test attached.

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5631/Ability to take a file-lvel snapshot from leveldb.

Summary:
A set of apis that allows an application to backup data from the
leveldb database based on a set of files.

Test Plan: unint test attached. more coming soon.

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5439/Clean up compiler warnings generated by -Wall option.

Summary:
Clean up compiler warnings generated by -Wall option.
make clean all OPT=-Wall

This is a pre-requisite before making a new release.

Test Plan: compile and run unit tests

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5019/fix db_test error with scribe logger turned on

Summary: as subject

Test Plan: db_test

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D4929/Introduce a new method Env->Fsync() that issues fsync (instead of fdatasync).

Summary:
Introduce a new method Env->Fsync() that issues fsync (instead of fdatasync).
This is needed for data durability when running on ext3 filesystems.
Added options to the benchmark db_bench to generate performance numbers
with either fsync or fdatasync enabled.

Cleaned up Makefile to build leveldb_shell only when building the thrift
leveldb server.

Test Plan: build and run benchmark

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D4911/add more logs

Summary:

as subject

Test Plan:db_test

Reviewers: dhruba/regression for trigger compaction logic

Summary: as subject

Test Plan: manually run db_bench confirmed

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D4809/adding a scribe logger in leveldb to log leveldb deploy stats

Summary:
as subject.

A new log is written to scribe via thrift client when a new db is opened and when there is
a compaction.

a new option var scribe_log_db_stats is added.

Test Plan: manually checked using command ""ptail -time 0 leveldb_deploy_stats""

Reviewers: dhruba

Differential Revision: https://reviews.facebook.net/D4659/Utility to dump manifest contents.

Summary:
./manifest_dump --file=/tmp/dbbench/MANIFEST-000002

Output looks like

manifest_file_number 30 next_file_number 31 last_sequence 388082 log_number 28  prev_log_number 0
--- level 0 ---
--- level 1 ---
--- level 2 ---
 5:3244155['0000000000000000' @ 1 : 1 .. '0000000000028220' @ 28221 : 1]
 7:3244177['0000000000028221' @ 28222 : 1 .. '0000000000056441' @ 56442 : 1]
 9:3244156['0000000000056442' @ 56443 : 1 .. '0000000000084662' @ 84663 : 1]
 11:3244178['0000000000084663' @ 84664 : 1 .. '0000000000112883' @ 112884 : 1]
 13:3244158['0000000000112884' @ 112885 : 1 .. '0000000000141104' @ 141105 : 1]
 15:3244176['0000000000141105' @ 141106 : 1 .. '0000000000169325' @ 169326 : 1]
 17:3244156['0000000000169326' @ 169327 : 1 .. '0000000000197546' @ 197547 : 1]
 19:3244178['0000000000197547' @ 197548 : 1 .. '0000000000225767' @ 225768 : 1]
 21:3244155['0000000000225768' @ 225769 : 1 .. '0000000000253988' @ 253989 : 1]
 23:3244179['0000000000253989' @ 253990 : 1 .. '0000000000282209' @ 282210 : 1]
 25:3244157['0000000000282210' @ 282211 : 1 .. '0000000000310430' @ 310431 : 1]
 27:3244176['0000000000310431' @ 310432 : 1 .. '0000000000338651' @ 338652 : 1]
 29:3244156['0000000000338652' @ 338653 : 1 .. '0000000000366872' @ 366873 : 1]
--- level 3 ---
--- level 4 ---
--- level 5 ---
--- level 6 ---

Test Plan: run on test directory created by dbbench

Reviewers: heyongqiang

Reviewed By: heyongqiang

CC: hustliubo

Differential Revision: https://reviews.facebook.net/D4743/add compaction log Summary:

Summary:
add compaction summary to log

log looks like:

2012/08/17-18:18:32.557334 7fdcaa2bb700 Compaction summary: Base level 0, input file:[11 9 7 ],[]

Test Plan: tested via db_test

Reviewers: dhruba

Differential Revision: https://reviews.facebook.net/D4749/"
,,Rocksdb,"put log in a seperate dir

Summary: added a new option db_log_dir, which points the log dir. Inside that dir, in order to make log names unique, the log file name is prefixed with the leveldb data dir absolute path.

Test Plan: db_test

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D5205/use ts as suffix for LOG.old files

Summary: as subject and only maintain 10 log files.

Test Plan: new test in db_test

Reviewers: dhruba

Differential Revision: https://reviews.facebook.net/D4731/"
,,Rocksdb,"Compile leveldb with gcc 4.7.1

Test Plan: run unit tests

Reviewers: heyongqiang

Reviewed By: heyongqiang

Differential Revision: https://reviews.facebook.net/D5163/merge 1.5

Summary:

as subject

Test Plan:

db_test table_test

Reviewers: dhruba/"
,,Rocksdb,"manifest_dump: Add --hex=1 option

Summary: Without this option, manifest_dump does not print binary keys for files in a human-readable way.

Reviewers: dhruba, sheki, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7425/"
,,Rocksdb,"sst_dump: Add --output_hex option and output the same format as ldb dump

Summary: Now sst_dump has the same option --output_hex as ""ldb dump"" and also share the same output format.  So we can do ""sst_dump ... | ldb load ..."" for an experiment.

Reviewers: dhruba, sheki, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7587/"
,,Rocksdb,"Enhanced ldb to support data access commands

Summary: Added put/get/scan/batchput/delete/approxsize

Test Plan: Added pyunit script to test the newly added commands

Reviewers: chip, leveldb

Reviewed By: chip

CC: zshao, emayanke

Differential Revision: https://reviews.facebook.net/D7947/"
,,Rocksdb,"Fixed issues Valgrind found.

Summary:
Found issues with `db_test` and `db_stress` when running valgrind.

`DBImpl` had an issue where if an compaction failed then it will use the uninitialised file size of an output file is used. This manifested as the final call to output to the log in `DoCompactionWork()` branching on uninitialized memory (all the way down in printf's innards).

Test Plan:
Ran `valgrind --track_origins=yes ./db_test` and `valgrind ./db_stress` to see if issues disappeared.

Ran `make check` to see if there were no regressions.

Reviewers: vamsi, dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8001/enhance dbstress to simulate hard crash

Summary:
dbstress has an option to reopen the database. Make it such that the
previous handle is not closed before we reopen, this simulates a
situation similar to a process crash.

Added new api to DMImpl to remove the lock file.

Test Plan: run db_stress

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D6777/"
,,Rocksdb,"Use fallocate to prevent excessive allocation of sst files and logs

Summary:
On some filesystems, pre-allocation can be a considerable
amount of space.  xfs in our production environment pre-allocates by
1GB, for instance.  By using fallocate to inform the kernel of our
expected file sizes, we eliminate this wasteage (that isn't recovered
until the file is closed which, in the case of LOG files, can be a
considerable amount of time).

Test Plan:
created an xfs loopback filesystem, mounted with
allocsize=4M, and ran db_stress.  LOG file without this change was 4M,
and with it it was 128k then grew to normal size.

Reviewers: dhruba

Reviewed By: dhruba

CC: adsharma, leveldb

Differential Revision: https://reviews.facebook.net/D7953/"
,,Rocksdb,"Fix a number of object lifetime/ownership issues

Summary:
Replace manual memory management with std::unique_ptr in a
number of places; not exhaustive, but this fixes a few leaks with file
handles as well as clarifies semantics of the ownership of file handles
with log classes.

Test Plan: db_stress, make check

Reviewers: dhruba

Reviewed By: dhruba

CC: zshao, leveldb, heyongqiang

Differential Revision: https://reviews.facebook.net/D8043/"
,,Rocksdb,"Fixed cache key for block cache

Summary:
Added function to `RandomAccessFile` to generate an unique ID for that file. Currently only `PosixRandomAccessFile` has this behaviour implemented and only on Linux.

Changed how key is generated in `Table::BlockReader`.

Added tests to check whether the unique ID is stable, unique and not a prefix of another unique ID. Added tests to see that `Table` uses the cache more efficiently.

Test Plan: make check

Reviewers: chip, vamsi, dhruba

Reviewed By: chip

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8145/"
,,Rocksdb,"Move WAL files to archive directory, instead of deleting.

Summary:
Create a directory ""archive"" in the DB directory.
During DeleteObsolteFiles move the WAL files (*.log) to the Archive directory,
instead of deleting.

Test Plan: Created a DB using DB_Bench. Reopened it. Checked if files move.

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D6975/"
Database Management,"Database Management, compression tasks",Rocksdb,"Enhanced ldb to support data access commands

Summary: Added put/get/scan/batchput/delete/approxsize

Test Plan: Added pyunit script to test the newly added commands

Reviewers: chip, leveldb

Reviewed By: chip

CC: zshao, emayanke

Differential Revision: https://reviews.facebook.net/D7947/ldb: Add command ""ldb query"" to support random read from the database

Summary: The queries will come from stdin.  One key per line.  The output will be in stdout, in the format of ""<key> ==> <value>"" if found, or ""<key>"" if not found.  ""--hex"" uses HEX-encoded keys and values in both input and output.

Test Plan: ldb query --db=leveldb_db --hex

Reviewers: dhruba, emayanke, sheki

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7617/ldb: add ""ldb load"" command

Summary: This command accepts key-value pairs from stdin with the same format of ""ldb dump"" command.  This allows us to try out different compression algorithms/block sizes easily.

Test Plan: dump, load, dump, verify the data is the same.

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7443/Fix ldb segfault and use static libsnappy for all builds

Summary:
Link statically against snappy, using the gvfs one for facebook
environments, and the bundled one otherwise.

In addition, fix a few minor segfaults in ldb when it couldn't open the
database, and update .gitignore to include a few other build artifacts.

Test Plan: make check

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D6855/LDB can read WAL.

Summary:
Add option to read WAL and print a summary for each record.
facebook task => #1885013

E.G. Output :
./ldb dump_wal --walfile=/tmp/leveldbtest-5907/dbbench/026122.log --header
Sequence,Count,ByteSize
49981,1,100033
49981,1,100033
49982,1,100033
49981,1,100033
49982,1,100033
49983,1,100033
49981,1,100033
49982,1,100033
49983,1,100033
49984,1,100033
49981,1,100033
49982,1,100033

Test Plan:
Works run
./ldb read_wal --wal-file=/tmp/leveldbtest-5907/dbbench/000078.log --header

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: emayanke, leveldb, zshao

Differential Revision: https://reviews.facebook.net/D6675/"
Database Management,"Database Management, compression tasks",Rocksdb,"Add zlib to our builds and tweak histogram output

Summary:
$SUBJECT -- cosmetic fix for histograms, print P75/P99, and
make sure zlib is enabled for our command line tools.

Test Plan: compile, test db_bench with --compression_type=zlib

Reviewers: heyongqiang

Reviewed By: heyongqiang

CC: adsharma, leveldb

Differential Revision: https://reviews.facebook.net/D8445/Performant util/histogram.

Summary:
Earlier way to record in histogram=>
Linear search BucketLimit array to find the bucket and increment the
counter
Current way to record in histogram=>
Store a HistMap statically which points the buckets of each value in the
range [kFirstValue, kLastValue);

In the proccess use vectors instead of array's and refactor some code to
HistogramHelper class.

Test Plan:
run db_bench with histogram=1 and see a histogram being
printed.

Reviewers: dhruba, chip, heyongqiang

Reviewed By: chip

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8265/"
,,Rocksdb,"Use CRC32 ss42 instruction. Load it dynamically.

Test Plan: make all check

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, zshao

Differential Revision: https://reviews.facebook.net/D7503/"
,,Rocksdb," Added methods to write small ints to bit streams.

Summary: Added BitStreamPutInt() and BitStreamGetInt() which take a stream of chars and can write integers of arbitrary bit sizes to that stream at arbitrary positions. There are also convenience versions of these functions that take std::strings and leveldb::Slices.

Test Plan: make check

Reviewers: sheki, vamsi, dhruba, emayanke

Reviewed By: vamsi

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7071/"
,,Rocksdb,"Fix a number of object lifetime/ownership issues

Summary:
Replace manual memory management with std::unique_ptr in a
number of places; not exhaustive, but this fixes a few leaks with file
handles as well as clarifies semantics of the ownership of file handles
with log classes.

Test Plan: db_stress, make check

Reviewers: dhruba

Reviewed By: dhruba

CC: zshao, leveldb, heyongqiang

Differential Revision: https://reviews.facebook.net/D8043/Fixed memory leak in ShardedLRUCache

Summary: `~ShardedLRUCache()` was empty despite `init()` allocating memory on the heap. Fixed the leak by freeing memory allocated by `init()`.

Test Plan:
make check

Ran valgrind on db_test before and after patch and saw leaked memory went down

Reviewers: vamsi, dhruba, emayanke, sheki

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7791/"
,,Rocksdb,"Performant util/histogram.

Summary:
Earlier way to record in histogram=>
Linear search BucketLimit array to find the bucket and increment the
counter
Current way to record in histogram=>
Store a HistMap statically which points the buckets of each value in the
range [kFirstValue, kLastValue);

In the proccess use vectors instead of array's and refactor some code to
HistogramHelper class.

Test Plan:
run db_bench with histogram=1 and see a histogram being
printed.

Reviewers: dhruba, chip, heyongqiang

Reviewed By: chip

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8265/"
Restructuring the code,Restructuring the code,Rocksdb,"Enhanced ldb to support data access commands

Summary: Added put/get/scan/batchput/delete/approxsize

Test Plan: Added pyunit script to test the newly added commands

Reviewers: chip, leveldb

Reviewed By: chip

CC: zshao, emayanke

Differential Revision: https://reviews.facebook.net/D7947/Fix a number of object lifetime/ownership issues

Summary:
Replace manual memory management with std::unique_ptr in a
number of places; not exhaustive, but this fixes a few leaks with file
handles as well as clarifies semantics of the ownership of file handles
with log classes.

Test Plan: db_stress, make check

Reviewers: dhruba

Reviewed By: dhruba

CC: zshao, leveldb, heyongqiang

Differential Revision: https://reviews.facebook.net/D8043/ldb: Add command ""ldb query"" to support random read from the database

Summary: The queries will come from stdin.  One key per line.  The output will be in stdout, in the format of ""<key> ==> <value>"" if found, or ""<key>"" if not found.  ""--hex"" uses HEX-encoded keys and values in both input and output.

Test Plan: ldb query --db=leveldb_db --hex

Reviewers: dhruba, emayanke, sheki

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7617/ldb: add ""ldb load"" command

Summary: This command accepts key-value pairs from stdin with the same format of ""ldb dump"" command.  This allows us to try out different compression algorithms/block sizes easily.

Test Plan: dump, load, dump, verify the data is the same.

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7443/ldb: Add compression and bloom filter options.

Summary:
Added the following two options:
[--bloom_bits=<int,e.g.:14>]
[--compression_type=<no|snappy|zlib|bzip2>]

These options will be used when ldb opens the leveldb database.

Test Plan: Tried by hand for both success and failure cases. We do need a test framework.

Reviewers: dhruba, emayanke, sheki

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7197/Fix LDB dumpwal to print the messages as in the file.

Summary:
StringStream.clear() does not clear the stream. It sets some flags.
Who knew? Fixing that is not printing the stuff again and again.

Test Plan: ran it on a local db

Reviewers: dhruba, emayanke

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D6795/LDB can read WAL.

Summary:
Add option to read WAL and print a summary for each record.
facebook task => #1885013

Test Plan:
Works run
./ldb read_wal --wal-file=/tmp/leveldbtest-5907/dbbench/000078.log --header

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: emayanke, leveldb, zshao

Differential Revision: https://reviews.facebook.net/D6675/LDB can read WAL.

Summary:
Add option to read WAL and print a summary for each record.
facebook task => #1885013

Test Plan:
Works run
./ldb read_wal --wal-file=/tmp/leveldbtest-5907/dbbench/000078.log --header

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: emayanke, leveldb, zshao

Differential Revision: https://reviews.facebook.net/D6675/"
Restructuring the code,Restructuring the code,Rocksdb,"Allow the logs to be purged by TTL.

Summary:
* Add a SplitByTTLLogger to enable this feature. In this diff I implemented generalized AutoSplitLoggerBase class to simplify the
development of such classes.
* Refactor the existing AutoSplitLogger and fix several bugs.

Test Plan:
* Added a unit tests for different types of ""auto splitable"" loggers individually.
* Tested the composited logger which allows the log files to be splitted by both TTL and log size.

Reviewers: heyongqiang, dhruba

Reviewed By: heyongqiang

CC: zshao, leveldb

Differential Revision: https://reviews.facebook.net/D8037/Fix poor error on num_levels mismatch and few other minor improvements

Summary:
Previously, if you opened a db with num_levels set lower than
the database, you received the unhelpful message ""Corruption:
VersionEdit: new-file entry.""  Now you get a more verbose message
describing the issue.

Also, fix handling of compression_levels (both the run-over-the-end
issue and the memory management of it).

Lastly, unique_ptr'ify a couple of minor calls.

Test Plan: make check

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8151/Use fallocate to prevent excessive allocation of sst files and logs

Summary:
On some filesystems, pre-allocation can be a considerable
amount of space.  xfs in our production environment pre-allocates by
1GB, for instance.  By using fallocate to inform the kernel of our
expected file sizes, we eliminate this wasteage (that isn't recovered
until the file is closed which, in the case of LOG files, can be a
considerable amount of time).

Test Plan:
created an xfs loopback filesystem, mounted with
allocsize=4M, and ran db_stress.  LOG file without this change was 4M,
and with it it was 128k then grew to normal size.

Reviewers: dhruba

Reviewed By: dhruba

CC: adsharma, leveldb

Differential Revision: https://reviews.facebook.net/D7953/rollover manifest file.

Summary:
Check in LogAndApply if the file size is more than the limit set in
Options.
Things to consider : will this be expensive?

Test Plan: make all check. Inputs on a new unit test?

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7701/Move WAL files to archive directory, instead of deleting.

Summary:
Create a directory ""archive"" in the DB directory.
During DeleteObsolteFiles move the WAL files (*.log) to the Archive directory,
instead of deleting.

Test Plan: Created a DB using DB_Bench. Reopened it. Checked if files move.

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D6975/Support taking a configurable number of  files from the same level to compact in a single compaction run.

Summary:
The compaction process takes some files from LevelK and
merges it into LevelK+1. The number of files it picks from
LevelK was capped such a way that the total amount of
data picked does not exceed the maxfilesize of that level.
This essentially meant that only one file from LevelK
is picked for a single compaction.

For bulkloads, we would like to take many many file from
LevelK and compact them using a single compaction run.

This patch introduces a option called the 'source_compaction_factor'
(similar to expanded_compaction_factor). It is a multiplier
that is multiplied by the maxfilesize of that level to arrive
at the limit that is used to throttle the number of source
files from LevelK.  For bulk loads, set source_compaction_factor
to a very high number so that multiple files from the same
level are picked for compaction in a single compaction.

The default value of source_compaction_factor is 1, so that
we can keep backward compatibilty with existing compaction semantics.

Test Plan: make clean check

Reviewers: emayanke, sheki

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D6867/Support to disable background compactions on a database.

Summary:
This option is needed for fast bulk uploads. The goal is to load
all the data into files in L0 without any interference from
background compactions.

Test Plan: make clean check

Reviewers: sheki

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D6849/"
,,Rocksdb,"Fixed cache key for block cache

Summary:
Added function to `RandomAccessFile` to generate an unique ID for that file. Currently only `PosixRandomAccessFile` has this behaviour implemented and only on Linux.

Changed how key is generated in `Table::BlockReader`.

Added tests to check whether the unique ID is stable, unique and not a prefix of another unique ID. Added tests to see that `Table` uses the cache more efficiently.

Test Plan: make check

Reviewers: chip, vamsi, dhruba

Reviewed By: chip

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8145/Use fallocate to prevent excessive allocation of sst files and logs

Summary:
On some filesystems, pre-allocation can be a considerable
amount of space.  xfs in our production environment pre-allocates by
1GB, for instance.  By using fallocate to inform the kernel of our
expected file sizes, we eliminate this wasteage (that isn't recovered
until the file is closed which, in the case of LOG files, can be a
considerable amount of time).

Test Plan:
created an xfs loopback filesystem, mounted with
allocsize=4M, and ran db_stress.  LOG file without this change was 4M,
and with it it was 128k then grew to normal size.

Reviewers: dhruba

Reviewed By: dhruba

CC: adsharma, leveldb

Differential Revision: https://reviews.facebook.net/D7953/Fix a number of object lifetime/ownership issues

Summary:
Replace manual memory management with std::unique_ptr in a
number of places; not exhaustive, but this fixes a few leaks with file
handles as well as clarifies semantics of the ownership of file handles
with log classes.

Test Plan: db_stress, make check

Reviewers: dhruba

Reviewed By: dhruba

CC: zshao, leveldb, heyongqiang

Differential Revision: https://reviews.facebook.net/D8043/Move WAL files to archive directory, instead of deleting.

Summary:
Create a directory ""archive"" in the DB directory.
During DeleteObsolteFiles move the WAL files (*.log) to the Archive directory,
instead of deleting.

Test Plan: Created a DB using DB_Bench. Reopened it. Checked if files move.

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D6975/"
API Management,API Management,Rocksdb,"GetUpdatesSince API to enable replication.

Summary:
How it works:
* GetUpdatesSince takes a SequenceNumber.
* A LogFile with the first SequenceNumber nearest and lesser than the requested Sequence Number is found.
* Seek in the logFile till the requested SeqNumber is found.
* Return an iterator which contains logic to return record's one by one.

Test Plan:
* Test case included to check the good code path.
* Will update with more test-cases.
* Feedback required on test-cases.

Reviewers: dhruba, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7119/"
,,Rocksdb,"Enhancements to rocksdb for better support for replication.

Summary:
1. The OpenForReadOnly() call should not lock the db. This is useful
so that multiple processes can open the same database concurrently
for reading.
2. GetUpdatesSince should not error out if the archive directory
does not exist.
3. A new constructor for WriteBatch that can takes a serialized
string as a parameter of the constructor.

Test Plan: make clean check

Reviewers: sheki

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7449/"
,,Rocksdb,"Improved CompactionFilter api:  pass in a opaque argument to CompactionFilter invocation.

Summary:
There are applications that operate on multiple leveldb instances.
These applications will like to pass in an opaque type for each
leveldb instance and this type should be passed back to the application
with every invocation of the CompactionFilter api.

Test Plan: Enehanced unit test for opaque parameter to CompactionFilter.

Reviewers: heyongqiang

Reviewed By: heyongqiang

CC: MarkCallaghan, sheki, emayanke

Differential Revision: https://reviews.facebook.net/D6711/"
,,Rocksdb,"Use a priority queue to merge files.

Summary:
Use a std::priority_queue in merger.cc instead of doing a o(n) search
every time.
Currently only the ForwardIteration uses a Priority Queue.

Test Plan: make all check

Reviewers: dhruba

Reviewed By: dhruba

CC: emayanke, zshao

Differential Revision: https://reviews.facebook.net/D7629/"
compression tasks,compression tasks,Rocksdb,"sst_dump: Error message should include the case that compression algorithms are not supported.

Summary: It took me almost a day to debug this. :(  Although I got to learn the file format as a by-product, this time could be saved if we have better error messages.

Test Plan: gmake clean all; sst_dump --hex --file=000005.sst

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7551/"
,,Rocksdb,"Fixed cache key for block cache

Summary:
Added function to `RandomAccessFile` to generate an unique ID for that file. Currently only `PosixRandomAccessFile` has this behaviour implemented and only on Linux.

Changed how key is generated in `Table::BlockReader`.

Added tests to check whether the unique ID is stable, unique and not a prefix of another unique ID. Added tests to see that `Table` uses the cache more efficiently.

Test Plan: make check

Reviewers: chip, vamsi, dhruba

Reviewed By: chip

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8145/Fix a number of object lifetime/ownership issues

Summary:
Replace manual memory management with std::unique_ptr in a
number of places; not exhaustive, but this fixes a few leaks with file
handles as well as clarifies semantics of the ownership of file handles
with log classes.

Test Plan: db_stress, make check

Reviewers: dhruba

Reviewed By: dhruba

CC: zshao, leveldb, heyongqiang

Differential Revision: https://reviews.facebook.net/D8043/"
,,Rocksdb,"Fix a number of object lifetime/ownership issues

Summary:
Replace manual memory management with std::unique_ptr in a
number of places; not exhaustive, but this fixes a few leaks with file
handles as well as clarifies semantics of the ownership of file handles
with log classes.

Test Plan: db_stress, make check

Reviewers: dhruba

Reviewed By: dhruba

CC: zshao, leveldb, heyongqiang

Differential Revision: https://reviews.facebook.net/D8043/Move WAL files to archive directory, instead of deleting.

Summary:
Create a directory ""archive"" in the DB directory.
During DeleteObsolteFiles move the WAL files (*.log) to the Archive directory,
instead of deleting.

Test Plan: Created a DB using DB_Bench. Reopened it. Checked if files move.

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D6975/"
Restructuring the code,"Restructuring the code, Compression Tasks, Thread management",Rocksdb,"Allow the logs to be purged by TTL.

Summary:
* Add a SplitByTTLLogger to enable this feature. In this diff I implemented generalized AutoSplitLoggerBase class to simplify the
development of such classes.
* Refactor the existing AutoSplitLogger and fix several bugs.

Test Plan:
* Added a unit tests for different types of ""auto splitable"" loggers individually.
* Tested the composited logger which allows the log files to be splitted by both TTL and log size.

Reviewers: heyongqiang, dhruba

Reviewed By: heyongqiang

CC: zshao, leveldb

Differential Revision: https://reviews.facebook.net/D8037/Fix poor error on num_levels mismatch and few other minor improvements

Summary:
Previously, if you opened a db with num_levels set lower than
the database, you received the unhelpful message ""Corruption:
VersionEdit: new-file entry.""  Now you get a more verbose message
describing the issue.

Also, fix handling of compression_levels (both the run-over-the-end
issue and the memory management of it).

Lastly, unique_ptr'ify a couple of minor calls.

Test Plan: make check

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8151/Stop continually re-creating build_version.c

Summary:
We continually rebuilt build_version.c because we put the
current date into it, but that's what __DATE__ already is.  This makes
builds faster.

This also fixes an issue with 'make clean FOO' not working properly.

Also tweak the build rules to be more consistent, always have warnings,
and add a 'make release' rule to handle flags for release builds.

Test Plan: make, make clean

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D8139/Use fallocate to prevent excessive allocation of sst files and logs

Summary:
On some filesystems, pre-allocation can be a considerable
amount of space.  xfs in our production environment pre-allocates by
1GB, for instance.  By using fallocate to inform the kernel of our
expected file sizes, we eliminate this wasteage (that isn't recovered
until the file is closed which, in the case of LOG files, can be a
considerable amount of time).

Test Plan:
created an xfs loopback filesystem, mounted with
allocsize=4M, and ran db_stress.  LOG file without this change was 4M,
and with it it was 128k then grew to normal size.

Reviewers: dhruba

Reviewed By: dhruba

CC: adsharma, leveldb

Differential Revision: https://reviews.facebook.net/D7953/Fix a number of object lifetime/ownership issues

Summary:
Replace manual memory management with std::unique_ptr in a
number of places; not exhaustive, but this fixes a few leaks with file
handles as well as clarifies semantics of the ownership of file handles
with log classes.

Test Plan: db_stress, make check

Reviewers: dhruba

Reviewed By: dhruba

CC: zshao, leveldb, heyongqiang

Differential Revision: https://reviews.facebook.net/D8043/Add counters to count gets and writes

Summary: Add Tickers to count Write's and Get's

Test Plan: make check

Reviewers: dhruba, chip

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7977/Fixed issues Valgrind found.

Summary:
Found issues with `db_test` and `db_stress` when running valgrind.

`DBImpl` had an issue where if an compaction failed then it will use the uninitialised file size of an output file is used. This manifested as the final call to output to the log in `DoCompactionWork()` branching on uninitialized memory (all the way down in printf's innards).

Test Plan:
Ran `valgrind --track_origins=yes ./db_test` and `valgrind ./db_stress` to see if issues disappeared.

Ran `make check` to see if there were no regressions.

Reviewers: vamsi, dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8001/Fixing some issues Valgrind found

Summary: Found some issues running Valgrind on `db_test` (there are still some outstanding ones) and fixed them.

Test Plan:
make check

ran `valgrind ./db_test` and saw that errors no longer occur

Reviewers: dhruba, vamsi, emayanke, sheki

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7803/Enhance ReadOnly mode to process the all committed transactions.

Summary:
Leveldb has an api OpenForReadOnly() that opens the database
in readonly mode. This call had an option to not process the
transaction log.  This patch removes this option and always
processes all transactions that had been committed. It has
been done in such a way that it does not create/write to
any new files in the process. The invariant of ""no-writes""
to the leveldb data directory is still true.

This enhancement allows multiple threads to open the same database
in readonly mode and access all trancations that were committed right
upto the OpenForReadOnly call.

I changed the public API to match the new semantics because
there are no users who are currently using this api.

Test Plan: make clean check

Reviewers: sheki

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7479/Enhancements to rocksdb for better support for replication.

Summary:
1. The OpenForReadOnly() call should not lock the db. This is useful
so that multiple processes can open the same database concurrently
for reading.
2. GetUpdatesSince should not error out if the archive directory
does not exist.
3. A new constructor for WriteBatch that can takes a serialized
string as a parameter of the constructor.

Test Plan: make clean check

Reviewers: sheki

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7449/Added meta-database support.

Summary:
Added kMetaDatabase for meta-databases in db/filename.h along with supporting
fuctions.
Fixed switch in DBImpl so that it also handles kMetaDatabase.
Fixed DestroyDB() that it can handle destroying meta-databases.

Test Plan: make check

Reviewers: sheki, emayanke, vamsi, dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D7245/Fix a bug. Where DestroyDB deletes a non-existant archive directory.

Summary:
C tests would fail sometimes as DestroyDB would return a Failure Status
message when deleting an archival directory which was not created
(WAL_ttl_seconds = 0).

Fix: Ignore the Status returned on Deleting Archival Directory.

Test Plan: * make check

Reviewers: dhruba, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7395/Fix Bug in Binary Search for files containing a seq no. and delete Archived Log Files during Destroy DB.

Summary:
* Fixed implementation bug in Binary_Searvch introduced in https://reviews.facebook.net/D7119
* Binary search is also overflow safe.
* Delete archive log files and archive dir during DestroyDB

Test Plan: make check

Reviewers: dhruba

CC: kosievdmerwe, emayanke

Differential Revision: https://reviews.facebook.net/D7263/Refactor GetArchivalDirectoryName to filename.h

Summary:
filename.h has functions to do similar things.
Moving code away from db_impl.cc

Test Plan: make check

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D7251/GetUpdatesSince API to enable replication.

Summary:
How it works:
* GetUpdatesSince takes a SequenceNumber.
* A LogFile with the first SequenceNumber nearest and lesser than the requested Sequence Number is found.
* Seek in the logFile till the requested SeqNumber is found.
* Return an iterator which contains logic to return record's one by one.

Test Plan:
* Test case included to check the good code path.
* Will update with more test-cases.
* Feedback required on test-cases.

Reviewers: dhruba, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7119/Move WAL files to archive directory, instead of deleting.

Summary:
Create a directory ""archive"" in the DB directory.
During DeleteObsolteFiles move the WAL files (*.log) to the Archive directory,
instead of deleting.

Test Plan: Created a DB using DB_Bench. Reopened it. Checked if files move.

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D6975/Delete non-visible keys during a compaction even in the presense of snapshots.

Summary:
 LevelDB should delete almost-new keys when a long-open snapshot exists.
The previous behavior is to keep all versions that were created after the
oldest open snapshot. This can lead to database size bloat for
high-update workloads when there are long-open snapshots and long-open
snapshot will be used for logical backup. By ""almost new"" I mean that the
key was updated more than once after the oldest snapshot.

If there were two snapshots with seq numbers s1 and s2 (s1 < s2), and if
we find two instances of the same key k1 that lie entirely within s1 and
s2 (i.e. s1 < k1 < s2), then the earlier version
of k1 can be safely deleted because that version is not visible in any snapshot.

Test Plan:
unit test attached
make clean check

Differential Revision: https://reviews.facebook.net/D6999/Print out status at the end of a compaction run.

Summary:
Print out status at the end of a compaction run. This helps in
debugging.

Test Plan: make clean check

Reviewers: sheki

Reviewed By: sheki

Differential Revision: https://reviews.facebook.net/D7035/Support to disable background compactions on a database.

Summary:
This option is needed for fast bulk uploads. The goal is to load
all the data into files in L0 without any interference from
background compactions.

Test Plan: make clean check

Reviewers: sheki

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D6849/enhance dbstress to simulate hard crash

Summary:
dbstress has an option to reopen the database. Make it such that the
previous handle is not closed before we reopen, this simulates a
situation similar to a process crash.

Added new api to DMImpl to remove the lock file.

Test Plan: run db_stress

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D6777/"
Memory management,Memory management,Rocksdb,"Fix a number of object lifetime/ownership issues

Summary:
Replace manual memory management with std::unique_ptr in a
number of places; not exhaustive, but this fixes a few leaks with file
handles as well as clarifies semantics of the ownership of file handles
with log classes.

Test Plan: db_stress, make check

Reviewers: dhruba

Reviewed By: dhruba

CC: zshao, leveldb, heyongqiang

Differential Revision: https://reviews.facebook.net/D8043/db_bench should use the default value for max_grandparent_overlap_factor.

Summary:
This was a peformance regression caused by https://reviews.facebook.net/D6729.
The default value of max_grandparent_overlap_factor was erroneously
set to 0 in db_bench.

This was causing compactions to create really really small files because the max_grandparent_overlap_factor was erroneously set to zero in the benchmark.

Test Plan: Run --benchmarks=overwrite

Reviewers: heyongqiang, emayanke, sheki, MarkCallaghan

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7797/Add --seed, --read_range to db_bench

Summary:
Adds the option --seed to db_bench to specify the base for the per-thread RNG.
When not set each thread uses the same value across runs of db_bench which defeats
IO stress testing.

Adds the option --read_range. When set to a value > 1 an iterator is created and
each query done for the randomread benchmark will do a range scan for that many
rows. When not set or set to 1 the existing behavior (a point lookup) is done.

Fixes a bug where a printf format string was missing.

Test Plan: run db_bench

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7749/Move WAL files to archive directory, instead of deleting.

Summary:
Create a directory ""archive"" in the DB directory.
During DeleteObsolteFiles move the WAL files (*.log) to the Archive directory,
instead of deleting.

Test Plan: Created a DB using DB_Bench. Reopened it. Checked if files move.

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D6975/Support taking a configurable number of  files from the same level to compact in a single compaction run.

Summary:
The compaction process takes some files from LevelK and
merges it into LevelK+1. The number of files it picks from
LevelK was capped such a way that the total amount of
data picked does not exceed the maxfilesize of that level.
This essentially meant that only one file from LevelK
is picked for a single compaction.

For bulkloads, we would like to take many many file from
LevelK and compact them using a single compaction run.

This patch introduces a option called the 'source_compaction_factor'
(similar to expanded_compaction_factor). It is a multiplier
that is multiplied by the maxfilesize of that level to arrive
at the limit that is used to throttle the number of source
files from LevelK.  For bulk loads, set source_compaction_factor
to a very high number so that multiple files from the same
level are picked for compaction in a single compaction.

The default value of source_compaction_factor is 1, so that
we can keep backward compatibilty with existing compaction semantics.

Test Plan: make clean check

Reviewers: emayanke, sheki

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D6867/Support to disable background compactions on a database.

Summary:
This option is needed for fast bulk uploads. The goal is to load
all the data into files in L0 without any interference from
background compactions.

Test Plan: make clean check

Reviewers: sheki

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D6849/Enhance db_bench to be able to specify a grandparent_overlap_factor.

Summary:
The value specified in max_grandparent_overlap_factor is used to
limit the file size in a compaction run. This patch makes it
configurable when using db_bench.

Test Plan: make clean db_bench

Reviewers: MarkCallaghan, heyongqiang

Reviewed By: heyongqiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D6729/"
,,Rocksdb,"Allow the logs to be purged by TTL.

Summary:
* Add a SplitByTTLLogger to enable this feature. In this diff I implemented generalized AutoSplitLoggerBase class to simplify the
development of such classes.
* Refactor the existing AutoSplitLogger and fix several bugs.

Test Plan:
* Added a unit tests for different types of ""auto splitable"" loggers individually.
* Tested the composited logger which allows the log files to be splitted by both TTL and log size.

Reviewers: heyongqiang, dhruba

Reviewed By: heyongqiang

CC: zshao, leveldb

Differential Revision: https://reviews.facebook.net/D8037/Refactor GetArchivalDirectoryName to filename.h

Summary:
filename.h has functions to do similar things.
Moving code away from db_impl.cc

Test Plan: make check

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D7251/"
,,Rocksdb,"Add counters to count gets and writes

Summary: Add Tickers to count Write's and Get's

Test Plan: make check

Reviewers: dhruba, chip

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7977/"
Compression tasks,compression tasks,Rocksdb,"Fix poor error on num_levels mismatch and few other minor improvements

Summary:
Previously, if you opened a db with num_levels set lower than
the database, you received the unhelpful message ""Corruption:
VersionEdit: new-file entry.""  Now you get a more verbose message
describing the issue.

Also, fix handling of compression_levels (both the run-over-the-end
issue and the memory management of it).

Lastly, unique_ptr'ify a couple of minor calls.

Test Plan: make check

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8151/"
Memory management,Memory management,Rocksdb,"Fix a number of object lifetime/ownership issues

Summary:
Replace manual memory management with std::unique_ptr in a
number of places; not exhaustive, but this fixes a few leaks with file
handles as well as clarifies semantics of the ownership of file handles
with log classes.

Test Plan: db_stress, make check

Reviewers: dhruba

Reviewed By: dhruba

CC: zshao, leveldb, heyongqiang

Differential Revision: https://reviews.facebook.net/D8043/Enhance ReadOnly mode to process the all committed transactions.

Summary:
Leveldb has an api OpenForReadOnly() that opens the database
in readonly mode. This call had an option to not process the
transaction log.  This patch removes this option and always
processes all transactions that had been committed. It has
been done in such a way that it does not create/write to
any new files in the process. The invariant of ""no-writes""
to the leveldb data directory is still true.

This enhancement allows multiple threads to open the same database
in readonly mode and access all trancations that were committed right
upto the OpenForReadOnly call.

I changed the public API to match the new semantics because
there are no users who are currently using this api.

Test Plan: make clean check

Reviewers: sheki

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7479/GetUpdatesSince API to enable replication.

Summary:
How it works:
* GetUpdatesSince takes a SequenceNumber.
* A LogFile with the first SequenceNumber nearest and lesser than the requested Sequence Number is found.
* Seek in the logFile till the requested SeqNumber is found.
* Return an iterator which contains logic to return record's one by one.

Test Plan:
* Test case included to check the good code path.
* Will update with more test-cases.
* Feedback required on test-cases.

Reviewers: dhruba, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7119/"
,,Rocksdb,"Add optional clang compile mode

Summary:
clang is an alternate compiler based on llvm.  It produces
nicer error messages and finds some bugs that gcc doesn't, such as the
size_t change in this file (which caused some write return values to be
misinterpreted!)

Clang isn't the default; to try it, do ""USE_CLANG=1 make"" or ""export
USE_CLANG=1"" then make as normal

Test Plan: ""make check"" and ""USE_CLANG=1 make check""

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D7899/"
,,Rocksdb,"Added meta-database support.

Summary:
Added kMetaDatabase for meta-databases in db/filename.h along with supporting
fuctions.
Fixed switch in DBImpl so that it also handles kMetaDatabase.
Fixed DestroyDB() that it can handle destroying meta-databases.

Test Plan: make check

Reviewers: sheki, emayanke, vamsi, dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D7245/"
,,Rocksdb,"rollover manifest file.

Summary:
Check in LogAndApply if the file size is more than the limit set in
Options.
Things to consider : will this be expensive?

Test Plan: make all check. Inputs on a new unit test?

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7701/Port fix for Leveldb manifest writing bug from Open-Source

Summary:
Pretty much a blind copy of the patch in open source.
Hope to get this in before we make a release

Test Plan: make clean check

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7809/Assertion failure while running with unit tests with OPT=-g

Summary:
When we expand the range of keys for a level 0 compaction, we
need to invoke ParentFilesInCompaction() only once for the
entire range of keys that is being compacted. We were invoking
it for each file that was being compacted, but this triggers
an assertion because each file's range were contiguous but
non-overlapping.

I renamed ParentFilesInCompaction to ParentRangeInCompaction
to adequately represent that it is the range-of-keys and
not individual files that we compact in a single compaction run.

Here is the assertion that is fixed by this patch.
db_test: db/version_set.cc:585: void leveldb::Version::ExtendOverlappingInputs(int, const leveldb::Slice&, const leveldb::Slice&, std::vector<leveldb::FileMetaData*, std::allocator<leveldb::FileMetaData*> >*, int): Assertion `user_cmp->Compare(flimit, user_begin) >= 0' failed.

Test Plan: make clean check OPT=-g

Reviewers: sheki

Reviewed By: sheki

CC: MarkCallaghan, emayanke, leveldb

Differential Revision: https://reviews.facebook.net/D6963/"
API Management,"Memory Management, API Usage",Rocksdb,"Fix poor error on num_levels mismatch and few other minor improvements

Summary:
Previously, if you opened a db with num_levels set lower than
the database, you received the unhelpful message ""Corruption:
VersionEdit: new-file entry.""  Now you get a more verbose message
describing the issue.

Also, fix handling of compression_levels (both the run-over-the-end
issue and the memory management of it).

Lastly, unique_ptr'ify a couple of minor calls.

Test Plan: make check

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8151/Use fallocate to prevent excessive allocation of sst files and logs

Summary:
On some filesystems, pre-allocation can be a considerable
amount of space.  xfs in our production environment pre-allocates by
1GB, for instance.  By using fallocate to inform the kernel of our
expected file sizes, we eliminate this wasteage (that isn't recovered
until the file is closed which, in the case of LOG files, can be a
considerable amount of time).

Test Plan:
created an xfs loopback filesystem, mounted with
allocsize=4M, and ran db_stress.  LOG file without this change was 4M,
and with it it was 128k then grew to normal size.

Reviewers: dhruba

Reviewed By: dhruba

CC: adsharma, leveldb

Differential Revision: https://reviews.facebook.net/D7953/Fix a number of object lifetime/ownership issues

Summary:
Replace manual memory management with std::unique_ptr in a
number of places; not exhaustive, but this fixes a few leaks with file
handles as well as clarifies semantics of the ownership of file handles
with log classes.

Test Plan: db_stress, make check

Reviewers: dhruba

Reviewed By: dhruba

CC: zshao, leveldb, heyongqiang

Differential Revision: https://reviews.facebook.net/D8043/rollover manifest file.

Summary:
Check in LogAndApply if the file size is more than the limit set in
Options.
Things to consider : will this be expensive?

Test Plan: make all check. Inputs on a new unit test?

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7701/Port fix for Leveldb manifest writing bug from Open-Source

Summary:
Pretty much a blind copy of the patch in open source.
Hope to get this in before we make a release

Test Plan: make clean check

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7809/Added meta-database support.

Summary:
Added kMetaDatabase for meta-databases in db/filename.h along with supporting
fuctions.
Fixed switch in DBImpl so that it also handles kMetaDatabase.
Fixed DestroyDB() that it can handle destroying meta-databases.

Test Plan: make check

Reviewers: sheki, emayanke, vamsi, dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D7245/GetSequence API in write batch.

Summary:
WriteBatch is now used by the GetUpdatesSinceAPI. This API is external
and will be used by the rocks server. Rocks Server and others will need
to know about the Sequence Number in the WriteBatch. This public method
will allow for that.

Test Plan: make all check.

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7293/Fix Bug in Binary Search for files containing a seq no. and delete Archived Log Files during Destroy DB.

Summary:
* Fixed implementation bug in Binary_Searvch introduced in https://reviews.facebook.net/D7119
* Binary search is also overflow safe.
* Delete archive log files and archive dir during DestroyDB

Test Plan: make check

Reviewers: dhruba

CC: kosievdmerwe, emayanke

Differential Revision: https://reviews.facebook.net/D7263/An public api to fetch the latest transaction id.

Summary:
Implement a interface to retrieve the most current transaction
id from the database.

Test Plan: Added unit test.

Reviewers: sheki

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7269/Refactor GetArchivalDirectoryName to filename.h

Summary:
filename.h has functions to do similar things.
Moving code away from db_impl.cc

Test Plan: make check

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D7251/GetUpdatesSince API to enable replication.

Summary:
How it works:
* GetUpdatesSince takes a SequenceNumber.
* A LogFile with the first SequenceNumber nearest and lesser than the requested Sequence Number is found.
* Seek in the logFile till the requested SeqNumber is found.
* Return an iterator which contains logic to return record's one by one.

Test Plan:
* Test case included to check the good code path.
* Will update with more test-cases.
* Feedback required on test-cases.

Reviewers: dhruba, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7119/Move WAL files to archive directory, instead of deleting.

Summary:
Create a directory ""archive"" in the DB directory.
During DeleteObsolteFiles move the WAL files (*.log) to the Archive directory,
instead of deleting.

Test Plan: Created a DB using DB_Bench. Reopened it. Checked if files move.

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D6975/Delete non-visible keys during a compaction even in the presense of snapshots.

Summary:
 LevelDB should delete almost-new keys when a long-open snapshot exists.
The previous behavior is to keep all versions that were created after the
oldest open snapshot. This can lead to database size bloat for
high-update workloads when there are long-open snapshots and long-open
snapshot will be used for logical backup. By ""almost new"" I mean that the
key was updated more than once after the oldest snapshot.

If there were two snapshots with seq numbers s1 and s2 (s1 < s2), and if
we find two instances of the same key k1 that lie entirely within s1 and
s2 (i.e. s1 < k1 < s2), then the earlier version
of k1 can be safely deleted because that version is not visible in any snapshot.

Test Plan:
unit test attached
make clean check

Differential Revision: https://reviews.facebook.net/D6999/Improved CompactionFilter api:  pass in a opaque argument to CompactionFilter invocation.

Summary:
There are applications that operate on multiple leveldb instances.
These applications will like to pass in an opaque type for each
leveldb instance and this type should be passed back to the application
with every invocation of the CompactionFilter api.

Test Plan: Enehanced unit test for opaque parameter to CompactionFilter.

Reviewers: heyongqiang

Reviewed By: heyongqiang

CC: MarkCallaghan, sheki, emayanke

Differential Revision: https://reviews.facebook.net/D6711/"
Memory management,Memory management,Rocksdb,"Fix a number of object lifetime/ownership issues

Summary:
Replace manual memory management with std::unique_ptr in a
number of places; not exhaustive, but this fixes a few leaks with file
handles as well as clarifies semantics of the ownership of file handles
with log classes.

Test Plan: db_stress, make check

Reviewers: dhruba

Reviewed By: dhruba

CC: zshao, leveldb, heyongqiang

Differential Revision: https://reviews.facebook.net/D8043/rollover manifest file.

Summary:
Check in LogAndApply if the file size is more than the limit set in
Options.
Things to consider : will this be expensive?

Test Plan: make all check. Inputs on a new unit test?

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7701/Fixed bug with seek compactions on Level 0

Summary: Due to how the code handled compactions in Level 0 in `PickCompaction()` it could be the case that two compactions on level 0 ran that produced tables in level 1 that overlap. However, this case seems like it would only occur on a seek compaction which is unlikely on level 0. Furthermore, level 0 and level 1 had to have a certain arrangement of files.

Test Plan:
make check

Reviewers: dhruba, vamsi

Reviewed By: dhruba

CC: leveldb, sheki

Differential Revision: https://reviews.facebook.net/D7923/Various build cleanups/improvements

Summary:
Specific changes:

1) Turn on -Werror so all warnings are errors
2) Fix some warnings the above now complains about
3) Add proper dependency support so changing a .h file forces a .c file
to rebuild
4) Automatically use fbcode gcc on any internal machine rather than
whatever system compiler is laying around
5) Fix jemalloc to once again be used in the builds (seemed like it
wasn't being?)
6) Fix issue where 'git' would fail in build_detect_version because of
LD_LIBRARY_PATH being set in the third-party build system

Test Plan:
make, make check, make clean, touch a header file, make sure
rebuild is expected

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D7887/fix warning for unused variable

Test Plan: compile

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7857/Port fix for Leveldb manifest writing bug from Open-Source

Summary:
Pretty much a blind copy of the patch in open source.
Hope to get this in before we make a release

Test Plan: make clean check

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7809/ExtendOverlappingInputs too slow for large databases.

Summary:
There was a bug in the ExtendOverlappingInputs method so that
the terminating condition for the backward search was incorrect.

Test Plan: make clean check

Reviewers: sheki, emayanke, MarkCallaghan

Reviewed By: MarkCallaghan

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7725/manifest_dump: Add --hex=1 option

Summary: Without this option, manifest_dump does not print binary keys for files in a human-readable way.

Reviewers: dhruba, sheki, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D7425/Print compaction score for every compaction run.

Summary:
A compaction is picked based on its score. It is useful to
print the compaction score in the LOG because it aids in
debugging. If one looks at the logs, one can find out why
a compaction was preferred over another.

Test Plan: make clean check

Differential Revision: https://reviews.facebook.net/D7137/Assertion failure while running with unit tests with OPT=-g

Summary:
When we expand the range of keys for a level 0 compaction, we
need to invoke ParentFilesInCompaction() only once for the
entire range of keys that is being compacted. We were invoking
it for each file that was being compacted, but this triggers
an assertion because each file's range were contiguous but
non-overlapping.

I renamed ParentFilesInCompaction to ParentRangeInCompaction
to adequately represent that it is the range-of-keys and
not individual files that we compact in a single compaction run.

Here is the assertion that is fixed by this patch.
db_test: db/version_set.cc:585: void leveldb::Version::ExtendOverlappingInputs(int, const leveldb::Slice&, const leveldb::Slice&, std::vector<leveldb::FileMetaData*, std::allocator<leveldb::FileMetaData*> >*, int): Assertion `user_cmp->Compare(flimit, user_begin) >= 0' failed.

Test Plan: make clean check OPT=-g

Reviewers: sheki

Reviewed By: sheki

CC: MarkCallaghan, emayanke, leveldb

Differential Revision: https://reviews.facebook.net/D6963/Support taking a configurable number of  files from the same level to compact in a single compaction run.

Summary:
The compaction process takes some files from LevelK and
merges it into LevelK+1. The number of files it picks from
LevelK was capped such a way that the total amount of
data picked does not exceed the maxfilesize of that level.
This essentially meant that only one file from LevelK
is picked for a single compaction.

For bulkloads, we would like to take many many file from
LevelK and compact them using a single compaction run.

This patch introduces a option called the 'source_compaction_factor'
(similar to expanded_compaction_factor). It is a multiplier
that is multiplied by the maxfilesize of that level to arrive
at the limit that is used to throttle the number of source
files from LevelK.  For bulk loads, set source_compaction_factor
to a very high number so that multiple files from the same
level are picked for compaction in a single compaction.

The default value of source_compaction_factor is 1, so that
we can keep backward compatibilty with existing compaction semantics.

Test Plan: make clean check

Reviewers: emayanke, sheki

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D6867/A major bug that was not considering the compaction score of the n-1 level.

Summary:
The method Finalize() recomputes the compaction score of each
level and then sorts these score from largest to smallest. The
idea is that the level with the largest compaction score will
be a better candidate for compaction.  There are usually very
few levels, and a bubble sort code was used to sort these
compaction scores. There existed a bug in the sorting code that
skipped looking at the score for the n-1 level. This meant that
even if the compaction score of the n-1 level is large, it will
not be picked for compaction.

This patch fixes the bug and also introduces ""asserts"" in the
code to detect any possible inconsistencies caused by future bugs.

This bug existed in the very first code change that introduced
multi-threaded compaction to the leveldb code. That version of
code was committed on Oct 19th via
https://github.com/facebook/leveldb/commit/1ca0584345af85d2dccc434f451218119626d36e

Test Plan: make clean check OPT=-g

Reviewers: emayanke, sheki, MarkCallaghan

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D6837/Fix compilation error introduced by previous commit
7889e094554dc5bba678a0bfa7fb5eca422c34de

Summary:
Fix compilation error introduced by previous commit
7889e094554dc5bba678a0bfa7fb5eca422c34de

Test Plan:
make clean check/Enhance manifest_dump to print each individual edit.

Summary:
The manifest file contains a series of edits. If the verbose
option is switched on, then print each individual edit in the
manifest file. This helps in debugging.

Test Plan: make clean manifest_dump

Reviewers: emayanke, sheki

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D6807/Fix asserts so that ""make check OPT=-g"" works on performance branch

Summary:
Compilation used to fail with the error:
db/version_set.cc:1773: error: ‘number_of_files_to_sort_’ is not a member of ‘leveldb::VersionSet’

I created a new method called CheckConsistencyForDeletes() so that
all the high cost checking is done only when OPT=-g is specified.

I also fixed a bug in PickCompactionBySize that was triggered when
OPT=-g was switched on. The base_index in the compaction record
was not set correctly.

Test Plan: make check OPT=-g

Differential Revision: https://reviews.facebook.net/D6687/"
,,Rocksdb,"Various build cleanups/improvements

Summary:
Specific changes:

1) Turn on -Werror so all warnings are errors
2) Fix some warnings the above now complains about
3) Add proper dependency support so changing a .h file forces a .c file
to rebuild
4) Automatically use fbcode gcc on any internal machine rather than
whatever system compiler is laying around
5) Fix jemalloc to once again be used in the builds (seemed like it
wasn't being?)
6) Fix issue where 'git' would fail in build_detect_version because of
LD_LIBRARY_PATH being set in the third-party build system

Test Plan:
make, make check, make clean, touch a header file, make sure
rebuild is expected

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D7887/Delete non-visible keys during a compaction even in the presense of snapshots.

Summary:
 LevelDB should delete almost-new keys when a long-open snapshot exists.
The previous behavior is to keep all versions that were created after the
oldest open snapshot. This can lead to database size bloat for
high-update workloads when there are long-open snapshots and long-open
snapshot will be used for logical backup. By ""almost new"" I mean that the
key was updated more than once after the oldest snapshot.

If there were two snapshots with seq numbers s1 and s2 (s1 < s2), and if
we find two instances of the same key k1 that lie entirely within s1 and
s2 (i.e. s1 < k1 < s2), then the earlier version
of k1 can be safely deleted because that version is not visible in any snapshot.

Test Plan:
unit test attached
make clean check

Differential Revision: https://reviews.facebook.net/D6999/"
,,Rocksdb,"Ability to configure bufferedio-reads, filesystem-readaheads and  mmap-read-write per database.

Summary:
This patch allows an application to specify whether to use bufferedio,
reads-via-mmaps and writes-via-mmaps per database. Earlier, there
was a global static variable that was used to configure this functionality.

The default setting remains the same (and is backward compatible):
 1. use bufferedio
 2. do not use mmaps for reads
 3. use mmap for writes
 4. use readaheads for reads needed for compaction

I also added a parameter to db_bench to be able to explicitly specify
whether to do readaheads for compactions or not.

Test Plan: make check

Reviewers: sheki, heyongqiang, MarkCallaghan

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9429/"
Memory Management,"Memory Management, Thread management",Rocksdb,"Timestamp and TTL Wrapper for rocksdb

Summary:
When opened with DBTimestamp::Open call, timestamps are prepended to and stripped from the value during subsequent Put and Get calls respectively. The Timestamp is used to discard values in Get and custom compaction filter which have exceeded their TTL which is specified during Open.
Have made a temporary change to Makefile to let us test with the temporary file TestTime.cc. Have also changed the private members of db_impl.h to protected to let them be inherited by the new class DBTimestamp

Test Plan: make db_timestamp; TestTime.cc(will not check it in) shows how to use the apis currently, but I will write unit-tests shortly

Reviewers: dhruba, vamsi, haobo, sheki, heyongqiang, vkrest

Reviewed By: vamsi

CC: zshao, xjin, vkrest, MarkCallaghan

Differential Revision: https://reviews.facebook.net/D10311/Make provision for db_stress to work with a pre-existing dir

Summary: The crash_test depends on db_stress to work with pre-existing dir

Test Plan: make db_stress; Run db_stress with 'destroy_db_initially=0'

Reviewers: vamsi, dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10041/db_stress #reopens should be less than ops_per_thread

Summary: For sanity w.r.t. the way we split up the reopens equally among the ops/thread

Test Plan: make db_stress; db_stress --ops_per_thread=10 --reopens=10 => error

Reviewers: vamsi, dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10023/Python script to periodically run and kill the db_stress test

Summary: The script runs and kills the stress test periodically. Default values have been used in the script now. Should I make this a part of the Makefile or automated rocksdb build? The values can be easily changed in the script right now, but should I add some support for variable values or input to the script? I believe the script achieves its objective of unsafe crashes and reopening to expect sanity in the database.

Test Plan: python tools/db_crashtest.py

Reviewers: dhruba, vamsi, MarkCallaghan

Reviewed By: vamsi

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9369/[Report the #gets and #founds in db_stress]

Summary:
Also added some comments and fixed some bugs in
stats reporting. Now the stats seem to match what is expected.

Revert Plan: OK

Task ID: #

Reviewers: emayanke, dhruba

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9081/Make db_stress Not purge redundant keys on some opens

Summary: In light of the new option introduced by commit 806e26435037f5e2eb3b8c2d1e5f278a86fdb2ba where the database has an option to compact before flushing to disk, we want the stress test to test both sides of the option. Have made it to 'deterministically' and configurably change that option for reopens.

Test Plan: make db_stress; ./db_stress with some differnet options

Reviewers: dhruba, vamsi

Reviewed By: dhruba

CC: leveldb, sheki

Differential Revision: https://reviews.facebook.net/D9165/[Add a second kind of verification to db_stress

Summary:
Currently the test tracks all writes in memory and
uses it for verification at the end. This has 4 problems:
(a) It needs mutex for each write to ensure in-memory update
and leveldb update are done atomically. This slows down the
benchmark.
(b) Verification phase at the end is time consuming as well
(c) Does not test batch writes or snapshots
(d) We cannot kill the test and restart multiple times in a
loop because in-memory state will be lost.

I am adding a FLAGS_multi that does MultiGet/MultiPut/MultiDelete
instead of get/put/delete to get/put/delete a group of related
keys with same values atomically. Every get retrieves the group
of keys and checks that their values are same. This does not have
the above problems but the downside is that it does less amount
of validation than the other approach.

Test Plan:
This whole this is a test! Here is a small run. I am doing larger run now.

[nponnekanti@dev902 /data/users/nponnekanti/rocksdb] ./db_stress --ops_per_thread=10000 --multi=1 --ops_per_key=25
LevelDB version     : 1.5
Number of threads   : 32
Ops per thread      : 10000
Read percentage     : 10
Delete percentage   : 30
Max key             : 2147483648
Num times DB reopens: 10
Num keys per lock   : 4
Compression         : snappy
------------------------------------------------
Creating 536870912 locks
2013/02/20-16:59:32  Starting database operations
Created bg thread 0x7f9ebcfff700
2013/02/20-16:59:37  Reopening database for the 1th time
2013/02/20-16:59:46  Reopening database for the 2th time
2013/02/20-16:59:57  Reopening database for the 3th time
2013/02/20-17:00:11  Reopening database for the 4th time
2013/02/20-17:00:25  Reopening database for the 5th time
2013/02/20-17:00:36  Reopening database for the 6th time
2013/02/20-17:00:47  Reopening database for the 7th time
2013/02/20-17:00:59  Reopening database for the 8th time
2013/02/20-17:01:10  Reopening database for the 9th time
2013/02/20-17:01:20  Reopening database for the 10th time
2013/02/20-17:01:31  Reopening database for the 11th time
2013/02/20-17:01:31  Starting verification
Stress Test : 109.125 micros/op 22191 ops/sec
            : Wrote 0.00 MB (0.23 MB/sec) (59% of 32 ops)
            : Deleted 10 times
2013/02/20-17:01:31  Verification successful

Revert Plan: OK

Task ID: #

Reviewers: dhruba, emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8733/Introduce histogram  in statistics.h

Summary:
* Introduce is histogram in statistics.h
* stop watch to measure time.
* introduce two timers as a poc.
Replaced NULL with nullptr to fight some lint errors
Should be useful for google.

Test Plan:
ran db_bench and check stats.
make all check

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8637/"
,,Rocksdb,"Do not allow Transaction Log Iterator to fall ahead when writer is writing the same file

Summary:
Store the last flushed, seq no. in db_impl. Check against it in
transaction Log iterator. Do not attempt to read ahead if we do not know
if the data is flushed completely.
Does not work if flush is disabled. Any ideas on fixing that?
* Minor change, iter->Next is called the first time automatically for
* the first time.

Test Plan:
existing test pass.
More ideas on testing this?
Planning to run some stress test.

Reviewers: dhruba, heyongqiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9087/"
,,Rocksdb,"[Rocksdb] Support Merge operation in rocksdb

Summary:
This diff introduces a new Merge operation into rocksdb.
The purpose of this review is mostly getting feedback from the team (everyone please) on the design.

Please focus on the four files under include/leveldb/, as they spell the client visible interface change.
include/leveldb/db.h
include/leveldb/merge_operator.h
include/leveldb/options.h
include/leveldb/write_batch.h

Please go over local/my_test.cc carefully, as it is a concerete use case.

Please also review the impelmentation files to see if the straw man implementation makes sense.

Note that, the diff does pass all make check and truly supports forward iterator over db and a version
of Get that's based on iterator.

Future work:
- Integration with compaction
- A raw Get implementation

I am working on a wiki that explains the design and implementation choices, but coding comes
just naturally and I think it might be a good idea to share the code earlier. The code is
heavily commented.

Test Plan: run all local tests

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: leveldb, zshao, sheki, emayanke, MarkCallaghan

Differential Revision: https://reviews.facebook.net/D9651/Codemod NULL to nullptr

Summary:
scripted NULL to nullptr in
* include/leveldb/
* db/
* table/
* util/

Test Plan: make all check

Reviewers: dhruba, emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9003/"
,,Rocksdb,"Fix valgrind errors in rocksdb tests: auto_roll_logger_test, reduce_levels_test

Summary: Fix for memory leaks in rocksdb tests. Also modified the variable NUM_FAILED_TESTS to print the actual number of failed tests.

Test Plan: make <test>; valgrind --leak-check=full ./<test>

Reviewers: sheki, dhruba

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9333/Fix the warning introduced by auto_roll_logger_test

Summary: Fix the warning [-Werror=format-security] and [-Werror=unused-result].

Test Plan:
enforced the Werror and run make

Task ID: 2101673

Blame Rev:

Reviewers: heyongqiang

Differential Revision: https://reviews.facebook.net/D8553/"
,,Rocksdb,"Codemod NULL to nullptr

Summary:
scripted NULL to nullptr in
* include/leveldb/
* db/
* table/
* util/

Test Plan: make all check

Reviewers: dhruba, emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9003/"
,,Rocksdb,"Enhance the ldb tool to support ttl databases

Summary: ldb works with raw data from the database and needs to be aware of ttl-database to work with it meaningfully. '-ttl' option now tells it that. Also added onto the ldb_test.py test. This option may be specified alongwith put, get, scan or dump. There is no support to provide a ttl-value and it uses default forever because there is no use-case for this currently.

Test Plan: make ldb_test; python tools/ldb_test.py

Reviewers: dhruba, sheki, haobo, vamsi

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10797/[RocksDB] Expose LDB functioanality as a library call - clients can build their own LDB binary with additional options

Summary: Primarily a refactor. Introduced LDBTool interface to which customers can plug in their options and this will create their own version of ldb tool.

Test Plan: made ldb tool and tried it.

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10191/Integrate the manifest_dump command with ldb

Summary:
Syntax:

   manifest_dump [--verbose] --num=<manifest_num>

Test Plan: - Tested on an example DB (see output in summary)

Reviewers: sheki, dhruba

Reviewed By: sheki

CC: leveldb, heyongqiang

Differential Revision: https://reviews.facebook.net/D9609/Fixing a careless mistake in ldb

Summary: negation of the condition checked currently had to be checkd actually

Test Plan: make ldb; python ldb_test.py

Reviewers: sheki, dhruba

Reviewed By: sheki

Differential Revision: https://reviews.facebook.net/D9459/Doing away with boost in ldb_cmd.h

Summary: boost functions cause complications while deploying to third-party

Test Plan: make

Reviewers: sheki, dhruba

Reviewed By: sheki

Differential Revision: https://reviews.facebook.net/D9441/[RocksDB] Add bulk_load option to Options and ldb

Summary:
Add a shortcut function to make it easier for people
to efficiently bulk_load data into RocksDB.

Test Plan:
Tried ldb with ""--bulk_load"" and ""--bulk_load --compact"" and verified the outcome.
Needs to consult the team on how to test this automatically.

Reviewers: sheki, dhruba, emayanke, heyongqiang

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8907/ldb waldump to print the keys along with other stats + NULL to nullptr in ldb_cmd.cc

Summary: LDB tool to print the deleted/put keys in hex in the wal file.

Test Plan: run ldb on a  db to check if output was satisfactory

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8691/Revert ""Fix for the weird behaviour encountered by ldb Get where it could read only the second-latest value""

This reverts commit 4c696ed0018800b62e2448a4ead438255140fc25./Fix for the weird behaviour encountered by ldb Get where it could read only the second-latest value

Summary:
flush_on_destroy has a default value of false and the memtable is flushed
in the dbimpl-destructor only when that is set to true. Because we want the memtable to be flushed everytime that
the destructor is called(db is closed) and the cases where we work with the memtable only are very less
it is a good idea to give this a default value of true. Thus the put from ldb
wil have its data flushed to disk in the destructor and the next Get will be able to
read it when opened with OpenForReadOnly. The reason that ldb could read the latest value when
the db was opened in the normal Open mode is that the Get from normal Open first reads
the memtable and directly finds the latest value written there and the Get from OpenForReadOnly
doesn't have access to the memtable (which is correct because all its Put/Modify) are disabled

Test Plan: make all; ldb put and get and scans

Reviewers: dhruba, heyongqiang, sheki

Reviewed By: heyongqiang

CC: kosievdmerwe, zshao, dilipj, kailiu

Differential Revision: https://reviews.facebook.net/D8631/"
,,Rocksdb,"Enhance the ldb tool to support ttl databases

Summary: ldb works with raw data from the database and needs to be aware of ttl-database to work with it meaningfully. '-ttl' option now tells it that. Also added onto the ldb_test.py test. This option may be specified alongwith put, get, scan or dump. There is no support to provide a ttl-value and it uses default forever because there is no use-case for this currently.

Test Plan: make ldb_test; python tools/ldb_test.py

Reviewers: dhruba, sheki, haobo, vamsi

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10797/[RocksDB] Expose LDB functioanality as a library call - clients can build their own LDB binary with additional options

Summary: Primarily a refactor. Introduced LDBTool interface to which customers can plug in their options and this will create their own version of ldb tool.

Test Plan: made ldb tool and tried it.

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10191/Integrate the manifest_dump command with ldb

Summary:
Syntax:

   manifest_dump [--verbose] --num=<manifest_num>

e.g.

$ ./ldb --db=/home/smarlow/tmp/testdb manifest_dump --num=12
manifest_file_number 13 next_file_number 14 last_sequence 3 log_number
11  prev_log_number 0
--- level 0 --- version# 0 ---
 6:116['a1' @ 1 : 1 .. 'a1' @ 1 : 1]
 10:130['a3' @ 2 : 1 .. 'a4' @ 3 : 1]
--- level 1 --- version# 0 ---
--- level 2 --- version# 0 ---
--- level 3 --- version# 0 ---
--- level 4 --- version# 0 ---
--- level 5 --- version# 0 ---
--- level 6 --- version# 0 ---

Test Plan: - Tested on an example DB (see output in summary)

Reviewers: sheki, dhruba

Reviewed By: sheki

CC: leveldb, heyongqiang

Differential Revision: https://reviews.facebook.net/D9609/Ability to configure bufferedio-reads, filesystem-readaheads and  mmap-read-write per database.

Summary:
This patch allows an application to specify whether to use bufferedio,
reads-via-mmaps and writes-via-mmaps per database. Earlier, there
was a global static variable that was used to configure this functionality.

The default setting remains the same (and is backward compatible):
 1. use bufferedio
 2. do not use mmaps for reads
 3. use mmap for writes
 4. use readaheads for reads needed for compaction

I also added a parameter to db_bench to be able to explicitly specify
whether to do readaheads for compactions or not.

Test Plan: make check

Reviewers: sheki, heyongqiang, MarkCallaghan

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9429/Removing boost from ldb_cmd.cc

Summary: Getting rid of boost in our github codebase which caused problems on third-party

Test Plan: make ldb; python tools/ldb_test.py

Reviewers: sheki, dhruba

Reviewed By: sheki

Differential Revision: https://reviews.facebook.net/D9543/Fix valgrind errors in rocksdb tests: auto_roll_logger_test, reduce_levels_test

Summary: Fix for memory leaks in rocksdb tests. Also modified the variable NUM_FAILED_TESTS to print the actual number of failed tests.

Test Plan: make <test>; valgrind --leak-check=full ./<test>

Reviewers: sheki, dhruba

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9333/[RocksDB] Add bulk_load option to Options and ldb

Summary:
Add a shortcut function to make it easier for people
to efficiently bulk_load data into RocksDB.

Test Plan:
Tried ldb with ""--bulk_load"" and ""--bulk_load --compact"" and verified the outcome.
Needs to consult the team on how to test this automatically.

Reviewers: sheki, dhruba, emayanke, heyongqiang

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8907/ldb waldump to print the keys along with other stats + NULL to nullptr in ldb_cmd.cc

Summary: LDB tool to print the deleted/put keys in hex in the wal file.

Test Plan: run ldb on a  db to check if output was satisfactory

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8691/"
,,Rocksdb,"[Rocksdb] Support Merge operation in rocksdb

Summary:
This diff introduces a new Merge operation into rocksdb.
The purpose of this review is mostly getting feedback from the team (everyone please) on the design.

Please focus on the four files under include/leveldb/, as they spell the client visible interface change.
include/leveldb/db.h
include/leveldb/merge_operator.h
include/leveldb/options.h
include/leveldb/write_batch.h

Please go over local/my_test.cc carefully, as it is a concerete use case.

Please also review the impelmentation files to see if the straw man implementation makes sense.

Note that, the diff does pass all make check and truly supports forward iterator over db and a version
of Get that's based on iterator.

Future work:
- Integration with compaction
- A raw Get implementation

I am working on a wiki that explains the design and implementation choices, but coding comes
just naturally and I think it might be a good idea to share the code earlier. The code is
heavily commented.

Test Plan: run all local tests

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: leveldb, zshao, sheki, emayanke, MarkCallaghan

Differential Revision: https://reviews.facebook.net/D9651/Set FD_CLOEXEC after each file open

Summary: as subject. This is causing problem in adsconv. Ideally, this flags should be set in open. But that is only supported in Linux kernel ?2.6.23 and glibc ?2.7.

Test Plan:
db_test

run db_test

Reviewers: dhruba, MarkCallaghan, haobo

Reviewed By: dhruba

CC: leveldb, chip

Differential Revision: https://reviews.facebook.net/D10089/Ability to configure bufferedio-reads, filesystem-readaheads and  mmap-read-write per database.

Summary:
This patch allows an application to specify whether to use bufferedio,
reads-via-mmaps and writes-via-mmaps per database. Earlier, there
was a global static variable that was used to configure this functionality.

The default setting remains the same (and is backward compatible):
 1. use bufferedio
 2. do not use mmaps for reads
 3. use mmap for writes
 4. use readaheads for reads needed for compaction

I also added a parameter to db_bench to be able to explicitly specify
whether to do readaheads for compactions or not.

Test Plan: make check

Reviewers: sheki, heyongqiang, MarkCallaghan

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9429/Fixed sign-comparison in rocksdb code-base and fixed Makefile

Summary: Makefile had options to ignore sign-comparisons and unused-parameters, which should be there. Also fixed the specific errors in the code-base

Test Plan: make

Reviewers: chip, dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9531/Ability for rocksdb to compact when flushing the in-memory memtable to a file in L0.

Summary:
Rocks accumulates recent writes and deletes in the in-memory memtable.
When the memtable is full, it writes the contents on the memtable to
a file in L0.

This patch removes redundant records at the time of the flush. If there
are multiple versions of the same key in the memtable, then only the
most recent one is dumped into the output file. The purging of
redundant records occur only if the most recent snapshot is earlier
than the earliest record in the memtable.

Should we switch on this feature by default or should we keep this feature
turned off in the default settings?

Test Plan: Added test case to db_test.cc

Reviewers: sheki, vamsi, emayanke, heyongqiang

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8991/Codemod NULL to nullptr

Summary:
scripted NULL to nullptr in
* include/leveldb/
* db/
* table/
* util/

Test Plan: make all check

Reviewers: dhruba, emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9003/"
,,Rocksdb,"[RocksDB] Fix LRUCache Eviction problem

Summary:
1. The stock LRUCache nukes itself whenever the working set (the total number of entries not released by client at a certain time) is bigger than the cache capacity.
See https://our.dev.facebook.com/intern/tasks/?t=2252281
2. There's a bug in shard calculation leading to segmentation fault when only one shard is needed.

Test Plan: make check

Reviewers: dhruba, heyongqiang

Reviewed By: heyongqiang

CC: leveldb, zshao, sheki

Differential Revision: https://reviews.facebook.net/D9927/"
Memory management,Memory management,Rocksdb,"Exit and Join the background compaction threads while running rocksdb tests

Summary:
The background compaction threads are never exitted and therefore caused
memory-leaks while running rpcksdb tests. Have changed the PosixEnv destructor to exit and join them and changed the tests likewise
The memory leaked has reduced from 320 bytes to 64 bytes in all the tests. The 64
bytes is relating to
pthread_exit, but still have to figure out why. The stack-trace right now with
table_test.cc = 64 bytes in 1 blocks are possibly lost in loss record 4 of 5
   at 0x475D8C: malloc (jemalloc.c:914)
   by 0x400D69E: _dl_map_object_deps (dl-deps.c:505)
   by 0x4013393: dl_open_worker (dl-open.c:263)
   by 0x400F015: _dl_catch_error (dl-error.c:178)
   by 0x4013B2B: _dl_open (dl-open.c:569)
   by 0x5D3E913: do_dlopen (dl-libc.c:86)
   by 0x400F015: _dl_catch_error (dl-error.c:178)
   by 0x5D3E9D6: __libc_dlopen_mode (dl-libc.c:47)
   by 0x5048BF3: pthread_cancel_init (unwind-forcedunwind.c:53)
   by 0x5048DC9: _Unwind_ForcedUnwind (unwind-forcedunwind.c:126)
   by 0x5046D9F: __pthread_unwind (unwind.c:130)
   by 0x50413A4: pthread_exit (pthreadP.h:289)

Test Plan: make all check

Reviewers: dhruba, sheki, haobo

Reviewed By: dhruba

CC: leveldb, chip

Differential Revision: https://reviews.facebook.net/D9573/Set FD_CLOEXEC after each file open

Summary: as subject. This is causing problem in adsconv. Ideally, this flags should be set in open. But that is only supported in Linux kernel ?2.6.23 and glibc ?2.7.

Test Plan:
db_test

run db_test

Reviewers: dhruba, MarkCallaghan, haobo

Reviewed By: dhruba

CC: leveldb, chip

Differential Revision: https://reviews.facebook.net/D10089/[RocksDB] env_posix cleanup

Summary:
1. SetBackgroundThreads was not thread safe
2. queue_size_ does not seem necessary
3. moved condition signal after shared state change. Even though the original
   order is in practice ok (because the mutex is still held), it looks fishy
   and non-intuitive.

Test Plan: make check

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, zshao

Differential Revision: https://reviews.facebook.net/D9825/Initialize variable in constructor for PosixEnv::checkedDiskForMmap_

Summary: This caused compilation problems on some gcc platforms during the third-partyrelease

Test Plan: make

Reviewers: sheki

Reviewed By: sheki

Differential Revision: https://reviews.facebook.net/D9627/Ability to configure bufferedio-reads, filesystem-readaheads and  mmap-read-write per database.

Summary:
This patch allows an application to specify whether to use bufferedio,
reads-via-mmaps and writes-via-mmaps per database. Earlier, there
was a global static variable that was used to configure this functionality.

The default setting remains the same (and is backward compatible):
 1. use bufferedio
 2. do not use mmaps for reads
 3. use mmap for writes
 4. use readaheads for reads needed for compaction

I also added a parameter to db_bench to be able to explicitly specify
whether to do readaheads for compactions or not.

Test Plan: make check

Reviewers: sheki, heyongqiang, MarkCallaghan

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9429/Use posix_fallocate as default.

Summary:
Ftruncate does not throw an error on disk-full. This causes Sig-bus in
the case where the database tries to issue a Put call on a full-disk.

Use posix_fallocate for allocation instead of truncate.
Add a check to use MMaped files only on ext4, xfs and tempfs, as
posix_fallocate is very slow on ext3 and older.

Test Plan: make all check

Reviewers: dhruba, chip

Reviewed By: dhruba

CC: adsharma, leveldb

Differential Revision: https://reviews.facebook.net/D9291/"
,,Rocksdb,"[Rocksdb] Support Merge operation in rocksdb

Summary:
This diff introduces a new Merge operation into rocksdb.
The purpose of this review is mostly getting feedback from the team (everyone please) on the design.

Please focus on the four files under include/leveldb/, as they spell the client visible interface change.
include/leveldb/db.h
include/leveldb/merge_operator.h
include/leveldb/options.h
include/leveldb/write_batch.h

Please go over local/my_test.cc carefully, as it is a concerete use case.

Please also review the impelmentation files to see if the straw man implementation makes sense.

Note that, the diff does pass all make check and truly supports forward iterator over db and a version
of Get that's based on iterator.

Future work:
- Integration with compaction
- A raw Get implementation

I am working on a wiki that explains the design and implementation choices, but coding comes
just naturally and I think it might be a good idea to share the code earlier. The code is
heavily commented.

Test Plan: run all local tests

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: leveldb, zshao, sheki, emayanke, MarkCallaghan

Differential Revision: https://reviews.facebook.net/D9651/"
Memory Management,"Memory Management, compression tasks",Rocksdb,"Add rate_delay_limit_milliseconds

Summary:
This adds the rate_delay_limit_milliseconds option to make the delay
configurable in MakeRoomForWrite when the max compaction score is too high.
This delay is called the Ln slowdown. This change also counts the Ln slowdown
per level to make it possible to see where the stalls occur.

From IO-bound performance testing, the Level N stalls occur:
* with compression -> at the largest uncompressed level. This makes sense
                      because compaction for compressed levels is much
                      slower. When Lx is uncompressed and Lx+1 is compressed
                      then files pile up at Lx because the (Lx,Lx+1)->Lx+1
                      compaction process is the first to be slowed by
                      compression.
* without compression -> at level 1

Task ID: #1832108

Blame Rev:

Test Plan:
run with real data, added test

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D9045/Codemod NULL to nullptr

Summary:
scripted NULL to nullptr in
* include/leveldb/
* db/
* table/
* util/

Test Plan: make all check

Reviewers: dhruba, emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9003/"
,,Rocksdb,"Measure compaction time.

Summary: just record time consumed in compaction

Test Plan: compile

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8781/Counters for bytes written and read.

Summary:
* Counters for bytes read and write.
as a part of this diff, I want to=>
* Measure compaction times. @dhruba can you point which function, should
* I time to get Compaction-times. Was looking at CompactRange.

Test Plan: db_test

Reviewers: dhruba, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8763/Introduce histogram  in statistics.h

Summary:
* Introduce is histogram in statistics.h
* stop watch to measure time.
* introduce two timers as a poc.
Replaced NULL with nullptr to fight some lint errors
Should be useful for google.

Test Plan:
ran db_bench and check stats.
make all check

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8637/"
,,Rocksdb,"[RocksDB] Refactor table.cc to reduce code duplication and improve readability.

Summary: In table.cc, the code section that reads in BlockContent and then put it into a Block, appears at least 4 times. This is too much duplication. BlockReader is much shorter after the change and reads way better. D10077 attempted that for index block read. This is a complete cleanup.

Test Plan: make check; ./db_stress

Reviewers: dhruba, sheki

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10527/Codemod NULL to nullptr

Summary:
scripted NULL to nullptr in
* include/leveldb/
* db/
* table/
* util/

Test Plan: make all check

Reviewers: dhruba, emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9003/"
,,Rocksdb,"[RocksDB] Refactor table.cc to reduce code duplication and improve readability.

Summary: In table.cc, the code section that reads in BlockContent and then put it into a Block, appears at least 4 times. This is too much duplication. BlockReader is much shorter after the change and reads way better. D10077 attempted that for index block read. This is a complete cleanup.

Test Plan: make check; ./db_stress

Reviewers: dhruba, sheki

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10527/"
Threads Management,Threads Management,Rocksdb,"[RocksDB] Expose compaction stalls via db_statistics

Test Plan: make check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10575/[RocksDB] Clear Archive WAL files

Summary:
WAL files are moved to archive directory and clear only at DB::Open.
Can lead to a lot of space consumption in a Database. Added logic to periodically clear Archive Directory too.

Test Plan: make all check + add unit test

Reviewers: dhruba, heyongqiang

Reviewed By: heyongqiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10617/[Rocksdb] Support Merge operation in rocksdb

Summary:
This diff introduces a new Merge operation into rocksdb.
The purpose of this review is mostly getting feedback from the team (everyone please) on the design.

Please focus on the four files under include/leveldb/, as they spell the client visible interface change.

Please go over local/my_test.cc carefully, as it is a concerete use case.

Please also review the impelmentation files to see if the straw man implementation makes sense.

Note that, the diff does pass all make check and truly supports forward iterator over db and a version
of Get that's based on iterator.

Future work:
- Integration with compaction
- A raw Get implementation

I am working on a wiki that explains the design and implementation choices, but coding comes
just naturally and I think it might be a good idea to share the code earlier. The code is
heavily commented.

Test Plan: run all local tests

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: leveldb, zshao, sheki, emayanke, MarkCallaghan

Differential Revision: https://reviews.facebook.net/D9651/[Rockdsdb] Better Error messages. Closing db instead of deleting db

Summary: A better error message. A local change. Did not look at other places where this could be done.

Test Plan: compile

Reviewers: dhruba, MarkCallaghan

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10251/[RocksDB] [Performance] Speed up FindObsoleteFiles

Summary:
FindObsoleteFiles was slow, holding the single big lock, resulted in bad p99 behavior.
Didn't profile anything, but several things could be improved:
1. VersionSet::AddLiveFiles works with std::set, which is by itself slow (a tree).
   You also don't know how many dynamic allocations occur just for building up this tree.
   switched to std::vector, also added logic to pre-calculate total size and do just one allocation
2. Don't see why env_->GetChildren() needs to be mutex proteced, moved to PurgeObsoleteFiles where
   mutex could be unlocked.
3. switched std::set to std:unordered_set, the conversion from vector is also inside PurgeObsoleteFiles
I have a feeling this should pretty much fix it.

Test Plan: make check;  db_stress

Reviewers: dhruba, heyongqiang, MarkCallaghan

Reviewed By: dhruba

CC: leveldb, zshao

Differential Revision: https://reviews.facebook.net/D10197/Prevent segfault in OpenCompactionOutputFile

Summary:
The segfault was happening because the program was unable to open a new
sst file (as part of the compaction) because the process ran out of
file descriptors.

The fix is to check the return status of the file creation before taking
any other action.

Program received signal SIGSEGV, Segmentation fault.
[Switching to Thread 0x7fabf03f9700 (LWP 29904)]
leveldb::DBImpl::OpenCompactionOutputFile (this=this@entry=0x7fabf9011400, compact=compact@entry=0x7fabf741a2b0) at db/db_impl.cc:1399
1399    db/db_impl.cc: No such file or directory.
(gdb) where

Test Plan: make check

Reviewers: MarkCallaghan, sheki

Reviewed By: MarkCallaghan

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10101/[RocksDB][Bug] Look at all the files, not just the first file in TransactionLogIter as BatchWrites can leave it in Limbo

Summary:
Transaction Log Iterator did not move to the next file in the series if there was a write batch at the end of the currentFile.
The solution is if the last seq no. of the current file is < RequestedSeqNo. Assume the first seqNo. of the next file has to satisfy the request.

Also major refactoring around the code. Moved opening the logreader to a seperate function, got rid of goto.

Test Plan: added a unit test for it.

Reviewers: dhruba, heyongqiang

Reviewed By: heyongqiang

CC: leveldb, emayanke

Differential Revision: https://reviews.facebook.net/D10029/Let's get rid of delete as much as possible, here are some examples.

Summary:
If a class owns an object:
 - If the object can be null => use a unique_ptr. no delete
 - If the object can not be null => don't even need new, let alone delete
 - for runtime sized array => use vector, no delete.

Test Plan: make check

Reviewers: dhruba, heyongqiang

Reviewed By: heyongqiang

CC: leveldb, zshao, sheki, emayanke, MarkCallaghan

Differential Revision: https://reviews.facebook.net/D9783/Use non-mmapd files for Write-Ahead Files

Summary:
Use non mmapd files for Write-Ahead log.
Earlier use of MMaped files. made the log iterator read ahead and miss records.
Now the reader and writer will point to the same physical location.

Test Plan: unit test included

Reviewers: dhruba, heyongqiang

Reviewed By: heyongqiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9741/[RocksDB] Minimize Mutex protected code section in the critical path

Summary: rocksdb uses a single global lock to protect in memory metadata. We should minimize the mutex protected code section to increase the effective parallelism of the program. See https://our.intern.facebook.com/intern/tasks/?t=2218928

Test Plan:
make check
db_bench

Reviewers: dhruba, heyongqiang

CC: zshao, leveldb

Differential Revision: https://reviews.facebook.net/D9705/Run compactions even if workload is readonly or read-mostly.

Summary:
The events that trigger compaction:
* opening the database
* Get -> only if seek compaction is not disabled and other checks are true
* MakeRoomForWrite -> when memtable is full
* BackgroundCall ->
  If the background thread is about to do a compaction run, it schedules
  a new background task to trigger a possible compaction. This will cause
  additional background threads to find and process other compactions that
  can run concurrently.

Test Plan: ran db_bench with overwrite and readonly alternatively.

Reviewers: sheki, MarkCallaghan

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9579/Ability to configure bufferedio-reads, filesystem-readaheads and  mmap-read-write per database.

Summary:
This patch allows an application to specify whether to use bufferedio,
reads-via-mmaps and writes-via-mmaps per database. Earlier, there
was a global static variable that was used to configure this functionality.

The default setting remains the same (and is backward compatible):
 1. use bufferedio
 2. do not use mmaps for reads
 3. use mmap for writes
 4. use readaheads for reads needed for compaction

I also added a parameter to db_bench to be able to explicitly specify
whether to do readaheads for compactions or not.

Test Plan: make check

Reviewers: sheki, heyongqiang, MarkCallaghan

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9429/Ignore a zero-sized file while looking for a seq-no in GetUpdatesSince

Summary:
Rocksdb can create 0 sized log files when it is opened and closed without any operations.
The GetUpdatesSince fails currently if there is a log file of size zero.

This diff fixes this. If there is a log file is 0, it is removed form the probable_file_list

Test Plan: unit test

Reviewers: dhruba, heyongqiang

Reviewed By: heyongqiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9507/Do not allow Transaction Log Iterator to fall ahead when writer is writing the same file

Summary:
Store the last flushed, seq no. in db_impl. Check against it in
transaction Log iterator. Do not attempt to read ahead if we do not know
if the data is flushed completely.
Does not work if flush is disabled. Any ideas on fixing that?
* Minor change, iter->Next is called the first time automatically for
* the first time.

Test Plan:
existing test pass.
More ideas on testing this?
Planning to run some stress test.

Reviewers: dhruba, heyongqiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9087/Fox db_stress crash by copying keys before changing sequencenum to zero.

Summary:
The compaction process zeros out sequence numbers if the output is
part of the bottommost level.
The Slice is supposed to refer to an immutable data buffer. The
merger that implements the priority queue while reading kvs as
the input of a compaction run reies on this fact. The bug was that
were updating the sequence number of a record in-place and that was
causing suceeding invocations of the merger to return kvs in
arbitrary order of sequence numbers.
The fix is to copy the key to a local memory buffer before setting
its seqno to 0.

Test Plan:
Set Options.purge_redundant_kvs_while_flush = false and then run
db_stress --ops_per_thread=1000 --max_key=320

Reviewers: emayanke, sheki

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9147/Add rate_delay_limit_milliseconds

Summary:
This adds the rate_delay_limit_milliseconds option to make the delay
configurable in MakeRoomForWrite when the max compaction score is too high.
This delay is called the Ln slowdown. This change also counts the Ln slowdown
per level to make it possible to see where the stalls occur.

From IO-bound performance testing, the Level N stalls occur:
* with compression -> at the largest uncompressed level. This makes sense
                      because compaction for compressed levels is much
                      slower. When Lx is uncompressed and Lx+1 is compressed
                      then files pile up at Lx because the (Lx,Lx+1)->Lx+1
                      compaction process is the first to be slowed by
                      compression.
* without compression -> at level 1

Task ID: #1832108

Blame Rev:

Test Plan:
run with real data, added test

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D9045/Abilty to support upto a million .sst files in the database

Summary:
There was an artifical limit of 50K files per database. This is
insifficient if the database is 1 TB in size and each file is 2 MB.

Test Plan: make check

Reviewers: sheki, emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8919/Counters for bytes written and read.

Summary:
* Counters for bytes read and write.
as a part of this diff, I want to=>
* Measure compaction times. @dhruba can you point which function, should
* I time to get Compaction-times. Was looking at CompactRange.

Test Plan: db_test

Reviewers: dhruba, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8763/Introduce histogram  in statistics.h

Summary:
* Introduce is histogram in statistics.h
* stop watch to measure time.
* introduce two timers as a poc.
Replaced NULL with nullptr to fight some lint errors
Should be useful for google.

Test Plan:
ran db_bench and check stats.
make all check

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8637/Zero out redundant sequence numbers for kvs to increase compression efficiency

Summary:
The sequence numbers in each record eat up plenty of space on storage.
The optimization zeroes out sequence numbers on kvs in the Lmax
layer that are earlier than the earliest snapshot.

Test Plan: Unit test attached.

Differential Revision: https://reviews.facebook.net/D8619/Fix for the weird behaviour encountered by ldb Get where it could read only the second-latest value

Summary:
flush_on_destroy has a default value of false and the memtable is flushed
in the dbimpl-destructor only when that is set to true. Because we want the memtable to be flushed everytime that
the destructor is called(db is closed) and the cases where we work with the memtable only are very less
it is a good idea to give this a default value of true. Thus the put from ldb
wil have its data flushed to disk in the destructor and the next Get will be able to
read it when opened with OpenForReadOnly. The reason that ldb could read the latest value when
the db was opened in the normal Open mode is that the Get from normal Open first reads
the memtable and directly finds the latest value written there and the Get from OpenForReadOnly
doesn't have access to the memtable (which is correct because all its Put/Modify) are disabled

Test Plan: make all; ldb put and get and scans

Reviewers: dhruba, heyongqiang, sheki

Reviewed By: heyongqiang

CC: kosievdmerwe, zshao, dilipj, kailiu

Differential Revision: https://reviews.facebook.net/D8631/"
Database Management,"DataBase Management,Thread management",Rocksdb,"[RocksDB] Fix ReadMissing in db_bench

Summary: D8943 Broke read_missing. Fix it by adding a ""."" at the end of the generated key

Test Plan: generate, print and check the key has a "".""

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10455/Add --writes_per_second rate limit, print p99.99 in histogram

Summary:
Adds the --writes_per_second rate limit for the readwhilewriting test.
The purpose is to optionally avoid saturating storage with writes & compaction
and test read response time when some writes are being done.

Changes the histogram code to also print the p99.99 value

Task ID: #

Blame Rev:

Test Plan:
make check, ran db_bench with it

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10305/Ability to configure bufferedio-reads, filesystem-readaheads and  mmap-read-write per database.

Summary:
This patch allows an application to specify whether to use bufferedio,
reads-via-mmaps and writes-via-mmaps per database. Earlier, there
was a global static variable that was used to configure this functionality.

The default setting remains the same (and is backward compatible):
 1. use bufferedio
 2. do not use mmaps for reads
 3. use mmap for writes
 4. use readaheads for reads needed for compaction

I also added a parameter to db_bench to be able to explicitly specify
whether to do readaheads for compactions or not.

Test Plan: make check

Reviewers: sheki, heyongqiang, MarkCallaghan

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9429/Fixed sign-comparison in rocksdb code-base and fixed Makefile

Summary: Makefile had options to ignore sign-comparisons and unused-parameters, which should be there. Also fixed the specific errors in the code-base

Test Plan: make

Reviewers: chip, dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9531/add --benchmarks=levelstats option to db_bench, prevent ""nan"" in stats output

Summary:
Add --benchmarks=levelstats option to report per-level stats (#files, #bytes)
Change readwhilewriting test to report response time for writes but exclude
them from the stats merged by all threads.
Prevent ""NaN"" in stats output by preventing division by 0.
Remove ""o"" file I committed by mistake.

Task ID: #

Blame Rev:

Test Plan:
make check

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D9513/Enhance db_bench

Summary:
Add --benchmarks=updaterandom for read-modify-write workloads. This is different
from --benchmarks=readrandomwriterandom in a few ways. First, an ""operation"" is the
combined time to do the read & write rather than treating them as two ops. Second,
the same key is used for the read & write.

Change RandomGenerator to support rows larger than 1M. That was using ""assert""
to fail and assert is compiled-away when -DNDEBUG is used.

Add more options to db_bench
--duration - sets the number of seconds for tests to run. When not set the
operation count continues to be the limit. This is used by random operation
tests.

--use_snapshot - when set GetSnapshot() is called prior to each random read.
This is to measure the overhead from using snapshots.

--get_approx - when set GetApproximateSizes() is called prior to each random
read. This is to measure the overhead for a query optimizer.

Task ID: #

Blame Rev:

Test Plan:
run db_bench

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D9267/Add rate_delay_limit_milliseconds

Summary:
This adds the rate_delay_limit_milliseconds option to make the delay
configurable in MakeRoomForWrite when the max compaction score is too high.
This delay is called the Ln slowdown. This change also counts the Ln slowdown
per level to make it possible to see where the stalls occur.

From IO-bound performance testing, the Level N stalls occur:
* with compression -> at the largest uncompressed level. This makes sense
                      because compaction for compressed levels is much
                      slower. When Lx is uncompressed and Lx+1 is compressed
                      then files pile up at Lx because the (Lx,Lx+1)->Lx+1
                      compaction process is the first to be slowed by
                      compression.
* without compression -> at level 1

Task ID: #1832108

Blame Rev:

Test Plan:
run with real data, added test

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D9045/enable the ability to set key size in db_bench in rocksdb

Summary:
1. the default value for key size is still 16
2. enable the ability to set the key size via command line --key_size=

Test Plan:
build & run db_banch and pass some value via command line.
verify it works correctly.

Reviewers: sheki

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8943/[Missed adding cmdline parsing for new flags added in D8685]

Summary:
I had added FLAGS_numdistinct and FLAGS_deletepercent for randomwithverify
but forgot to add cmdline parsing for those flags.

Created bg thread 0x7fc31c7ff700
randomwithverify :       4.920 micros/op 203233 ops/sec; ( get:900000 put:50000 del:50000 total:1000000 found:445522)

Revert Plan: OK

Task ID: #

Reviewers: dhruba, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8769/[Add randomwithverify benchmark option]

Summary: Added RandomWithVerify benchmark option.

Created bg thread 0x7fa9c3fff700
randomwithverify :       5.004 micros/op 199836 ops/sec; ( get:900000 put:80000 del:20000 total:1000000 found:711992)

Revert Plan: OK

Task ID: #

Reviewers: dhruba, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8685/Introduce histogram  in statistics.h

Summary:
* Introduce is histogram in statistics.h
* stop watch to measure time.
* introduce two timers as a poc.
Replaced NULL with nullptr to fight some lint errors
Should be useful for google.

Test Plan:
ran db_bench and check stats.
make all check

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8637/"
Data Conversion,"Data Conversion,Thread management",Rocksdb,"[Rocksdb] Support Merge operation in rocksdb

Summary:
This diff introduces a new Merge operation into rocksdb.
The purpose of this review is mostly getting feedback from the team (everyone please) on the design.

Please focus on the four files under include/leveldb/, as they spell the client visible interface change.
include/leveldb/db.h
include/leveldb/merge_operator.h
include/leveldb/options.h
include/leveldb/write_batch.h

Please go over local/my_test.cc carefully, as it is a concerete use case.

Please also review the impelmentation files to see if the straw man implementation makes sense.

Note that, the diff does pass all make check and truly supports forward iterator over db and a version
of Get that's based on iterator.

Future work:
- Integration with compaction
- A raw Get implementation

I am working on a wiki that explains the design and implementation choices, but coding comes
just naturally and I think it might be a good idea to share the code earlier. The code is
heavily commented.

Test Plan: run all local tests

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: leveldb, zshao, sheki, emayanke, MarkCallaghan

Differential Revision: https://reviews.facebook.net/D9651/Fox db_stress crash by copying keys before changing sequencenum to zero.

Summary:
The compaction process zeros out sequence numbers if the output is
part of the bottommost level.
The Slice is supposed to refer to an immutable data buffer. The
merger that implements the priority queue while reading kvs as
the input of a compaction run reies on this fact. The bug was that
were updating the sequence number of a record in-place and that was
causing suceeding invocations of the merger to return kvs in
arbitrary order of sequence numbers.
The fix is to copy the key to a local memory buffer before setting
its seqno to 0.

Test Plan:
Set Options.purge_redundant_kvs_while_flush = false and then run
db_stress --ops_per_thread=1000 --max_key=320

Reviewers: emayanke, sheki

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9147/Zero out redundant sequence numbers for kvs to increase compression efficiency

Summary:
The sequence numbers in each record eat up plenty of space on storage.
The optimization zeroes out sequence numbers on kvs in the Lmax
layer that are earlier than the earliest snapshot.

Test Plan: Unit test attached.

Differential Revision: https://reviews.facebook.net/D8619/"
,,Rocksdb,"Ability to configure bufferedio-reads, filesystem-readaheads and  mmap-read-write per database.

Summary:
This patch allows an application to specify whether to use bufferedio,
reads-via-mmaps and writes-via-mmaps per database. Earlier, there
was a global static variable that was used to configure this functionality.

The default setting remains the same (and is backward compatible):
 1. use bufferedio
 2. do not use mmaps for reads
 3. use mmap for writes
 4. use readaheads for reads needed for compaction

I also added a parameter to db_bench to be able to explicitly specify
whether to do readaheads for compactions or not.

Test Plan: make check

Reviewers: sheki, heyongqiang, MarkCallaghan

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9429/Refactor statistics. Remove individual functions like incNumFileOpens

Summary:
Use only the counter mechanism. Do away with
incNumFileOpens, incNumFileClose, incNumFileErrors
s/NULL/nullptr/g in db/table_cache.cc

Test Plan: make clean check

Reviewers: dhruba, heyongqiang, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8841/"
,,Rocksdb,"Introduce histogram  in statistics.h

Summary:
* Introduce is histogram in statistics.h
* stop watch to measure time.
* introduce two timers as a poc.
Replaced NULL with nullptr to fight some lint errors
Should be useful for google.

Test Plan:
ran db_bench and check stats.
make all check

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8637/"
,,Rocksdb,"[Rocksdb] Support Merge operation in rocksdb

Summary:
This diff introduces a new Merge operation into rocksdb.
The purpose of this review is mostly getting feedback from the team (everyone please) on the design.

Please focus on the four files under include/leveldb/, as they spell the client visible interface change.
include/leveldb/db.h
include/leveldb/merge_operator.h
include/leveldb/options.h
include/leveldb/write_batch.h

Please go over local/my_test.cc carefully, as it is a concerete use case.

Please also review the impelmentation files to see if the straw man implementation makes sense.

Note that, the diff does pass all make check and truly supports forward iterator over db and a version
of Get that's based on iterator.

Future work:
- Integration with compaction
- A raw Get implementation

I am working on a wiki that explains the design and implementation choices, but coding comes
just naturally and I think it might be a good idea to share the code earlier. The code is
heavily commented.

Test Plan: run all local tests

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: leveldb, zshao, sheki, emayanke, MarkCallaghan

Differential Revision: https://reviews.facebook.net/D9651/Ability for rocksdb to compact when flushing the in-memory memtable to a file in L0.

Summary:
Rocks accumulates recent writes and deletes in the in-memory memtable.
When the memtable is full, it writes the contents on the memtable to
a file in L0.

This patch removes redundant records at the time of the flush. If there
are multiple versions of the same key in the memtable, then only the
most recent one is dumped into the output file. The purging of
redundant records occur only if the most recent snapshot is earlier
than the earliest record in the memtable.

Should we switch on this feature by default or should we keep this feature
turned off in the default settings?

Test Plan: Added test case to db_test.cc

Reviewers: sheki, vamsi, emayanke, heyongqiang

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8991/"
,,Rocksdb,"[Rocksdb] Support Merge operation in rocksdb

Summary:
This diff introduces a new Merge operation into rocksdb.
The purpose of this review is mostly getting feedback from the team (everyone please) on the design.

Please focus on the four files under include/leveldb/, as they spell the client visible interface change.
include/leveldb/db.h
include/leveldb/merge_operator.h
include/leveldb/options.h
include/leveldb/write_batch.h

Please go over local/my_test.cc carefully, as it is a concerete use case.

Please also review the impelmentation files to see if the straw man implementation makes sense.

Note that, the diff does pass all make check and truly supports forward iterator over db and a version
of Get that's based on iterator.

Future work:
- Integration with compaction
- A raw Get implementation

I am working on a wiki that explains the design and implementation choices, but coding comes
just naturally and I think it might be a good idea to share the code earlier. The code is
heavily commented.

Test Plan: run all local tests

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: leveldb, zshao, sheki, emayanke, MarkCallaghan

Differential Revision: https://reviews.facebook.net/D9651/Fix for the weird behaviour encountered by ldb Get where it could read only the second-latest value

Summary:
Changed the Get and Scan options with openForReadOnly mode to have access to the memtable.
Changed the visibility of NewInternalIterator in db_impl from private to protected so that
the derived class db_impl_read_only can call that in its NewIterator function for the
scan case. The previous approach which changed the default for flush_on_destroy_ from false to true
caused many problems in the unit tests due to empty sst files that it created. All
unit tests pass now.

Test Plan: make clean; make all check; ldb put and get and scans

Reviewers: dhruba, heyongqiang, sheki

Reviewed By: dhruba

CC: kosievdmerwe, zshao, dilipj, kailiu

Differential Revision: https://reviews.facebook.net/D8697/"
,,Rocksdb,"[RocksDB][Bug] Look at all the files, not just the first file in TransactionLogIter as BatchWrites can leave it in Limbo

Summary:
Transaction Log Iterator did not move to the next file in the series if there was a write batch at the end of the currentFile.
The solution is if the last seq no. of the current file is < RequestedSeqNo. Assume the first seqNo. of the next file has to satisfy the request.

Also major refactoring around the code. Moved opening the logreader to a seperate function, got rid of goto.

Test Plan: added a unit test for it.

Reviewers: dhruba, heyongqiang

Reviewed By: heyongqiang

CC: leveldb, emayanke

Differential Revision: https://reviews.facebook.net/D10029/TransactionLogIter should stall at the last record. Currently it errors out

Summary:
* Add a method to check if the log reader is at EOF.
* If we know a record has been flushed force the log_reader to believe it is not at EOF, using a new method UnMarkEof().

This does not work with MMpaed files.

Test Plan: added a unit test.

Reviewers: dhruba, heyongqiang

Reviewed By: heyongqiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9567/Do not allow Transaction Log Iterator to fall ahead when writer is writing the same file

Summary:
Store the last flushed, seq no. in db_impl. Check against it in
transaction Log iterator. Do not attempt to read ahead if we do not know
if the data is flushed completely.
Does not work if flush is disabled. Any ideas on fixing that?
* Minor change, iter->Next is called the first time automatically for
* the first time.

Test Plan:
existing test pass.
More ideas on testing this?
Planning to run some stress test.

Reviewers: dhruba, heyongqiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9087/Codemod NULL to nullptr

Summary:
scripted NULL to nullptr in
* include/leveldb/
* db/
* table/
* util/

Test Plan: make all check

Reviewers: dhruba, emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9003/"
,,Rocksdb,"Ability for rocksdb to compact when flushing the in-memory memtable to a file in L0.

Summary:
Rocks accumulates recent writes and deletes in the in-memory memtable.
When the memtable is full, it writes the contents on the memtable to
a file in L0.

This patch removes redundant records at the time of the flush. If there
are multiple versions of the same key in the memtable, then only the
most recent one is dumped into the output file. The purging of
redundant records occur only if the most recent snapshot is earlier
than the earliest record in the memtable.

Should we switch on this feature by default or should we keep this feature
turned off in the default settings?

Test Plan: Added test case to db_test.cc

Reviewers: sheki, vamsi, emayanke, heyongqiang

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8991/"
,,Rocksdb,"Fix more signed-unsigned comparisons

Summary: Some comparisons left in log_test.cc and db_test.cc complained by make

Test Plan: make

Reviewers: dhruba, sheki

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D9537/"
,,Rocksdb,"[Rocksdb] Support Merge operation in rocksdb

Summary:
This diff introduces a new Merge operation into rocksdb.
The purpose of this review is mostly getting feedback from the team (everyone please) on the design.

Please focus on the four files under include/leveldb/, as they spell the client visible interface change.
include/leveldb/db.h
include/leveldb/merge_operator.h
include/leveldb/options.h
include/leveldb/write_batch.h

Please go over local/my_test.cc carefully, as it is a concerete use case.

Please also review the impelmentation files to see if the straw man implementation makes sense.

Note that, the diff does pass all make check and truly supports forward iterator over db and a version
of Get that's based on iterator.

Future work:
- Integration with compaction
- A raw Get implementation

I am working on a wiki that explains the design and implementation choices, but coding comes
just naturally and I think it might be a good idea to share the code earlier. The code is
heavily commented.

Test Plan: run all local tests

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: leveldb, zshao, sheki, emayanke, MarkCallaghan

Differential Revision: https://reviews.facebook.net/D9651/Fix for the weird behaviour encountered by ldb Get where it could read only the second-latest value

Summary:
Changed the Get and Scan options with openForReadOnly mode to have access to the memtable.
Changed the visibility of NewInternalIterator in db_impl from private to protected so that
the derived class db_impl_read_only can call that in its NewIterator function for the
scan case. The previous approach which changed the default for flush_on_destroy_ from false to true
caused many problems in the unit tests due to empty sst files that it created. All
unit tests pass now.

Test Plan: make clean; make all check; ldb put and get and scans

Reviewers: dhruba, heyongqiang, sheki

Reviewed By: dhruba

CC: kosievdmerwe, zshao, dilipj, kailiu

Differential Revision: https://reviews.facebook.net/D8697/Revert ""Fix for the weird behaviour encountered by ldb Get where it could read only the second-latest value""

This reverts commit 4c696ed0018800b62e2448a4ead438255140fc25./Fix for the weird behaviour encountered by ldb Get where it could read only the second-latest value

Summary:
flush_on_destroy has a default value of false and the memtable is flushed
in the dbimpl-destructor only when that is set to true. Because we want the memtable to be flushed everytime that
the destructor is called(db is closed) and the cases where we work with the memtable only are very less
it is a good idea to give this a default value of true. Thus the put from ldb
wil have its data flushed to disk in the destructor and the next Get will be able to
read it when opened with OpenForReadOnly. The reason that ldb could read the latest value when
the db was opened in the normal Open mode is that the Get from normal Open first reads
the memtable and directly finds the latest value written there and the Get from OpenForReadOnly
doesn't have access to the memtable (which is correct because all its Put/Modify) are disabled

Test Plan: make all; ldb put and get and scans

Reviewers: dhruba, heyongqiang, sheki

Reviewed By: heyongqiang

CC: kosievdmerwe, zshao, dilipj, kailiu

Differential Revision: https://reviews.facebook.net/D8631/"
Compression tasks,"Compression tasks, Thread management",Rocksdb,"[RocksDB] Cleanup compaction filter to use a class interface, instead of function pointer and additional context pointer.

Summary:
This diff replaces compaction_filter_args and CompactionFilter with a single compaction_filter parameter. It gives CompactionFilter better encapsulation and a similar look to Comparator and MergeOpertor, which improves consistency of the overall interface.
The change is not backward compatible. Nevertheless, the two references in fbcode are not in production yet.

Test Plan: make check

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, zshao

Differential Revision: https://reviews.facebook.net/D10773/[RocksDB] fix compaction filter trigger condition

Summary:
Currently, compaction filter is run on internal key older than the oldest snapshot, which is incorrect.
Compaction filter should really be run on the most recent internal key when there is no external snapshot.

Test Plan: make check; db_stress

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D10641/[RocksDB] Clear Archive WAL files

Summary:
WAL files are moved to archive directory and clear only at DB::Open.
Can lead to a lot of space consumption in a Database. Added logic to periodically clear Archive Directory too.

Test Plan: make all check + add unit test

Reviewers: dhruba, heyongqiang

Reviewed By: heyongqiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10617/[Rocksdb] Support Merge operation in rocksdb

Summary:
This diff introduces a new Merge operation into rocksdb.
The purpose of this review is mostly getting feedback from the team (everyone please) on the design.

Please focus on the four files under include/leveldb/, as they spell the client visible interface change.
include/leveldb/db.h
include/leveldb/merge_operator.h
include/leveldb/options.h
include/leveldb/write_batch.h

Please go over local/my_test.cc carefully, as it is a concerete use case.

Please also review the impelmentation files to see if the straw man implementation makes sense.

Note that, the diff does pass all make check and truly supports forward iterator over db and a version
of Get that's based on iterator.

Future work:
- Integration with compaction
- A raw Get implementation

I am working on a wiki that explains the design and implementation choices, but coding comes
just naturally and I think it might be a good idea to share the code earlier. The code is
heavily commented.

Test Plan: run all local tests

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: leveldb, zshao, sheki, emayanke, MarkCallaghan

Differential Revision: https://reviews.facebook.net/D9651/[RocksDB][Bug] Look at all the files, not just the first file in TransactionLogIter as BatchWrites can leave it in Limbo

Summary:
Transaction Log Iterator did not move to the next file in the series if there was a write batch at the end of the currentFile.
The solution is if the last seq no. of the current file is < RequestedSeqNo. Assume the first seqNo. of the next file has to satisfy the request.

Also major refactoring around the code. Moved opening the logreader to a seperate function, got rid of goto.

Test Plan: added a unit test for it.

Reviewers: dhruba, heyongqiang

Reviewed By: heyongqiang

CC: leveldb, emayanke

Differential Revision: https://reviews.facebook.net/D10029/[Rocksdb] Recover last updated sequence number from manifest also.

Summary:
During recovery, last_updated_manifest number was not set if there were no records in the Write-ahead log.
Now check for the recovered manifest also and set last_updated_manifest file to the max value.

Test Plan: unit test

Reviewers: heyongqiang

Reviewed By: heyongqiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9891/[Rocksdb] Fix Crash on finding a db with no log files. Error out instead

Summary:
If the vector returned by GetUpdatesSince is empty, it is still returned to the
user. This causes it throw an std::range error.
The probable file list is checked and it returns an IOError status instead of OK now.

Test Plan: added a unit test.

Reviewers: dhruba, heyongqiang

Reviewed By: heyongqiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9771/Use non-mmapd files for Write-Ahead Files

Summary:
Use non mmapd files for Write-Ahead log.
Earlier use of MMaped files. made the log iterator read ahead and miss records.
Now the reader and writer will point to the same physical location.

There is no perf regression :
./db_bench --benchmarks=fillseq --db=/dev/shm/mmap_test --num=$(million 20) --use_existing_db=0 --threads=2
with This diff :
fillseq      :      10.756 micros/op 185281 ops/sec;   20.5 MB/s
without this dif :
fillseq      :      11.085 micros/op 179676 ops/sec;   19.9 MB/s

Test Plan: unit test included

Reviewers: dhruba, heyongqiang

Reviewed By: heyongqiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9741/TransactionLogIter should stall at the last record. Currently it errors out

Summary:
* Add a method to check if the log reader is at EOF.
* If we know a record has been flushed force the log_reader to believe it is not at EOF, using a new method UnMarkEof().

This does not work with MMpaed files.

Test Plan: added a unit test.

Reviewers: dhruba, heyongqiang

Reviewed By: heyongqiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9567/Ability to configure bufferedio-reads, filesystem-readaheads and  mmap-read-write per database.

Summary:
This patch allows an application to specify whether to use bufferedio,
reads-via-mmaps and writes-via-mmaps per database. Earlier, there
was a global static variable that was used to configure this functionality.

The default setting remains the same (and is backward compatible):
 1. use bufferedio
 2. do not use mmaps for reads
 3. use mmap for writes
 4. use readaheads for reads needed for compaction

I also added a parameter to db_bench to be able to explicitly specify
whether to do readaheads for compactions or not.

Test Plan: make check

Reviewers: sheki, heyongqiang, MarkCallaghan

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9429/Ignore a zero-sized file while looking for a seq-no in GetUpdatesSince

Summary:
Rocksdb can create 0 sized log files when it is opened and closed without any operations.
The GetUpdatesSince fails currently if there is a log file of size zero.

This diff fixes this. If there is a log file is 0, it is removed form the probable_file_list

Test Plan: unit test

Reviewers: dhruba, heyongqiang

Reviewed By: heyongqiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9507/Do not allow Transaction Log Iterator to fall ahead when writer is writing the same file

Summary:
Store the last flushed, seq no. in db_impl. Check against it in
transaction Log iterator. Do not attempt to read ahead if we do not know
if the data is flushed completely.
Does not work if flush is disabled. Any ideas on fixing that?
* Minor change, iter->Next is called the first time automatically for
* the first time.

Test Plan:
existing test pass.
More ideas on testing this?
Planning to run some stress test.

Reviewers: dhruba, heyongqiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9087/Add rate_delay_limit_milliseconds

Summary:
This adds the rate_delay_limit_milliseconds option to make the delay
configurable in MakeRoomForWrite when the max compaction score is too high.
This delay is called the Ln slowdown. This change also counts the Ln slowdown
per level to make it possible to see where the stalls occur.

From IO-bound performance testing, the Level N stalls occur:
* with compression -> at the largest uncompressed level. This makes sense
                      because compaction for compressed levels is much
                      slower. When Lx is uncompressed and Lx+1 is compressed
                      then files pile up at Lx because the (Lx,Lx+1)->Lx+1
                      compaction process is the first to be slowed by
                      compression.
* without compression -> at level 1

Task ID: #1832108

Blame Rev:

Test Plan:
run with real data, added test

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D9045/Ability for rocksdb to compact when flushing the in-memory memtable to a file in L0.

Summary:
Rocks accumulates recent writes and deletes in the in-memory memtable.
When the memtable is full, it writes the contents on the memtable to
a file in L0.

This patch removes redundant records at the time of the flush. If there
are multiple versions of the same key in the memtable, then only the
most recent one is dumped into the output file. The purging of
redundant records occur only if the most recent snapshot is earlier
than the earliest record in the memtable.

Should we switch on this feature by default or should we keep this feature
turned off in the default settings?

Test Plan: Added test case to db_test.cc

Reviewers: sheki, vamsi, emayanke, heyongqiang

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D8991/Codemod NULL to nullptr

Summary:
scripted NULL to nullptr in
* include/leveldb/
* db/
* table/
* util/

Test Plan: make all check

Reviewers: dhruba, emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9003/Zero out redundant sequence numbers for kvs to increase compression efficiency

Summary:
The sequence numbers in each record eat up plenty of space on storage.
The optimization zeroes out sequence numbers on kvs in the Lmax
layer that are earlier than the earliest snapshot.

Test Plan: Unit test attached.

Differential Revision: https://reviews.facebook.net/D8619/"
Threads Management,Threads Management,Rocksdb,"[Rocksdb] Support Merge operation in rocksdb

Summary:
This diff introduces a new Merge operation into rocksdb.
The purpose of this review is mostly getting feedback from the team (everyone please) on the design.

Please focus on the four files under include/leveldb/, as they spell the client visible interface change.
include/leveldb/db.h
include/leveldb/merge_operator.h
include/leveldb/options.h
include/leveldb/write_batch.h

Please go over local/my_test.cc carefully, as it is a concerete use case.

Please also review the impelmentation files to see if the straw man implementation makes sense.

Note that, the diff does pass all make check and truly supports forward iterator over db and a version
of Get that's based on iterator.

Future work:
- Integration with compaction
- A raw Get implementation

I am working on a wiki that explains the design and implementation choices, but coding comes
just naturally and I think it might be a good idea to share the code earlier. The code is
heavily commented.

Test Plan: run all local tests

Reviewers: dhruba, heyongqiang

Reviewed By: dhruba

CC: leveldb, zshao, sheki, emayanke, MarkCallaghan

Differential Revision: https://reviews.facebook.net/D9651/[RocksDB] [Performance] Speed up FindObsoleteFiles

Summary:
FindObsoleteFiles was slow, holding the single big lock, resulted in bad p99 behavior.
Didn't profile anything, but several things could be improved:
1. VersionSet::AddLiveFiles works with std::set, which is by itself slow (a tree).
   You also don't know how many dynamic allocations occur just for building up this tree.
   switched to std::vector, also added logic to pre-calculate total size and do just one allocation
2. Don't see why env_->GetChildren() needs to be mutex proteced, moved to PurgeObsoleteFiles where
   mutex could be unlocked.
3. switched std::set to std:unordered_set, the conversion from vector is also inside PurgeObsoleteFiles
I have a feeling this should pretty much fix it.

Test Plan: make check;  db_stress

Reviewers: dhruba, heyongqiang, MarkCallaghan

Reviewed By: dhruba

CC: leveldb, zshao

Differential Revision: https://reviews.facebook.net/D10197/Ability to configure bufferedio-reads, filesystem-readaheads and  mmap-read-write per database.

Summary:
This patch allows an application to specify whether to use bufferedio,
reads-via-mmaps and writes-via-mmaps per database. Earlier, there
was a global static variable that was used to configure this functionality.

The default setting remains the same (and is backward compatible):
 1. use bufferedio
 2. do not use mmaps for reads
 3. use mmap for writes
 4. use readaheads for reads needed for compaction

I also added a parameter to db_bench to be able to explicitly specify
whether to do readaheads for compactions or not.

Test Plan: make check

Reviewers: sheki, heyongqiang, MarkCallaghan

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9429/DO not report level size as zero when there are no files in L0

Summary:
Instead of checking for number of files in L0. Check for number of files in the requested level.

Bug introduced in D4929 (diff trying to do too many things).

Test Plan: db_test.

Reviewers: dhruba, MarkCallaghan

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D9483/Prevent segfault because SizeUnderCompaction was called without any locks.

Summary:
SizeBeingCompacted was called without any lock protection. This causes
crashes, especially when running db_bench with value_size=128K.
The fix is to compute SizeUnderCompaction while holding the mutex and
passing in these values into the call to Finalize.


Test Plan:
make check

I am running db_bench with a value size of 128K to see if the segfault is fixed.

Reviewers: MarkCallaghan, sheki, emayanke

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9279/A mechanism to detect manifest file write errors and put db in readonly mode.

Summary:
If there is an error while writing an edit to the manifest file, the manifest
file is closed and reopened to check if the edit made it in. However, if the
re-opening of the manifest is unsuccessful and options.paranoid_checks is set
t true, then the db refuses to accept new puts, effectively putting the db
in readonly mode.

In a future diff, I would like to make the default value of paranoid_check
to true.

Test Plan: make check

Reviewers: sheki

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9201/Codemod NULL to nullptr

Summary:
scripted NULL to nullptr in

Test Plan: make all check

Reviewers: dhruba, emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9003/Fix unit test failure in db_filename.cc

Summary:
    c_test: db/filename.cc:74: std::string leveldb::DescriptorFileName(const string&,....

Test Plan:
  this is a failure in a unit test

Differential Revision: https://reviews.facebook.net/D8667/"
,,Rocksdb,"Fix valgrind errors in rocksdb tests: auto_roll_logger_test, reduce_levels_test

Summary: Fix for memory leaks in rocksdb tests. Also modified the variable NUM_FAILED_TESTS to print the actual number of failed tests.

Test Plan: make <test>; valgrind --leak-check=full ./<test>

Reviewers: sheki, dhruba

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9333/"
,,Rocksdb,"TransactionLogIter should stall at the last record. Currently it errors out

Summary:
* Add a method to check if the log reader is at EOF.
* If we know a record has been flushed force the log_reader to believe it is not at EOF, using a new method UnMarkEof().

This does not work with MMpaed files.

Test Plan: added a unit test.

Reviewers: dhruba, heyongqiang

Reviewed By: heyongqiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D9567/"
,,Rocksdb,"[RocksDB] Improve sst_dump to take user key range

Summary: The ability to dump internal keys associated with certain user keys, directly from sst files, is very useful for diagnosis. Will incorporate it directly into ldb later.

Test Plan: run it

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12075/"
API Management,"API Usage, Thread management",Rocksdb,"Add prefix scans to db_stress (and bug fix in prefix scan)

Summary: Added support for prefix scans.

Test Plan: ./db_stress --max_key=4096 --ops_per_thread=10000

Reviewers: dhruba, vamsi

Reviewed By: vamsi

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12267/Use KeyMayExist for WriteBatch-Deletes

Summary:
Introduced KeyMayExist checking during writebatch-delete and removed from Outer Delete API because it uses writebatch-delete.
Added code to skip getting Table from disk if not already present in table_cache.
Some renaming of variables.
Introduced KeyMayExistImpl which allows checking since specified sequence number in GetImpl useful to check partially written writebatch.
Changed KeyMayExist to not be pure virtual and provided a default implementation.
Expanded unit-tests in db_test to check appropriately.
Ran db_stress for 1 hour with ./db_stress --max_key=100000 --ops_per_thread=10000000 --delpercent=50 --filter_deletes=1 --statistics=1.

Test Plan: db_stress;make check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D11745/Make rocksdb-deletes faster using bloom filter

Summary:
Wrote a new function in db_impl.c-CheckKeyMayExist that calls Get but with a new parameter turned on which makes Get return false only if bloom filters can guarantee that key is not in database. Delete calls this function and if the option- deletes_use_filter is turned on and CheckKeyMayExist returns false, the delete will be dropped saving:
1. Put of delete type
2. Space in the db,and
3. Compaction time

Test Plan:
make all check;
will run db_stress and db_bench and enhance unit-test once the basic design gets approved

Reviewers: dhruba, haobo, vamsi

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11607/Renamed 'hybrid_compaction' tp be ""Universal Compaction'.

Summary:
All the universal compaction parameters are encapsulated in
a new file universal_compaction.h

Test Plan:
make check/[RocksDB] Add mmap_read option for db_stress

Summary: as title, also removed an incorrect assertion

Test Plan: make check; db_stress --mmap_read=1; db_stress --mmap_read=0

Reviewers: dhruba, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11367/Enhance dbstress to allow specifying compaction trigger for L0.

Summary:
Rocksdb allos specifying the number of files in L0 that triggers
compactions. Expose this api as a command line parameter for
running db_stress.

Test Plan: Run test

Reviewers: sheki, emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11343/[Kill randomly at various points in source code for testing]

Summary:
This is initial version. A few ways in which this could
be extended in the future are:
(a) Killing from more places in source code
(b) Hashing stack and using that hash in determining whether to crash.
    This is to avoid crashing more often at source lines that are executed
    more often.
(c) Raising exceptions or returning errors instead of killing

Test Plan:
This whole thing is for testing.

Here is part of output:

python2.7 tools/db_crashtest2.py -d 600
Running db_stress

Created bg thread 0x7ff0137ff700
No lock creation because test_batches_snapshots set
2013/04/26-17:56:15  Starting database operations
... finished 90000 ops

Revert Plan: OK

Task ID: #2252691

Reviewers: dhruba, emayanke

Reviewed By: emayanke

CC: leveldb, haobo

Differential Revision: https://reviews.facebook.net/D10581/"
Threads Management,Threads Management,Rocksdb,"[RocksDB] Fix PosixLogger and AutoRollLogger thread safety

Summary:
PosixLogger and AutoRollLogger do not seem to be thread safe.
For PosixLogger, log_size_ is not atomically updated.
For AutoRollLogger, the underlying logger_ might be deleted by
one thread while still being accessed by another.

Test Plan: make check

Reviewers: kailiu, dhruba, heyongqiang

Reviewed By: kailiu

CC: leveldb, zshao, sheki

Differential Revision: https://reviews.facebook.net/D9699/"
,,Rocksdb,"[RocksDB] Simplify StopWatch implementation

Summary:
Make stop watch a simple implementation, instead of subclass of a virtual class
Allocate stop watches off the stack instead of heap.
Code is more terse now.

Test Plan: make all check, db_bench with --statistics=1

Reviewers: haobo, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10809/"
,,Rocksdb,"[RocksDB] Improve manifest dump to print internal keys in hex for version edits.

Summary: Currently, VersionEdit::DebugString always display internal keys in the original ascii format. This could cause manifest dump to be truncated if internal keys contain special charactors (like null). Also added an option --input_key_hex for ldb idump to indicate that the passed in user keys are in hex.

Test Plan: run ldb manifest_dump

Reviewers: dhruba, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12111/[RocksDB] Support internal key/value dump for ldb

Summary: This diff added a command 'idump' to ldb tool, which dumps the internal key/value pairs. It could be useful for diagnosis and estimating the per user key 'overhead'. Also cleaned up the ldb code a bit where I touched.

Test Plan: make check; ldb idump

Reviewers: emayanke, sheki, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11517/Introducing timeranged scan, timeranged dump in ldb. Also the ability to count in time-batches during Dump

Summary:
Scan and Dump commands in ldb use iterator. We need to also print timestamp for ttl databases for debugging. For this I create a TtlIterator class pointer in these functions and assign it the value of Iterator pointer which actually points to t TtlIterator object, and access the new function ValueWithTS which can return TS also. Buckets feature for dump command: gives a count of different key-values in the specified time-range distributed across the time-range partitioned according to bucket-size. start_time and end_time are specified in unixtimestamp and bucket in seconds on the user-commandline
Have commented out 3 ines from ldb_test.py so that the test does not break right now. It breaks because timestamp is also printed now and I have to look at wildcards in python to compare properly.

Test Plan: python tools/ldb_test.py

Reviewers: vamsi, dhruba, haobo, sheki

Reviewed By: vamsi

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11403/"
,,Rocksdb,"Make arena block size configurable

Summary:
Add an option for arena block size, default value 4096 bytes. Arena will allocate blocks with such size.

I am not sure about passing parameter to skiplist in the new virtualized framework, though I talked to Jim a bit. So add Jim as reviewer.

Test Plan:
new unit test, I am running db_test.

For passing paramter from configured option to Arena, I tried tests like:

  TEST(DBTest, Arena_Option) {
  std::string dbname = test::TmpDir() + ""/db_arena_option_test"";
  DestroyDB(dbname, Options());

  DB* db = nullptr;
  Options opts;
  opts.create_if_missing = true;
  opts.arena_block_size = 1000000; // tested 99, 999999
  Status s = DB::Open(opts, dbname, &db);
  db->Put(WriteOptions(), ""a"", ""123"");
  }

and printed some debug info. The results look good. Any suggestion for such a unit-test?

Reviewers: haobo, dhruba, emayanke, jpaton

Reviewed By: dhruba

CC: leveldb, zshao

Differential Revision: https://reviews.facebook.net/D11799/"
,,Rocksdb,"[RocksDB] Improve manifest dump to print internal keys in hex for version edits.

Summary: Currently, VersionEdit::DebugString always display internal keys in the original ascii format. This could cause manifest dump to be truncated if internal keys contain special charactors (like null). Also added an option --input_key_hex for ldb idump to indicate that the passed in user keys are in hex.

Test Plan: run ldb manifest_dump

Reviewers: dhruba, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12111/[RocksDB] Support internal key/value dump for ldb

Summary: This diff added a command 'idump' to ldb tool, which dumps the internal key/value pairs. It could be useful for diagnosis and estimating the per user key 'overhead'. Also cleaned up the ldb code a bit where I touched.

Test Plan: make check; ldb idump

Reviewers: emayanke, sheki, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11517/Simplify bucketing logic in ldb-ttl

Summary: [start_time, end_time) is waht I'm following for the buckets and the whole time-range. Also cleaned up some code in db_ttl.* Not correcting the spacing/indenting convention for util/ldb_cmd.cc in this diff.

Test Plan: python ldb_test.py, make ttl_test, Run mcrocksdb-backup tool, Run the ldb tool on 2 mcrocksdb production backups form sigmafio033.prn1

Reviewers: vamsi, haobo

Reviewed By: vamsi

Differential Revision: https://reviews.facebook.net/D11433/Introducing timeranged scan, timeranged dump in ldb. Also the ability to count in time-batches during Dump

Summary:
Scan and Dump commands in ldb use iterator. We need to also print timestamp for ttl databases for debugging. For this I create a TtlIterator class pointer in these functions and assign it the value of Iterator pointer which actually points to t TtlIterator object, and access the new function ValueWithTS which can return TS also. Buckets feature for dump command: gives a count of different key-values in the specified time-range distributed across the time-range partitioned according to bucket-size. start_time and end_time are specified in unixtimestamp and bucket in seconds on the user-commandline
Have commented out 3 ines from ldb_test.py so that the test does not break right now. It breaks because timestamp is also printed now and I have to look at wildcards in python to compare properly.

Test Plan: python tools/ldb_test.py

Reviewers: vamsi, dhruba, haobo, sheki

Reviewed By: vamsi

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11403/"
,,Rocksdb,"[RocksDB] Support internal key/value dump for ldb

Summary: This diff added a command 'idump' to ldb tool, which dumps the internal key/value pairs. It could be useful for diagnosis and estimating the per user key 'overhead'. Also cleaned up the ldb code a bit where I touched.

Test Plan: make check; ldb idump

Reviewers: emayanke, sheki, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11517/"
Memory management,Memory management,Rocksdb,"Add options to dump.

Summary: added options to Dump() I missed in D12027.  I also ran a script to look for other missing options and found a couple which I added.  Should we also print anything for ""PrepareForBulkLoad"", ""memtable_factory"", and ""statistics""?  Or should we leave those alone since it's not easy to print useful info for those?

Test Plan: run anything and look at LOG file to make sure these are printed now.

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12219/Separate compaction filter for each compaction

Summary:
If we have same compaction filter for each compaction,
application cannot know about the different compaction processes.
Later on, we can put in more details in compaction filter for the
application to consume and use it according to its needs. For e.g. In
the universal compaction, we have a compaction process involving all the
files while others don't involve all the files. Applications may want to
collect some stats only when during full compaction.

Test Plan: run existing unit tests

Reviewers: haobo, dhruba

Reviewed By: dhruba

CC: xinyaohu, leveldb

Differential Revision: https://reviews.facebook.net/D12057/Fix unit tests/bugs for universal compaction (first step)

Summary:
This is the first step to fix unit tests and bugs for universal
compactiion. I added universal compaction option to ChangeOptions(), and
fixed all unit tests calling ChangeOptions(). Some of these tests
obviously assume more than 1 level and check file number/values in level
1 or above levels. I set kSkipUniversalCompaction for these tests.

The major bug I found is manual compaction with universal compaction never stops. I have put a fix for
it.

I have also set universal compaction as the default compaction and found
at least 20+ unit tests failing. I haven't looked into the details. The
next step is to check all unit tests without calling ChangeOptions().

Test Plan: make all check

Reviewers: dhruba, haobo

Differential Revision: https://reviews.facebook.net/D12051/Add soft and hard rate limit support

Summary:
This diff adds support for both soft and hard rate limiting. The following changes are included:

1) Options.rate_limit is renamed to Options.hard_rate_limit.
2) Options.rate_limit_delay_milliseconds is renamed to Options.rate_limit_delay_max_milliseconds.
3) Options.soft_rate_limit is added.
4) If the maximum compaction score is > hard_rate_limit and rate_limit_delay_max_milliseconds == 0, then writes are delayed by 1 ms at a time until the max compaction score falls below hard_rate_limit.
5) If the max compaction score is > soft_rate_limit but <= hard_rate_limit, then writes are delayed by 0-1 ms depending on how close we are to hard_rate_limit.
6) Users can disable 4 by setting hard_rate_limit = 0. They can add a limit to the maximum amount of time waited by setting rate_limit_delay_max_milliseconds > 0. Thus, the old behavior can be preserved by setting soft_rate_limit = 0, which is the default.

Test Plan:
make -j32 check
./db_stress

Reviewers: dhruba, haobo, MarkCallaghan

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12003/Make arena block size configurable

Summary:
Add an option for arena block size, default value 4096 bytes. Arena will allocate blocks with such size.

I am not sure about passing parameter to skiplist in the new virtualized framework, though I talked to Jim a bit. So add Jim as reviewer.

Test Plan:
new unit test, I am running db_test.

For passing paramter from configured option to Arena, I tried tests like:

  TEST(DBTest, Arena_Option) {
  std::string dbname = test::TmpDir() + ""/db_arena_option_test"";
  DestroyDB(dbname, Options());

  DB* db = nullptr;
  Options opts;
  opts.create_if_missing = true;
  opts.arena_block_size = 1000000; // tested 99, 999999
  Status s = DB::Open(opts, dbname, &db);
  db->Put(WriteOptions(), ""a"", ""123"");
  }

and printed some debug info. The results look good. Any suggestion for such a unit-test?

Reviewers: haobo, dhruba, emayanke, jpaton

Reviewed By: dhruba

CC: leveldb, zshao

Differential Revision: https://reviews.facebook.net/D11799/Virtualize SkipList Interface

Summary: This diff virtualizes the skiplist interface so that users can provide their own implementation of a backing store for MemTables. Eventually, the backing store will be responsible for its own synchronization, allowing users (and us) to experiment with different lockless implementations.

Test Plan:
make clean
make -j32 check
./db_stress

Reviewers: dhruba, emayanke, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11739/Use KeyMayExist for WriteBatch-Deletes

Summary:
Introduced KeyMayExist checking during writebatch-delete and removed from Outer Delete API because it uses writebatch-delete.
Added code to skip getting Table from disk if not already present in table_cache.
Some renaming of variables.
Introduced KeyMayExistImpl which allows checking since specified sequence number in GetImpl useful to check partially written writebatch.
Changed KeyMayExist to not be pure virtual and provided a default implementation.
Expanded unit-tests in db_test to check appropriately.
Ran db_stress for 1 hour with ./db_stress --max_key=100000 --ops_per_thread=10000000 --delpercent=50 --filter_deletes=1 --statistics=1.

Test Plan: db_stress;make check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D11745/Fix merge problems with options.

Summary:
Fix merge problems with options.

Test Plan:

Reviewers:

CC:

Task ID: #

Blame Rev:/Make rocksdb-deletes faster using bloom filter

Summary:
Wrote a new function in db_impl.c-CheckKeyMayExist that calls Get but with a new parameter turned on which makes Get return false only if bloom filters can guarantee that key is not in database. Delete calls this function and if the option- deletes_use_filter is turned on and CheckKeyMayExist returns false, the delete will be dropped saving:
1. Put of delete type
2. Space in the db,and
3. Compaction time

Test Plan:
make all check;
will run db_stress and db_bench and enhance unit-test once the basic design gets approved

Reviewers: dhruba, haobo, vamsi

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11607/Renamed 'hybrid_compaction' tp be ""Universal Compaction'.

Summary:
All the universal compaction parameters are encapsulated in
a new file universal_compaction.h

Test Plan:
make check/[RocksDB] Option for incremental sync

Summary: This diff added an option to control the incremenal sync frequency. db_bench has a new flag bytes_per_sync for easy tuning exercise.

Test Plan: make check; db_bench

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11295/Compact multiple memtables before flushing to storage.

Summary:
Merge multiple multiple memtables in memory before writing it
out to a file in L0.

There is a new config parameter min_write_buffer_number_to_merge
that specifies the number of write buffers that should be merged
together to a single file in storage. The system will not flush
wrte buffers to storage unless at least these many buffers have
accumulated in memory.
The default value of this new parameter is 1, which means that
a write buffer will be immediately flushed to disk as soon it is
ready.

Test Plan: make check

Differential Revision: https://reviews.facebook.net/D11241/[RocksDB] [Performance] Allow different posix advice to be applied to the same table file

Summary:
Current posix advice implementation ties up the access pattern hint with the creation of a file.
It is not possible to apply different advice for different access (random get vs compaction read),
without keeping two open files for the same table. This patch extended the RandomeAccessFile interface
to accept new access hint at anytime. Particularly, we are able to set different access hint on the same
table file based on when/how the file is used.
Two options are added to set the access hint, after the file is first opened and after the file is being
compacted.

Test Plan: make check; db_stress; db_bench

Reviewers: dhruba

Reviewed By: dhruba

CC: MarkCallaghan, leveldb

Differential Revision: https://reviews.facebook.net/D10905/add block deviation option to terminate a block before it exceeds block_size

Summary: a new option block_size_deviation is added.

Test Plan: run db_test and db_bench

Reviewers: dhruba, haobo

Reviewed By: haobo

Differential Revision: https://reviews.facebook.net/D10821/[RocksDB] dump leveldb.stats periodically in LOG file.

Summary:
Added an option stats_dump_period_sec to dump leveldb.stats to LOG periodically for diagnosis.
By defauly, it's set to a very big number 3600 (1 hour).

Test Plan: make check;

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, zshao

Differential Revision: https://reviews.facebook.net/D10761/"
,,Rocksdb,"[RocksDB] Include 64bit random number generator

Summary: As title.

Test Plan: make check;

Reviewers: chip, MarkCallaghan

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11061/"
Database Management,"Database Management, Thread management",Rocksdb,"[RocksDB] Option for incremental sync

Summary: This diff added an option to control the incremenal sync frequency. db_bench has a new flag bytes_per_sync for easy tuning exercise.

Test Plan: make check; db_bench

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11295/[RocksDB] Sync file to disk incrementally

Summary:
During compaction, we sync the output files after they are fully written out. This causes unnecessary blocking of the compaction thread and burstiness of the write traffic.
This diff simply asks the OS to sync data incrementally as they are written, on the background. The hope is that, at the final sync, most of the data are already on disk and we would block less on the sync call. Thus, each compaction runs faster and we could use fewer number of compaction threads to saturate IO.
In addition, the write traffic will be smoothed out, hopefully reducing the IO P99 latency too.

Some quick tests show 10~20% improvement in per thread compaction throughput. Combined with posix advice on compaction read, just 5 threads are enough to almost saturate the udb flash bandwidth for 800 bytes write only benchmark.
What's more promising is that, with saturated IO, iostat shows average wait time is actually smoother and much smaller.
For the write only test 800bytes test:
Before the change:  await  occillate between 10ms and 3ms
After the change: await ranges 1-3ms

Will test against read-modify-write workload too, see if high read latency P99 could be resolved.

Will introduce a parameter to control the sync interval in a follow up diff after cleaning up EnvOptions.

Test Plan: make check; db_bench; db_stress

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11115/[RocksDB] cleanup EnvOptions

Summary:
This diff simplifies EnvOptions by treating it as POD, similar to Options.
- virtual functions are removed and member fields are accessed directly.
- StorageOptions is removed.
- Options.allow_readahead and Options.allow_readahead_compactions are deprecated.
- Unused global variables are removed: useOsBuffer, useFsReadAhead, useMmapRead, useMmapWrite

Test Plan: make check; db_stress

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11175/[RocksDB] [Performance] Allow different posix advice to be applied to the same table file

Summary:
Current posix advice implementation ties up the access pattern hint with the creation of a file.
It is not possible to apply different advice for different access (random get vs compaction read),
without keeping two open files for the same table. This patch extended the RandomeAccessFile interface
to accept new access hint at anytime. Particularly, we are able to set different access hint on the same
table file based on when/how the file is used.
Two options are added to set the access hint, after the file is first opened and after the file is being
compacted.

Test Plan: make check; db_stress; db_bench

Reviewers: dhruba

Reviewed By: dhruba

CC: MarkCallaghan, leveldb

Differential Revision: https://reviews.facebook.net/D10905/[Kill randomly at various points in source code for testing]

Summary:
This is initial version. A few ways in which this could
be extended in the future are:
(a) Killing from more places in source code
(b) Hashing stack and using that hash in determining whether to crash.
    This is to avoid crashing more often at source lines that are executed
    more often.
(c) Raising exceptions or returning errors instead of killing

Test Plan:
This whole thing is for testing.

Here is part of output:

python2.7 tools/db_crashtest2.py -d 600
Running db_stress

------------------------------------------------
Created bg thread 0x7ff0137ff700
No lock creation because test_batches_snapshots set
2013/04/26-17:56:15  Starting database operations
... finished 90000 ops

Revert Plan: OK

Task ID: #2252691

Reviewers: dhruba, emayanke

Reviewed By: emayanke

CC: leveldb, haobo

Differential Revision: https://reviews.facebook.net/D10581/"
,,Rocksdb,"[RocksDB] cleanup EnvOptions

Summary:
This diff simplifies EnvOptions by treating it as POD, similar to Options.
- virtual functions are removed and member fields are accessed directly.
- StorageOptions is removed.
- Options.allow_readahead and Options.allow_readahead_compactions are deprecated.
- Unused global variables are removed: useOsBuffer, useFsReadAhead, useMmapRead, useMmapWrite

Test Plan: make check; db_stress

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11175/"
,,Rocksdb,"[RocksDB] [MergeOperator] The new Merge Interface! Uses merge sequences.

Summary:
Here are the major changes to the Merge Interface. It has been expanded
to handle cases where the MergeOperator is not associative. It does so by stacking
up merge operations while scanning through the key history (i.e.: during Get() or
Compaction), until a valid Put/Delete/end-of-history is encountered; it then
applies all of the merge operations in the correct sequence starting with the
base/sentinel value.

I have also introduced an ""AssociativeMerge"" function which allows the user to
take advantage of associative merge operations (such as in the case of counters).
The implementation will always attempt to merge the operations/operands themselves
together when they are encountered, and will resort to the ""stacking"" method if
and only if the ""associative-merge"" fails.

This implementation is conjectured to allow MergeOperator to handle the general
case, while still providing the user with the ability to take advantage of certain
efficiencies in their own merge-operator / data-structure.

NOTE: This is a preliminary diff. This must still go through a lot of review,
revision, and testing. Feedback welcome!

Test Plan:
  -This is a preliminary diff. I have only just begun testing/debugging it.
  -I will be testing this with the existing MergeOperator use-cases and unit-tests
(counters, string-append, and redis-lists)
  -I will be ""desk-checking"" and walking through the code with the help gdb.
  -I will find a way of stress-testing the new interface / implementation using
db_bench, db_test, merge_test, and/or db_stress.
  -I will ensure that my tests cover all cases: Get-Memtable,
Get-Immutable-Memtable, Get-from-Disk, Iterator-Range-Scan, Flush-Memtable-to-L0,
Compaction-L0-L1, Compaction-Ln-L(n+1), Put/Delete found, Put/Delete not-found,
end-of-history, end-of-file, etc.
  -A lot of feedback from the reviewers.

Reviewers: haobo, dhruba, zshao, emayanke

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11499/"
Threads Management,Threads Management,Rocksdb,"Use KeyMayExist for WriteBatch-Deletes

Summary:
Introduced KeyMayExist checking during writebatch-delete and removed from Outer Delete API because it uses writebatch-delete.
Added code to skip getting Table from disk if not already present in table_cache.
Some renaming of variables.
Introduced KeyMayExistImpl which allows checking since specified sequence number in GetImpl useful to check partially written writebatch.
Changed KeyMayExist to not be pure virtual and provided a default implementation.
Expanded unit-tests in db_test to check appropriately.
Ran db_stress for 1 hour with ./db_stress --max_key=100000 --ops_per_thread=10000000 --delpercent=50 --filter_deletes=1 --statistics=1.

Test Plan: db_stress;make check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D11745/"
,,Rocksdb,"Make rocksdb-deletes faster using bloom filter

Summary:
Wrote a new function in db_impl.c-CheckKeyMayExist that calls Get but with a new parameter turned on which makes Get return false only if bloom filters can guarantee that key is not in database. Delete calls this function and if the option- deletes_use_filter is turned on and CheckKeyMayExist returns false, the delete will be dropped saving:
1. Put of delete type
2. Space in the db,and
3. Compaction time

Test Plan:
make all check;
will run db_stress and db_bench and enhance unit-test once the basic design gets approved

Reviewers: dhruba, haobo, vamsi

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11607/[rocksdb] names for all metrics provided in statistics.h

Summary: Provide a  map of histograms and ticker vs strings. Fb303 libraries can use this to provide the mapping. We will not have to duplicate the code during release.

Test Plan: db_bench with statistics=1

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11145/"
,,Rocksdb,"Support user's compaction filter in TTL logic

Summary: TTL uses compaction filter to purge key-values and required the user to not pass one. This diff makes it accommodating of user's compaciton filter. Added test to ttl_test

Test Plan: make; ./ttl_test

Reviewers: dhruba, haobo, vamsi

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11973/Make Write API work for TTL databases

Summary: Added logic to make another WriteBatch with Timestamps during the Write function execution in TTL class. Also expanded the ttl_test to test for it. Have done nothing for Merge for now.

Test Plan: make ttl_test;./ttl_test

Reviewers: haobo, vamsi, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10827/"
,,Rocksdb,"[RocksDB] [MergeOperator] The new Merge Interface! Uses merge sequences.

Summary:
Here are the major changes to the Merge Interface. It has been expanded
to handle cases where the MergeOperator is not associative. It does so by stacking
up merge operations while scanning through the key history (i.e.: during Get() or
Compaction), until a valid Put/Delete/end-of-history is encountered; it then
applies all of the merge operations in the correct sequence starting with the
base/sentinel value.

I have also introduced an ""AssociativeMerge"" function which allows the user to
take advantage of associative merge operations (such as in the case of counters).
The implementation will always attempt to merge the operations/operands themselves
together when they are encountered, and will resort to the ""stacking"" method if
and only if the ""associative-merge"" fails.

This implementation is conjectured to allow MergeOperator to handle the general
case, while still providing the user with the ability to take advantage of certain
efficiencies in their own merge-operator / data-structure.

NOTE: This is a preliminary diff. This must still go through a lot of review,
revision, and testing. Feedback welcome!

Test Plan:
  -This is a preliminary diff. I have only just begun testing/debugging it.
  -I will be testing this with the existing MergeOperator use-cases and unit-tests
(counters, string-append, and redis-lists)
  -I will be ""desk-checking"" and walking through the code with the help gdb.
  -I will find a way of stress-testing the new interface / implementation using
db_bench, db_test, merge_test, and/or db_stress.
  -I will ensure that my tests cover all cases: Get-Memtable,
Get-Immutable-Memtable, Get-from-Disk, Iterator-Range-Scan, Flush-Memtable-to-L0,
Compaction-L0-L1, Compaction-Ln-L(n+1), Put/Delete found, Put/Delete not-found,
end-of-history, end-of-file, etc.
  -A lot of feedback from the reviewers.

Reviewers: haobo, dhruba, zshao, emayanke

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11499/Support user's compaction filter in TTL logic

Summary: TTL uses compaction filter to purge key-values and required the user to not pass one. This diff makes it accommodating of user's compaciton filter. Added test to ttl_test

Test Plan: make; ./ttl_test

Reviewers: dhruba, haobo, vamsi

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11973/Merge operator for ttl

Summary: Implemented a TtlMergeOperator class which inherits from MergeOperator and is TTL aware. It strips out timestamp from existing_value and attaches timestamp to new_value, calling user-provided-Merge in between.

Test Plan: make all check

Reviewers: haobo, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11775/Expand KeyMayExist to return the proper value if it can be found in memory and also check block_cache

Summary: Removed KeyMayExistImpl because KeyMayExist demanded Get like semantics now. Removed no_io from memtable and imm because we need the proper value now and shouldn't just stop when we see Merge in memtable. Added checks to block_cache. Updated documentation and unit-test

Test Plan: make all check;db_stress for 1 hour

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11853/Make rocksdb-deletes faster using bloom filter

Summary:
Wrote a new function in db_impl.c-CheckKeyMayExist that calls Get but with a new parameter turned on which makes Get return false only if bloom filters can guarantee that key is not in database. Delete calls this function and if the option- deletes_use_filter is turned on and CheckKeyMayExist returns false, the delete will be dropped saving:
1. Put of delete type
2. Space in the db,and
3. Compaction time

Test Plan:
make all check;
will run db_stress and db_bench and enhance unit-test once the basic design gets approved

Reviewers: dhruba, haobo, vamsi

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11607/"
,,Rocksdb,"Expose base db object from ttl wrapper

Summary: rocksdb replicaiton will need this when writing value+TS from master to slave 'as is'

Test Plan: make

Reviewers: dhruba, vamsi, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11919/Merge operator for ttl

Summary: Implemented a TtlMergeOperator class which inherits from MergeOperator and is TTL aware. It strips out timestamp from existing_value and attaches timestamp to new_value, calling user-provided-Merge in between.

Test Plan: make all check

Reviewers: haobo, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11775/Expand KeyMayExist to return the proper value if it can be found in memory and also check block_cache

Summary: Removed KeyMayExistImpl because KeyMayExist demanded Get like semantics now. Removed no_io from memtable and imm because we need the proper value now and shouldn't just stop when we see Merge in memtable. Added checks to block_cache. Updated documentation and unit-test

Test Plan: make all check;db_stress for 1 hour

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11853/Make rocksdb-deletes faster using bloom filter

Summary:
Wrote a new function in db_impl.c-CheckKeyMayExist that calls Get but with a new parameter turned on which makes Get return false only if bloom filters can guarantee that key is not in database. Delete calls this function and if the option- deletes_use_filter is turned on and CheckKeyMayExist returns false, the delete will be dropped saving:
1. Put of delete type
2. Space in the db,and
3. Compaction time

Test Plan:
make all check;
will run db_stress and db_bench and enhance unit-test once the basic design gets approved

Reviewers: dhruba, haobo, vamsi

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11607/Simplify bucketing logic in ldb-ttl

Summary: [start_time, end_time) is waht I'm following for the buckets and the whole time-range. Also cleaned up some code in db_ttl.* Not correcting the spacing/indenting convention for util/ldb_cmd.cc in this diff.

Test Plan: python ldb_test.py, make ttl_test, Run mcrocksdb-backup tool, Run the ldb tool on 2 mcrocksdb production backups form sigmafio033.prn1

Reviewers: vamsi, haobo

Reviewed By: vamsi

Differential Revision: https://reviews.facebook.net/D11433/Very basic Multiget and simple test cases.

Summary:
Implemented the MultiGet operator which takes in a list of keys
and returns their associated values. Currently uses std::vector as its
container data structure. Otherwise, it works identically to ""Get"".

Test Plan:
 1. make db_test      ; compile it
 2. ./db_test         ; test it
 3. make all check    ; regress / run all tests
 4. make release      ; (optional) compile with release settings

Reviewers: haobo, MarkCallaghan, dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10875/"
Memory management,Memory management,Rocksdb,"Prefix filters for scans (v4)

Summary: Similar to v2 (db and table code understands prefixes), but use ReadOptions as in v3.  Also, make the CreateFilter code faster and cleaner.

Test Plan: make db_test; export LEVELDB_TESTS=PrefixScan; ./db_test

Reviewers: dhruba

Reviewed By: dhruba

CC: haobo, emayanke

Differential Revision: https://reviews.facebook.net/D12027/Expand KeyMayExist to return the proper value if it can be found in memory and also check block_cache

Summary: Removed KeyMayExistImpl because KeyMayExist demanded Get like semantics now. Removed no_io from memtable and imm because we need the proper value now and shouldn't just stop when we see Merge in memtable. Added checks to block_cache. Updated documentation and unit-test

Test Plan: make all check;db_stress for 1 hour

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11853/Use KeyMayExist for WriteBatch-Deletes

Summary:
Introduced KeyMayExist checking during writebatch-delete and removed from Outer Delete API because it uses writebatch-delete.
Added code to skip getting Table from disk if not already present in table_cache.
Some renaming of variables.
Introduced KeyMayExistImpl which allows checking since specified sequence number in GetImpl useful to check partially written writebatch.
Changed KeyMayExist to not be pure virtual and provided a default implementation.
Expanded unit-tests in db_test to check appropriately.
Ran db_stress for 1 hour with ./db_stress --max_key=100000 --ops_per_thread=10000000 --delpercent=50 --filter_deletes=1 --statistics=1.

Test Plan: db_stress;make check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D11745/Make rocksdb-deletes faster using bloom filter

Summary:
Wrote a new function in db_impl.c-CheckKeyMayExist that calls Get but with a new parameter turned on which makes Get return false only if bloom filters can guarantee that key is not in database. Delete calls this function and if the option- deletes_use_filter is turned on and CheckKeyMayExist returns false, the delete will be dropped saving:
1. Put of delete type
2. Space in the db,and
3. Compaction time

Test Plan:
make all check;
will run db_stress and db_bench and enhance unit-test once the basic design gets approved

Reviewers: dhruba, haobo, vamsi

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11607/[Rocksdb] measure table open io in a histogram

Summary: Table is setup for compaction using Table::SetupForCompaction. So read block calls can be differentiated b/w Gets/Compaction. Use this and measure times.

Test Plan: db_bench --statistics=1

Reviewers: dhruba, haobo

Reviewed By: haobo

CC: leveldb, MarkCallaghan

Differential Revision: https://reviews.facebook.net/D11217/"
,,Rocksdb,"add block deviation option to terminate a block before it exceeds block_size

Summary: a new option block_size_deviation is added.

Test Plan: run db_test and db_bench

Reviewers: dhruba, haobo

Reviewed By: haobo

Differential Revision: https://reviews.facebook.net/D10821/"
Threads Management,Threads Management,Rocksdb,"Add prefix scans to db_stress (and bug fix in prefix scan)

Summary: Added support for prefix scans.

Test Plan: ./db_stress --max_key=4096 --ops_per_thread=10000

Reviewers: dhruba, vamsi

Reviewed By: vamsi

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12267/Prefix filters for scans (v4)

Summary: Similar to v2 (db and table code understands prefixes), but use ReadOptions as in v3.  Also, make the CreateFilter code faster and cleaner.

Test Plan: make db_test; export LEVELDB_TESTS=PrefixScan; ./db_test

Reviewers: dhruba

Reviewed By: dhruba

CC: haobo, emayanke

Differential Revision: https://reviews.facebook.net/D12027/"
,,Rocksdb,"Prefix filters for scans (v4)

Summary: Similar to v2 (db and table code understands prefixes), but use ReadOptions as in v3.  Also, make the CreateFilter code faster and cleaner.

Test Plan: make db_test; export LEVELDB_TESTS=PrefixScan; ./db_test

Reviewers: dhruba

Reviewed By: dhruba

CC: haobo, emayanke

Differential Revision: https://reviews.facebook.net/D12027/[Rocksdb] measure table open io in a histogram

Summary: Table is setup for compaction using Table::SetupForCompaction. So read block calls can be differentiated b/w Gets/Compaction. Use this and measure times.

Test Plan: db_bench --statistics=1

Reviewers: dhruba, haobo

Reviewed By: haobo

CC: leveldb, MarkCallaghan

Differential Revision: https://reviews.facebook.net/D11217/"
,,Rocksdb,"Prefix filters for scans (v4)

Summary: Similar to v2 (db and table code understands prefixes), but use ReadOptions as in v3.  Also, make the CreateFilter code faster and cleaner.

Test Plan: make db_test; export LEVELDB_TESTS=PrefixScan; ./db_test

Reviewers: dhruba

Reviewed By: dhruba

CC: haobo, emayanke

Differential Revision: https://reviews.facebook.net/D12027/"
,Thread management,Rocksdb,"Use KeyMayExist for WriteBatch-Deletes

Summary:
Introduced KeyMayExist checking during writebatch-delete and removed from Outer Delete API because it uses writebatch-delete.
Added code to skip getting Table from disk if not already present in table_cache.
Some renaming of variables.
Introduced KeyMayExistImpl which allows checking since specified sequence number in GetImpl useful to check partially written writebatch.
Changed KeyMayExist to not be pure virtual and provided a default implementation.
Expanded unit-tests in db_test to check appropriately.
Ran db_stress for 1 hour with ./db_stress --max_key=100000 --ops_per_thread=10000000 --delpercent=50 --filter_deletes=1 --statistics=1.

Test Plan: db_stress;make check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D11745/[RocksDB] cleanup EnvOptions

Summary:
This diff simplifies EnvOptions by treating it as POD, similar to Options.
- virtual functions are removed and member fields are accessed directly.
- StorageOptions is removed.
- Options.allow_readahead and Options.allow_readahead_compactions are deprecated.
- Unused global variables are removed: useOsBuffer, useFsReadAhead, useMmapRead, useMmapWrite

Test Plan: make check; db_stress

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11175/"
Memory management,Memory management,Rocksdb,"Prefix filters for scans (v4)

Summary: Similar to v2 (db and table code understands prefixes), but use ReadOptions as in v3.  Also, make the CreateFilter code faster and cleaner.

Test Plan: make db_test; export LEVELDB_TESTS=PrefixScan; ./db_test

Reviewers: dhruba

Reviewed By: dhruba

CC: haobo, emayanke

Differential Revision: https://reviews.facebook.net/D12027/Fix unit tests/bugs for universal compaction (first step)

Summary:
This is the first step to fix unit tests and bugs for universal
compactiion. I added universal compaction option to ChangeOptions(), and
fixed all unit tests calling ChangeOptions(). Some of these tests
obviously assume more than 1 level and check file number/values in level
1 or above levels. I set kSkipUniversalCompaction for these tests.

The major bug I found is manual compaction with universal compaction never stops. I have put a fix for
it.

I have also set universal compaction as the default compaction and found
at least 20+ unit tests failing. I haven't looked into the details. The
next step is to check all unit tests without calling ChangeOptions().

Test Plan: make all check

Reviewers: dhruba, haobo

Differential Revision: https://reviews.facebook.net/D12051/[RocksDB] [MergeOperator] The new Merge Interface! Uses merge sequences.

Summary:
Here are the major changes to the Merge Interface. It has been expanded
to handle cases where the MergeOperator is not associative. It does so by stacking
up merge operations while scanning through the key history (i.e.: during Get() or
Compaction), until a valid Put/Delete/end-of-history is encountered; it then
applies all of the merge operations in the correct sequence starting with the
base/sentinel value.

I have also introduced an ""AssociativeMerge"" function which allows the user to
take advantage of associative merge operations (such as in the case of counters).
The implementation will always attempt to merge the operations/operands themselves
together when they are encountered, and will resort to the ""stacking"" method if
and only if the ""associative-merge"" fails.

This implementation is conjectured to allow MergeOperator to handle the general
case, while still providing the user with the ability to take advantage of certain
efficiencies in their own merge-operator / data-structure.

NOTE: This is a preliminary diff. This must still go through a lot of review,
revision, and testing. Feedback welcome!

Test Plan:
  -This is a preliminary diff. I have only just begun testing/debugging it.
  -I will be testing this with the existing MergeOperator use-cases and unit-tests
(counters, string-append, and redis-lists)
  -I will be ""desk-checking"" and walking through the code with the help gdb.
  -I will find a way of stress-testing the new interface / implementation using
db_bench, db_test, merge_test, and/or db_stress.
  -I will ensure that my tests cover all cases: Get-Memtable,
Get-Immutable-Memtable, Get-from-Disk, Iterator-Range-Scan, Flush-Memtable-to-L0,
Compaction-L0-L1, Compaction-Ln-L(n+1), Put/Delete found, Put/Delete not-found,
end-of-history, end-of-file, etc.
  -A lot of feedback from the reviewers.

Reviewers: haobo, dhruba, zshao, emayanke

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11499/Add soft_rate_limit stats

Summary: This diff adds histogram stats for soft_rate_limit stalls. It also renames the old rate_limit stats to hard_rate_limit.

Test Plan: make -j32 check

Reviewers: dhruba, haobo, MarkCallaghan

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12021/Add soft and hard rate limit support

Summary:
This diff adds support for both soft and hard rate limiting. The following changes are included:

1) Options.rate_limit is renamed to Options.hard_rate_limit.
2) Options.rate_limit_delay_milliseconds is renamed to Options.rate_limit_delay_max_milliseconds.
3) Options.soft_rate_limit is added.
4) If the maximum compaction score is > hard_rate_limit and rate_limit_delay_max_milliseconds == 0, then writes are delayed by 1 ms at a time until the max compaction score falls below hard_rate_limit.
5) If the max compaction score is > soft_rate_limit but <= hard_rate_limit, then writes are delayed by 0-1 ms depending on how close we are to hard_rate_limit.
6) Users can disable 4 by setting hard_rate_limit = 0. They can add a limit to the maximum amount of time waited by setting rate_limit_delay_max_milliseconds > 0. Thus, the old behavior can be preserved by setting soft_rate_limit = 0, which is the default.

Test Plan:
make -j32 check
./db_stress

Reviewers: dhruba, haobo, MarkCallaghan

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12003/Expand KeyMayExist to return the proper value if it can be found in memory and also check block_cache

Summary: Removed KeyMayExistImpl because KeyMayExist demanded Get like semantics now. Removed no_io from memtable and imm because we need the proper value now and shouldn't just stop when we see Merge in memtable. Added checks to block_cache. Updated documentation and unit-test

Test Plan: make all check;db_stress for 1 hour

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11853/Slow down writes gradually rather than suddenly

Summary:
Currently, when a certain number of level0 files (level0_slowdown_writes_trigger) are present, RocksDB will slow down each write by 1ms. There is a second limit of level0 files at which RocksDB will stop writes altogether (level0_stop_writes_trigger).

This patch enables the user to supply a third parameter specifying the number of files at which Rocks will start slowing down writes (level0_start_slowdown_writes). When this number is reached, Rocks will slow down writes as a quadratic function of level0_slowdown_writes_trigger - num_level0_files.

For some workloads, this improves latency and throughput. I will post some stats momentarily in https://our.intern.facebook.com/intern/tasks/?t=2613384.

Test Plan:
make -j32 check
./db_stress
./db_bench

Reviewers: dhruba, haobo, MarkCallaghan, xjin

Reviewed By: xjin

CC: leveldb, xjin, zshao

Differential Revision: https://reviews.facebook.net/D11859/Make arena block size configurable

Summary:
Add an option for arena block size, default value 4096 bytes. Arena will allocate blocks with such size.

I am not sure about passing parameter to skiplist in the new virtualized framework, though I talked to Jim a bit. So add Jim as reviewer.

Test Plan:
new unit test, I am running db_test.

For passing paramter from configured option to Arena, I tried tests like:

  TEST(DBTest, Arena_Option) {
  std::string dbname = test::TmpDir() + ""/db_arena_option_test"";
  DestroyDB(dbname, Options());

  DB* db = nullptr;
  Options opts;
  opts.create_if_missing = true;
  opts.arena_block_size = 1000000; // tested 99, 999999
  Status s = DB::Open(opts, dbname, &db);
  db->Put(WriteOptions(), ""a"", ""123"");
  }

and printed some debug info. The results look good. Any suggestion for such a unit-test?

Reviewers: haobo, dhruba, emayanke, jpaton

Reviewed By: dhruba

CC: leveldb, zshao

Differential Revision: https://reviews.facebook.net/D11799/Don't use redundant Env::NowMicros() calls

Summary: After my patch for stall histograms, there are redundant calls to NowMicros() by both the stop watches and DBImpl::MakeRoomForWrites. So I removed the redundant calls such that the information is gotten from the stopwatch.

Test Plan:
make clean
make -j32 check

Reviewers: dhruba, haobo, MarkCallaghan

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11883/Add stall counts to statistics

Summary: Previously, statistics are kept on how much time is spent on stalls of different types. This patch adds support for keeping number of stalls of each type. For example, instead of just reporting how many microseconds are spent waiting for memtables to be compacted, it will also report how many times a write stalled for that to occur.

Test Plan:
make -j32 check
./db_stress

# Not really sure what else should be done...

Reviewers: dhruba, MarkCallaghan, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11841/Virtualize SkipList Interface

Summary: This diff virtualizes the skiplist interface so that users can provide their own implementation of a backing store for MemTables. Eventually, the backing store will be responsible for its own synchronization, allowing users (and us) to experiment with different lockless implementations.

Test Plan:
make clean
make -j32 check
./db_stress

Reviewers: dhruba, emayanke, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11739/If disable wal is set, then batch commits are avoided.

Summary:
rocksdb uses batch commit to write to transaction log. But if
disable wal is set, then writes to transaction log are anyways
avoided. In this case, there is not much value-add to batch things,
batching can cause unnecessary delays to Puts().
This patch avoids batching when disableWal is set.

Test Plan:
make check.

I am running db_stress now.

Reviewers: haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11763/Use KeyMayExist for WriteBatch-Deletes

Summary:
Introduced KeyMayExist checking during writebatch-delete and removed from Outer Delete API because it uses writebatch-delete.
Added code to skip getting Table from disk if not already present in table_cache.
Some renaming of variables.
Introduced KeyMayExistImpl which allows checking since specified sequence number in GetImpl useful to check partially written writebatch.
Changed KeyMayExist to not be pure virtual and provided a default implementation.
Expanded unit-tests in db_test to check appropriately.
Ran db_stress for 1 hour with ./db_stress --max_key=100000 --ops_per_thread=10000000 --delpercent=50 --filter_deletes=1 --statistics=1.

Test Plan: db_stress;make check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D11745/[RocksDB] Enable manual compaction to move files back to an appropriate level.

Summary: As title. This diff added an option reduce_level to CompactRange. When set to true, it will try to move the files back to the minimum level sufficient to hold the data set. Note that the default is set to true now, just to excerise it in all existing tests. Will set the default to false before check-in, for backward compatibility.

Test Plan: make check;

Reviewers: dhruba, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11553/Make rocksdb-deletes faster using bloom filter

Summary:
Wrote a new function in db_impl.c-CheckKeyMayExist that calls Get but with a new parameter turned on which makes Get return false only if bloom filters can guarantee that key is not in database. Delete calls this function and if the option- deletes_use_filter is turned on and CheckKeyMayExist returns false, the delete will be dropped saving:
1. Put of delete type
2. Space in the db,and
3. Compaction time

Test Plan:
make all check;
will run db_stress and db_bench and enhance unit-test once the basic design gets approved

Reviewers: dhruba, haobo, vamsi

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11607/[RocksDB] Provide contiguous sequence number even in case of write failure

Summary: Replication logic would be simplifeid if we can guarantee that write sequence number is always contiguous, even if write failure occurs. Dhruba and I looked at the sequence number generation part of the code. It seems fixable. Note that if WAL was successful and insert into memtable was not, we would be in an unfortunate state. The approach in this diff is : IO error is expected and error status will be returned to client, sequence number will not be advanced; In-mem error is not expected and we panic.

Test Plan: make check; db_stress

Reviewers: dhruba, sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11439/Renamed 'hybrid_compaction' tp be ""Universal Compaction'.

Summary:
All the universal compaction parameters are encapsulated in
a new file universal_compaction.h

Test Plan:
make check/Reduce write amplification by merging files in L0 back into L0

Summary:
There is a new option called hybrid_mode which, when switched on,
causes HBase style compactions.  Files from L0 are
compacted back into L0. This meat of this compaction algorithm
is in PickCompactionHybrid().

All files reside in L0. That means all files have overlapping
keys. Each file has a time-bound, i.e. each file contains a
range of keys that were inserted around the same time. The
start-seqno and the end-seqno refers to the timeframe when
these keys were inserted.  Files that have contiguous seqno
are compacted together into a larger file. All files are
ordered from most recent to the oldest.

The current compaction algorithm starts to look for
candidate files starting from the most recent file. It continues to
add more files to the same compaction run as long as the
sum of the files chosen till now is smaller than the next
candidate file size. This logic needs to be debated
and validated.

The above logic should reduce write amplification to a
large extent... will publish numbers shortly.

Test Plan: dbstress runs for 6 hours with no data corruption (tested so far).

Differential Revision: https://reviews.facebook.net/D11289/[rocksdb][refactor] statistic printing code to one place

Summary: $title

Test Plan: db_bench --statistics=1

Reviewers: haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11373/Compact multiple memtables before flushing to storage.

Summary:
Merge multiple multiple memtables in memory before writing it
out to a file in L0.

There is a new config parameter min_write_buffer_number_to_merge
that specifies the number of write buffers that should be merged
together to a single file in storage. The system will not flush
wrte buffers to storage unless at least these many buffers have
accumulated in memory.
The default value of this new parameter is 1, which means that
a write buffer will be immediately flushed to disk as soon it is
ready.

Test Plan: make check

Differential Revision: https://reviews.facebook.net/D11241/[RocksDB] Compaction Filter Cleanup

Summary: This hopefully gives the right semantics to compaction filter. Will write a small wiki to explain the ideas.

Test Plan: make check; db_stress

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11121/[RocksDB] cleanup EnvOptions

Summary:
This diff simplifies EnvOptions by treating it as POD, similar to Options.
- virtual functions are removed and member fields are accessed directly.
- StorageOptions is removed.
- Options.allow_readahead and Options.allow_readahead_compactions are deprecated.
- Unused global variables are removed: useOsBuffer, useFsReadAhead, useMmapRead, useMmapWrite

Test Plan: make check; db_stress

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11175/Very basic Multiget and simple test cases.

Summary:
Implemented the MultiGet operator which takes in a list of keys
and returns their associated values. Currently uses std::vector as its
container data structure. Otherwise, it works identically to ""Get"".

Test Plan:
 1. make db_test      ; compile it
 2. ./db_test         ; test it
 3. make all check    ; regress / run all tests
 4. make release      ; (optional) compile with release settings

Reviewers: haobo, MarkCallaghan, dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10875/Improve output for GetProperty('leveldb.stats')

Summary:
Display separate values for read, write & total compaction IO.
Display compaction amplification and write amplification.
Add similar values for the period since the last call to GetProperty. Results since the server started
are reported as ""cumulative"" stats. Results since the last call to GetProperty are reported as
""interval"" stats.

Level  Files Size(MB) Time(sec)  Read(MB) Write(MB)    Rn(MB)  Rnp1(MB)  Wnew(MB) Amplify Read(MB/s) Write(MB/s)      Rn     Rnp1     Wnp1     NewW    Count  Ln-stall
----------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0        7       13        21         0       211         0         0       211     0.0       0.0        10.1        0        0        0        0      113       0.0
  1       79      157        88       993       989       198       795       194     9.0      11.3        11.2      106      405      502       97       14       0.0
  2       19       36         5        63        63        37        27        36     2.4      12.3        12.2       19       14       32       18       12       0.0
>>>>>>>>>>>>>>>>>>>>>>>>> text below has been is new and/or reformatted
Uptime(secs): 122.2 total, 0.9 interval
Compaction IO cumulative (GB): 0.21 new, 1.03 read, 1.23 write, 2.26 read+write
Compaction IO cumulative (MB/sec): 1.7 new, 8.6 read, 10.3 write, 19.0 read+write
Amplification cumulative: 6.0 write, 11.0 compaction
Compaction IO interval (MB): 5.59 new, 0.00 read, 5.59 write, 5.59 read+write
Compaction IO interval (MB/sec): 6.5 new, 0.0 read, 6.5 write, 6.5 read+write
Amplification interval: 1.0 write, 1.0 compaction
>>>>>>>>>>>>>>>>>>>>>>>> text above is new and/or reformatted
Stalls(secs): 90.574 level0_slowdown, 0.000 level0_numfiles, 10.165 memtable_compaction, 0.000 leveln_slowdown

Task ID: #

Blame Rev:

Test Plan:
make check, run db_bench

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11049/[RocksDB] Add score column to leveldb.stats

Summary: Added the 'score' column to the compaction stats output, which shows the level total size devided by level target size. Could be useful when monitoring compaction decisions...

Test Plan: make check; db_bench

Reviewers: dhruba

CC: leveldb, MarkCallaghan

Differential Revision: https://reviews.facebook.net/D11025/[RocksDB] Dump counters and histogram data periodically with compaction stats

Summary: As title

Test Plan: make check

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10995/[RocksDB] Fix MaybeDumpStats

Summary: MaybeDumpStats was causing lock problem

Test Plan: make check; db_stress

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D10935/[RocksDB] dump leveldb.stats periodically in LOG file.

Summary:
Added an option stats_dump_period_sec to dump leveldb.stats to LOG periodically for diagnosis.
By defauly, it's set to a very big number 3600 (1 hour).

Test Plan: make check;

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, zshao

Differential Revision: https://reviews.facebook.net/D10761/[RocksDB] Introduce an option to skip log error on recovery

Summary:
Currently, with paranoid_check on, DB::Open will fail on any log read error on recovery.
If client is ok with losing most recent updates, we could simply skip those errors.
However, it's important to introduce an additional flag, so that paranoid_check can
still guard against more serious problems.

Test Plan: make check; db_stress

Reviewers: dhruba, emayanke

Reviewed By: emayanke

CC: leveldb, emayanke

Differential Revision: https://reviews.facebook.net/D10869/"
Memory management,Memory management,Rocksdb,"Add soft and hard rate limit support

Summary:
This diff adds support for both soft and hard rate limiting. The following changes are included:

1) Options.rate_limit is renamed to Options.hard_rate_limit.
2) Options.rate_limit_delay_milliseconds is renamed to Options.rate_limit_delay_max_milliseconds.
3) Options.soft_rate_limit is added.
4) If the maximum compaction score is > hard_rate_limit and rate_limit_delay_max_milliseconds == 0, then writes are delayed by 1 ms at a time until the max compaction score falls below hard_rate_limit.
5) If the max compaction score is > soft_rate_limit but <= hard_rate_limit, then writes are delayed by 0-1 ms depending on how close we are to hard_rate_limit.
6) Users can disable 4 by setting hard_rate_limit = 0. They can add a limit to the maximum amount of time waited by setting rate_limit_delay_max_milliseconds > 0. Thus, the old behavior can be preserved by setting soft_rate_limit = 0, which is the default.

Test Plan:
make -j32 check
./db_stress

Reviewers: dhruba, haobo, MarkCallaghan

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12003/Use KeyMayExist for WriteBatch-Deletes

Summary:
Introduced KeyMayExist checking during writebatch-delete and removed from Outer Delete API because it uses writebatch-delete.
Added code to skip getting Table from disk if not already present in table_cache.
Some renaming of variables.
Introduced KeyMayExistImpl which allows checking since specified sequence number in GetImpl useful to check partially written writebatch.
Changed KeyMayExist to not be pure virtual and provided a default implementation.
Expanded unit-tests in db_test to check appropriately.
Ran db_stress for 1 hour with ./db_stress --max_key=100000 --ops_per_thread=10000000 --delpercent=50 --filter_deletes=1 --statistics=1.

Test Plan: db_stress;make check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D11745/Make rocksdb-deletes faster using bloom filter

Summary:
Wrote a new function in db_impl.c-CheckKeyMayExist that calls Get but with a new parameter turned on which makes Get return false only if bloom filters can guarantee that key is not in database. Delete calls this function and if the option- deletes_use_filter is turned on and CheckKeyMayExist returns false, the delete will be dropped saving:
1. Put of delete type
2. Space in the db,and
3. Compaction time

Test Plan:
make all check;
will run db_stress and db_bench and enhance unit-test once the basic design gets approved

Reviewers: dhruba, haobo, vamsi

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11607/Renamed 'hybrid_compaction' tp be ""Universal Compaction'.

Summary:
All the universal compaction parameters are encapsulated in
a new file universal_compaction.h

Test Plan:
make check/[RocksDB] Option for incremental sync

Summary: This diff added an option to control the incremenal sync frequency. db_bench has a new flag bytes_per_sync for easy tuning exercise.

Test Plan: make check; db_bench

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11295/Compact multiple memtables before flushing to storage.

Summary:
Merge multiple multiple memtables in memory before writing it
out to a file in L0.

There is a new config parameter min_write_buffer_number_to_merge
that specifies the number of write buffers that should be merged
together to a single file in storage. The system will not flush
wrte buffers to storage unless at least these many buffers have
accumulated in memory.
The default value of this new parameter is 1, which means that
a write buffer will be immediately flushed to disk as soon it is
ready.

Test Plan: make check

Differential Revision: https://reviews.facebook.net/D11241/[Rocksdb] Implement filluniquerandom

Summary:
Use a bit set to keep track of which random number is generated.
        Currently only supports single-threaded. All our perf tests are run with threads=1
        Copied over bitset implementation from common/datastructures

Test Plan: printed the generated keys, and verified all keys were present.

Reviewers: MarkCallaghan, haobo, dhruba

Reviewed By: MarkCallaghan

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11247/Fix db_bench for release build.

Test Plan: make release

Reviewers: haobo, dhruba, jpaton

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11307/[Rocksdb] [Multiget] Introduced multiget into db_bench

Summary:
Preliminary! Introduced the --use_multiget=1 and --keys_per_multiget=n
flags for db_bench. Also updated and tested the ReadRandom() method
to include an option to use multiget. By default,
keys_per_multiget=100.

Preliminary tests imply that multiget is at least 1.25x faster per
key than regular get.

Will continue adding Multiget for ReadMissing, ReadHot,
RandomWithVerify, ReadRandomWriteRandom; soon. Will also think
about ways to better verify benchmarks.

Test Plan:
1. make db_bench
2. ./db_bench --benchmarks=fillrandom
3. ./db_bench --benchmarks=readrandom --use_existing_db=1
	      --use_multiget=1 --threads=4 --keys_per_multiget=100
4. ./db_bench --benchmarks=readrandom --use_existing_db=1
	      --threads=4
5. Verify ops/sec (and 1000000 of 1000000 keys found)

Reviewers: haobo, MarkCallaghan, dhruba

Reviewed By: MarkCallaghan

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11127/[RocksDB] cleanup EnvOptions

Summary:
This diff simplifies EnvOptions by treating it as POD, similar to Options.
- virtual functions are removed and member fields are accessed directly.
- StorageOptions is removed.
- Options.allow_readahead and Options.allow_readahead_compactions are deprecated.
- Unused global variables are removed: useOsBuffer, useFsReadAhead, useMmapRead, useMmapWrite

Test Plan: make check; db_stress

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11175/[RocksDB] Introduce Fast Mutex option

Summary:
This diff adds an option to specify whether PTHREAD_MUTEX_ADAPTIVE_NP will be enabled for the rocksdb single big kernel lock. db_bench also have this option now.
Quickly tested 8 thread cpu bound 100 byte random read.
No fast mutex: ~750k/s ops
With fast mutex: ~880k/s ops

Test Plan: make check; db_bench; db_stress

Reviewers: dhruba

CC: MarkCallaghan, leveldb

Differential Revision: https://reviews.facebook.net/D11031/[RocksDB] [Performance] Allow different posix advice to be applied to the same table file

Summary:
Current posix advice implementation ties up the access pattern hint with the creation of a file.
It is not possible to apply different advice for different access (random get vs compaction read),
without keeping two open files for the same table. This patch extended the RandomeAccessFile interface
to accept new access hint at anytime. Particularly, we are able to set different access hint on the same
table file based on when/how the file is used.
Two options are added to set the access hint, after the file is first opened and after the file is being
compacted.

Test Plan: make check; db_stress; db_bench

Reviewers: dhruba

Reviewed By: dhruba

CC: MarkCallaghan, leveldb

Differential Revision: https://reviews.facebook.net/D10905/Ability to set different size fanout multipliers for every level.

Summary:
There is an existing field Options.max_bytes_for_level_multiplier that
sets the multiplier for the size of each level in the database.

This patch introduces the ability to set different multipliers
for every level in the database. The size of a level is determined
by using both max_bytes_for_level_multiplier as well as the
per-level fanout.

size of level[i] = size of level[i-1] * max_bytes_for_level_multiplier
                   * fanout[i-1]

The default value of fanout is 1, so that it is backward compatible.

Test Plan: make check

Reviewers: haobo, emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10863/"
,,Rocksdb,"Expand KeyMayExist to return the proper value if it can be found in memory and also check block_cache

Summary: Removed KeyMayExistImpl because KeyMayExist demanded Get like semantics now. Removed no_io from memtable and imm because we need the proper value now and shouldn't just stop when we see Merge in memtable. Added checks to block_cache. Updated documentation and unit-test

Test Plan: make all check;db_stress for 1 hour

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11853/Make rocksdb-deletes faster using bloom filter

Summary:
Wrote a new function in db_impl.c-CheckKeyMayExist that calls Get but with a new parameter turned on which makes Get return false only if bloom filters can guarantee that key is not in database. Delete calls this function and if the option- deletes_use_filter is turned on and CheckKeyMayExist returns false, the delete will be dropped saving:
1. Put of delete type
2. Space in the db,and
3. Compaction time

Test Plan:
make all check;
will run db_stress and db_bench and enhance unit-test once the basic design gets approved

Reviewers: dhruba, haobo, vamsi

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11607/Compact multiple memtables before flushing to storage.

Summary:
Merge multiple multiple memtables in memory before writing it
out to a file in L0.

There is a new config parameter min_write_buffer_number_to_merge
that specifies the number of write buffers that should be merged
together to a single file in storage. The system will not flush
wrte buffers to storage unless at least these many buffers have
accumulated in memory.
The default value of this new parameter is 1, which means that
a write buffer will be immediately flushed to disk as soon it is
ready.

Test Plan: make check

Differential Revision: https://reviews.facebook.net/D11241/"
,,Rocksdb,"Fix refering freed memory in earlier commit.

Summary: Fix refering freed memory in earlier commit by https://reviews.facebook.net/D11181

Test Plan: make check

Reviewers: haobo, sheki

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11193/"
,Thread management,Rocksdb,"Use KeyMayExist for WriteBatch-Deletes

Summary:
Introduced KeyMayExist checking during writebatch-delete and removed from Outer Delete API because it uses writebatch-delete.
Added code to skip getting Table from disk if not already present in table_cache.
Some renaming of variables.
Introduced KeyMayExistImpl which allows checking since specified sequence number in GetImpl useful to check partially written writebatch.
Changed KeyMayExist to not be pure virtual and provided a default implementation.
Expanded unit-tests in db_test to check appropriately.
Ran db_stress for 1 hour with ./db_stress --max_key=100000 --ops_per_thread=10000000 --delpercent=50 --filter_deletes=1 --statistics=1.

Test Plan: db_stress;make check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D11745/[RocksDB] cleanup EnvOptions

Summary:
This diff simplifies EnvOptions by treating it as POD, similar to Options.
- virtual functions are removed and member fields are accessed directly.
- StorageOptions is removed.
- Options.allow_readahead and Options.allow_readahead_compactions are deprecated.
- Unused global variables are removed: useOsBuffer, useFsReadAhead, useMmapRead, useMmapWrite

Test Plan: make check; db_stress

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11175/[RocksDB] [Performance] Allow different posix advice to be applied to the same table file

Summary:
Current posix advice implementation ties up the access pattern hint with the creation of a file.
It is not possible to apply different advice for different access (random get vs compaction read),
without keeping two open files for the same table. This patch extended the RandomeAccessFile interface
to accept new access hint at anytime. Particularly, we are able to set different access hint on the same
table file based on when/how the file is used.
Two options are added to set the access hint, after the file is first opened and after the file is being
compacted.

Test Plan: make check; db_stress; db_bench

Reviewers: dhruba

Reviewed By: dhruba

CC: MarkCallaghan, leveldb

Differential Revision: https://reviews.facebook.net/D10905/"
Memory management,Memory management,Rocksdb,"Implement log blobs

Summary:
This patch adds the ability for the user to add sequences of arbitrary data (blobs) to write batches. These blobs are saved to the log along with everything else in the write batch. You can add multiple blobs per WriteBatch and the ordering of blobs, puts, merges, and deletes are preserved.

Blobs are not saves to SST files. RocksDB ignores blobs in every way except for writing them to the log.

Before committing this patch, I need to add some test code. But I'm submitting it now so people can comment on the API.

Test Plan: make -j32 check

Reviewers: dhruba, haobo, vamsi

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12195/Expand KeyMayExist to return the proper value if it can be found in memory and also check block_cache

Summary: Removed KeyMayExistImpl because KeyMayExist demanded Get like semantics now. Removed no_io from memtable and imm because we need the proper value now and shouldn't just stop when we see Merge in memtable. Added checks to block_cache. Updated documentation and unit-test

Test Plan: make all check;db_stress for 1 hour

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11853/Use KeyMayExist for WriteBatch-Deletes

Summary:
Introduced KeyMayExist checking during writebatch-delete and removed from Outer Delete API because it uses writebatch-delete.
Added code to skip getting Table from disk if not already present in table_cache.
Some renaming of variables.
Introduced KeyMayExistImpl which allows checking since specified sequence number in GetImpl useful to check partially written writebatch.
Changed KeyMayExist to not be pure virtual and provided a default implementation.
Expanded unit-tests in db_test to check appropriately.
Ran db_stress for 1 hour with ./db_stress --max_key=100000 --ops_per_thread=10000000 --delpercent=50 --filter_deletes=1 --statistics=1.

Test Plan: db_stress;make check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D11745/"
,,Rocksdb,"[RocksDB] Expose DBStatistics

Summary: Make Statistics usable by client

Test Plan: make check; db_bench

Reviewers: dhruba

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D10899/"
,,Rocksdb,"Reduce write amplification by merging files in L0 back into L0

Summary:
There is a new option called hybrid_mode which, when switched on,
causes HBase style compactions.  Files from L0 are
compacted back into L0. This meat of this compaction algorithm
is in PickCompactionHybrid().

All files reside in L0. That means all files have overlapping
keys. Each file has a time-bound, i.e. each file contains a
range of keys that were inserted around the same time. The
start-seqno and the end-seqno refers to the timeframe when
these keys were inserted.  Files that have contiguous seqno
are compacted together into a larger file. All files are
ordered from most recent to the oldest.

The current compaction algorithm starts to look for
candidate files starting from the most recent file. It continues to
add more files to the same compaction run as long as the
sum of the files chosen till now is smaller than the next
candidate file size. This logic needs to be debated
and validated.

The above logic should reduce write amplification to a
large extent... will publish numbers shortly.

Test Plan: dbstress runs for 6 hours with no data corruption (tested so far).

Differential Revision: https://reviews.facebook.net/D11289/"
,,Rocksdb,"[RocksDB] Enable manual compaction to move files back to an appropriate level.

Summary: As title. This diff added an option reduce_level to CompactRange. When set to true, it will try to move the files back to the minimum level sufficient to hold the data set. Note that the default is set to true now, just to excerise it in all existing tests. Will set the default to false before check-in, for backward compatibility.

Test Plan: make check;

Reviewers: dhruba, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11553/"
Memory management,Memory management,Rocksdb,"Add soft and hard rate limit support

Summary:
This diff adds support for both soft and hard rate limiting. The following changes are included:

1) Options.rate_limit is renamed to Options.hard_rate_limit.
2) Options.rate_limit_delay_milliseconds is renamed to Options.rate_limit_delay_max_milliseconds.
3) Options.soft_rate_limit is added.
4) If the maximum compaction score is > hard_rate_limit and rate_limit_delay_max_milliseconds == 0, then writes are delayed by 1 ms at a time until the max compaction score falls below hard_rate_limit.
5) If the max compaction score is > soft_rate_limit but <= hard_rate_limit, then writes are delayed by 0-1 ms depending on how close we are to hard_rate_limit.
6) Users can disable 4 by setting hard_rate_limit = 0. They can add a limit to the maximum amount of time waited by setting rate_limit_delay_max_milliseconds > 0. Thus, the old behavior can be preserved by setting soft_rate_limit = 0, which is the default.

Test Plan:
make -j32 check
./db_stress

Reviewers: dhruba, haobo, MarkCallaghan

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12003/Expand KeyMayExist to return the proper value if it can be found in memory and also check block_cache

Summary: Removed KeyMayExistImpl because KeyMayExist demanded Get like semantics now. Removed no_io from memtable and imm because we need the proper value now and shouldn't just stop when we see Merge in memtable. Added checks to block_cache. Updated documentation and unit-test

Test Plan: make all check;db_stress for 1 hour

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11853/Use KeyMayExist for WriteBatch-Deletes

Summary:
Introduced KeyMayExist checking during writebatch-delete and removed from Outer Delete API because it uses writebatch-delete.
Added code to skip getting Table from disk if not already present in table_cache.
Some renaming of variables.
Introduced KeyMayExistImpl which allows checking since specified sequence number in GetImpl useful to check partially written writebatch.
Changed KeyMayExist to not be pure virtual and provided a default implementation.
Expanded unit-tests in db_test to check appropriately.
Ran db_stress for 1 hour with ./db_stress --max_key=100000 --ops_per_thread=10000000 --delpercent=50 --filter_deletes=1 --statistics=1.

Test Plan: db_stress;make check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D11745/[RocksDB] Enable manual compaction to move files back to an appropriate level.

Summary: As title. This diff added an option reduce_level to CompactRange. When set to true, it will try to move the files back to the minimum level sufficient to hold the data set. Note that the default is set to true now, just to excerise it in all existing tests. Will set the default to false before check-in, for backward compatibility.

Test Plan: make check;

Reviewers: dhruba, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11553/Make rocksdb-deletes faster using bloom filter

Summary:
Wrote a new function in db_impl.c-CheckKeyMayExist that calls Get but with a new parameter turned on which makes Get return false only if bloom filters can guarantee that key is not in database. Delete calls this function and if the option- deletes_use_filter is turned on and CheckKeyMayExist returns false, the delete will be dropped saving:
1. Put of delete type
2. Space in the db,and
3. Compaction time

Test Plan:
make all check;
will run db_stress and db_bench and enhance unit-test once the basic design gets approved

Reviewers: dhruba, haobo, vamsi

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11607/Very basic Multiget and simple test cases.

Summary:
Implemented the MultiGet operator which takes in a list of keys
and returns their associated values. Currently uses std::vector as its
container data structure. Otherwise, it works identically to ""Get"".

Test Plan:
 1. make db_test      ; compile it
 2. ./db_test         ; test it
 3. make all check    ; regress / run all tests
 4. make release      ; (optional) compile with release settings

Reviewers: haobo, MarkCallaghan, dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10875/Improve output for GetProperty('leveldb.stats')

Summary:
Display separate values for read, write & total compaction IO.
Display compaction amplification and write amplification.
Add similar values for the period since the last call to GetProperty. Results since the server started
are reported as ""cumulative"" stats. Results since the last call to GetProperty are reported as
""interval"" stats.

Level  Files Size(MB) Time(sec)  Read(MB) Write(MB)    Rn(MB)  Rnp1(MB)  Wnew(MB) Amplify Read(MB/s) Write(MB/s)      Rn     Rnp1     Wnp1     NewW    Count  Ln-stall
----------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0        7       13        21         0       211         0         0       211     0.0       0.0        10.1        0        0        0        0      113       0.0
  1       79      157        88       993       989       198       795       194     9.0      11.3        11.2      106      405      502       97       14       0.0
  2       19       36         5        63        63        37        27        36     2.4      12.3        12.2       19       14       32       18       12       0.0
>>>>>>>>>>>>>>>>>>>>>>>>> text below has been is new and/or reformatted
Uptime(secs): 122.2 total, 0.9 interval
Compaction IO cumulative (GB): 0.21 new, 1.03 read, 1.23 write, 2.26 read+write
Compaction IO cumulative (MB/sec): 1.7 new, 8.6 read, 10.3 write, 19.0 read+write
Amplification cumulative: 6.0 write, 11.0 compaction
Compaction IO interval (MB): 5.59 new, 0.00 read, 5.59 write, 5.59 read+write
Compaction IO interval (MB/sec): 6.5 new, 0.0 read, 6.5 write, 6.5 read+write
Amplification interval: 1.0 write, 1.0 compaction
>>>>>>>>>>>>>>>>>>>>>>>> text above is new and/or reformatted
Stalls(secs): 90.574 level0_slowdown, 0.000 level0_numfiles, 10.165 memtable_compaction, 0.000 leveln_slowdown

Task ID: #

Blame Rev:

Test Plan:
make check, run db_bench

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11049/[RocksDB] dump leveldb.stats periodically in LOG file.

Summary:
Added an option stats_dump_period_sec to dump leveldb.stats to LOG periodically for diagnosis.
By defauly, it's set to a very big number 3600 (1 hour).

Test Plan: make check;

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, zshao

Differential Revision: https://reviews.facebook.net/D10761/"
,,Rocksdb,"Counter for merge failure

Summary:
With Merge returning bool, it can keep failing silently(eg. While faling to fetch timestamp in TTL). We need to detect this through a rocksdb counter which can get bumped whenever Merge returns false. This will also be super-useful for the mcrocksdb-counter service where Merge may fail.
Added a counter NUMBER_MERGE_FAILURES and appropriately updated db/merge_helper.cc

I felt that it would be better to directly add counter-bumping in Merge as a default function of MergeOperator class but user should not be aware of this, so this approach seems better to me.

Test Plan: make all check

Reviewers: dnicholas, haobo, dhruba, vamsi

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12129/[RocksDB] [MergeOperator] The new Merge Interface! Uses merge sequences.

Summary:
Here are the major changes to the Merge Interface. It has been expanded
to handle cases where the MergeOperator is not associative. It does so by stacking
up merge operations while scanning through the key history (i.e.: during Get() or
Compaction), until a valid Put/Delete/end-of-history is encountered; it then
applies all of the merge operations in the correct sequence starting with the
base/sentinel value.

I have also introduced an ""AssociativeMerge"" function which allows the user to
take advantage of associative merge operations (such as in the case of counters).
The implementation will always attempt to merge the operations/operands themselves
together when they are encountered, and will resort to the ""stacking"" method if
and only if the ""associative-merge"" fails.

This implementation is conjectured to allow MergeOperator to handle the general
case, while still providing the user with the ability to take advantage of certain
efficiencies in their own merge-operator / data-structure.

NOTE: This is a preliminary diff. This must still go through a lot of review,
revision, and testing. Feedback welcome!

Test Plan:
  -This is a preliminary diff. I have only just begun testing/debugging it.
  -I will be testing this with the existing MergeOperator use-cases and unit-tests
(counters, string-append, and redis-lists)
  -I will be ""desk-checking"" and walking through the code with the help gdb.
  -I will find a way of stress-testing the new interface / implementation using
db_bench, db_test, merge_test, and/or db_stress.
  -I will ensure that my tests cover all cases: Get-Memtable,
Get-Immutable-Memtable, Get-from-Disk, Iterator-Range-Scan, Flush-Memtable-to-L0,
Compaction-L0-L1, Compaction-Ln-L(n+1), Put/Delete found, Put/Delete not-found,
end-of-history, end-of-file, etc.
  -A lot of feedback from the reviewers.

Reviewers: haobo, dhruba, zshao, emayanke

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11499/Expand KeyMayExist to return the proper value if it can be found in memory and also check block_cache

Summary: Removed KeyMayExistImpl because KeyMayExist demanded Get like semantics now. Removed no_io from memtable and imm because we need the proper value now and shouldn't just stop when we see Merge in memtable. Added checks to block_cache. Updated documentation and unit-test

Test Plan: make all check;db_stress for 1 hour

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11853/Make rocksdb-deletes faster using bloom filter

Summary:
Wrote a new function in db_impl.c-CheckKeyMayExist that calls Get but with a new parameter turned on which makes Get return false only if bloom filters can guarantee that key is not in database. Delete calls this function and if the option- deletes_use_filter is turned on and CheckKeyMayExist returns false, the delete will be dropped saving:
1. Put of delete type
2. Space in the db,and
3. Compaction time

Test Plan:
make all check;
will run db_stress and db_bench and enhance unit-test once the basic design gets approved

Reviewers: dhruba, haobo, vamsi

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11607/Compact multiple memtables before flushing to storage.

Summary:
Merge multiple multiple memtables in memory before writing it
out to a file in L0.

There is a new config parameter min_write_buffer_number_to_merge
that specifies the number of write buffers that should be merged
together to a single file in storage. The system will not flush
wrte buffers to storage unless at least these many buffers have
accumulated in memory.
The default value of this new parameter is 1, which means that
a write buffer will be immediately flushed to disk as soon it is
ready.

Test Plan: make check

Differential Revision: https://reviews.facebook.net/D11241/"
Memory management,Memory management,Rocksdb,"[RocksDB] Expose thread local perf counter for low overhead, per call level performance statistics.

Summary:
As title. No locking/atomic is needed due to thread local. There is also no need to modify the existing client interface, in order to expose related counters.

perf_context_test shows a simple example of retrieving the number of user key comparison done for each put and get call. More counters could be added later.

Sample output
./perf_context_test 1000000
==== Test PerfContextTest.KeyComparisonCount
Inserting 1000000 key/value pairs
...
total user key comparison get: 43446523
total user key comparison put: 8017877
max user key comparison get: 88939
avg user key comparison get:43

Basically, the current skiplist does well on average, but could perform poorly in extreme cases.

Test Plan: run perf_context_test <total number of entries to put/get>

Reviewers: dhruba

Differential Revision: https://reviews.facebook.net/D12225/Fix refering freed memory in earlier commit.

Summary: Fix refering freed memory in earlier commit by https://reviews.facebook.net/D11181

Test Plan: make check

Reviewers: haobo, sheki

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11193/Print name of user comparator in LOG.

Summary:
The current code prints the name of the InternalKeyComparator
in the log file. We would also like to print the name of the
user-specified comparator for easier debugging.

Test Plan: make check

Reviewers: sheki

Reviewed By: sheki

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11181/"
,,Rocksdb,"Expand KeyMayExist to return the proper value if it can be found in memory and also check block_cache

Summary: Removed KeyMayExistImpl because KeyMayExist demanded Get like semantics now. Removed no_io from memtable and imm because we need the proper value now and shouldn't just stop when we see Merge in memtable. Added checks to block_cache. Updated documentation and unit-test

Test Plan: make all check;db_stress for 1 hour

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11853/Virtualize SkipList Interface

Summary: This diff virtualizes the skiplist interface so that users can provide their own implementation of a backing store for MemTables. Eventually, the backing store will be responsible for its own synchronization, allowing users (and us) to experiment with different lockless implementations.

Test Plan:
make clean
make -j32 check
./db_stress

Reviewers: dhruba, emayanke, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11739/Make rocksdb-deletes faster using bloom filter

Summary:
Wrote a new function in db_impl.c-CheckKeyMayExist that calls Get but with a new parameter turned on which makes Get return false only if bloom filters can guarantee that key is not in database. Delete calls this function and if the option- deletes_use_filter is turned on and CheckKeyMayExist returns false, the delete will be dropped saving:
1. Put of delete type
2. Space in the db,and
3. Compaction time

Test Plan:
make all check;
will run db_stress and db_bench and enhance unit-test once the basic design gets approved

Reviewers: dhruba, haobo, vamsi

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11607/"
,,Rocksdb,"Compact multiple memtables before flushing to storage.

Summary:
Merge multiple multiple memtables in memory before writing it
out to a file in L0.

There is a new config parameter min_write_buffer_number_to_merge
that specifies the number of write buffers that should be merged
together to a single file in storage. The system will not flush
wrte buffers to storage unless at least these many buffers have
accumulated in memory.
The default value of this new parameter is 1, which means that
a write buffer will be immediately flushed to disk as soon it is
ready.

Test Plan: make check

Differential Revision: https://reviews.facebook.net/D11241/"
Memory management,Memory management,Rocksdb,"Expand KeyMayExist to return the proper value if it can be found in memory and also check block_cache

Summary: Removed KeyMayExistImpl because KeyMayExist demanded Get like semantics now. Removed no_io from memtable and imm because we need the proper value now and shouldn't just stop when we see Merge in memtable. Added checks to block_cache. Updated documentation and unit-test

Test Plan: make all check;db_stress for 1 hour

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11853/Use KeyMayExist for WriteBatch-Deletes

Summary:
Introduced KeyMayExist checking during writebatch-delete and removed from Outer Delete API because it uses writebatch-delete.
Added code to skip getting Table from disk if not already present in table_cache.
Some renaming of variables.
Introduced KeyMayExistImpl which allows checking since specified sequence number in GetImpl useful to check partially written writebatch.
Changed KeyMayExist to not be pure virtual and provided a default implementation.
Expanded unit-tests in db_test to check appropriately.
Ran db_stress for 1 hour with ./db_stress --max_key=100000 --ops_per_thread=10000000 --delpercent=50 --filter_deletes=1 --statistics=1.

Test Plan: db_stress;make check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D11745/Make rocksdb-deletes faster using bloom filter

Summary:
Wrote a new function in db_impl.c-CheckKeyMayExist that calls Get but with a new parameter turned on which makes Get return false only if bloom filters can guarantee that key is not in database. Delete calls this function and if the option- deletes_use_filter is turned on and CheckKeyMayExist returns false, the delete will be dropped saving:
1. Put of delete type
2. Space in the db,and
3. Compaction time

Test Plan:
make all check;
will run db_stress and db_bench and enhance unit-test once the basic design gets approved

Reviewers: dhruba, haobo, vamsi

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11607/Reduce write amplification by merging files in L0 back into L0

Summary:
There is a new option called hybrid_mode which, when switched on,
causes HBase style compactions.  Files from L0 are
compacted back into L0. This meat of this compaction algorithm
is in PickCompactionHybrid().

All files reside in L0. That means all files have overlapping
keys. Each file has a time-bound, i.e. each file contains a
range of keys that were inserted around the same time. The
start-seqno and the end-seqno refers to the timeframe when
these keys were inserted.  Files that have contiguous seqno
are compacted together into a larger file. All files are
ordered from most recent to the oldest.

The current compaction algorithm starts to look for
candidate files starting from the most recent file. It continues to
add more files to the same compaction run as long as the
sum of the files chosen till now is smaller than the next
candidate file size. This logic needs to be debated
and validated.

The above logic should reduce write amplification to a
large extent... will publish numbers shortly.

Test Plan: dbstress runs for 6 hours with no data corruption (tested so far).

Differential Revision: https://reviews.facebook.net/D11289/[RocksDB] cleanup EnvOptions

Summary:
This diff simplifies EnvOptions by treating it as POD, similar to Options.
- virtual functions are removed and member fields are accessed directly.
- StorageOptions is removed.
- Options.allow_readahead and Options.allow_readahead_compactions are deprecated.
- Unused global variables are removed: useOsBuffer, useFsReadAhead, useMmapRead, useMmapWrite

Test Plan: make check; db_stress

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11175/"
Memory management,Memory management,Rocksdb,"Implement log blobs

Summary:
This patch adds the ability for the user to add sequences of arbitrary data (blobs) to write batches. These blobs are saved to the log along with everything else in the write batch. You can add multiple blobs per WriteBatch and the ordering of blobs, puts, merges, and deletes are preserved.

Blobs are not saves to SST files. RocksDB ignores blobs in every way except for writing them to the log.

Before committing this patch, I need to add some test code. But I'm submitting it now so people can comment on the API.

Test Plan: make -j32 check

Reviewers: dhruba, haobo, vamsi

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12195/Prefix filters for scans (v4)

Summary: Similar to v2 (db and table code understands prefixes), but use ReadOptions as in v3.  Also, make the CreateFilter code faster and cleaner.

Test Plan: make db_test; export LEVELDB_TESTS=PrefixScan; ./db_test

Reviewers: dhruba

Reviewed By: dhruba

CC: haobo, emayanke

Differential Revision: https://reviews.facebook.net/D12027/Universal Compaction should keep DeleteMarkers unless it is the earliest file.

Summary:
The pre-existing code was purging a DeleteMarker if thay key did not
exist in deeper levels.  But in the Universal Compaction Style, all
files are in Level0. For compaction runs that did not include the
earliest file, we were erroneously purging the DeleteMarkers.

The fix is to purge DeleteMarkers only if the compaction includes
the earlist file.

Test Plan: DBTest.Randomized triggers this code path.

Differential Revision: https://reviews.facebook.net/D12081/Fix unit tests for universal compaction (step 2)

Summary:
Continue fixing existing unit tests for universal compaction. I have
tried to apply universal compaction to all unit tests those haven't
called ChangeOptions(). I left a few which are either apparently not
applicable to universal compaction (because they check files/keys/values
at level 1 or above levels), or apparently not related to compaction
(e.g., open a file, open a db).

I also add a new unit test for universal compaction.

Good news is I didn't see any bugs during this round.

Test Plan: Ran ""make all check"" yesterday. Has rebased and is rerunning

Reviewers: haobo, dhruba

Differential Revision: https://reviews.facebook.net/D12135/Fix unit tests/bugs for universal compaction (first step)

Summary:
This is the first step to fix unit tests and bugs for universal
compactiion. I added universal compaction option to ChangeOptions(), and
fixed all unit tests calling ChangeOptions(). Some of these tests
obviously assume more than 1 level and check file number/values in level
1 or above levels. I set kSkipUniversalCompaction for these tests.

The major bug I found is manual compaction with universal compaction never stops. I have put a fix for
it.

I have also set universal compaction as the default compaction and found
at least 20+ unit tests failing. I haven't looked into the details. The
next step is to check all unit tests without calling ChangeOptions().

Test Plan: make all check

Reviewers: dhruba, haobo

Differential Revision: https://reviews.facebook.net/D12051/Expand KeyMayExist to return the proper value if it can be found in memory and also check block_cache

Summary: Removed KeyMayExistImpl because KeyMayExist demanded Get like semantics now. Removed no_io from memtable and imm because we need the proper value now and shouldn't just stop when we see Merge in memtable. Added checks to block_cache. Updated documentation and unit-test

Test Plan: make all check;db_stress for 1 hour

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11853/Use KeyMayExist for WriteBatch-Deletes

Summary:
Introduced KeyMayExist checking during writebatch-delete and removed from Outer Delete API because it uses writebatch-delete.
Added code to skip getting Table from disk if not already present in table_cache.
Some renaming of variables.
Introduced KeyMayExistImpl which allows checking since specified sequence number in GetImpl useful to check partially written writebatch.
Changed KeyMayExist to not be pure virtual and provided a default implementation.
Expanded unit-tests in db_test to check appropriately.
Ran db_stress for 1 hour with ./db_stress --max_key=100000 --ops_per_thread=10000000 --delpercent=50 --filter_deletes=1 --statistics=1.

Test Plan: db_stress;make check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D11745/[RocksDB] Enable manual compaction to move files back to an appropriate level.

Summary: As title. This diff added an option reduce_level to CompactRange. When set to true, it will try to move the files back to the minimum level sufficient to hold the data set. Note that the default is set to true now, just to excerise it in all existing tests. Will set the default to false before check-in, for backward compatibility.

Test Plan: make check;

Reviewers: dhruba, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11553/Make rocksdb-deletes faster using bloom filter

Summary:
Wrote a new function in db_impl.c-CheckKeyMayExist that calls Get but with a new parameter turned on which makes Get return false only if bloom filters can guarantee that key is not in database. Delete calls this function and if the option- deletes_use_filter is turned on and CheckKeyMayExist returns false, the delete will be dropped saving:
1. Put of delete type
2. Space in the db,and
3. Compaction time

Test Plan:
make all check;
will run db_stress and db_bench and enhance unit-test once the basic design gets approved

Reviewers: dhruba, haobo, vamsi

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11607/Reduce write amplification by merging files in L0 back into L0

Summary:
There is a new option called hybrid_mode which, when switched on,
causes HBase style compactions.  Files from L0 are
compacted back into L0. This meat of this compaction algorithm
is in PickCompactionHybrid().

All files reside in L0. That means all files have overlapping
keys. Each file has a time-bound, i.e. each file contains a
range of keys that were inserted around the same time. The
start-seqno and the end-seqno refers to the timeframe when
these keys were inserted.  Files that have contiguous seqno
are compacted together into a larger file. All files are
ordered from most recent to the oldest.

The current compaction algorithm starts to look for
candidate files starting from the most recent file. It continues to
add more files to the same compaction run as long as the
sum of the files chosen till now is smaller than the next
candidate file size. This logic needs to be debated
and validated.

The above logic should reduce write amplification to a
large extent... will publish numbers shortly.

Test Plan: dbstress runs for 6 hours with no data corruption (tested so far).

Differential Revision: https://reviews.facebook.net/D11289/[RocksDB] cleanup EnvOptions

Summary:
This diff simplifies EnvOptions by treating it as POD, similar to Options.
- virtual functions are removed and member fields are accessed directly.
- StorageOptions is removed.
- Options.allow_readahead and Options.allow_readahead_compactions are deprecated.
- Unused global variables are removed: useOsBuffer, useFsReadAhead, useMmapRead, useMmapWrite

Test Plan: make check; db_stress

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11175/Fixed valgrind errors/Very basic Multiget and simple test cases.

Summary:
Implemented the MultiGet operator which takes in a list of keys
and returns their associated values. Currently uses std::vector as its
container data structure. Otherwise, it works identically to ""Get"".

Test Plan:
 1. make db_test      ; compile it
 2. ./db_test         ; test it
 3. make all check    ; regress / run all tests
 4. make release      ; (optional) compile with release settings

Reviewers: haobo, MarkCallaghan, dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10875/"
Memory management,Memory management,Rocksdb,"Universal Compaction should keep DeleteMarkers unless it is the earliest file.

Summary:
The pre-existing code was purging a DeleteMarker if thay key did not
exist in deeper levels.  But in the Universal Compaction Style, all
files are in Level0. For compaction runs that did not include the
earliest file, we were erroneously purging the DeleteMarkers.

The fix is to purge DeleteMarkers only if the compaction includes
the earlist file.

Test Plan: DBTest.Randomized triggers this code path.

Differential Revision: https://reviews.facebook.net/D12081/Fix unit tests/bugs for universal compaction (first step)

Summary:
This is the first step to fix unit tests and bugs for universal
compactiion. I added universal compaction option to ChangeOptions(), and
fixed all unit tests calling ChangeOptions(). Some of these tests
obviously assume more than 1 level and check file number/values in level
1 or above levels. I set kSkipUniversalCompaction for these tests.

The major bug I found is manual compaction with universal compaction never stops. I have put a fix for
it.

I have also set universal compaction as the default compaction and found
at least 20+ unit tests failing. I haven't looked into the details. The
next step is to check all unit tests without calling ChangeOptions().

Test Plan: make all check

Reviewers: dhruba, haobo

Differential Revision: https://reviews.facebook.net/D12051/Merge branch 'performance' of github.com:facebook/rocksdb into performance

Conflicts:
	db/builder.cc
	db/db_impl.cc
	db/version_set.cc
	include/leveldb/statistics.h/[RocksDB] [MergeOperator] The new Merge Interface! Uses merge sequences.

Summary:
Here are the major changes to the Merge Interface. It has been expanded
to handle cases where the MergeOperator is not associative. It does so by stacking
up merge operations while scanning through the key history (i.e.: during Get() or
Compaction), until a valid Put/Delete/end-of-history is encountered; it then
applies all of the merge operations in the correct sequence starting with the
base/sentinel value.

I have also introduced an ""AssociativeMerge"" function which allows the user to
take advantage of associative merge operations (such as in the case of counters).
The implementation will always attempt to merge the operations/operands themselves
together when they are encountered, and will resort to the ""stacking"" method if
and only if the ""associative-merge"" fails.

This implementation is conjectured to allow MergeOperator to handle the general
case, while still providing the user with the ability to take advantage of certain
efficiencies in their own merge-operator / data-structure.

NOTE: This is a preliminary diff. This must still go through a lot of review,
revision, and testing. Feedback welcome!

Test Plan:
  -This is a preliminary diff. I have only just begun testing/debugging it.
  -I will be testing this with the existing MergeOperator use-cases and unit-tests
(counters, string-append, and redis-lists)
  -I will be ""desk-checking"" and walking through the code with the help gdb.
  -I will find a way of stress-testing the new interface / implementation using
db_bench, db_test, merge_test, and/or db_stress.
  -I will ensure that my tests cover all cases: Get-Memtable,
Get-Immutable-Memtable, Get-from-Disk, Iterator-Range-Scan, Flush-Memtable-to-L0,
Compaction-L0-L1, Compaction-Ln-L(n+1), Put/Delete found, Put/Delete not-found,
end-of-history, end-of-file, etc.
  -A lot of feedback from the reviewers.

Reviewers: haobo, dhruba, zshao, emayanke

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11499/Expand KeyMayExist to return the proper value if it can be found in memory and also check block_cache

Summary: Removed KeyMayExistImpl because KeyMayExist demanded Get like semantics now. Removed no_io from memtable and imm because we need the proper value now and shouldn't just stop when we see Merge in memtable. Added checks to block_cache. Updated documentation and unit-test

Test Plan: make all check;db_stress for 1 hour

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11853/Use KeyMayExist for WriteBatch-Deletes

Summary:
Introduced KeyMayExist checking during writebatch-delete and removed from Outer Delete API because it uses writebatch-delete.
Added code to skip getting Table from disk if not already present in table_cache.
Some renaming of variables.
Introduced KeyMayExistImpl which allows checking since specified sequence number in GetImpl useful to check partially written writebatch.
Changed KeyMayExist to not be pure virtual and provided a default implementation.
Expanded unit-tests in db_test to check appropriately.
Ran db_stress for 1 hour with ./db_stress --max_key=100000 --ops_per_thread=10000000 --delpercent=50 --filter_deletes=1 --statistics=1.

Test Plan: db_stress;make check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D11745/Merge branch 'master' into performance

Conflicts:
	db/version_set.cc
	include/leveldb/options.h
	util/options.cc/Make file-sizes and grandparentoverlap to be unsigned to avoid bad comparisions.

Summary:
The maxGrandParentOverlapBytes_ was signed which was causing
an erroneous comparision between signed and unsigned longs.
This, in turn, was causing compaction-created-output-files
to be very small in size.

Test Plan: make check

Differential Revision: https://reviews.facebook.net/D11727/Make rocksdb-deletes faster using bloom filter

Summary:
Wrote a new function in db_impl.c-CheckKeyMayExist that calls Get but with a new parameter turned on which makes Get return false only if bloom filters can guarantee that key is not in database. Delete calls this function and if the option- deletes_use_filter is turned on and CheckKeyMayExist returns false, the delete will be dropped saving:
1. Put of delete type
2. Space in the db,and
3. Compaction time

Test Plan:
make all check;
will run db_stress and db_bench and enhance unit-test once the basic design gets approved

Reviewers: dhruba, haobo, vamsi

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11607/Rename PickCompactionHybrid to PickCompactionUniversal.

Summary:
Rename PickCompactionHybrid to PickCompactionUniversal.
Changed a few LOG message from ""Hybrid:"" to ""Universal:"".

Test Plan:

Reviewers:

CC:

Task ID: #

Blame Rev:/Renamed 'hybrid_compaction' tp be ""Universal Compaction'.

Summary:
All the universal compaction parameters are encapsulated in
a new file universal_compaction.h

Test Plan:
make check/Reduce write amplification by merging files in L0 back into L0

Summary:
There is a new option called hybrid_mode which, when switched on,
causes HBase style compactions.  Files from L0 are
compacted back into L0. This meat of this compaction algorithm
is in PickCompactionHybrid().

All files reside in L0. That means all files have overlapping
keys. Each file has a time-bound, i.e. each file contains a
range of keys that were inserted around the same time. The
start-seqno and the end-seqno refers to the timeframe when
these keys were inserted.  Files that have contiguous seqno
are compacted together into a larger file. All files are
ordered from most recent to the oldest.

The current compaction algorithm starts to look for
candidate files starting from the most recent file. It continues to
add more files to the same compaction run as long as the
sum of the files chosen till now is smaller than the next
candidate file size. This logic needs to be debated
and validated.

The above logic should reduce write amplification to a
large extent... will publish numbers shortly.

Test Plan: dbstress runs for 6 hours with no data corruption (tested so far).

Differential Revision: https://reviews.facebook.net/D11289/[rocksdb] do not trim range for level0 in manual compaction

Summary:
https://code.google.com/p/leveldb/issues/detail?can=1&q=178&colspec=ID%20Type%20Status%20Priority%20Milestone%20Owner%20Summary&id=178

Ported the solution as is to RocksDB.

Test Plan: moved the unit test as manual_compaction_test

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11331/[RocksDB] cleanup EnvOptions

Summary:
This diff simplifies EnvOptions by treating it as POD, similar to Options.
- virtual functions are removed and member fields are accessed directly.
- StorageOptions is removed.
- Options.allow_readahead and Options.allow_readahead_compactions are deprecated.
- Unused global variables are removed: useOsBuffer, useFsReadAhead, useMmapRead, useMmapWrite

Test Plan: make check; db_stress

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11175/Do not submit multiple simultaneous seek-compaction requests.

Summary:
The code was such that if multi-threaded-compactions as well
as seek compaction are enabled then it submits multiple
compaction request for the same range of keys. This causes
extraneous sst-files to accumulate at various levels.

Test Plan:
I am not able to write a very good unit test for this one
but can easily reproduce this bug with 'dbstress' with the
following options.

batch=1;maxk=100000000;ops=100000000;ro=0;fm=2;bpl=10485760;of=500000; wbn=3; mbc=20; mb=2097152; wbs=4194304; dds=1; sync=0;  t=32; bs=16384; cs=1048576; of=500000; ./db_stress --disable_seek_compaction=0 --mmap_read=0 --threads=$t --block_size=$bs --cache_size=$cs --open_files=$of --verify_checksum=1 --db=/data/mysql/leveldb/dbstress.dir --sync=$sync --disable_wal=1 --disable_data_sync=$dds --write_buffer_size=$wbs --target_file_size_base=$mb --target_file_size_multiplier=$fm --max_write_buffer_number=$wbn --max_background_compactions=$mbc --max_bytes_for_level_base=$bpl --reopen=$ro --ops_per_thread=$ops --max_key=$maxk --test_batches_snapshots=$batch

Reviewers: leveldb, emayanke

Reviewed By: emayanke

Differential Revision: https://reviews.facebook.net/D11055/[RocksDB] [Performance] Allow different posix advice to be applied to the same table file

Summary:
Current posix advice implementation ties up the access pattern hint with the creation of a file.
It is not possible to apply different advice for different access (random get vs compaction read),
without keeping two open files for the same table. This patch extended the RandomeAccessFile interface
to accept new access hint at anytime. Particularly, we are able to set different access hint on the same
table file based on when/how the file is used.
Two options are added to set the access hint, after the file is first opened and after the file is being
compacted.

Test Plan: make check; db_stress; db_bench

Reviewers: dhruba

Reviewed By: dhruba

CC: MarkCallaghan, leveldb

Differential Revision: https://reviews.facebook.net/D10905/"
,,Rocksdb,"Expose base db object from ttl wrapper

Summary: rocksdb replicaiton will need this when writing value+TS from master to slave 'as is'

Test Plan: make

Reviewers: dhruba, vamsi, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11919/Merge operator for ttl

Summary: Implemented a TtlMergeOperator class which inherits from MergeOperator and is TTL aware. It strips out timestamp from existing_value and attaches timestamp to new_value, calling user-provided-Merge in between.

Test Plan: make all check

Reviewers: haobo, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11775/"
,,Rocksdb,"[RocksDB] [MergeOperator] The new Merge Interface! Uses merge sequences.

Summary:
Here are the major changes to the Merge Interface. It has been expanded
to handle cases where the MergeOperator is not associative. It does so by stacking
up merge operations while scanning through the key history (i.e.: during Get() or
Compaction), until a valid Put/Delete/end-of-history is encountered; it then
applies all of the merge operations in the correct sequence starting with the
base/sentinel value.

I have also introduced an ""AssociativeMerge"" function which allows the user to
take advantage of associative merge operations (such as in the case of counters).
The implementation will always attempt to merge the operations/operands themselves
together when they are encountered, and will resort to the ""stacking"" method if
and only if the ""associative-merge"" fails.

This implementation is conjectured to allow MergeOperator to handle the general
case, while still providing the user with the ability to take advantage of certain
efficiencies in their own merge-operator / data-structure.

NOTE: This is a preliminary diff. This must still go through a lot of review,
revision, and testing. Feedback welcome!

Test Plan:
  -This is a preliminary diff. I have only just begun testing/debugging it.
  -I will be testing this with the existing MergeOperator use-cases and unit-tests
(counters, string-append, and redis-lists)
  -I will be ""desk-checking"" and walking through the code with the help gdb.
  -I will find a way of stress-testing the new interface / implementation using
db_bench, db_test, merge_test, and/or db_stress.
  -I will ensure that my tests cover all cases: Get-Memtable,
Get-Immutable-Memtable, Get-from-Disk, Iterator-Range-Scan, Flush-Memtable-to-L0,
Compaction-L0-L1, Compaction-Ln-L(n+1), Put/Delete found, Put/Delete not-found,
end-of-history, end-of-file, etc.
  -A lot of feedback from the reviewers.

Reviewers: haobo, dhruba, zshao, emayanke

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11499/Record the number of open db iterators.

Summary: Enhance the statitics to report the number of open db iterators.

Test Plan: make check

Reviewers: haobo, emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10983/"
,,Rocksdb,"Implement log blobs

Summary:
This patch adds the ability for the user to add sequences of arbitrary data (blobs) to write batches. These blobs are saved to the log along with everything else in the write batch. You can add multiple blobs per WriteBatch and the ordering of blobs, puts, merges, and deletes are preserved.

Blobs are not saves to SST files. RocksDB ignores blobs in every way except for writing them to the log.

Before committing this patch, I need to add some test code. But I'm submitting it now so people can comment on the API.

Test Plan: make -j32 check

Reviewers: dhruba, haobo, vamsi

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12195/"
,,Rocksdb,"Implement log blobs

Summary:
This patch adds the ability for the user to add sequences of arbitrary data (blobs) to write batches. These blobs are saved to the log along with everything else in the write batch. You can add multiple blobs per WriteBatch and the ordering of blobs, puts, merges, and deletes are preserved.

Blobs are not saves to SST files. RocksDB ignores blobs in every way except for writing them to the log.

Before committing this patch, I need to add some test code. But I'm submitting it now so people can comment on the API.

Test Plan: make -j32 check

Reviewers: dhruba, haobo, vamsi

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12195/[RocksDB] [MergeOperator] The new Merge Interface! Uses merge sequences.

Summary:
Here are the major changes to the Merge Interface. It has been expanded
to handle cases where the MergeOperator is not associative. It does so by stacking
up merge operations while scanning through the key history (i.e.: during Get() or
Compaction), until a valid Put/Delete/end-of-history is encountered; it then
applies all of the merge operations in the correct sequence starting with the
base/sentinel value.

I have also introduced an ""AssociativeMerge"" function which allows the user to
take advantage of associative merge operations (such as in the case of counters).
The implementation will always attempt to merge the operations/operands themselves
together when they are encountered, and will resort to the ""stacking"" method if
and only if the ""associative-merge"" fails.

This implementation is conjectured to allow MergeOperator to handle the general
case, while still providing the user with the ability to take advantage of certain
efficiencies in their own merge-operator / data-structure.

NOTE: This is a preliminary diff. This must still go through a lot of review,
revision, and testing. Feedback welcome!

Test Plan:
  -This is a preliminary diff. I have only just begun testing/debugging it.
  -I will be testing this with the existing MergeOperator use-cases and unit-tests
(counters, string-append, and redis-lists)
  -I will be ""desk-checking"" and walking through the code with the help gdb.
  -I will find a way of stress-testing the new interface / implementation using
db_bench, db_test, merge_test, and/or db_stress.
  -I will ensure that my tests cover all cases: Get-Memtable,
Get-Immutable-Memtable, Get-from-Disk, Iterator-Range-Scan, Flush-Memtable-to-L0,
Compaction-L0-L1, Compaction-Ln-L(n+1), Put/Delete found, Put/Delete not-found,
end-of-history, end-of-file, etc.
  -A lot of feedback from the reviewers.

Reviewers: haobo, dhruba, zshao, emayanke

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11499/Expand KeyMayExist to return the proper value if it can be found in memory and also check block_cache

Summary: Removed KeyMayExistImpl because KeyMayExist demanded Get like semantics now. Removed no_io from memtable and imm because we need the proper value now and shouldn't just stop when we see Merge in memtable. Added checks to block_cache. Updated documentation and unit-test

Test Plan: make all check;db_stress for 1 hour

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11853/Make arena block size configurable

Summary:
Add an option for arena block size, default value 4096 bytes. Arena will allocate blocks with such size.

I am not sure about passing parameter to skiplist in the new virtualized framework, though I talked to Jim a bit. So add Jim as reviewer.

Test Plan:
new unit test, I am running db_test.

For passing paramter from configured option to Arena, I tried tests like:

  TEST(DBTest, Arena_Option) {
  std::string dbname = test::TmpDir() + ""/db_arena_option_test"";
  DestroyDB(dbname, Options());

  DB* db = nullptr;
  Options opts;
  opts.create_if_missing = true;
  opts.arena_block_size = 1000000; // tested 99, 999999
  Status s = DB::Open(opts, dbname, &db);
  db->Put(WriteOptions(), ""a"", ""123"");
  }

and printed some debug info. The results look good. Any suggestion for such a unit-test?

Reviewers: haobo, dhruba, emayanke, jpaton

Reviewed By: dhruba

CC: leveldb, zshao

Differential Revision: https://reviews.facebook.net/D11799/Virtualize SkipList Interface

Summary: This diff virtualizes the skiplist interface so that users can provide their own implementation of a backing store for MemTables. Eventually, the backing store will be responsible for its own synchronization, allowing users (and us) to experiment with different lockless implementations.

Test Plan:
make clean
make -j32 check
./db_stress

Reviewers: dhruba, emayanke, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11739/Make rocksdb-deletes faster using bloom filter

Summary:
Wrote a new function in db_impl.c-CheckKeyMayExist that calls Get but with a new parameter turned on which makes Get return false only if bloom filters can guarantee that key is not in database. Delete calls this function and if the option- deletes_use_filter is turned on and CheckKeyMayExist returns false, the delete will be dropped saving:
1. Put of delete type
2. Space in the db,and
3. Compaction time

Test Plan:
make all check;
will run db_stress and db_bench and enhance unit-test once the basic design gets approved

Reviewers: dhruba, haobo, vamsi

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11607/[RocksDB] [Performance Bug] MemTable::Get Slow

Summary:
The merge operator diff introduced a performance problem in MemTable::Get.
An exit condition is missed when the current key does not match the user key.
This could lead to full memtable scan if the user key is not found.

Test Plan: make check; db_bench

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D10851/"
,,Rocksdb,"[Rocksdb] Log on disable/enable file deletions

Summary: as title

Test Plan: compile

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11085/"
,Thread management,Rocksdb,"[RocksDB] Introduce Fast Mutex option

Summary:
This diff adds an option to specify whether PTHREAD_MUTEX_ADAPTIVE_NP will be enabled for the rocksdb single big kernel lock. db_bench also have this option now.
Quickly tested 8 thread cpu bound 100 byte random read.
No fast mutex: ~750k/s ops
With fast mutex: ~880k/s ops

Test Plan: make check; db_bench; db_stress

Reviewers: dhruba

CC: MarkCallaghan, leveldb

Differential Revision: https://reviews.facebook.net/D11031/"
,Thread management,Rocksdb,"Fix Zlib_Compress and Zlib_Uncompress

Summary:
Zlib_{Compress,Uncompress} did not handle very small input buffers properly. In addition, they did not call inflate/deflate until Z_STREAM_END was returned; it was possible for them to exit when only Z_OK had returned.

This diff also fixes a bunch of lint errors.

Test Plan: Run make check

Reviewers: dhruba, sheki, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11301/[RocksDB] Introduce Fast Mutex option

Summary:
This diff adds an option to specify whether PTHREAD_MUTEX_ADAPTIVE_NP will be enabled for the rocksdb single big kernel lock. db_bench also have this option now.
Quickly tested 8 thread cpu bound 100 byte random read.
No fast mutex: ~750k/s ops
With fast mutex: ~880k/s ops

Test Plan: make check; db_bench; db_stress

Reviewers: dhruba

CC: MarkCallaghan, leveldb

Differential Revision: https://reviews.facebook.net/D11031/"
,,Rocksdb,"Make ""Table"" pluggable

Summary: This patch makes Table and TableBuilder a abstract class and make all the implementation of the current table into BlockedBasedTable and BlockedBasedTable Builder.

Test Plan: Make db_test.cc to work with block based table. Add a new test simple_table_db_test.cc where a different simple table format is implemented.

Reviewers: dhruba, haobo, kailiu, emayanke, vamsi

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13521/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/"
,,Rocksdb,"Migrate names of properties from 'leveldb' prefix to 'rocksdb' prefix.

Summary: Migrate names of properties from 'leveldb' prefix to 'rocksdb' prefix.

Test Plan: make check

Reviewers: emayanke, haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13311/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/"
,Thread management,Rocksdb,"Add the index/filter block cache

Summary: This diff leverage the existing block cache and extend it to cache index/filter block.

Test Plan:
Added new tests in db_test and table_test

The correctness is checked by:

1. make check
2. make valgrind_check

Performance is test by:

1. 10 times of build_tools/regression_build_test.sh on two versions of rocksdb before/after the code change. Test results suggests no significant difference between them. For the two key operatons `overwrite` and `readrandom`, the average iops are both 20k and ~260k, with very small variance).
2. db_stress.

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, haobo, xjin

Differential Revision: https://reviews.facebook.net/D13167/Conversion of db_bench, db_stress and db_repl_stress to use gflags

Summary: Converted db_stress, db_repl_stress and db_bench to use gflags

Test Plan: I tested by printing out all the flags from old and new versions. Tried defaults, + various combinations with ""interesting flags"". Also, tested by running db_crashtest.py and db_crashtest2.py.

Reviewers: emayanke, dhruba, haobo, kailiu, sdong

Reviewed By: emayanke

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D13581/Migrate names of properties from 'leveldb' prefix to 'rocksdb' prefix.

Summary: Migrate names of properties from 'leveldb' prefix to 'rocksdb' prefix.

Test Plan: make check

Reviewers: emayanke, haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13311/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/Triggering verify for gets also

Summary: Will use iterators to verify keys in the db for half of its keys and Gets for the other half.

Test Plan: ./db_stress --max_key=1000 --ops_per_thread=100

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13227/Phase 2 of iterator stress test

Summary: Using an iterator instead of the Get method, each thread goes through a portion of the database and verifies values by comparing to the shared state.

Test Plan:
./db_stress --db=/tmp/tmppp --max_key=10000 --ops_per_thread=10000

To test some basic cases, the following lines can be added (each set in turn) to the verifyDb method with the following expected results:

    // Should abort with ""Unexpected value found""
    shared.Delete(start);

    // Should abort with ""Value not found""
    WriteOptions write_opts;
    db_->Delete(write_opts, Key(start));

    // Should succeed
    WriteOptions write_opts;
    shared.Delete(start);
     db_->Delete(write_opts, Key(start));

    // Should abort with ""Value not found""
    WriteOptions write_opts;
    db_->Delete(write_opts, Key(start + (end-start)/2));

    // Should abort with ""Value not found""
    db_->Delete(write_opts, Key(end-1));

    // Should abort with ""Unexpected value""
    shared.Delete(end-1);

    // Should abort with ""Unexpected value""
    shared.Delete(start + (end-start)/2);

    // Should abort with ""Value not found""
    db_->Delete(write_opts, Key(start));
    shared.Delete(start);
    db_->Delete(write_opts, Key(end-1));
    db_->Delete(write_opts, Key(end-2));

To test the out of range abort, change the key in the for loop to Key(i+1), so that the key defined by the index i is now outside of the supposed range of the database.

Reviewers: emayanke

Reviewed By: emayanke

CC: dhruba, xjin

Differential Revision: https://reviews.facebook.net/D13071/Phase 1 of an iterator stress test

Summary:
Added MultiIterate() which does a seek and some Next/Prev
calls.  Iterator status is checked only, no data integrity check

Test Plan:
make db_stress
./db_stress --iterpercent=<nonzero value> --readpercent=, etc.

Reviewers: emayanke, dhruba, xjin

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12915/Added a parameter to limit the maximum space amplification for universal compaction.

Summary:
Added a new field called max_size_amplification_ratio in the
CompactionOptionsUniversal structure. This determines the maximum
percentage overhead of space amplification.

The size amplification is defined to be the ratio between the size of
the oldest file to the sum of the sizes of all other files. If the
size amplification exceeds the specified value, then min_merge_width
and max_merge_width are ignored and a full compaction of all files is done.
A value of 10 means that the size a database that stores 100 bytes
of user data could occupy 110 bytes of physical storage.

Test Plan: Unit test DBTest.UniversalCompactionSpaceAmplification added.

Reviewers: haobo, emayanke, xjin

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12825/Add three new MemTableRep's

Summary:
This patch adds three new MemTableRep's: UnsortedRep, PrefixHashRep, and VectorRep.

UnsortedRep stores keys in an std::unordered_map of std::sets. When an iterator is ed, it dumps the keys into an std::set and iterates over that.

VectorRep stores keys in an std::vector. When an iterator is ed, it creates a copy of the vector and sorts it using std::sort. The iterator accesses that new vector.

PrefixHashRep stores keys in an unordered_map mapping prefixes to ordered sets.

I also added one API change. I added a function MemTableRep::MarkImmutable. This function is called when the rep is added to the immutable list. It doesn't do anything yet, but it seems like that could be useful. In particular, for the vectorrep, it means we could elide the extra copy and just sort in place. The only reason I haven't done that yet is because the use of the ArenaAllocator complicates things (I can elaborate on this if needed).

Test Plan:
make -j32 check
./db_stress --memtablerep=vector
./db_stress --memtablerep=unsorted
./db_stress --memtablerep=prefixhash --prefix_size=10

Reviewers: dhruba, haobo, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12117/Benchmarking for Merge Operator

Summary:
Updated db_bench and utilities/merge_operators.h to allow for dynamic benchmarking
of merge operators in db_bench. Added a new test (--benchmarks=mergerandom), which performs
a bunch of random Merge() operations over random keys. Also added a ""--merge_operator="" flag
so that the tester can easily benchmark different merge operators. Currently supports
the PutOperator and UInt64Add operator. Support for stringappend or list append may come later.

Test Plan:
	1. make db_bench
	2. Test the PutOperator (simulating Put) as follows:
./db_bench --benchmarks=fillrandom,readrandom,updaterandom,readrandom,mergerandom,readrandom --merge_operator=put
--threads=2

3. Test the UInt64AddOperator (simulating numeric addition) similarly:
./db_bench --value_size=8 --benchmarks=fillrandom,readrandom,updaterandom,readrandom,mergerandom,readrandom
--merge_operator=uint64add --threads=2

Reviewers: haobo, dhruba, zshao, MarkCallaghan

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11535/Minor fix to current codes

Summary:
Minor fix to current codes, including: coding style, output format,
comments. No major logic change. There are only 2 real changes, please see my inline comments.

Test Plan: make all check

Reviewers: haobo, dhruba, emayanke

Differential Revision: https://reviews.facebook.net/D12297/"
,,Rocksdb,"WAL log retention policy based on archive size.

Summary:
Archive cleaning will still happen every WAL_ttl seconds
but archived logs will be deleted only if archive size
is greater then a WAL_size_limit value.
Empty archived logs will be deleted evety WAL_ttl.

Test Plan:
1. Unit tests pass.
2. Benchmark.

Reviewers: emayanke, dhruba, haobo, sdong, kailiu, igor

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13869/Conversion of db_bench, db_stress and db_repl_stress to use gflags

Summary: Converted db_stress, db_repl_stress and db_bench to use gflags

Test Plan: I tested by printing out all the flags from old and new versions. Tried defaults, + various combinations with ""interesting flags"". Also, tested by running db_crashtest.py and db_crashtest2.py.

Reviewers: emayanke, dhruba, haobo, kailiu, sdong

Reviewed By: emayanke

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D13581/API for getting archived log files

Summary: Also expanded class LogFile to have startSequene and FileSize and exposed it publicly

Test Plan: make all check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12087/"
,,Rocksdb,"Flush the log outside of lock

Summary:
Added a new call LogFlush() that flushes the log contents to the OS buffers. We never call it with lock held.

We call it once for every Read/Write and often in compaction/flush process so the frequency should not be a problem.

Test Plan: db_test

Reviewers: dhruba, haobo, kailiu, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13935/Log flush every 0 seconds

Summary: We have to be able to catch last few log outputs before a crash

Test Plan: no

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13917/Flush Log every 5 seconds

Summary: This might help with p99 performance, but does not solve the real problem. More discussion on #2947135

Test Plan: make check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13809/"
,,Rocksdb,"Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/"
Performance Management,"Data conversion, Compression Tasks",Rocksdb,"[RocksDB] Added nano second stopwatch and new perf counters to track block read cost

Summary: The pupose of this diff is to expose per user-call level precise timing of block read, so that we can answer questions like: a Get() costs me 100ms, is that somehow related to loading blocks from file system, or sth else? We will answer that with EXACTLY how many blocks have been read, how much time was spent on transfering the bytes from os, how much time was spent on checksum verification and how much time was spent on block decompression, just for that one Get. A nano second stopwatch was introduced to track time with higher precision. The cost/precision of the stopwatch is also measured in unit-test. On my dev box, retrieving one time instance costs about 30ns, on average. The deviation of timing results is good enough to track 100ns-1us level events. And the overhead could be safely ignored for 100us level events (10000 instances/s), for example, a viewstate thrift call.

Test Plan: perf_context_test, also testing with viewstate shadow traffic.

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D12351/"
,,Rocksdb,"Task #3071144 Enhance ldb (db dump tool for leveldb) to report row counters for each row type

Summary: Added an option --count_delim=<char> which takes the given character as delimiter ('.' by default) and reports count of each row type found in the db

Reviewers: vamsi, dhruba, emayanke, kailiu

Reviewed By: vamsi

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13815/New ldb command to convert compaction style

Summary:
Add new command ""change_compaction_style"" to ldb tool. For
universal->level, it shows ""nothing to do"". For level->universal, it
compacts all files into a single one and moves the file to level 0.

Also add check for number of files at level 1+ when opening db with
universal compaction style.

Test Plan:
'make all check'. New unit test for internal convertion function. Also manully test various
cmd like:

./ldb change_compaction_style --old_compaction_style=0
--new_compaction_style=1 --db=/tmp/leveldbtest-3088/db_test

Reviewers: haobo, dhruba

Reviewed By: haobo

CC: vamsi, emayanke

Differential Revision: https://reviews.facebook.net/D12603/"
,,Rocksdb,"Fixing the warning messages captured under mac os # Consider using `git commit -m 'One line title' && arc diff`. # You will save time by running lint and unit in the background.

Summary: The work to make sure mac os compiles rocksdb is not completed yet. But at least we can start cleaning some warnings captured only by g++ from mac os..

Test Plan: ran make in mac os

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14049/"
,,Rocksdb,"LRUCache to try to clean entries not referenced first.

Summary:
With this patch, when LRUCache.Insert() is called and the cache is full, it will first try to free up entries whose reference counter is 1 (would become 0 after remo\
ving from the cache). We do it in two passes, in the first pass, we only try to release those unreferenced entries. If we cannot free enough space after traversing t\
he first remove_scan_cnt_ entries, we start from the beginning again and remove those entries being used.

Test Plan: add two unit tests to cover the codes

Reviewers: dhruba, haobo, emayanke

Reviewed By: emayanke

CC: leveldb, emayanke, xjin

Differential Revision: https://reviews.facebook.net/D13377/Move delete and free outside of crtical section

Summary: Split Unref into two parts -> cheap and expensive. Try to call expensive Unref outside of critical section to decrease lock contention.

Test Plan: unittests

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb, kailiu

Differential Revision: https://reviews.facebook.net/D13299/Minor fixes found while trying to compile it using clang on Mac OS X/"
,,Rocksdb,"Migrate names of properties from 'leveldb' prefix to 'rocksdb' prefix.

Summary: Migrate names of properties from 'leveldb' prefix to 'rocksdb' prefix.

Test Plan: make check

Reviewers: emayanke, haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13311/"
,,Rocksdb,"Fix the string format issue

Summary:

mac and our dev server has totally differnt definition of uint64_t, therefore fixing the warning in mac has actually made code in linux uncompileable.

Test Plan:

make clean && make -j32/Fixing the warning messages captured under mac os # Consider using `git commit -m 'One line title' && arc diff`. # You will save time by running lint and unit in the background.

Summary: The work to make sure mac os compiles rocksdb is not completed yet. But at least we can start cleaning some warnings captured only by g++ from mac os..

Test Plan: ran make in mac os

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14049/Task #3071144 Enhance ldb (db dump tool for leveldb) to report row counters for each row type

Summary: Added an option --count_delim=<char> which takes the given character as delimiter ('.' by default) and reports count of each row type found in the db


Reviewers: vamsi, dhruba, emayanke, kailiu

Reviewed By: vamsi

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13815/New ldb command to convert compaction style

Summary:
Add new command ""change_compaction_style"" to ldb tool. For
universal->level, it shows ""nothing to do"". For level->universal, it
compacts all files into a single one and moves the file to level 0.

Also add check for number of files at level 1+ when opening db with
universal compaction style.

Test Plan:
'make all check'. New unit test for internal convertion function. Also manully test various
cmd like:

./ldb change_compaction_style --old_compaction_style=0
--new_compaction_style=1 --db=/tmp/leveldbtest-3088/db_test

Reviewers: haobo, dhruba

Reviewed By: haobo

CC: vamsi, emayanke

Differential Revision: https://reviews.facebook.net/D12603/"
,,Rocksdb,"New ldb command to convert compaction style

Summary:
Add new command ""change_compaction_style"" to ldb tool. For
universal->level, it shows ""nothing to do"". For level->universal, it
compacts all files into a single one and moves the file to level 0.

Also add check for number of files at level 1+ when opening db with
universal compaction style.

Test Plan:
'make all check'. New unit test for internal convertion function. Also manully test various
cmd like:

./ldb change_compaction_style --old_compaction_style=0
--new_compaction_style=1 --db=/tmp/leveldbtest-3088/db_test

Reviewers: haobo, dhruba

Reviewed By: haobo

CC: vamsi, emayanke

Differential Revision: https://reviews.facebook.net/D12603/"
,Thread management,Rocksdb,"Env class that can randomly read and write

Summary: I have implemented basic simple use case that I need for External Value Store I'm working on. There is a potential for making this prettier by refactoring/combining WritableFile and RandomAccessFile, avoiding some copypasta. However, I decided to implement just the basic functionality, so I can continue working on the other diff.

Test Plan: Added a unittest

Reviewers: dhruba, haobo, kailiu

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13365/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/Implement apis in the Environment to clear out pages in the OS cache.

Summary:
Added a new api to the Environment that allows clearing out not-needed
pages from the OS cache. This will be helpful when the compressed
block cache replaces the OS cache.

Test Plan: EnvPosixTest.InvalidateCache

Reviewers: haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13041/[RocksDB] fix build env_test

Summary: move the TwoPools test to the end of thread related tests. Otherwise, the SetBackgroundThreads call would increase the Low pool size and affect the result of other tests.

Test Plan: make env_test; ./env_test

Reviewers: dhruba, emayanke, xjin

Reviewed By: xjin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12939/[RocksDB] Enhance Env to support two thread pools LOW and HIGH

Summary:
this is the ground work for separating memtable flush jobs to their own thread pool.
Both SetBackgroundThreads and Schedule take a third parameter Priority to indicate which thread pool they are working on. The names LOW and HIGH are just identifiers for two different thread pools, and does not indicate real difference in 'priority'. We can set number of threads in the pools independently.
The thread pool implementation is refactored.

Test Plan: make check

Reviewers: dhruba, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12885/"
Memory management,Memory management,Rocksdb,"Fixing the warning messages captured under mac os # Consider using `git commit -m 'One line title' && arc diff`. # You will save time by running lint and unit in the background.

Summary: The work to make sure mac os compiles rocksdb is not completed yet. But at least we can start cleaning some warnings captured only by g++ from mac os..

Test Plan: ran make in mac os

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14049/Speed up FindObsoleteFiles

Summary:
Here's one solution we discussed on speeding up FindObsoleteFiles. Keep a set of all files in DBImpl and update the set every time we create a file. I probably missed few other spots where we create a file.

It might speed things up a bit, but makes code uglier. I don't really like it.

Much better approach would be to abstract all file handling to a separate class. Think of it as layer between DBImpl and Env. Having a separate class deal with file namings and deletion would benefit both code cleanliness (especially with huge DBImpl) and speed things up. It will take a huge effort to do this, though.

Let's discuss offline today.

Test Plan: Ran ./db_stress, verified that files are getting deleted

Reviewers: dhruba, haobo, kailiu, emayanke

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D13827/WAL log retention policy based on archive size.

Summary:
Archive cleaning will still happen every WAL_ttl seconds
but archived logs will be deleted only if archive size
is greater then a WAL_size_limit value.
Empty archived logs will be deleted evety WAL_ttl.

Test Plan:
1. Unit tests pass.
2. Benchmark.

Reviewers: emayanke, dhruba, haobo, sdong, kailiu, igor

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13869/In-place updates for equal keys and similar sized values

Summary:
Currently for each put, a fresh memory is allocated, and a new entry is added to the memtable with a new sequence number irrespective of whether the key already exists in the memtable. This diff is an attempt to update the value inplace for existing keys. It currently handles a very simple case:
1. Key already exists in the current memtable. Does not inplace update values in immutable memtable or snapshot
2. Latest value type is a 'put' ie kTypeValue
3. New value size is less than existing value, to avoid reallocating memory

TODO: For a put of an existing key, deallocate memory take by values, for other value types till a kTypeValue is found, ie. remove kTypeMerge.
TODO: Update the transaction log, to allow consistent reload of the memtable.

Test Plan: Added a unit test verifying the inplace update. But some other unit tests broken due to invalid sequence number checks. WIll fix them next.

Reviewers: xinyaohu, sumeet, haobo, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12423

Automatic commit by arc/Make ""Table"" pluggable

Summary: This patch makes Table and TableBuilder a abstract class and make all the implementation of the current table into BlockedBasedTable and BlockedBasedTable Builder.

Test Plan: Make db_test.cc to work with block based table. Add a new test simple_table_db_test.cc where a different simple table format is implemented.

Reviewers: dhruba, haobo, kailiu, emayanke, vamsi

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13521/Support user-defined table stats collector

Summary:
1. Added a new option that support user-defined table stats collection.
2. Added a deleted key stats collector in `utilities`

Test Plan:
Added a unit test for newly added code.
Also ran make check to make sure other tests are not broken.

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13491/LRUCache to try to clean entries not referenced first.

Summary:
With this patch, when LRUCache.Insert() is called and the cache is full, it will first try to free up entries whose reference counter is 1 (would become 0 after remo\
ving from the cache). We do it in two passes, in the first pass, we only try to release those unreferenced entries. If we cannot free enough space after traversing t\
he first remove_scan_cnt_ entries, we start from the beginning again and remove those entries being used.

Test Plan: add two unit tests to cover the codes

Reviewers: dhruba, haobo, emayanke

Reviewed By: emayanke

CC: leveldb, emayanke, xjin

Differential Revision: https://reviews.facebook.net/D13377/[Rocksdb] Submit mem table flush job in a different thread pool

Summary: As title. This is just a quick hack and not ready for commit. fails a lot of unit test. I will test/debug it directly in ViewState shadow .

Test Plan: Try it in shadow test.

Reviewers: dhruba, xjin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12933/Added a parameter to limit the maximum space amplification for universal compaction.

Summary:
Added a new field called max_size_amplification_ratio in the
CompactionOptionsUniversal structure. This determines the maximum
percentage overhead of space amplification.

The size amplification is defined to be the ratio between the size of
the oldest file to the sum of the sizes of all other files. If the
size amplification exceeds the specified value, then min_merge_width
and max_merge_width are ignored and a full compaction of all files is done.
A value of 10 means that the size a database that stores 100 bytes
of user data could occupy 110 bytes of physical storage.

Test Plan: Unit test DBTest.UniversalCompactionSpaceAmplification added.

Reviewers: haobo, emayanke, xjin

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12825/[RocksDB] Remove Log file immediately after memtable flush

Summary: As title. The DB log file life cycle is tied up with the memtable it backs. Once the memtable is flushed to sst and committed, we should be able to delete the log file, without holding the mutex. This is part of the bigger change to avoid FindObsoleteFiles at runtime. It deals with log files. sst files will be dealt with later.

Test Plan: make check; db_bench

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11709/An iterator may automatically invoke reseeks.

Summary:
An iterator invokes reseek if the number of sequential skips over the
same userkey exceeds a configured number. This makes iter->Next()
faster (bacause of fewer key compares) if a large number of
adjacent internal keys in a table (sst or memtable) have the
same userkey.

Test Plan: Unit test DBTest.IterReseek.

Reviewers: emayanke, haobo, xjin

Reviewed By: xjin

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D11865/"
,,Rocksdb,"LRUCache to try to clean entries not referenced first.

Summary:
With this patch, when LRUCache.Insert() is called and the cache is full, it will first try to free up entries whose reference counter is 1 (would become 0 after remo\
ving from the cache). We do it in two passes, in the first pass, we only try to release those unreferenced entries. If we cannot free enough space after traversing t\
he first remove_scan_cnt_ entries, we start from the beginning again and remove those entries being used.

Test Plan: add two unit tests to cover the codes

Reviewers: dhruba, haobo, emayanke

Reviewed By: emayanke

CC: leveldb, emayanke, xjin

Differential Revision: https://reviews.facebook.net/D13377/Minor: Fix a lint error in cache_test.cc

Summary:
As title. Fix an lint error:

Lint: CppLint Error
Single-argument constructor 'Value(int v)' may inadvertently be used as a type conversion constructor. Prefix the function with the 'explicit' keyword to avoid this, or add an /* implicit */ comment to suppress this warning.

Test Plan: N/A

Reviewers: emayanke, haobo, dhruba

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13401/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/"
,,Rocksdb,"WriteBatch::Put() overload that gathers key and value from arrays of slices

Summary: In our project, when writing to the database, we want to form the value as the concatenation of a small header and a larger payload.  It's a shame to have to copy the payload just so we can give RocksDB API a linear view of the value.  Since RocksDB makes a copy internally, it's easy to support gather writes.

Test Plan: write_batch_test, new test case

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13947/Add three new MemTableRep's

Summary:
This patch adds three new MemTableRep's: UnsortedRep, PrefixHashRep, and VectorRep.

UnsortedRep stores keys in an std::unordered_map of std::sets. When an iterator is ed, it dumps the keys into an std::set and iterates over that.

VectorRep stores keys in an std::vector. When an iterator is ed, it creates a copy of the vector and sorts it using std::sort. The iterator accesses that new vector.

PrefixHashRep stores keys in an unordered_map mapping prefixes to ordered sets.

I also added one API change. I added a function MemTableRep::MarkImmutable. This function is called when the rep is added to the immutable list. It doesn't do anything yet, but it seems like that could be useful. In particular, for the vectorrep, it means we could elide the extra copy and just sort in place. The only reason I haven't done that yet is because the use of the ArenaAllocator complicates things (I can elaborate on this if needed).

Test Plan:
make -j32 check
./db_stress --memtablerep=vector
./db_stress --memtablerep=unsorted
./db_stress --memtablerep=prefixhash --prefix_size=10

Reviewers: dhruba, haobo, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12117/"
Compression tasks,"Compression Tasks, Thread management",Rocksdb,"Implement a compressed block cache.

Summary:
Rocksdb can now support a uncompressed block cache, or a compressed
block cache or both. Lookups first look for a block in the
uncompressed cache, if it is not found only then it is looked up
in the compressed cache. If it is found in the compressed cache,
then it is uncompressed and inserted into the uncompressed cache.

It is possible that the same block resides in the compressed cache
as well as the uncompressed cache at the same time. Both caches
have their own individual LRU policy.

Test Plan: Unit test case attached.

Reviewers: kailiu, sdong, haobo, leveldb

Reviewed By: haobo

CC: xjin, haobo

Differential Revision: https://reviews.facebook.net/D12675/Dbid feature

Summary:
Create a new type of file on startup if it doesn't already exist called DBID.
This will store a unique number generated from boost library's uuid header file.
The use-case is to identify the case of a db losing all its data and coming back up either empty or from an image(backup/live replica's recovery)
the key point to note is that DBID is not stored in a backup or db snapshot
It's preferable to use Boost for uuid because:
1) A non-standard way of generating uuid is not good
2) /proc/sys/kernel/random/uuid generates a uuid but only on linux environments and the solution would not be clean
3) c++ doesn't have any direct way to get a uuid
4) Boost is a very good library that was already having linkage in rocksdb from third-party
Note: I had to update the TOOLCHAIN_REV in build files to get latest verison of boost from third-party as the older version had a bug.
I had to put Wno-uninitialized in Makefile because boost-1.51 has an unitialized variable and rocksdb would not comiple otherwise. Latet open-source for boost is 1.54 but is not there in third-party. I have notified the concerned people in fbcode about it.
@kailiu : While releasing to third-party, an additional dependency will need to be created for boost in TARGETS file. I can help identify.

Test Plan:
Expand db_test to test 2 cases
1) Restarting db with Id file present - verify that no change to Id
2)Restarting db with Id file deleted - verify that a different Id is there after reopen
Also run make all check

Reviewers: dhruba, haobo, kailiu, sdong

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13587/Env class that can randomly read and write

Summary: I have implemented basic simple use case that I need for External Value Store I'm working on. There is a potential for making this prettier by refactoring/combining WritableFile and RandomAccessFile, avoiding some copypasta. However, I decided to implement just the basic functionality, so I can continue working on the other diff.

Test Plan: Added a unittest

Reviewers: dhruba, haobo, kailiu

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13365/Migrate names of properties from 'leveldb' prefix to 'rocksdb' prefix.

Summary: Migrate names of properties from 'leveldb' prefix to 'rocksdb' prefix.

Test Plan: make check

Reviewers: emayanke, haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13311/Implement apis in the Environment to clear out pages in the OS cache.

Summary:
Added a new api to the Environment that allows clearing out not-needed
pages from the OS cache. This will be helpful when the compressed
block cache replaces the OS cache.

Test Plan: EnvPosixTest.InvalidateCache

Reviewers: haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13041/[RocksDB] Enhance Env to support two thread pools LOW and HIGH

Summary:
this is the ground work for separating memtable flush jobs to their own thread pool.
Both SetBackgroundThreads and Schedule take a third parameter Priority to indicate which thread pool they are working on. The names LOW and HIGH are just identifiers for two different thread pools, and does not indicate real difference in 'priority'. We can set number of threads in the pools independently.
The thread pool implementation is refactored.

Test Plan: make check

Reviewers: dhruba, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12885/[RocksDB] Added nano second stopwatch and new perf counters to track block read cost

Summary: The pupose of this diff is to expose per user-call level precise timing of block read, so that we can answer questions like: a Get() costs me 100ms, is that somehow related to loading blocks from file system, or sth else? We will answer that with EXACTLY how many blocks have been read, how much time was spent on transfering the bytes from os, how much time was spent on checksum verification and how much time was spent on block decompression, just for that one Get. A nano second stopwatch was introduced to track time with higher precision. The cost/precision of the stopwatch is also measured in unit-test. On my dev box, retrieving one time instance costs about 30ns, on average. The deviation of timing results is good enough to track 100ns-1us level events. And the overhead could be safely ignored for 100us level events (10000 instances/s), for example, a viewstate thrift call.

Test Plan: perf_context_test, also testing with viewstate shadow traffic.

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D12351/"
,,Rocksdb,"Prefix scan: db_bench and bug fixes

Summary: If use_prefix_filters is set and read_range>1, then the random seeks will set a the prefix filter to be the prefix of the key which was randomly selected as the target.  Still need to add statistics (perhaps in a separate diff).

Test Plan: ./db_bench --benchmarks=fillseq,prefixscanrandom --num=10000000 --statistics=1 --use_prefix_blooms=1 --use_prefix_api=1 --bloom_bits=10

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, haobo

Differential Revision: https://reviews.facebook.net/D12273/"
,,Rocksdb,"Prefix scan: db_bench and bug fixes

Summary: If use_prefix_filters is set and read_range>1, then the random seeks will set a the prefix filter to be the prefix of the key which was randomly selected as the target.  Still need to add statistics (perhaps in a separate diff).

Test Plan: ./db_bench --benchmarks=fillseq,prefixscanrandom --num=10000000 --statistics=1 --use_prefix_blooms=1 --use_prefix_api=1 --bloom_bits=10

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, haobo

Differential Revision: https://reviews.facebook.net/D12273/Expose statistic for sequence number and implement setTickerCount

Summary: statistic for sequence number is needed by wormhole. setTickerCount is demanded for this statistic. I can't simply recordTick(max_sequence) when db recovers because the statistic iobject is owned by client and may/may not be reset during reopen. Eg. statistic is reset in mcrocksdb whereas it is not in db_stress. Therefore it is best to go with setTickerCount

Test Plan: ./db_stress ... --statistics=1 and observed expected sequence number

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12327/"
,,Rocksdb,"Fixing the warning messages captured under mac os # Consider using `git commit -m 'One line title' && arc diff`. # You will save time by running lint and unit in the background.

Summary: The work to make sure mac os compiles rocksdb is not completed yet. But at least we can start cleaning some warnings captured only by g++ from mac os..

Test Plan: ran make in mac os

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14049/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/KeyMayExist for ttl

Summary: value needed to be filtered of timestamp

Test Plan: ./ttl_test

Reviewers: dhruba, haobo, vamsi

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12657/"
,,Rocksdb,"Merge operator fixes part 1.

Summary:
-Added null checks and revisions to DBIter::MergeValuesNewToOld()
-Added DBIter test to stringappend_test
-Major fix with Merge and TTL
More plans for fixes later.

Test Plan:
-make clean; make stringappend_test -j 32; ./stringappend_test
-make all check;

Reviewers: haobo, emayanke, vamsi, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12315/"
,,Rocksdb,Minor fixes found while trying to compile it using clang on Mac OS X/
,Thread management,Rocksdb,"Merge operator fixes part 1.

Summary:
-Added null checks and revisions to DBIter::MergeValuesNewToOld()
-Added DBIter test to stringappend_test
-Major fix with Merge and TTL
More plans for fixes later.

Test Plan:
-make clean; make stringappend_test -j 32; ./stringappend_test
-make all check;

Reviewers: haobo, emayanke, vamsi, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12315/Benchmarking for Merge Operator

Summary:
Updated db_bench and utilities/merge_operators.h to allow for dynamic benchmarking
of merge operators in db_bench. Added a new test (--benchmarks=mergerandom), which performs
a bunch of random Merge() operations over random keys. Also added a ""--merge_operator="" flag
so that the tester can easily benchmark different merge operators. Currently supports
the PutOperator and UInt64Add operator. Support for stringappend or list append may come later.

Test Plan:
	1. make db_bench
	2. Test the PutOperator (simulating Put) as follows:
./db_bench --benchmarks=fillrandom,readrandom,updaterandom,readrandom,mergerandom,readrandom --merge_operator=put
--threads=2

3. Test the UInt64AddOperator (simulating numeric addition) similarly:
./db_bench --value_size=8 --benchmarks=fillrandom,readrandom,updaterandom,readrandom,mergerandom,readrandom
--merge_operator=uint64add --threads=2

Reviewers: haobo, dhruba, zshao, MarkCallaghan

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11535/"
,,Rocksdb,"Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/Made merge_oprator a shared_ptr; and added TTL unit tests

Test Plan:
- make all check;
- make release;
- make stringappend_test; ./stringappend_test

Reviewers: haobo, emayanke

Reviewed By: haobo

CC: leveldb, kailiu

Differential Revision: https://reviews.facebook.net/D12381/Merge operator fixes part 1.

Summary:
-Added null checks and revisions to DBIter::MergeValuesNewToOld()
-Added DBIter test to stringappend_test
-Major fix with Merge and TTL
More plans for fixes later.

Test Plan:
-make clean; make stringappend_test -j 32; ./stringappend_test
-make all check;

Reviewers: haobo, emayanke, vamsi, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12315/"
,,Rocksdb,"Introduced a new flag non_blocking_io in ReadOptions.

Summary:
If ReadOptions.non_blocking_io is set to true, then KeyMayExists
and Iterators will return data that is cached in RAM.
If the Iterator needs to do IO from storage to serve the data,
then the Iterator.status() will return Status::IsRetry().

Test Plan:
Enhanced unit test DBTest.KeyMayExist to detect if there were are IOs
issues from storage. Added DBTest.NonBlockingIteration to verify
nonblocking Iterations.

Reviewers: emayanke, haobo

Reviewed By: haobo

CC: leveldb

Maniphest Tasks: T63

Differential Revision: https://reviews.facebook.net/D12531/Revert ""Prefix scan: db_bench and bug fixes""

This reverts commit c2bd8f4824bda98db8699f1e08d6969cf21ef86f./Prefix scan: db_bench and bug fixes

Summary: If use_prefix_filters is set and read_range>1, then the random seeks will set a the prefix filter to be the prefix of the key which was randomly selected as the target.  Still need to add statistics (perhaps in a separate diff).

Test Plan: ./db_bench --benchmarks=fillseq,prefixscanrandom --num=10000000 --statistics=1 --use_prefix_blooms=1 --use_prefix_api=1 --bloom_bits=10

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, haobo

Differential Revision: https://reviews.facebook.net/D12273/"
,,Rocksdb,"[RocksDB] Minor iterator cleanup

Summary: Was going through the iterator related code, did some cleanup along the way. Basically replaced array with vector and adopted range based loop where applicable.

Test Plan: make check; make valgrind_check

Reviewers: dhruba, emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12435/"
,,Rocksdb,"Fix a bug in table builder

Summary:
In talbe.cc, when reading the metablock, it uses BytewiseComparator();
However in table_builder.cc, we use r->options.comparator. After tracing
the creation of r->options.comparator, I found this comparator is an
InternalKeyComparator, which wraps the user defined comparator(details
can be found in DBImpl::SanitizeOptions().

I encountered this problem when adding metadata about ""bloom filter""
before. With different comparator, we may fail to do the binary sort.

Current code works well since there is only one entry in meta block.

Test Plan:
make all check

I've also tested this change in https://reviews.facebook.net/D8283 before.

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13335/"
,,Rocksdb,"Change a typo in method signature/Fix a bug in table builder

Summary:
In talbe.cc, when reading the metablock, it uses BytewiseComparator();
However in table_builder.cc, we use r->options.comparator. After tracing
the creation of r->options.comparator, I found this comparator is an
InternalKeyComparator, which wraps the user defined comparator(details
can be found in DBImpl::SanitizeOptions().

I encountered this problem when adding metadata about ""bloom filter""
before. With different comparator, we may fail to do the binary sort.

Current code works well since there is only one entry in meta block.

Test Plan:
make all check

I've also tested this change in https://reviews.facebook.net/D8283 before.

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13335/"
,,Rocksdb,"Revert ""Prefix scan: db_bench and bug fixes""

This reverts commit c2bd8f4824bda98db8699f1e08d6969cf21ef86f./Prefix scan: db_bench and bug fixes

Summary: If use_prefix_filters is set and read_range>1, then the random seeks will set a the prefix filter to be the prefix of the key which was randomly selected as the target.  Still need to add statistics (perhaps in a separate diff).

Test Plan: ./db_bench --benchmarks=fillseq,prefixscanrandom --num=10000000 --statistics=1 --use_prefix_blooms=1 --use_prefix_api=1 --bloom_bits=10

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, haobo

Differential Revision: https://reviews.facebook.net/D12273/"
,,Rocksdb,"Implement a compressed block cache.

Summary:
Rocksdb can now support a uncompressed block cache, or a compressed
block cache or both. Lookups first look for a block in the
uncompressed cache, if it is not found only then it is looked up
in the compressed cache. If it is found in the compressed cache,
then it is uncompressed and inserted into the uncompressed cache.

It is possible that the same block resides in the compressed cache
as well as the uncompressed cache at the same time. Both caches
have their own individual LRU policy.

Test Plan: Unit test case attached.

Reviewers: kailiu, sdong, haobo, leveldb

Reviewed By: haobo

CC: xjin, haobo

Differential Revision: https://reviews.facebook.net/D12675/"
,Thread management,Rocksdb,"Fix stress test failure when using mmap-reads.

Summary:
The mmap-read file->Read() does not use the scratch buffer to
read in file-contents.

Test Plan: ./db_stress --test_batches_snapshots=1 --ops_per_thread=100000000 --threads=32 --write_buffer_size=4194304 --destroy_db_initially=0 --reopen=0 --readpercent=45 --prefixpercent=5 --writepercent=35 --delpercent=5 --iterpercent=10 --db=/tmp/dhruba --max_key=100000000 --disable_seek_compaction=0 --mmap_read=1 --block_size=16384 --cache_size=1048576 --open_files=500000 --verify_checksum=1 --sync=1 --disable_wal=0 --disable_data_sync=0 --target_file_size_base=2097152 --target_file_size_multiplier=2 --max_write_buffer_number=3 --max_background_compactions=20 --max_bytes_for_level_base=10485760 --filter_deletes=0

Reviewers: haobo, kailiu

Reviewed By: kailiu

CC: leveldb, kailiu, emayanke

Differential Revision: https://reviews.facebook.net/D13923/Implement a compressed block cache.

Summary:
Rocksdb can now support a uncompressed block cache, or a compressed
block cache or both. Lookups first look for a block in the
uncompressed cache, if it is not found only then it is looked up
in the compressed cache. If it is found in the compressed cache,
then it is uncompressed and inserted into the uncompressed cache.

It is possible that the same block resides in the compressed cache
as well as the uncompressed cache at the same time. Both caches
have their own individual LRU policy.

Test Plan: Unit test case attached.

Reviewers: kailiu, sdong, haobo, leveldb

Reviewed By: haobo

CC: xjin, haobo

Differential Revision: https://reviews.facebook.net/D12675/"
Compression tasks,"Compression Tasks, Thread management",Rocksdb,"Implement a compressed block cache.

Summary:
Rocksdb can now support a uncompressed block cache, or a compressed
block cache or both. Lookups first look for a block in the
uncompressed cache, if it is not found only then it is looked up
in the compressed cache. If it is found in the compressed cache,
then it is uncompressed and inserted into the uncompressed cache.

It is possible that the same block resides in the compressed cache
as well as the uncompressed cache at the same time. Both caches
have their own individual LRU policy.

Test Plan: Unit test case attached.

Reviewers: kailiu, sdong, haobo, leveldb

Reviewed By: haobo

CC: xjin, haobo

Differential Revision: https://reviews.facebook.net/D12675/[RocksDB] Added nano second stopwatch and new perf counters to track block read cost

Summary: The pupose of this diff is to expose per user-call level precise timing of block read, so that we can answer questions like: a Get() costs me 100ms, is that somehow related to loading blocks from file system, or sth else? We will answer that with EXACTLY how many blocks have been read, how much time was spent on transfering the bytes from os, how much time was spent on checksum verification and how much time was spent on block decompression, just for that one Get. A nano second stopwatch was introduced to track time with higher precision. The cost/precision of the stopwatch is also measured in unit-test. On my dev box, retrieving one time instance costs about 30ns, on average. The deviation of timing results is good enough to track 100ns-1us level events. And the overhead could be safely ignored for 100us level events (10000 instances/s), for example, a viewstate thrift call.

Test Plan: perf_context_test, also testing with viewstate shadow traffic.

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D12351/"
DataBase Management,DataBase Management,Rocksdb,"Add the index/filter block cache

Summary: This diff leverage the existing block cache and extend it to cache index/filter block.

Test Plan:
Added new tests in db_test and table_test

The correctness is checked by:

1. make check
2. make valgrind_check

Performance is test by:

1. 10 times of build_tools/regression_build_test.sh on two versions of rocksdb before/after the code change. Test results suggests no significant difference between them. For the two key operatons `overwrite` and `readrandom`, the average iops are both 20k and ~260k, with very small variance).
2. db_stress.

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, haobo, xjin

Differential Revision: https://reviews.facebook.net/D13167/Move down the time consuming tests in table_test

Summary:

it helps us to better check the tests we really care.

Test Plan:

make/Provide mechanism to configure when to flush the block

Summary: Allow block based table to configure the way flushing the blocks. This feature will allow us to add support for prefix-aligned block.

Test Plan: make check

Reviewers: dhruba, haobo, sdong, igor

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13875/Follow-up Cleaning-up After D13521

Summary:
This patch is to address @haobo's comments on D13521:
1. rename Table to be TableReader and make its factory function to be GetTableReader
2. move the compression type selection logic out of TableBuilder but to compaction logic
3. more accurate comments
4. Move stat name constants into BlockBasedTable implementation.
5. remove some uncleaned codes in simple_table_db_test

Test Plan: pass test suites.

Reviewers: haobo, dhruba, kailiu

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13785/Make ""Table"" pluggable

Summary: This patch makes Table and TableBuilder a abstract class and make all the implementation of the current table into BlockedBasedTable and BlockedBasedTable Builder.

Test Plan: Make db_test.cc to work with block based table. Add a new test simple_table_db_test.cc where a different simple table format is implemented.

Reviewers: dhruba, haobo, kailiu, emayanke, vamsi

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13521/Fix the valgrind error in newly added unittests for table stats

Summary:

Previous the newly added test called NewBloomFilter without releasing it at the end of the test, which resulted in memory leak and was detected by valgrind.

Test Plan:

Ran valgrind test./Add bloom filter to predefined table stats

Summary: As title.

Test Plan: Updated the unit tests to make sure new statistic is correctly written/read.

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13497/Add statistics to sst file

Summary:
So far we only have key/value pairs as well as bloom filter stored in the
sst file.  It will be great if we are able to store more metadata about
this table itself, for example, the entry size, bloom filter name, etc.

This diff is the first step of this effort. It allows table to keep the
basic statistics mentioned in http://fburl.com/14995441, as well as
allowing writing user-collected stats to stats block.

After this diff, we will figure out the interface of how to allow user to collect their interested statistics.

Test Plan:
1. Added several unit tests.
2. Ran `make check` to ensure it doesn't break other tests.

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13419/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/"
,Thread management,Rocksdb,"Fixing build failure

Summary: virtual NewRandomRWFile is not implemented on EnvHdfs, causing build failure.

Test Plan: make clean; make all check

Reviewers: dhruba, haobo, kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13383/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/[RocksDB] Enhance Env to support two thread pools LOW and HIGH

Summary:
this is the ground work for separating memtable flush jobs to their own thread pool.
Both SetBackgroundThreads and Schedule take a third parameter Priority to indicate which thread pool they are working on. The names LOW and HIGH are just identifiers for two different thread pools, and does not indicate real difference in 'priority'. We can set number of threads in the pools independently.
The thread pool implementation is refactored.

Test Plan: make check

Reviewers: dhruba, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12885/"
,,Rocksdb,"Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/Prefix scan: db_bench and bug fixes

Summary: If use_prefix_filters is set and read_range>1, then the random seeks will set a the prefix filter to be the prefix of the key which was randomly selected as the target.  Still need to add statistics (perhaps in a separate diff).

Test Plan: ./db_bench --benchmarks=fillseq,prefixscanrandom --num=10000000 --statistics=1 --use_prefix_blooms=1 --use_prefix_api=1 --bloom_bits=10

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, haobo

Differential Revision: https://reviews.facebook.net/D12273/"
Data Conversion,"Data Conversion, Thread management",Rocksdb,"Fix the string format issue

Summary:

mac and our dev server has totally differnt definition of uint64_t, therefore fixing the warning in mac has actually made code in linux uncompileable.

Test Plan:

make clean && make -j32/Fix deleting files

Summary: One more fix! In some cases, our filenames start with ""/"". Apparently, env_ can't handle filenames with double //

Test Plan:
deletefile_test does not include this line in the LOG anymore:
2013/11/12-18:11:43.150149 7fe4a6fff700 RenameFile logfile #3 FAILED -- IO error: /tmp/rocksdbtest-3574/deletefile_test//000003.log: No such file or directory

Reviewers: dhruba, haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14055/Fixing the warning messages captured under mac os # Consider using `git commit -m 'One line title' && arc diff`. # You will save time by running lint and unit in the background.

Summary: The work to make sure mac os compiles rocksdb is not completed yet. But at least we can start cleaning some warnings captured only by g++ from mac os..

Test Plan: ran make in mac os

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14049/Small changes in Deleting obsolete files

Summary:
@haobo's suggestions from https://reviews.facebook.net/D13827

Renaming some variables, deprecating purge_log_after_flush, changing for loop into auto for loop.

I have not implemented deleting objects outside of mutex yet because it would require a big code change - we would delete object in db_impl, which currently does not know anything about object because it's defined in version_edit.h (FileMetaData). We should do it at some point, though.

Test Plan: Ran deletefile_test

Reviewers: haobo

Reviewed By: haobo

CC: leveldb, haobo

Differential Revision: https://reviews.facebook.net/D14025/Combine two FindObsoleteFiles()

Summary: We don't need to call FindObsoleteFiles() twice

Test Plan: deletefile_test

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14007/Speed up FindObsoleteFiles

Summary:
Here's one solution we discussed on speeding up FindObsoleteFiles. Keep a set of all files in DBImpl and update the set every time we create a file. I probably missed few other spots where we create a file.

It might speed things up a bit, but makes code uglier. I don't really like it.

Much better approach would be to abstract all file handling to a separate class. Think of it as layer between DBImpl and Env. Having a separate class deal with file namings and deletion would benefit both code cleanliness (especially with huge DBImpl) and speed things up. It will take a huge effort to do this, though.

Let's discuss offline today.

Test Plan: Ran ./db_stress, verified that files are getting deleted

Reviewers: dhruba, haobo, kailiu, emayanke

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D13827/Flush the log outside of lock

Summary:
Added a new call LogFlush() that flushes the log contents to the OS buffers. We never call it with lock held.

We call it once for every Read/Write and often in compaction/flush process so the frequency should not be a problem.

Test Plan: db_test

Reviewers: dhruba, haobo, kailiu, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13935/[RocksDB] Generalize prefix-aware iterator to be used for more than one Seek

Summary: Added a prefix_seek flag in ReadOptions to indicate that Seek is prefix aware(might not return data with different prefix), and also not bound to a specific prefix. Multiple Seeks and range scans can be invoked on the same iterator. If a specific prefix is specified, this flag will be ignored. Just a quick prototype that works for PrefixHashRep, the new lockless memtable could be easily extended with this support too.

Test Plan: test it on Leaf

Reviewers: dhruba, kailiu, sdong, igor

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13929/WAL log retention policy based on archive size.

Summary:
Archive cleaning will still happen every WAL_ttl seconds
but archived logs will be deleted only if archive size
is greater then a WAL_size_limit value.
Empty archived logs will be deleted evety WAL_ttl.

Test Plan:
1. Unit tests pass.
2. Benchmark.

Reviewers: emayanke, dhruba, haobo, sdong, kailiu, igor

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13869/Implement a compressed block cache.

Summary:
Rocksdb can now support a uncompressed block cache, or a compressed
block cache or both. Lookups first look for a block in the
uncompressed cache, if it is not found only then it is looked up
in the compressed cache. If it is found in the compressed cache,
then it is uncompressed and inserted into the uncompressed cache.

It is possible that the same block resides in the compressed cache
as well as the uncompressed cache at the same time. Both caches
have their own individual LRU policy.

Test Plan: Unit test case attached.

Reviewers: kailiu, sdong, haobo, leveldb

Reviewed By: haobo

CC: xjin, haobo

Differential Revision: https://reviews.facebook.net/D12675/Follow-up Cleaning-up After D13521

Summary:
This patch is to address @haobo's comments on D13521:
1. rename Table to be TableReader and make its factory function to be GetTableReader
2. move the compression type selection logic out of TableBuilder but to compaction logic
3. more accurate comments
4. Move stat name constants into BlockBasedTable implementation.
5. remove some uncleaned codes in simple_table_db_test

Test Plan: pass test suites.

Reviewers: haobo, dhruba, kailiu

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13785/If a Put fails, fail all other puts

Summary:
When a Put fails, it can leave database in a messy state. We don't want to pretend that everything is OK when it may not be. We fail every write following the failed one.

I added checks for corruption to DBImpl::Write(). Is there anywhere else I need to add them?

Test Plan: Corruption unit test.

Reviewers: dhruba, haobo, kailiu

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13671/Unify DeleteFile and DeleteWalFiles

Summary:
This is to simplify rocksdb public APIs and improve the code quality.
Created an additional parameter to ParseFileName for log sub type and improved the code for deleting a wal file.
Wrote exhaustive unit-tests in delete_file_test
Unification of other redundant APIs can be taken up in a separate diff

Test Plan: Expanded delete_file test

Reviewers: dhruba, haobo, kailiu, sdong

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13647/Fix the log number bug when updating MANIFEST file

Summary:
Crash may occur during the flushes of more than two mem tables.

As the info log suggested, even when both were successfully flushed,
the recovery process still pick up one of the memtable's log for recovery.

This diff fix the problem by setting the correct ""log number"" in MANIFEST.

Test Plan: make test; deployed to leaf4 and make sure it doesn't result in crashes of this type.

Reviewers: haobo, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13659/Dbid feature

Summary:
Create a new type of file on startup if it doesn't already exist called DBID.
This will store a unique number generated from boost library's uuid header file.
The use-case is to identify the case of a db losing all its data and coming back up either empty or from an image(backup/live replica's recovery)
the key point to note is that DBID is not stored in a backup or db snapshot
It's preferable to use Boost for uuid because:
1) A non-standard way of generating uuid is not good
2) /proc/sys/kernel/random/uuid generates a uuid but only on linux environments and the solution would not be clean
3) c++ doesn't have any direct way to get a uuid
4) Boost is a very good library that was already having linkage in rocksdb from third-party
Note: I had to update the TOOLCHAIN_REV in build files to get latest verison of boost from third-party as the older version had a bug.
I had to put Wno-uninitialized in Makefile because boost-1.51 has an unitialized variable and rocksdb would not comiple otherwise. Latet open-source for boost is 1.54 but is not there in third-party. I have notified the concerned people in fbcode about it.
@kailiu : While releasing to third-party, an additional dependency will need to be created for boost in TARGETS file. I can help identify.

Test Plan:
Expand db_test to test 2 cases
1) Restarting db with Id file present - verify that no change to Id
2)Restarting db with Id file deleted - verify that a different Id is there after reopen
Also run make all check

Reviewers: dhruba, haobo, kailiu, sdong

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13587/Universal Compaction to Have a Size Percentage Threshold To Decide Whether to Compress

Summary:
This patch adds a option for universal compaction to allow us to only compress output files if the files compacted previously did not yet reach a specified ratio, to save CPU costs in some cases.

Compression is always skipped for flushing. This is because the size information is not easy to evaluate for flushing case. We can improve it later.

Test Plan:
add test
DBTest.UniversalCompactionCompressRatio1 and DBTest.UniversalCompactionCompressRatio12

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13467/Enable background flush thread by default and fix issues related to it

Summary:
Enable background flush thread in this patch and fix unit tests with:
(1) After background flush, schedule a background compaction if condition satisfied;
(2) Fix a bug that if universal compaction is enabled and number of levels are set to be 0, compaction will not be automatically triggered
(3) Fix unit tests to wait for compaction to finish instead of flush, before checking the compaction results.

Test Plan: pass all unit tests

Reviewers: haobo, xjin, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13461/Change Function names from Compaction->Flush When they really mean Flush

Summary: When I debug the unit test failures when enabling background flush thread, I feel the function names can be made clearer for people to understand. Also, if the names are fixed, in many places, some tests' bugs are obvious (and some of those tests are failing). This patch is to clean it up for future maintenance.

Test Plan: Run test suites.

Reviewers: haobo, dhruba, xjin

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13431/Add option for storing transaction logs in a separate dir

Summary: In some cases, you might not want to store the data log (write ahead log) files in the same dir as the sst files. An example use case is leaf, which stores sst files in tmpfs. And would like to save the log files in a separate dir (disk) to save memory.

Test Plan: make all. Ran db_test test. A few test failing. P2785018. If you guys don't see an obvious problem with the code, maybe somebody from the rocksdb team could help me debug the issue here. Running this on leaf worked well. I could see logs stored on disk, and deleted appropriately after compactions. Obviously this is only one set of options. The unit tests cover different options. Seems like I'm missing some edge cases.

Reviewers: dhruba, haobo, leveldb

CC: xinyaohu, sumeet

Differential Revision: https://reviews.facebook.net/D13239/Migrate names of properties from 'leveldb' prefix to 'rocksdb' prefix.

Summary: Migrate names of properties from 'leveldb' prefix to 'rocksdb' prefix.

Test Plan: make check

Reviewers: emayanke, haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13311/[RocksDB] Still honor DisableFileDeletions when purge_log_after_memtable_flush is on

Summary: as title

Test Plan: make check

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13263/[Rocksdb] Submit mem table flush job in a different thread pool

Summary: As title. This is just a quick hack and not ready for commit. fails a lot of unit test. I will test/debug it directly in ViewState shadow .

Test Plan: Try it in shadow test.

Reviewers: dhruba, xjin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12933/[RocksDB] Move last_sequence and last_flushed_sequence_ update back into lock protected area

Summary: A previous diff moved these outside of lock protected area. Moved back in now. Also moved tmp_batch_ update outside of lock protected area, as only the single write thread can access it.

Test Plan: make check

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13137/[RocksDB] Remove Log file immediately after memtable flush

Summary: As title. The DB log file life cycle is tied up with the memtable it backs. Once the memtable is flushed to sst and committed, we should be able to delete the log file, without holding the mutex. This is part of the bigger change to avoid FindObsoleteFiles at runtime. It deals with log files. sst files will be dealt with later.

Test Plan: make check; db_bench

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11709/Return pathname relative to db dir in LogFile and cleanup AppendSortedWalsOfType

Summary: So that replication can just download from wherever LogFile.Pathname is pointing them.

Test Plan: make all check;./db_repl_stress

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12609/New ldb command to convert compaction style

Summary:
Add new command ""change_compaction_style"" to ldb tool. For
universal->level, it shows ""nothing to do"". For level->universal, it
compacts all files into a single one and moves the file to level 0.

Also add check for number of files at level 1+ when opening db with
universal compaction style.

Test Plan:
'make all check'. New unit test for internal convertion function. Also manully test various
cmd like:

./ldb change_compaction_style --old_compaction_style=0
--new_compaction_style=1 --db=/tmp/leveldbtest-3088/db_test

Reviewers: haobo, dhruba

Reviewed By: haobo

CC: vamsi, emayanke

Differential Revision: https://reviews.facebook.net/D12603/Fix build caused by DeleteFile not tolerating / at the beginning

Summary: db->DeleteFile calls ParseFileName to check name that was returned for sst file. Now, sst filename is returned using TableFileName which uses MakeFileName. This puts a / at the front of the name and ParseFileName doesn't like that. Changed ParseFileName to tolerate /s at the beginning. The test delet_file_test used to pass earlier because this behaviour of MakeFileName had been changed a while back to not return a / during which delete_file_test was checked in. But MakeFileName had to be reverted to add / at the front because GetLiveFiles used at many places outside rocksdb used the previous behaviour of MakeFileName.

Test Plan: make;./delete_filetest;make all check

Reviewers: dhruba, haobo, vamsi

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12663/Cleanup DeleteFile API

Summary:
The DeleteFile API was removing files inside the db-lock. This
is now changed to remove files outside the db-lock.
The GetLiveFilesMetadata() returns the smallest and largest
seqnuence number of each file as well.

Test Plan: deletefile_test

Reviewers: emayanke, haobo

Reviewed By: haobo

CC: leveldb

Maniphest Tasks: T63

Differential Revision: https://reviews.facebook.net/D12567/[RocksDB] Fix TransformRepFactory related valgrind problem

Summary: Let TransformRepFactory own the passed in transform. Also make it better encapsulated.

Test Plan: make valgrind_check;

Reviewers: dhruba, emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12591/Introduced a new flag non_blocking_io in ReadOptions.

Summary:
If ReadOptions.non_blocking_io is set to true, then KeyMayExists
and Iterators will return data that is cached in RAM.
If the Iterator needs to do IO from storage to serve the data,
then the Iterator.status() will return Status::IsRetry().

Test Plan:
Enhanced unit test DBTest.KeyMayExist to detect if there were are IOs
issues from storage. Added DBTest.NonBlockingIteration to verify
nonblocking Iterations.

Reviewers: emayanke, haobo

Reviewed By: haobo

CC: leveldb

Maniphest Tasks: T63

Differential Revision: https://reviews.facebook.net/D12531/[RocksDB] move stats counting outside of mutex protected region for DB::Get()

Summary:
As title. This is possible as tickers are atomic now.
db_bench on high qps in-memory muti-thread random get workload, showed ~5% throughput improvement.

Test Plan: make check; db_bench; db_stress

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12555/Add three new MemTableRep's

Summary:
This patch adds three new MemTableRep's: UnsortedRep, PrefixHashRep, and VectorRep.

UnsortedRep stores keys in an std::unordered_map of std::sets. When an iterator is requested, it dumps the keys into an std::set and iterates over that.

VectorRep stores keys in an std::vector. When an iterator is requested, it creates a copy of the vector and sorts it using std::sort. The iterator accesses that new vector.

PrefixHashRep stores keys in an unordered_map mapping prefixes to ordered sets.

I also added one API change. I added a function MemTableRep::MarkImmutable. This function is called when the rep is added to the immutable list. It doesn't do anything yet, but it seems like that could be useful. In particular, for the vectorrep, it means we could elide the extra copy and just sort in place. The only reason I haven't done that yet is because the use of the ArenaAllocator complicates things (I can elaborate on this if needed).


Reviewers: dhruba, haobo, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12117/Pull from https://reviews.facebook.net/D10917

Summary: Pull Mark's patch and slightly revise it. I revised another place in db_impl.cc with similar new formula.

Test Plan:
make all check. Also run ""time ./db_bench --num=2500000000 --numdistinct=2200000000"". It has run for 20+ hours and hasn't finished. Looks good so far:

Reviewers: MarkCallaghan, haobo, dhruba, chip

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D12441/Add APIs to query SST file metadata and to delete specific SST files

Summary: An api to query the level, key ranges, size etc for each SST file and an api to delete a specific file from the db and all associated state in the bookkeeping datastructures.

Notes: Editing the manifest version does not release the obsolete files right away. However deleting the file directly will mess up the iterator. We may need a more aggressive/timely file deletion api.

I have used std::unique_ptr - will switch to boost:: since this is external. thoughts?

Unit test is fragile right now as it expects the compaction at certain levels.

Test Plan: unittest

Reviewers: dhruba, vamsi, emayanke

CC: zshao, leveldb, haobo

Task ID: #

Blame Rev:/API for getting archived log files

Summary: Also expanded class LogFile to have startSequene and FileSize and exposed it publicly

Test Plan: make all check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12087/Expose statistic for sequence number and implement setTickerCount

Summary: statistic for sequence number is needed by wormhole. setTickerCount is demanded for this statistic. I can't simply recordTick(max_sequence) when db recovers because the statistic iobject is owned by client and may/may not be reset during reopen. Eg. statistic is reset in mcrocksdb whereas it is not in db_stress. Therefore it is best to go with setTickerCount

Test Plan: ./db_stress ... --statistics=1 and observed expected sequence number

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12327/"
DataBase Management,"DataBase Management, Thread management",Rocksdb,"Conversion of db_bench, db_stress and db_repl_stress to use gflags

Summary: Converted db_stress, db_repl_stress and db_bench to use gflags

Test Plan: I tested by printing out all the flags from old and new versions. Tried defaults, + various combinations with ""interesting flags"". Also, tested by running db_crashtest.py and db_crashtest2.py.

Reviewers: emayanke, dhruba, haobo, kailiu, sdong

Reviewed By: emayanke

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D13581/LRUCache to try to clean entries not referenced first.

Summary:
With this patch, when LRUCache.Insert() is called and the cache is full, it will first try to free up entries whose reference counter is 1 (would become 0 after remo\
ving from the cache). We do it in two passes, in the first pass, we only try to release those unreferenced entries. If we cannot free enough space after traversing t\
he first remove_scan_cnt_ entries, we start from the beginning again and remove those entries being used.

Test Plan: add two unit tests to cover the codes

Reviewers: dhruba, haobo, emayanke

Reviewed By: emayanke

CC: leveldb, emayanke, xjin

Differential Revision: https://reviews.facebook.net/D13377/Migrate names of properties from 'leveldb' prefix to 'rocksdb' prefix.

Summary: Migrate names of properties from 'leveldb' prefix to 'rocksdb' prefix.

Test Plan: make check

Reviewers: emayanke, haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13311/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/Better locking in vectorrep that increases throughput to match speed of storage.

Summary:
There is a use-case where we want to insert data into rocksdb as
fast as possible. Vector rep is used for this purpose.

The background flush thread needs to flush the vectorrep to
storage. It acquires the dblock then sorts the vector, releases
the dblock and then writes the sorted vector to storage. This is
suboptimal because the lock is held during the sort, which
prevents new writes for occuring.

This patch moves the sorting of the vector rep to outside the
db mutex. Performance is now as fastas the underlying storage
system. If you are doing buffered writes to rocksdb files, then
you can observe throughput upwards of 200 MB/sec writes.

This is an early draft and not yet ready to be reviewed.

Test Plan:
make check

Task ID: #

Blame Rev:

Reviewers: haobo

Reviewed By: haobo

CC: leveldb, haobo

Differential Revision: https://reviews.facebook.net/D12987/Revert ""Minor fixes found while trying to compile it using clang on Mac OS X""

This reverts commit 5f2c136c328a8dbb6c3cb3818881e30eeb916cd6./Added a parameter to limit the maximum space amplification for universal compaction.

Summary:
Added a new field called max_size_amplification_ratio in the
CompactionOptionsUniversal structure. This determines the maximum
percentage overhead of space amplification.

The size amplification is defined to be the ratio between the size of
the oldest file to the sum of the sizes of all other files. If the
size amplification exceeds the specified value, then min_merge_width
and max_merge_width are ignored and a full compaction of all files is done.
A value of 10 means that the size a database that stores 100 bytes
of user data could occupy 110 bytes of physical storage.

Test Plan: Unit test DBTest.UniversalCompactionSpaceAmplification added.

Reviewers: haobo, emayanke, xjin

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12825/[RocksDB] Remove Log file immediately after memtable flush

Summary: As title. The DB log file life cycle is tied up with the memtable it backs. Once the memtable is flushed to sst and committed, we should be able to delete the log file, without holding the mutex. This is part of the bigger change to avoid FindObsoleteFiles at runtime. It deals with log files. sst files will be dealt with later.

Test Plan: make check; db_bench

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11709/Internal/user key bug fix.

Summary: Fix code so that the filter_block layer only assumes keys are internal when prefix_extractor is set.

Test Plan: ./filter_block_test

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12501/Add three new MemTableRep's

Summary:
This patch adds three new MemTableRep's: UnsortedRep, PrefixHashRep, and VectorRep.

UnsortedRep stores keys in an std::unordered_map of std::sets. When an iterator is ed, it dumps the keys into an std::set and iterates over that.

VectorRep stores keys in an std::vector. When an iterator is ed, it creates a copy of the vector and sorts it using std::sort. The iterator accesses that new vector.

PrefixHashRep stores keys in an unordered_map mapping prefixes to ordered sets.

I also added one API change. I added a function MemTableRep::MarkImmutable. This function is called when the rep is added to the immutable list. It doesn't do anything yet, but it seems like that could be useful. In particular, for the vectorrep, it means we could elide the extra copy and just sort in place. The only reason I haven't done that yet is because the use of the ArenaAllocator complicates things (I can elaborate on this if needed).

Test Plan:
make -j32 check
./db_stress --memtablerep=vector
./db_stress --memtablerep=unsorted
./db_stress --memtablerep=prefixhash --prefix_size=10

Reviewers: dhruba, haobo, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12117/ from https://reviews.facebook.net/D10917

Summary:  Mark's patch and slightly revise it. I revised another place in db_impl.cc with similar new formula.

Test Plan:
make all check. Also run ""time ./db_bench --num=2500000000 --numdistinct=2200000000"". It has run for 20+ hours and hasn't finished. Looks good so far:

Installed stack trace handler for SIGILL SIGSEGV SIGBUS SIGABRT
LevelDB:    version 2.0
Date:       Tue Aug 20 23:11:55 2013
CPU:        32 * Intel(R) Xeon(R) CPU E5-2660 0 @ 2.20GHz
CPUCache:   20480 KB
Keys:       16 bytes each
Values:     100 bytes each (50 bytes after compression)
Entries:    2500000000
RawSize:    276565.6 MB (estimated)
FileSize:   157356.3 MB (estimated)
Write rate limit: 0
Compression: snappy


Reviewers: MarkCallaghan, haobo, dhruba, chip

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D12441/Prefix scan: db_bench and bug fixes

Summary: If use_prefix_filters is set and read_range>1, then the random seeks will set a the prefix filter to be the prefix of the key which was randomly selected as the target.  Still need to add statistics (perhaps in a separate diff).

Test Plan: ./db_bench --benchmarks=fillseq,prefixscanrandom --num=10000000 --statistics=1 --use_prefix_blooms=1 --use_prefix_api=1 --bloom_bits=10

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, haobo

Differential Revision: https://reviews.facebook.net/D12273/Tiny fix to db_bench for make release.

Summary:
In release, ""found variable assigned but not used anywhere"". Changed it to work with
assert. Someone accept this :).

Test Plan: make release -j 32

Reviewers: haobo, dhruba, emayanke

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12309/Benchmarking for Merge Operator

Summary:
Updated db_bench and utilities/merge_operators.h to allow for dynamic benchmarking
of merge operators in db_bench. Added a new test (--benchmarks=mergerandom), which performs
a bunch of random Merge() operations over random keys. Also added a ""--merge_operator="" flag
so that the tester can easily benchmark different merge operators. Currently supports
the PutOperator and UInt64Add operator. Support for stringappend or list append may come later.

Test Plan:
	1. make db_bench
	2. Test the PutOperator (simulating Put) as follows:
./db_bench --benchmarks=fillrandom,readrandom,updaterandom,readrandom,mergerandom,readrandom --merge_operator=put
--threads=2

3. Test the UInt64AddOperator (simulating numeric addition) similarly:
./db_bench --value_size=8 --benchmarks=fillrandom,readrandom,updaterandom,readrandom,mergerandom,readrandom
--merge_operator=uint64add --threads=2

Reviewers: haobo, dhruba, zshao, MarkCallaghan

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D11535/Minor fix to current codes

Summary:
Minor fix to current codes, including: coding style, output format,
comments. No major logic change. There are only 2 real changes, please see my inline comments.

Test Plan: make all check

Reviewers: haobo, dhruba, emayanke

Differential Revision: https://reviews.facebook.net/D12297/"
,,Rocksdb,"Dbid feature

Summary:
Create a new type of file on startup if it doesn't already exist called DBID.
This will store a unique number generated from boost library's uuid header file.
The use-case is to identify the case of a db losing all its data and coming back up either empty or from an image(backup/live replica's recovery)
the key point to note is that DBID is not stored in a backup or db snapshot
It's preferable to use Boost for uuid because:
1) A non-standard way of generating uuid is not good
2) /proc/sys/kernel/random/uuid generates a uuid but only on linux environments and the solution would not be clean
3) c++ doesn't have any direct way to get a uuid
4) Boost is a very good library that was already having linkage in rocksdb from third-party
Note: I had to update the TOOLCHAIN_REV in build files to get latest verison of boost from third-party as the older version had a bug.
I had to put Wno-uninitialized in Makefile because boost-1.51 has an unitialized variable and rocksdb would not comiple otherwise. Latet open-source for boost is 1.54 but is not there in third-party. I have notified the concerned people in fbcode about it.
@kailiu : While releasing to third-party, an additional dependency will need to be created for boost in TARGETS file. I can help identify.

Test Plan:
Expand db_test to test 2 cases
1) Restarting db with Id file present - verify that no change to Id
2)Restarting db with Id file deleted - verify that a different Id is there after reopen
Also run make all check

Reviewers: dhruba, haobo, kailiu, sdong

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13587/"
,,Rocksdb,"Implementing DynamicIterator for TransformRepNoLock

Summary: What @haobo done with TransformRep, now in TransformRepNoLock. Similar implementation, except that I made DynamicIterator a subclass of Iterator which makes me have less iterator initializations.

Test Plan: ./prefix_test. Seeing huge savings vs. TransformRep again!

Reviewers: dhruba, haobo, sdong, kailiu

Reviewed By: haobo

CC: leveldb, haobo

Differential Revision: https://reviews.facebook.net/D13953/[RocksDB] Fix skiplist sequential insertion optimization

Summary: The original optimization missed updating links other than the lowest level.

Test Plan: make check; perf_context_test

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, adsharma

Differential Revision: https://reviews.facebook.net/D13119/"
,,Rocksdb,"Flush was hanging because the configured options specified that more than 1 memtable need to be merged.

Summary:
There is an config option called Options.min_write_buffer_number_to_merge
that specifies the minimum number of write buffers to merge in memory
before flushing to a file in L0. But in the the case when the db is
being closed, we should not be using this config, instead we should
flush whatever write buffers were available at that time.

Test Plan: Unit test attached.

Reviewers: haobo, emayanke

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12717/"
 Restructuring the code,"DataBase Management, Restructuring the code, Compression tasks",Rocksdb,"Follow-up Cleaning-up After D13521

Summary:
This patch is to address @haobo's comments on D13521:
1. rename Table to be TableReader and make its factory function to be GetTableReader
2. move the compression type selection logic out of TableBuilder but to compaction logic
3. more accurate comments
4. Move stat name constants into BlockBasedTable implementation.
5. remove some uncleaned codes in simple_table_db_test

Test Plan: pass test suites.

Reviewers: haobo, dhruba, kailiu

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13785/LRUCache to try to clean entries not referenced first.

Summary:
With this patch, when LRUCache.Insert() is called and the cache is full, it will first try to free up entries whose reference counter is 1 (would become 0 after remo\
ving from the cache). We do it in two passes, in the first pass, we only try to release those unreferenced entries. If we cannot free enough space after traversing t\
he first remove_scan_cnt_ entries, we start from the beginning again and remove those entries being used.

Test Plan: add two unit tests to cover the codes

Reviewers: dhruba, haobo, emayanke

Reviewed By: emayanke

CC: leveldb, emayanke, xjin

Differential Revision: https://reviews.facebook.net/D13377/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/Introduced a new flag non_blocking_io in ReadOptions.

Summary:
If ReadOptions.non_blocking_io is set to true, then KeyMayExists
and Iterators will return data that is cached in RAM.
If the Iterator needs to do IO from storage to serve the data,
then the Iterator.status() will return Status::IsRetry().

Test Plan:
Enhanced unit test DBTest.KeyMayExist to detect if there were are IOs
issues from storage. Added DBTest.NonBlockingIteration to verify
nonblocking Iterations.

Reviewers: emayanke, haobo

Reviewed By: haobo

CC: leveldb

Maniphest Tasks: T63

Differential Revision: https://reviews.facebook.net/D12531/Internal/user key bug fix.

Summary: Fix code so that the filter_block layer only assumes keys are internal when prefix_extractor is set.

Test Plan: ./filter_block_test

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12501/Prefix scan: db_bench and bug fixes

Summary: If use_prefix_filters is set and read_range>1, then the random seeks will set a the prefix filter to be the prefix of the key which was randomly selected as the target.  Still need to add statistics (perhaps in a separate diff).

Test Plan: ./db_bench --benchmarks=fillseq,prefixscanrandom --num=10000000 --statistics=1 --use_prefix_blooms=1 --use_prefix_api=1 --bloom_bits=10

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, haobo

Differential Revision: https://reviews.facebook.net/D12273/"
,,Rocksdb,"WriteBatch::Put() overload that gathers key and value from arrays of slices

Summary: In our project, when writing to the database, we want to form the value as the concatenation of a small header and a larger payload.  It's a shame to have to copy the payload just so we can give RocksDB API a linear view of the value.  Since RocksDB makes a copy internally, it's easy to support gather writes.

Test Plan: write_batch_test, new test case

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13947/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/Allow WriteBatch::Handler to abort iteration

Summary:
Sometimes you don't need to iterate through the whole WriteBatch. This diff makes the Handler member functions return a bool that indicates whether to abort or not. If they return true, the iteration stops.

One thing I just thought of is that this will break backwards-compability. Maybe it would be better to add a virtual member function WriteBatch::Handler::ShouldAbort() that returns false by default. Comments ed.

I still have to add a new unit test for the abort code, but let's finalize the API first.

Test Plan: make -j32 check

Reviewers: dhruba, haobo, vamsi, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12339/"
 Restructuring the code,"DataBase Management, Restructuring the code, Compression tasks",Rocksdb,"Follow-up Cleaning-up After D13521

Summary:
This patch is to address @haobo's comments on D13521:
1. rename Table to be TableReader and make its factory function to be GetTableReader
2. move the compression type selection logic out of TableBuilder but to compaction logic
3. more accurate comments
4. Move stat name constants into BlockBasedTable implementation.
5. remove some uncleaned codes in simple_table_db_test

Test Plan: pass test suites.

Reviewers: haobo, dhruba, kailiu

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13785/Make ""Table"" pluggable

Summary: This patch makes Table and TableBuilder a abstract class and make all the implementation of the current table into BlockedBasedTable and BlockedBasedTable Builder.

Test Plan: Make db_test.cc to work with block based table. Add a new test simple_table_db_test.cc where a different simple table format is implemented.

Reviewers: dhruba, haobo, kailiu, emayanke, vamsi

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13521/Universal Compaction to Have a Size Percentage Threshold To Decide Whether to Compress

Summary:
This patch adds a option for universal compaction to allow us to only compress output files if the files compacted previously did not yet reach a specified ratio, to save CPU costs in some cases.

Compression is always skipped for flushing. This is because the size information is not easy to evaluate for flushing case. We can improve it later.

Test Plan:
add test
DBTest.UniversalCompactionCompressRatio1 and DBTest.UniversalCompactionCompressRatio12

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13467/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/"
,,Rocksdb,"Add backward compatible option in GetLiveFiles to choose whether to not Flush first

Summary:
As explained in comments in GetLiveFiles in db.h, this option will cause flush to be skipped in GetLiveFiles because some use-cases use GetSortedWalFiles after GetLiveFiles to generate more complete snapshots.
Using GetSortedWalFiles after GetLiveFiles allows us to not Flush in GetLiveFiles first because wals have everything.
Note: file deletions will be disabled before calling GLF or GSWF so live logs will not move to archive logs or get delted.
Note: Manifest file is truncated to a proper value in GLF, so it will always reply from the proper wal files on a restart

Test Plan: make

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13257/New ldb command to convert compaction style

Summary:
Add new command ""change_compaction_style"" to ldb tool. For
universal->level, it shows ""nothing to do"". For level->universal, it
compacts all files into a single one and moves the file to level 0.

Also add check for number of files at level 1+ when opening db with
universal compaction style.

Test Plan:
'make all check'. New unit test for internal convertion function. Also manully test various
cmd like:

./ldb change_compaction_style --old_compaction_style=0
--new_compaction_style=1 --db=/tmp/leveldbtest-3088/db_test

Reviewers: haobo, dhruba

Reviewed By: haobo

CC: vamsi, emayanke

Differential Revision: https://reviews.facebook.net/D12603/"
DataBase Management,DataBase Management,Rocksdb,"[RocksDB] Add perf_context.wal_write_time to track time spent on writing the recovery log.

Summary: as title

Test Plan: make check; ./perf_context_test

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13629/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/[RocksDB] Added perf counters to track skipped internal keys during iteration

Summary: as title. unit test not polished. this is for a quick live test

Test Plan: live

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13221/[RocbsDB] Add an option to enable set based memtable for perf_context_test

Summary:
as title.
Some result:

-- Sequential insertion of 1M key/value with stock skip list (all in on memtable)
time ./perf_context_test  --total_keys=1000000  --use_set_based_memetable=0
Inserting 1000000 key/value pairs


Worst case comparison for a Put is 88933 (skiplist) vs 71 (set based memetable)

Of course, there's other in-efficiency in set based memtable implementation, which lead to the overall worst performance. However, P99 behavior advantage is very very obvious.

Test Plan: ./perf_context_test and viewstate shadow testing

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13095/[RocksDB] Unit test to show Seek key comparison number

Summary: Added SeekKeyComparison to show the uer key comparison incurred by Seek.

Test Plan:
make perf_context_test
export LEVELDB_TESTS=DBTest.SeekKeyComparison
./perf_context_test --write_buffer_size=500000 --total_keys=10000
./perf_context_test --write_buffer_size=250000 --total_keys=10000

Reviewers: dhruba, xjin

Reviewed By: xjin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12843/[RocksDB] Added nano second stopwatch and new perf counters to track block read cost

Summary: The pupose of this diff is to expose per user-call level precise timing of block read, so that we can answer questions like: a Get() costs me 100ms, is that somehow related to loading blocks from file system, or sth else? We will answer that with EXACTLY how many blocks have been read, how much time was spent on transfering the bytes from os, how much time was spent on checksum verification and how much time was spent on block decompression, just for that one Get. A nano second stopwatch was introduced to track time with higher precision. The cost/precision of the stopwatch is also measured in unit-test. On my dev box, retrieving one time instance costs about 30ns, on average. The deviation of timing results is good enough to track 100ns-1us level events. And the overhead could be safely ignored for 100us level events (10000 instances/s), for example, a viewstate thrift call.

Test Plan: perf_context_test, also testing with viewstate shadow traffic.

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D12351/"
,Thread management,Rocksdb,"Small changes in Deleting obsolete files

Summary:
@haobo's suggestions from https://reviews.facebook.net/D13827

Renaming some variables, deprecating purge_log_after_flush, changing for loop into auto for loop.

I have not implemented deleting objects outside of mutex yet because it would require a big code change - we would delete object in db_impl, which currently does not know anything about object because it's defined in version_edit.h (FileMetaData). We should do it at some point, though.

Test Plan: Ran deletefile_test

Reviewers: haobo

Reviewed By: haobo

CC: leveldb, haobo

Differential Revision: https://reviews.facebook.net/D14025/Fix valgrind check by initialising DeletionState.

Summary:
The valgrind error was introduced by commit
1510339e5257073af82f8c07e6fa1f2a9144e6aa. Initialize DeletionState
in constructor.

Test Plan: valgrind --leak-check=yes ./deletefile_test

Reviewers: igor, kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13983/Speed up FindObsoleteFiles

Summary:
Here's one solution we discussed on speeding up FindObsoleteFiles. Keep a set of all files in DBImpl and update the set every time we create a file. I probably missed few other spots where we create a file.

It might speed things up a bit, but makes code uglier. I don't really like it.

Much better approach would be to abstract all file handling to a separate class. Think of it as layer between DBImpl and Env. Having a separate class deal with file namings and deletion would benefit both code cleanliness (especially with huge DBImpl) and speed things up. It will take a huge effort to do this, though.

Let's discuss offline today.

Test Plan: Ran ./db_stress, verified that files are getting deleted

Reviewers: dhruba, haobo, kailiu, emayanke

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D13827/Change Function names from Compaction->Flush When they really mean Flush

Summary: When I debug the unit test failures when enabling background flush thread, I feel the function names can be made clearer for people to understand. Also, if the names are fixed, in many places, some tests' bugs are obvious (and some of those tests are failing). This patch is to clean it up for future maintenance.

Test Plan: Run test suites.

Reviewers: haobo, dhruba, xjin

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13431/Add backward compatible option in GetLiveFiles to choose whether to not Flush first

Summary:
As explained in comments in GetLiveFiles in db.h, this option will cause flush to be skipped in GetLiveFiles because some use-cases use GetSortedWalFiles after GetLiveFiles to generate more complete snapshots.
Using GetSortedWalFiles after GetLiveFiles allows us to not Flush in GetLiveFiles first because wals have everything.
Note: file deletions will be disabled before calling GLF or GSWF so live logs will not move to archive logs or get delted.
Note: Manifest file is truncated to a proper value in GLF, so it will always reply from the proper wal files on a restart

Test Plan: make

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13257/[Rocksdb] Submit mem table flush job in a different thread pool

Summary: As title. This is just a quick hack and not ready for commit. fails a lot of unit test. I will test/debug it directly in ViewState shadow .

Test Plan: Try it in shadow test.

Reviewers: dhruba, xjin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12933/Add APIs to query SST file metadata and to delete specific SST files

Summary: An api to query the level, key ranges, size etc for each SST file and an api to delete a specific file from the db and all associated state in the bookkeeping datastructures.

Notes: Editing the manifest version does not release the obsolete files right away. However deleting the file directly will mess up the iterator. We may need a more aggressive/timely file deletion api.

I have used std::unique_ptr - will switch to boost:: since this is external. thoughts?

Unit test is fragile right now as it expects the compaction at certain levels.

Test Plan: unittest

Reviewers: dhruba, vamsi, emayanke

CC: zshao, leveldb, haobo

Task ID: #

Blame Rev:/API for getting archived log files

Summary: Also expanded class LogFile to have startSequene and FileSize and exposed it publicly

Test Plan: make all check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12087/"
,,Rocksdb,"Fixing the warning messages captured under mac os # Consider using `git commit -m 'One line title' && arc diff`. # You will save time by running lint and unit in the background.

Summary: The work to make sure mac os compiles rocksdb is not completed yet. But at least we can start cleaning some warnings captured only by g++ from mac os..

Test Plan: ran make in mac os

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14049/Fix the gcc warning for unused variable

Summary: Fix the unused variable warning for `first` when running `make release`

Test Plan:
make
make check

Reviewers: dhruba, igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13695/Fix the log number bug when updating MANIFEST file

Summary:
Crash may occur during the flushes of more than two mem tables.

As the info log suggested, even when both were successfully flushed,
the recovery process still pick up one of the memtable's log for recovery.

This diff fix the problem by setting the correct ""log number"" in MANIFEST.

Test Plan: make test; deployed to leaf4 and make sure it doesn't result in crashes of this type.

Reviewers: haobo, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13659/Flush was hanging because the configured options specified that more than 1 memtable need to be merged.

Summary:
There is an config option called Options.min_write_buffer_number_to_merge
that specifies the minimum number of write buffers to merge in memory
before flushing to a file in L0. But in the the case when the db is
being closed, we should not be using this config, instead we should
flush whatever write buffers were available at that time.

Test Plan: Unit test attached.

Reviewers: haobo, emayanke

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12717/"
,,Rocksdb,"[RocksDB] Generalize prefix-aware iterator to be used for more than one Seek

Summary: Added a prefix_seek flag in ReadOptions to indicate that Seek is prefix aware(might not return data with different prefix), and also not bound to a specific prefix. Multiple Seeks and range scans can be invoked on the same iterator. If a specific prefix is specified, this flag will be ignored. Just a quick prototype that works for PrefixHashRep, the new lockless memtable could be easily extended with this support too.

Test Plan: test it on Leaf

Reviewers: dhruba, kailiu, sdong, igor

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13929/In-place updates for equal keys and similar sized values

Summary:
Currently for each put, a fresh memory is allocated, and a new entry is added to the memtable with a new sequence number irrespective of whether the key already exists in the memtable. This diff is an attempt to update the value inplace for existing keys. It currently handles a very simple case:
1. Key already exists in the current memtable. Does not inplace update values in immutable memtable or snapshot
2. Latest value type is a 'put' ie kTypeValue
3. New value size is less than existing value, to avoid reallocating memory

TODO: For a put of an existing key, deallocate memory take by values, for other value types till a kTypeValue is found, ie. remove kTypeMerge.
TODO: Update the transaction log, to allow consistent reload of the memtable.

Test Plan: Added a unit test verifying the inplace update. But some other unit tests broken due to invalid sequence number checks. WIll fix them next.

Reviewers: xinyaohu, sumeet, haobo, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12423

Automatic commit by arc/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/Add three new MemTableRep's

Summary:
This patch adds three new MemTableRep's: UnsortedRep, PrefixHashRep, and VectorRep.

UnsortedRep stores keys in an std::unordered_map of std::sets. When an iterator is ed, it dumps the keys into an std::set and iterates over that.

VectorRep stores keys in an std::vector. When an iterator is ed, it creates a copy of the vector and sorts it using std::sort. The iterator accesses that new vector.

PrefixHashRep stores keys in an unordered_map mapping prefixes to ordered sets.

I also added one API change. I added a function MemTableRep::MarkImmutable. This function is called when the rep is added to the immutable list. It doesn't do anything yet, but it seems like that could be useful. In particular, for the vectorrep, it means we could elide the extra copy and just sort in place. The only reason I haven't done that yet is because the use of the ArenaAllocator complicates things (I can elaborate on this if needed).

Test Plan:
make -j32 check
./db_stress --memtablerep=vector
./db_stress --memtablerep=unsorted
./db_stress --memtablerep=prefixhash --prefix_size=10

Reviewers: dhruba, haobo, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12117/"
Restructuring the code,"Restructuring the code, database Management, performance Management",Rocksdb,"Small changes in Deleting obsolete files

Summary:
@haobo's suggestions from https://reviews.facebook.net/D13827

Renaming some variables, deprecating purge_log_after_flush, changing for loop into auto for loop.

I have not implemented deleting objects outside of mutex yet because it would require a big code change - we would delete object in db_impl, which currently does not know anything about object because it's defined in version_edit.h (FileMetaData). We should do it at some point, though.

Test Plan: Ran deletefile_test

Reviewers: haobo

Reviewed By: haobo

CC: leveldb, haobo

Differential Revision: https://reviews.facebook.net/D14025/Universal Compaction to Have a Size Percentage Threshold To Decide Whether to Compress

Summary:
This patch adds a option for universal compaction to allow us to only compress output files if the files compacted previously did not yet reach a specified ratio, to save CPU costs in some cases.

Compression is always skipped for flushing. This is because the size information is not easy to evaluate for flushing case. We can improve it later.

Test Plan:
add test
DBTest.UniversalCompactionCompressRatio1 and DBTest.UniversalCompactionCompressRatio12

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13467/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/Prefix scan: db_bench and bug fixes

Summary: If use_prefix_filters is set and read_range>1, then the random seeks will set a the prefix filter to be the prefix of the key which was randomly selected as the target.  Still need to add statistics (perhaps in a separate diff).

Test Plan: ./db_bench --benchmarks=fillseq,prefixscanrandom --num=10000000 --statistics=1 --use_prefix_blooms=1 --use_prefix_api=1 --bloom_bits=10

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, haobo

Differential Revision: https://reviews.facebook.net/D12273/Add APIs to query SST file metadata and to delete specific SST files

Summary: An api to query the level, key ranges, size etc for each SST file and an api to delete a specific file from the db and all associated state in the bookkeeping datastructures.

Notes: Editing the manifest version does not release the obsolete files right away. However deleting the file directly will mess up the iterator. We may need a more aggressive/timely file deletion api.

I have used std::unique_ptr - will switch to boost:: since this is external. thoughts?

Unit test is fragile right now as it expects the compaction at certain levels.

Test Plan: unittest

Reviewers: dhruba, vamsi, emayanke

CC: zshao, leveldb, haobo

Task ID: #

Blame Rev:/"
DataBase Management,"Database Management,Thread management",Rocksdb,"Add the index/filter block cache

Summary: This diff leverage the existing block cache and extend it to cache index/filter block.

Test Plan:
Added new tests in db_test and table_test

The correctness is checked by:

1. make check
2. make valgrind_check

Performance is test by:

1. 10 times of build_tools/regression_build_test.sh on two versions of rocksdb before/after the code change. Test results suggests no significant difference between them. For the two key operatons `overwrite` and `readrandom`, the average iops are both 20k and ~260k, with very small variance).
2. db_stress.

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, haobo, xjin

Differential Revision: https://reviews.facebook.net/D13167/Fixing the warning messages captured under mac os # Consider using `git commit -m 'One line title' && arc diff`. # You will save time by running lint and unit in the background.

Summary: The work to make sure mac os compiles rocksdb is not completed yet. But at least we can start cleaning some warnings captured only by g++ from mac os..

Test Plan: ran make in mac os

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14049/Fix failure in rocksdb unit test CompressedCache

Summary:
The problem was that there was only a single key-value in a block
and its compressibility was less than 88%. Rocksdb refuses to
compress a block unless its compresses to lesser than 88% of its
original size. If a block is not compressed, it does nto get inserted
into the compressed block cache.

Create the test data so that multiple records fit into the same
data block. This increases the compressibility of these data block.

Test Plan: ./db_test

Reviewers: kailiu, haobo

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13905/Fixed valgrind error in DBTest.CompressedCache

Summary:
Fixed valgrind error in DBTest.CompressedCache.
This fixes the valgrind error (thanks to Haobo). I am still trying to reproduce the test-failure case deterministically.

Test Plan: db_test

Reviewers: haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13899/Making the transaction log iterator more robust

Summary:
strict essentially means that we MUST find the startsequence. Thus we should return if starteSequence is not found in the first file in case strict is set. This will take care of ending the iterator in case of permanent gaps due to corruptions in the log files
Also created NextImpl function that will have internal variable to distinguish whether Next is being called from StartSequence or by application.
Set NotFoudn::gaps status to give an indication of gaps happeneing.
Polished the inline documentation at various places

Test Plan:
* db_repl_stress test
* db_test relating to transaction log iterator
* fbcode/wormhole/rocksdb/rocks_log_iterator
* sigma production machine sigmafio032.prn1

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13689/Implement a compressed block cache.

Summary:
Rocksdb can now support a uncompressed block cache, or a compressed
block cache or both. Lookups first look for a block in the
uncompressed cache, if it is not found only then it is looked up
in the compressed cache. If it is found in the compressed cache,
then it is uncompressed and inserted into the uncompressed cache.

It is possible that the same block resides in the compressed cache
as well as the uncompressed cache at the same time. Both caches
have their own individual LRU policy.

Test Plan: Unit test case attached.

Reviewers: kailiu, sdong, haobo, leveldb

Reviewed By: haobo

CC: xjin, haobo

Differential Revision: https://reviews.facebook.net/D12675/In-place updates for equal keys and similar sized values

Summary:
Currently for each put, a fresh memory is allocated, and a new entry is added to the memtable with a new sequence number irrespective of whether the key already exists in the memtable. This diff is an attempt to update the value inplace for existing keys. It currently handles a very simple case:
1. Key already exists in the current memtable. Does not inplace update values in immutable memtable or snapshot
2. Latest value type is a 'put' ie kTypeValue
3. New value size is less than existing value, to avoid reallocating memory

TODO: For a put of an existing key, deallocate memory take by values, for other value types till a kTypeValue is found, ie. remove kTypeMerge.
TODO: Update the transaction log, to allow consistent reload of the memtable.

Test Plan: Added a unit test verifying the inplace update. But some other unit tests broken due to invalid sequence number checks. WIll fix them next.

Reviewers: xinyaohu, sumeet, haobo, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12423

Automatic commit by arc/If a Put fails, fail all other puts

Summary:
When a Put fails, it can leave database in a messy state. We don't want to pretend that everything is OK when it may not be. We fail every write following the failed one.

I added checks for corruption to DBImpl::Write(). Is there anywhere else I need to add them?

Test Plan: Corruption unit test.

Reviewers: dhruba, haobo, kailiu

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13671/Unify DeleteFile and DeleteWalFiles

Summary:
This is to simplify rocksdb public APIs and improve the code quality.
Created an additional parameter to ParseFileName for log sub type and improved the code for deleting a wal file.
Wrote exhaustive unit-tests in delete_file_test
Unification of other redundant APIs can be taken up in a separate diff

Test Plan: Expanded delete_file test

Reviewers: dhruba, haobo, kailiu, sdong

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13647/Dbid feature

Summary:
Create a new type of file on startup if it doesn't already exist called DBID.
This will store a unique number generated from boost library's uuid header file.
The use-case is to identify the case of a db losing all its data and coming back up either empty or from an image(backup/live replica's recovery)
the key point to note is that DBID is not stored in a backup or db snapshot
It's preferable to use Boost for uuid because:
1) A non-standard way of generating uuid is not good
2) /proc/sys/kernel/random/uuid generates a uuid but only on linux environments and the solution would not be clean
3) c++ doesn't have any direct way to get a uuid
4) Boost is a very good library that was already having linkage in rocksdb from third-party
Note: I had to update the TOOLCHAIN_REV in build files to get latest verison of boost from third-party as the older version had a bug.
I had to put Wno-uninitialized in Makefile because boost-1.51 has an unitialized variable and rocksdb would not comiple otherwise. Latet open-source for boost is 1.54 but is not there in third-party. I have notified the concerned people in fbcode about it.
@kailiu : While releasing to third-party, an additional dependency will need to be created for boost in TARGETS file. I can help identify.

Test Plan:
Expand db_test to test 2 cases
1) Restarting db with Id file present - verify that no change to Id
2)Restarting db with Id file deleted - verify that a different Id is there after reopen
Also run make all check

Reviewers: dhruba, haobo, kailiu, sdong

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13587/Fix Bug: iterator.Prev() or iterator.SeekToLast() might return the first element instead of the correct one

Summary:
Recent patch https://reviews.facebook.net/D11865 introduced a regression bug:

DBIter::FindPrevUserEntry(), which is called by DBIter::Prev() (and also implicitly if calling iterator.SeekToLast())  might do issue a seek when having skipped too many entries. If the skipped entry just before the seek() is a delete, the saved key is erased so that it seeks to the front, so Prev() would return the first element.

This patch fixes the bug by not doing seek() in DBIter::FindNextUserEntry() if saved key has been erased.

Test Plan: Add a test DBTest.IterPrevMaxSkip which would fail without the patch and would pass with the change.

Reviewers: dhruba, xjin, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13557/Universal Compaction to Have a Size Percentage Threshold To Decide Whether to Compress

Summary:
This patch adds a option for universal compaction to allow us to only compress output files if the files compacted previously did not yet reach a specified ratio, to save CPU costs in some cases.

Compression is always skipped for flushing. This is because the size information is not easy to evaluate for flushing case. We can improve it later.

Test Plan:
add test
DBTest.UniversalCompactionCompressRatio1 and DBTest.UniversalCompactionCompressRatio12

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13467/Change Function names from Compaction->Flush When they really mean Flush

Summary: When I debug the unit test failures when enabling background flush thread, I feel the function names can be made clearer for people to understand. Also, if the names are fixed, in many places, some tests' bugs are obvious (and some of those tests are failing). This patch is to clean it up for future maintenance.

Test Plan: Run test suites.

Reviewers: haobo, dhruba, xjin

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13431/Add option for storing transaction logs in a separate dir

Summary: In some cases, you might not want to store the data log (write ahead log) files in the same dir as the sst files. An example use case is leaf, which stores sst files in tmpfs. And would like to save the log files in a separate dir (disk) to save memory.

Test Plan: make all. Ran db_test test. A few test failing. P2785018. If you guys don't see an obvious problem with the code, maybe somebody from the rocksdb team could help me debug the issue here. Running this on leaf worked well. I could see logs stored on disk, and deleted appropriately after compactions. Obviously this is only one set of options. The unit tests cover different options. Seems like I'm missing some edge cases.

Reviewers: dhruba, haobo, leveldb

CC: xinyaohu, sumeet

Differential Revision: https://reviews.facebook.net/D13239/Make db_test more robust

Summary: While working on D13239, I noticed that the same options are not used for opening and destroying at db. So adding that. Also added asserts for successful DestroyDB calls.

Test Plan: Ran unit tests. Atleast 1 unit test is failing. They failures are a result of some past logic change. I'm not really planning to fix those. But I would like to check this in. And hopefully the respective unit test owners can fix the broken tests

Reviewers: leveldb, haobo

CC: xinyaohu, sumeet, dhruba

Differential Revision: https://reviews.facebook.net/D13329/Unit test failure in DBTest.NumImmutableMemTable.

Summary:
Previous patch introduced a unit test failure in
DBTest.NumImmutableMemTable because of change in property names.

Test Plan:

Reviewers:

CC:

Task ID: #

Blame Rev:/Migrate names of properties from 'leveldb' prefix to 'rocksdb' prefix.

Summary: Migrate names of properties from 'leveldb' prefix to 'rocksdb' prefix.

Test Plan: make check

Reviewers: emayanke, haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13311/[RocksDB] Added a property ""leveldb.num-immutable-mem-table"" so that Flush can be called without blocking, and application still has a way to check when it's done also without blocking.

Summary: as title

Test Plan: DBTest.NumImmutableMemTable

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13305/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/Add backward compatible option in GetLiveFiles to choose whether to not Flush first

Summary:
As explained in comments in GetLiveFiles in db.h, this option will cause flush to be skipped in GetLiveFiles because some use-cases use GetSortedWalFiles after GetLiveFiles to generate more complete snapshots.
Using GetSortedWalFiles after GetLiveFiles allows us to not Flush in GetLiveFiles first because wals have everything.
Note: file deletions will be disabled before calling GLF or GSWF so live logs will not move to archive logs or get delted.
Note: Manifest file is truncated to a proper value in GLF, so it will always reply from the proper wal files on a restart

Test Plan: make

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13257/Fix SIGSEGV issue in universal compaction

Summary:
We saw SIGSEGV when set options.num_levels=1 in universal compaction
style. Dug into this issue for a while, and finally found the root cause (thank Haobo for discussion).

Test Plan: Add new unit test. It throws SIGSEGV without this change. Also run ""make all check"".

Reviewers: haobo, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13251/New unit test for iterator with snapshot

Summary:
I played with the reported bug about iterator with snapshot:
https://code.google.com/p/leveldb/issues/detail?id=200.

I turned the original test program
(https://code.google.com/p/leveldb/issues/attachmentText?id=200&aid=2000000000&name=test.cc&token=7uOUQW-HFlbAFMUm7EqtaAEy7Tw%3A1378320724136)
into a new unit test, but I cannot reproduce the problem. Notice lines
31-34 in above link. I have ran the new test with and without such Put()
operations. Both succeed.

So this diff simply adds the test, without changing any source codes.

Test Plan: run new test.

Reviewers: dhruba, haobo, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12735/The vector rep implementation was segfaulting because of incorrect initialization of vector.

Summary:
The constructor for Vector memtable has a parameter called 'count'
that specifies the capacity of the vector to be reserved at allocation
time. It was incorrectly used to initialize the size of the vector.

Test Plan: Enhanced db_test.

Reviewers: haobo, xjin, emayanke

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13083/[RocksDB] Fix DBTest.UniversalCompactionSizeAmplification too

Summary: as title

Test Plan: make db_test; ./db_test

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13005/[RocksDB] Fix DBTest.UniversalCompactionTrigger to reflect the correct compaction trigger condition.

Summary: as title

Test Plan: make db_test; ./db_test

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12981/Added a parameter to limit the maximum space amplification for universal compaction.

Summary:
Added a new field called max_size_amplification_ratio in the
CompactionOptionsUniversal structure. This determines the maximum
percentage overhead of space amplification.

The size amplification is defined to be the ratio between the size of
the oldest file to the sum of the sizes of all other files. If the
size amplification exceeds the specified value, then min_merge_width
and max_merge_width are ignored and a full compaction of all files is done.
A value of 10 means that the size a database that stores 100 bytes
of user data could occupy 110 bytes of physical storage.

Test Plan: Unit test DBTest.UniversalCompactionSpaceAmplification added.

Reviewers: haobo, emayanke, xjin

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12825/Flush was hanging because the configured options specified that more than 1 memtable need to be merged.

Summary:
There is an config option called Options.min_write_buffer_number_to_merge
that specifies the minimum number of write buffers to merge in memory
before flushing to a file in L0. But in the the case when the db is
being closed, we should not be using this config, instead we should
flush whatever write buffers were available at that time.

Test Plan: Unit test attached.

Reviewers: haobo, emayanke

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12717/An iterator may automatically invoke reseeks.

Summary:
An iterator invokes reseek if the number of sequential skips over the
same userkey exceeds a configured number. This makes iter->Next()
faster (bacause of fewer key compares) if a large number of
adjacent internal keys in a table (sst or memtable) have the
same userkey.

Test Plan: Unit test DBTest.IterReseek.

Reviewers: emayanke, haobo, xjin

Reviewed By: xjin

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D11865/New ldb command to convert compaction style

Summary:
Add new command ""change_compaction_style"" to ldb tool. For
universal->level, it shows ""nothing to do"". For level->universal, it
compacts all files into a single one and moves the file to level 0.

Also add check for number of files at level 1+ when opening db with
universal compaction style.

Test Plan:
'make all check'. New unit test for internal convertion function. Also manully test various
cmd like:

./ldb change_compaction_style --old_compaction_style=0
--new_compaction_style=1 --db=/tmp/leveldbtest-3088/db_test

Reviewers: haobo, dhruba

Reviewed By: haobo

CC: vamsi, emayanke

Differential Revision: https://reviews.facebook.net/D12603/Introduced a new flag non_blocking_io in ReadOptions.

Summary:
If ReadOptions.non_blocking_io is set to true, then KeyMayExists
and Iterators will return data that is cached in RAM.
If the Iterator needs to do IO from storage to serve the data,
then the Iterator.status() will return Status::IsRetry().

Test Plan:
Enhanced unit test DBTest.KeyMayExist to detect if there were are IOs
issues from storage. Added DBTest.NonBlockingIteration to verify
nonblocking Iterations.

Reviewers: emayanke, haobo

Reviewed By: haobo

CC: leveldb

Maniphest Tasks: T63

Differential Revision: https://reviews.facebook.net/D12531/Add three new MemTableRep's

Summary:
This patch adds three new MemTableRep's: UnsortedRep, PrefixHashRep, and VectorRep.

UnsortedRep stores keys in an std::unordered_map of std::sets. When an iterator is requested, it dumps the keys into an std::set and iterates over that.

VectorRep stores keys in an std::vector. When an iterator is requested, it creates a copy of the vector and sorts it using std::sort. The iterator accesses that new vector.

PrefixHashRep stores keys in an unordered_map mapping prefixes to ordered sets.

I also added one API change. I added a function MemTableRep::MarkImmutable. This function is called when the rep is added to the immutable list. It doesn't do anything yet, but it seems like that could be useful. In particular, for the vectorrep, it means we could elide the extra copy and just sort in place. The only reason I haven't done that yet is because the use of the ArenaAllocator complicates things (I can elaborate on this if needed).

Test Plan:
make -j32 check
./db_stress --memtablerep=vector
./db_stress --memtablerep=unsorted
./db_stress --memtablerep=prefixhash --prefix_size=10

Reviewers: dhruba, haobo, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12117/Made merge_oprator a shared_ptr; and added TTL unit tests

Test Plan:
- make all check;
- make release;
- make stringappend_test; ./stringappend_test

Reviewers: haobo, emayanke

Reviewed By: haobo

CC: leveldb, kailiu

Differential Revision: https://reviews.facebook.net/D12381/API for getting archived log files

Summary: Also expanded class LogFile to have startSequene and FileSize and exposed it publicly

Test Plan: make all check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12087/"
,,Rocksdb,"Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/Merge operator fixes part 1.

Summary:
-Added null checks and revisions to DBIter::MergeValuesNewToOld()
-Added DBIter test to stringappend_test
-Major fix with Merge and TTL
More plans for fixes later.

Test Plan:
-make clean; make stringappend_test -j 32; ./stringappend_test
-make all check;

Reviewers: haobo, emayanke, vamsi, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12315/"
DataBase Management,"DataBase Management, Performanc Management",Rocksdb,"Make ""Table"" pluggable

Summary: This patch makes Table and TableBuilder a abstract class and make all the implementation of the current table into BlockedBasedTable and BlockedBasedTable Builder.

Test Plan: Make db_test.cc to work with block based table. Add a new test simple_table_db_test.cc where a different simple table format is implemented.

Reviewers: dhruba, haobo, kailiu, emayanke, vamsi

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13521/Universal Compaction to Have a Size Percentage Threshold To Decide Whether to Compress

Summary:
This patch adds a option for universal compaction to allow us to only compress output files if the files compacted previously did not yet reach a specified ratio, to save CPU costs in some cases.

Compression is always skipped for flushing. This is because the size information is not easy to evaluate for flushing case. We can improve it later.

Test Plan:
add test
DBTest.UniversalCompactionCompressRatio1 and DBTest.UniversalCompactionCompressRatio12

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13467/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/Minor fixes found while trying to compile it using clang on Mac OS X/"
Restructuring the code,"Data conversion, Restructuring the code, performance management",Rocksdb,"Fix the string format issue

Summary:

mac and our dev server has totally differnt definition of uint64_t, therefore fixing the warning in mac has actually made code in linux uncompileable.

Test Plan:

make clean && make -j32/Fixing the warning messages captured under mac os # Consider using `git commit -m 'One line title' && arc diff`. # You will save time by running lint and unit in the background.

Summary: The work to make sure mac os compiles rocksdb is not completed yet. But at least we can start cleaning some warnings captured only by g++ from mac os..

Test Plan: ran make in mac os

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14049/Small changes in Deleting obsolete files

Summary:
@haobo's suggestions from https://reviews.facebook.net/D13827

Renaming some variables, deprecating purge_log_after_flush, changing for loop into auto for loop.

I have not implemented deleting objects outside of mutex yet because it would require a big code change - we would delete object in db_impl, which currently does not know anything about object because it's defined in version_edit.h (FileMetaData). We should do it at some point, though.

Test Plan: Ran deletefile_test

Reviewers: haobo

Reviewed By: haobo

CC: leveldb, haobo

Differential Revision: https://reviews.facebook.net/D14025/Speed up FindObsoleteFiles

Summary:
Here's one solution we discussed on speeding up FindObsoleteFiles. Keep a set of all files in DBImpl and update the set every time we create a file. I probably missed few other spots where we create a file.

It might speed things up a bit, but makes code uglier. I don't really like it.

Much better approach would be to abstract all file handling to a separate class. Think of it as layer between DBImpl and Env. Having a separate class deal with file namings and deletion would benefit both code cleanliness (especially with huge DBImpl) and speed things up. It will take a huge effort to do this, though.

Let's discuss offline today.

Test Plan: Ran ./db_stress, verified that files are getting deleted

Reviewers: dhruba, haobo, kailiu, emayanke

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D13827/Flush the log outside of lock

Summary:
Added a new call LogFlush() that flushes the log contents to the OS buffers. We never call it with lock held.

We call it once for every Read/Write and often in compaction/flush process so the frequency should not be a problem.

Test Plan: db_test

Reviewers: dhruba, haobo, kailiu, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13935/Move I/O outside of lock

Summary:
I'm figuring out how Version[Set, Edit, ] classes work and I stumbled on this.

It doesn't seem that the comment is accurate anymore. What I read is when the manifest grows too big, create a new file (and not only when we call LogAndApply for the first time).

Test Plan: make check (currently running)

Reviewers: dhruba, haobo, kailiu, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13839/[RocksDB] Add OnCompactionStart to CompactionFilter class

Summary: This is to give application compaction filter a chance to access context information of a specific compaction run. For example, depending on whether a compaction goes through all data files, the application could do things differently.

Test Plan: make check

Reviewers: dhruba, kailiu, sdong

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13683/Follow-up Cleaning-up After D13521

Summary:
This patch is to address @haobo's comments on D13521:
1. rename Table to be TableReader and make its factory function to be GetTableReader
2. move the compression type selection logic out of TableBuilder but to compaction logic
3. more accurate comments
4. Move stat name constants into BlockBasedTable implementation.
5. remove some uncleaned codes in simple_table_db_test

Test Plan: pass test suites.

Reviewers: haobo, dhruba, kailiu

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13785/Universal Compaction to Have a Size Percentage Threshold To Decide Whether to Compress

Summary:
This patch adds a option for universal compaction to allow us to only compress output files if the files compacted previously did not yet reach a specified ratio, to save CPU costs in some cases.

Compression is always skipped for flushing. This is because the size information is not easy to evaluate for flushing case. We can improve it later.

Test Plan:
add test
DBTest.UniversalCompactionCompressRatio1 and DBTest.UniversalCompactionCompressRatio12

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13467/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/[RocksDB] Universal compaction trigger condition minor fix

Summary: Currently, when total number of files reaches level0_file_num_compaction_trigger, universal compaction will schedule a compaction job, but the job will not honor the compaction until the total number of files is level0_file_num_compaction_trigger+1. Fixed the condition for consistent behavior (start compaction on reaching level0_file_num_compaction_trigger).

Test Plan: make check; db_stress

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12945/Added a parameter to limit the maximum space amplification for universal compaction.

Summary:
Added a new field called max_size_amplification_ratio in the
CompactionOptionsUniversal structure. This determines the maximum
percentage overhead of space amplification.

The size amplification is defined to be the ratio between the size of
the oldest file to the sum of the sizes of all other files. If the
size amplification exceeds the specified value, then min_merge_width
and max_merge_width are ignored and a full compaction of all files is done.
A value of 10 means that the size a database that stores 100 bytes
of user data could occupy 110 bytes of physical storage.

Test Plan: Unit test DBTest.UniversalCompactionSpaceAmplification added.

Reviewers: haobo, emayanke, xjin

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12825/Cleanup DeleteFile API

Summary:
The DeleteFile API was removing files inside the db-lock. This
is now changed to remove files outside the db-lock.
The GetLiveFilesMetadata() returns the smallest and largest
seqnuence number of each file as well.

Test Plan: deletefile_test

Reviewers: emayanke, haobo

Reviewed By: haobo

CC: leveldb

Maniphest Tasks: T63

Differential Revision: https://reviews.facebook.net/D12567/Introduced a new flag non_blocking_io in ReadOptions.

Summary:
If ReadOptions.non_blocking_io is set to true, then KeyMayExists
and Iterators will return data that is cached in RAM.
If the Iterator needs to do IO from storage to serve the data,
then the Iterator.status() will return Status::IsRetry().

Test Plan:
Enhanced unit test DBTest.KeyMayExist to detect if there were are IOs
issues from storage. Added DBTest.NonBlockingIteration to verify
nonblocking Iterations.

Reviewers: emayanke, haobo

Reviewed By: haobo

CC: leveldb

Maniphest Tasks: T63

Differential Revision: https://reviews.facebook.net/D12531/Internal/user key bug fix.

Summary: Fix code so that the filter_block layer only assumes keys are internal when prefix_extractor is set.

Test Plan: ./filter_block_test

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12501/Revert ""Prefix scan: db_bench and bug fixes""

This reverts commit c2bd8f4824bda98db8699f1e08d6969cf21ef86f./Prefix scan: db_bench and bug fixes

Summary: If use_prefix_filters is set and read_range>1, then the random seeks will set a the prefix filter to be the prefix of the key which was randomly selected as the target.  Still need to add statistics (perhaps in a separate diff).

Test Plan: ./db_bench --benchmarks=fillseq,prefixscanrandom --num=10000000 --statistics=1 --use_prefix_blooms=1 --use_prefix_api=1 --bloom_bits=10

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, haobo

Differential Revision: https://reviews.facebook.net/D12273/Add APIs to query SST file metadata and to delete specific SST files

Summary: An api to query the level, key ranges, size etc for each SST file and an api to delete a specific file from the db and all associated state in the bookkeeping datastructures.

Notes: Editing the manifest version does not release the obsolete files right away. However deleting the file directly will mess up the iterator. We may need a more aggressive/timely file deletion api.

I have used std::unique_ptr - will switch to boost:: since this is external. thoughts?

Unit test is fragile right now as it expects the compaction at certain levels.

Test Plan: unittest

Reviewers: dhruba, vamsi, emayanke

CC: zshao, leveldb, haobo

Task ID: #

Blame Rev:/[RocksDB] Minor iterator cleanup

Summary: Was going through the iterator related code, did some cleanup along the way. Basically replaced array with vector and adopted range based loop where applicable.

Test Plan: make check; make valgrind_check

Reviewers: dhruba, emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12435/Made merge_oprator a shared_ptr; and added TTL unit tests

Test Plan:
- make all check;
- make release;
- make stringappend_test; ./stringappend_test

Reviewers: haobo, emayanke

Reviewed By: haobo

CC: leveldb, kailiu

Differential Revision: https://reviews.facebook.net/D12381/"
,,Rocksdb,"Fix Bug: iterator.Prev() or iterator.SeekToLast() might return the first element instead of the correct one

Summary:
Recent patch https://reviews.facebook.net/D11865 introduced a regression bug:

DBIter::FindPrevUserEntry(), which is called by DBIter::Prev() (and also implicitly if calling iterator.SeekToLast())  might do issue a seek when having skipped too many entries. If the skipped entry just before the seek() is a delete, the saved key is erased so that it seeks to the front, so Prev() would return the first element.

This patch fixes the bug by not doing seek() in DBIter::FindNextUserEntry() if saved key has been erased.

Test Plan: Add a test DBTest.IterPrevMaxSkip which would fail without the patch and would pass with the change.

Reviewers: dhruba, xjin, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13557/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/An iterator may automatically invoke reseeks.

Summary:
An iterator invokes reseek if the number of sequential skips over the
same userkey exceeds a configured number. This makes iter->Next()
faster (bacause of fewer key compares) if a large number of
adjacent internal keys in a table (sst or memtable) have the
same userkey.

Test Plan: Unit test DBTest.IterReseek.

Reviewers: emayanke, haobo, xjin

Reviewed By: xjin

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D11865/Merge operator fixes part 1.

Summary:
-Added null checks and revisions to DBIter::MergeValuesNewToOld()
-Added DBIter test to stringappend_test
-Major fix with Merge and TTL
More plans for fixes later.

Test Plan:
-make clean; make stringappend_test -j 32; ./stringappend_test
-make all check;

Reviewers: haobo, emayanke, vamsi, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12315/"
Compression tasks,"Compression tasks, performance management",Rocksdb,"Universal Compaction to Have a Size Percentage Threshold To Decide Whether to Compress

Summary:
This patch adds a option for universal compaction to allow us to only compress output files if the files compacted previously did not yet reach a specified ratio, to save CPU costs in some cases.

Compression is always skipped for flushing. This is because the size information is not easy to evaluate for flushing case. We can improve it later.

Test Plan:
add test
DBTest.UniversalCompactionCompressRatio1 and DBTest.UniversalCompactionCompressRatio12

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13467/Migrate names of properties from 'leveldb' prefix to 'rocksdb' prefix.

Summary: Migrate names of properties from 'leveldb' prefix to 'rocksdb' prefix.

Test Plan: make check

Reviewers: emayanke, haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13311/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/"
,,Rocksdb,"WriteBatch::Put() overload that gathers key and value from arrays of slices

Summary: In our project, when writing to the database, we want to form the value as the concatenation of a small header and a larger payload.  It's a shame to have to copy the payload just so we can give RocksDB API a linear view of the value.  Since RocksDB makes a copy internally, it's easy to support gather writes.

Test Plan: write_batch_test, new test case

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13947/In-place updates for equal keys and similar sized values

Summary:
Currently for each put, a fresh memory is allocated, and a new entry is added to the memtable with a new sequence number irrespective of whether the key already exists in the memtable. This diff is an attempt to update the value inplace for existing keys. It currently handles a very simple case:
1. Key already exists in the current memtable. Does not inplace update values in immutable memtable or snapshot
2. Latest value type is a 'put' ie kTypeValue
3. New value size is less than existing value, to avoid reallocating memory

TODO: For a put of an existing key, deallocate memory take by values, for other value types till a kTypeValue is found, ie. remove kTypeMerge.
TODO: Update the transaction log, to allow consistent reload of the memtable.

Test Plan: Added a unit test verifying the inplace update. But some other unit tests broken due to invalid sequence number checks. WIll fix them next.

Reviewers: xinyaohu, sumeet, haobo, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12423

Automatic commit by arc/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/Allow WriteBatch::Handler to abort iteration

Summary:
Sometimes you don't need to iterate through the whole WriteBatch. This diff makes the Handler member functions return a bool that indicates whether to abort or not. If they return true, the iteration stops.

One thing I just thought of is that this will break backwards-compability. Maybe it would be better to add a virtual member function WriteBatch::Handler::ShouldAbort() that returns false by default. Comments ed.

I still have to add a new unit test for the abort code, but let's finalize the API first.

Test Plan: make -j32 check

Reviewers: dhruba, haobo, vamsi, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12339/"
,,Rocksdb,"Unify DeleteFile and DeleteWalFiles

Summary:
This is to simplify rocksdb public APIs and improve the code quality.
Created an additional parameter to ParseFileName for log sub type and improved the code for deleting a wal file.
Wrote exhaustive unit-tests in delete_file_test
Unification of other redundant APIs can be taken up in a separate diff

Test Plan: Expanded delete_file test

Reviewers: dhruba, haobo, kailiu, sdong

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13647/Dbid feature

Summary:
Create a new type of file on startup if it doesn't already exist called DBID.
This will store a unique number generated from boost library's uuid header file.
The use-case is to identify the case of a db losing all its data and coming back up either empty or from an image(backup/live replica's recovery)
the key point to note is that DBID is not stored in a backup or db snapshot
It's preferable to use Boost for uuid because:
1) A non-standard way of generating uuid is not good
2) /proc/sys/kernel/random/uuid generates a uuid but only on linux environments and the solution would not be clean
3) c++ doesn't have any direct way to get a uuid
4) Boost is a very good library that was already having linkage in rocksdb from third-party
Note: I had to update the TOOLCHAIN_REV in build files to get latest verison of boost from third-party as the older version had a bug.
I had to put Wno-uninitialized in Makefile because boost-1.51 has an unitialized variable and rocksdb would not comiple otherwise. Latet open-source for boost is 1.54 but is not there in third-party. I have notified the concerned people in fbcode about it.
@kailiu : While releasing to third-party, an additional dependency will need to be created for boost in TARGETS file. I can help identify.

Test Plan:
Expand db_test to test 2 cases
1) Restarting db with Id file present - verify that no change to Id
2)Restarting db with Id file deleted - verify that a different Id is there after reopen
Also run make all check

Reviewers: dhruba, haobo, kailiu, sdong

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13587/"
,,Rocksdb,"[RocksDB] Generalize prefix-aware iterator to be used for more than one Seek

Summary: Added a prefix_seek flag in ReadOptions to indicate that Seek is prefix aware(might not return data with different prefix), and also not bound to a specific prefix. Multiple Seeks and range scans can be invoked on the same iterator. If a specific prefix is specified, this flag will be ignored. Just a quick prototype that works for PrefixHashRep, the new lockless memtable could be easily extended with this support too.

Test Plan: test it on Leaf

Reviewers: dhruba, kailiu, sdong, igor

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13929/In-place updates for equal keys and similar sized values

Summary:
Currently for each put, a fresh memory is allocated, and a new entry is added to the memtable with a new sequence number irrespective of whether the key already exists in the memtable. This diff is an attempt to update the value inplace for existing keys. It currently handles a very simple case:
1. Key already exists in the current memtable. Does not inplace update values in immutable memtable or snapshot
2. Latest value type is a 'put' ie kTypeValue
3. New value size is less than existing value, to avoid reallocating memory

TODO: For a put of an existing key, deallocate memory take by values, for other value types till a kTypeValue is found, ie. remove kTypeMerge.
TODO: Update the transaction log, to allow consistent reload of the memtable.

Test Plan: Added a unit test verifying the inplace update. But some other unit tests broken due to invalid sequence number checks. WIll fix them next.

Reviewers: xinyaohu, sumeet, haobo, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12423

Automatic commit by arc/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/Add three new MemTableRep's

Summary:
This patch adds three new MemTableRep's: UnsortedRep, PrefixHashRep, and VectorRep.

UnsortedRep stores keys in an std::unordered_map of std::sets. When an iterator is ed, it dumps the keys into an std::set and iterates over that.

VectorRep stores keys in an std::vector. When an iterator is ed, it creates a copy of the vector and sorts it using std::sort. The iterator accesses that new vector.

PrefixHashRep stores keys in an unordered_map mapping prefixes to ordered sets.

I also added one API change. I added a function MemTableRep::MarkImmutable. This function is called when the rep is added to the immutable list. It doesn't do anything yet, but it seems like that could be useful. In particular, for the vectorrep, it means we could elide the extra copy and just sort in place. The only reason I haven't done that yet is because the use of the ArenaAllocator complicates things (I can elaborate on this if needed).

Test Plan:
make -j32 check
./db_stress --memtablerep=vector
./db_stress --memtablerep=unsorted
./db_stress --memtablerep=prefixhash --prefix_size=10

Reviewers: dhruba, haobo, emayanke

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12117/Made merge_oprator a shared_ptr; and added TTL unit tests

Test Plan:
- make all check;
- make release;
- make stringappend_test; ./stringappend_test

Reviewers: haobo, emayanke

Reviewed By: haobo

CC: leveldb, kailiu

Differential Revision: https://reviews.facebook.net/D12381/"
,,Rocksdb,"Speed up FindObsoleteFiles

Summary:
Here's one solution we discussed on speeding up FindObsoleteFiles. Keep a set of all files in DBImpl and update the set every time we create a file. I probably missed few other spots where we create a file.

It might speed things up a bit, but makes code uglier. I don't really like it.

Much better approach would be to abstract all file handling to a separate class. Think of it as layer between DBImpl and Env. Having a separate class deal with file namings and deletion would benefit both code cleanliness (especially with huge DBImpl) and speed things up. It will take a huge effort to do this, though.

Let's discuss offline today.

Test Plan: Ran ./db_stress, verified that files are getting deleted

Reviewers: dhruba, haobo, kailiu, emayanke

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D13827/Add option for storing transaction logs in a separate dir

Summary: In some cases, you might not want to store the data log (write ahead log) files in the same dir as the sst files. An example use case is leaf, which stores sst files in tmpfs. And would like to save the log files in a separate dir (disk) to save memory.

Test Plan: make all. Ran db_test test. A few test failing. P2785018. If you guys don't see an obvious problem with the code, maybe somebody from the rocksdb team could help me debug the issue here. Running this on leaf worked well. I could see logs stored on disk, and deleted appropriately after compactions. Obviously this is only one set of options. The unit tests cover different options. Seems like I'm missing some edge cases.

Reviewers: dhruba, haobo, leveldb

CC: xinyaohu, sumeet

Differential Revision: https://reviews.facebook.net/D13239/Change namespace from leveldb to rocksdb

Summary:
Change namespace from leveldb to rocksdb. This allows a single
application to link in open-source leveldb code as well as
rocksdb code into the same process.

Test Plan: compile rocksdb

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13287/Return pathname relative to db dir in LogFile and cleanup AppendSortedWalsOfType

Summary: So that replication can just download from wherever LogFile.Pathname is pointing them.

Test Plan: make all check;./db_repl_stress

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12609/API for getting archived log files

Summary: Also expanded class LogFile to have startSequene and FileSize and exposed it publicly

Test Plan: make all check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D12087/"
,,Rocksdb,"Make table properties shareable

Summary:
We are going to expose properties of all tables to end users through ""some"" db interface.
However, current design doesn't naturally fit for this need, which is because:

1. If a table presents in table cache, we cannot simply return the reference to its table properties, because the table may be destroy after compaction (and we don't want to hold the ref of the version).
2. Copy table properties is OK, but it's slow.

Thus in this diff, I change the table reader's interface to return a shared pointer (for const table properties), instead a const refernce.

Test Plan: `make check` passed

Reviewers: haobo, sdong, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15999/Add support for plain table format to sst_dump.

Summary:
This diff enables the command line tool `sst_dump` to work for sst files
under plain table format.  Changes include:
  * In tools/sst_dump.cc:
    - add support for plain table format
    - display prefix_extractor information when --show_properties is on
  * In table/format.cc
    - Now the table magic number of a Footer can be later initialized
      via ReadFooterFromFile().
  * In table/meta_bocks:
    - add function ReadTableMagicNumber() that reads the magic number of
      the specified file.

Minor fixes:
 - remove a duplicate #include in table/table_test.cc
 - fix a commentary typo in include/rocksdb/memtablerep.h
 - fix lint errors.

Test Plan:
Runs sst_dump with both block-based and plain-table format files with
different arguments, specifically those with --show-properties and --from.

* sample output:
  https://reviews.facebook.net/P261

Reviewers: kailiu, sdong, xjin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15903/[Performance Branch] PlainTable to encode rows with seqID 0, value type using 1 internal byte.

Summary: In PlainTable, use one single byte to represent 8 bytes of internal bytes, if seqID = 0 and it is value type (which should be common for bottom most files). It is to save 7 bytes for uncompressed cases.

Test Plan: make all check

Reviewers: haobo, dhruba, kailiu

Reviewed By: haobo

CC: igor, leveldb

Differential Revision: https://reviews.facebook.net/D15489/First phase API clean up

Summary:
Addressed all the issues in https://reviews.facebook.net/D15447.
Now most table-related modules are hidden from user land.

Test Plan: make check

Reviewers: sdong, haobo, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15525/Allow command line tool sst-dump to display table properties.

Summary:
Add option '--show_properties' to sst_dump tool to allow displaying
property block of the specified files.

Test Plan:
Run sst_dump with the following arguments, which covers cases affected by
this diff:

  1. with only --file
  2. with both --file and --show_properties
  3. with --file, --show_properties, and --from

Reviewers: kailiu, xjin

Differential Revision: https://reviews.facebook.net/D15453/"
Database Management,"Database Management, compression tasks",Rocksdb,"Support for LZ4 compression./Killing Transform Rep

Summary:
Let's get rid of TransformRep and it's children. We have confirmed that HashSkipListRep works better with multifeed, so there is no benefit to keeping this around.

This diff is mostly just deleting references to obsoleted functions. I also have a diff for fbcode that we'll need to push when we switch to new release.

I had to expose HashSkipListRepFactory in the client header files because db_impl.cc needs access to GetTransform() function for SanitizeOptions.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14397/Fix two nasty use-after-free-bugs

Summary:
These bugs were caught by ASAN crash test.
1. The first one, in table/filter_block.cc is very nasty. We first reference entries_ and store the reference to Slice prev. Then, we call entries_.append(), which can change the reference. The Slice prev now points to junk.
2. The second one is a bug in a test, so it's not very serious. Once we set read_opts.prefix, we never clear it, so some other function might still reference it.

Test Plan: asan crash test now runs more than 5 mins. Before, it failed immediately. I will run the full one, but the full one takes quite some time (5 hours)

Reviewers: dhruba, haobo, kailiu

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14223/make util/env_posix.cc work under mac

Summary: This diff invoves some more complicated issues in the posix environment.

Test Plan: works under mac os. will need to verify dev box.

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14061/"
,,Rocksdb,"Fix some 32-bit compile errors

Summary: RocksDB doesn't compile on 32-bit architecture apparently. This is attempt to fix some of 32-bit errors. They are reported here: https://gist.github.com/paxos/8789697

Test Plan: RocksDB still compiles on 64-bit :)

Reviewers: kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15825/"
,,Rocksdb,"convert Tickers back to array with padding and alignment

Summary:
Pad each Ticker structure to be 64 bytes and make them align on 64 bytes
boundary to avoid cache line false sharing issue.
Please refer to task 3615553 for more details

Test Plan:
db_bench

LevelDB:    version 2.0s
Date:       Wed Jan 29 12:23:17 2014
CPU:        32 * Intel(R) Xeon(R) CPU E5-2660 0 @ 2.20GHz
CPUCache:   20480 KB
rocksdb.build.overwrite.qps 49638
rocksdb.build.overwrite.p50_micros 58.73
rocksdb.build.overwrite.p75_micros 210.56
rocksdb.build.overwrite.p99_micros 733.28
rocksdb.build.fillseq.qps 366729
rocksdb.build.fillseq.p50_micros 1.00
rocksdb.build.fillseq.p75_micros 1.00
rocksdb.build.fillseq.p99_micros 2.65
rocksdb.build.readrandom.qps 1152995
rocksdb.build.readrandom.p50_micros 11.27
rocksdb.build.readrandom.p75_micros 15.69
rocksdb.build.readrandom.p99_micros 33.59
rocksdb.build.readrandom_smallblockcache.qps 956047
rocksdb.build.readrandom_smallblockcache.p50_micros 15.23
rocksdb.build.readrandom_smallblockcache.p75_micros 17.31
rocksdb.build.readrandom_smallblockcache.p99_micros 31.49
rocksdb.build.readrandom_memtable_sst.qps 1105183
rocksdb.build.readrandom_memtable_sst.p50_micros 12.04
rocksdb.build.readrandom_memtable_sst.p75_micros 15.78
rocksdb.build.readrandom_memtable_sst.p99_micros 32.49
rocksdb.build.readrandom_fillunique_random.qps 487856
rocksdb.build.readrandom_fillunique_random.p50_micros 29.65
rocksdb.build.readrandom_fillunique_random.p75_micros 40.93
rocksdb.build.readrandom_fillunique_random.p99_micros 78.68
rocksdb.build.memtablefillrandom.qps 91304
rocksdb.build.memtablefillrandom.p50_micros 171.05
rocksdb.build.memtablefillrandom.p75_micros 196.12
rocksdb.build.memtablefillrandom.p99_micros 291.73
rocksdb.build.memtablereadrandom.qps 1340411
rocksdb.build.memtablereadrandom.p50_micros 9.48
rocksdb.build.memtablereadrandom.p75_micros 13.95
rocksdb.build.memtablereadrandom.p99_micros 30.36
rocksdb.build.readwhilewriting.qps 491004
rocksdb.build.readwhilewriting.p50_micros 29.58
rocksdb.build.readwhilewriting.p75_micros 40.34
rocksdb.build.readwhilewriting.p99_micros 76.78

Reviewers: igor, haobo

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15573/Fix performance regression in statistics

Summary:
For some reason, D15099 caused a big performance regression: https://fburl.com/16059000

After digging a bit, I figured out that the reason was that std::atomic_uint_fast64_t was allocated in an array. When I switched from an array to vector, the QPS returned to the previous level. I'm not sure why this is happening, but this diff seems to fix the performance regression.

Test Plan: I ran the regression script, observed the performance going back to normal

Reviewers: tnovak, kailiu, haobo

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15375/Fix a Statistics-related unit test faulure

Summary:
In my MacOS, the member variables are populated with random numbers after initialization.
This diff fixes it by fill these arrays with 0.

Test Plan: make && ./table_test

Reviewers: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15315/Statistics code cleanup

Summary: I'm separating code-cleanup part of https://reviews.facebook.net/D14517. This will make D14517 easier to understand and this diff easier to review.

Test Plan: make check

Reviewers: haobo, kailiu, sdong, dhruba, tnovak

Reviewed By: tnovak

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15099/"
,,Rocksdb,"Avoid malloc in NotFound key status if no message is given.

Summary:
In some places we have NotFound status created with empty message, but it doesn't avoid a malloc. With this patch, the malloc is avoided for that case.

The motivation of it is that I found in db_bench readrandom test when all keys are not existing, about 4% of the total running time is spent on malloc of Status, plus a similar amount of CPU spent on free of them, which is not necessary.

Test Plan: make all check

Reviewers: dhruba, haobo, igor

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14691/"
,Thread management,Rocksdb,"Don't LogFlush() in foreground threads

Summary: So fflush() takes a lock which is heavyweight. I added flush_pending_, but more importantly, I removed LogFlush() from foreground threads.

Test Plan: ./db_test

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14535/"
,,Rocksdb,"Print stack trace on assertion failure

Summary:
This will help me a lot! When we hit an assertion in unittest, we get the whole stack trace now.

Also, changed stack trace a bit, we now include actual demangled C++ class::function symbols!

Test Plan: Added ASSERT_TRUE(false) to a test, observed a stack trace

Reviewers: haobo, dhruba, kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14499/"
,,Rocksdb,"[RocksDB] [Performance Branch] Added dynamic bloom, to be used for memable non-existing key filtering

Summary: as title

Test Plan: dynamic_bloom_test

Reviewers: dhruba, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14385/"
,,Rocksdb,"Aggressively inlining the short functions in coding.cc

Summary:
This diff takes an even more aggressive way to inline the functions. A decent rule that I followed is ""not inline a function if it is more than 10 lines long.""

Normally optimizing code by inline is ugly and hard to control, but since one of our usecase has significant amount of CPU used in functions from coding.cc, I'd like to try this diff out.

Test Plan:
1. the size for some .o file increased a little bit, but most less than 1%. So I think the negative impact of inline is negligible.
2. As the regression test shows (ran for 10 times and I calculated the average number)

    Metrics                                         Befor    After
    ========================================================================
    rocksdb.build.fillseq.qps                       426595   444515    (+4.6%)
    rocksdb.build.memtablefillrandom.qps            121739   123110
    rocksdb.build.memtablereadrandom.qps            1285103  1280520
    rocksdb.build.overwrite.qps                     125816   135570    (+9%)
    rocksdb.build.readrandom_fillunique_random.qps  285995   296863
    rocksdb.build.readrandom_memtable_sst.qps       1027132  1027279
    rocksdb.build.readrandom.qps                    1041427  1054665
    rocksdb.build.readrandom_smallblockcache.qps    1028631  1038433
    rocksdb.build.readwhilewriting.qps              918352   914629

Reviewers: haobo, sdong, igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15291/"
,,Rocksdb,"Statistics code cleanup

Summary: I'm separating code-cleanup part of https://reviews.facebook.net/D14517. This will make D14517 easier to understand and this diff easier to review.

Test Plan: make check

Reviewers: haobo, kailiu, sdong, dhruba, tnovak

Reviewed By: tnovak

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15099/StopWatch not to get time if it is created for statistics and it is disabled

Summary: Currently, even if statistics is not enabled, StopWatch only for the stats still gets the time of the day, which is wasteful. This patch adds a new option to StopWatch to disable this get in this case.

Test Plan: make all check

Reviewers: dhruba, haobo, igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14703/[RocksDB] Use raw pointer instead of shared pointer when passing Statistics object internally

Summary: liveness of the statistics object is already ensured by the shared pointer in DB options. There's no reason to pass again shared pointer among internal functions. Raw pointer is sufficient and efficient.

Test Plan: make check

Reviewers: dhruba, MarkCallaghan, igor

Reviewed By: dhruba

CC: leveldb, reconnect.grayhat

Differential Revision: https://reviews.facebook.net/D14289/"
,,Rocksdb,"Fsync directory after we create a new file

Summary:
@dhruba, I'm not sure where we need to sync the directory. I implemented the function in Env() and added the dir sync just after we close the newly created file in the builder.

Should I also add FsyncDir() to new files that get created by a compaction?

Test Plan: Confirmed that FsyncDir is returning Status::OK()

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D14751/"
,,Rocksdb,"Clean up arena API

Summary:
Easy thing goes first. This patch moves arena to internal dir; based
on which, the coming patch will deal with memtable_rep.

Test Plan: make check

Reviewers: haobo, sdong, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15615/Separate the aligned and unaligned memory allocation

Summary: Use two vectors for different types of memory allocation.

Test Plan: run all unit tests.

Reviewers: haobo, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15027/"
,,Rocksdb,"Statistics code cleanup

Summary: I'm separating code-cleanup part of https://reviews.facebook.net/D14517. This will make D14517 easier to understand and this diff easier to review.

Test Plan: make check

Reviewers: haobo, kailiu, sdong, dhruba, tnovak

Reviewed By: tnovak

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15099/"
,,Rocksdb,"Merge branch 'master' into performance

Conflicts:
	Makefile
	db/db_impl.cc
	db/db_test.cc
	db/memtable_list.cc
	db/memtable_list.h
	table/block_based_table_reader.cc
	table/table_test.cc
	util/cache.cc
	util/coding.cc/Add a call DisownData() to Cache, which should speed up shutdown

Summary: On a shutdown, freeing memory takes a long time. If we're shutting down, we don't really care about memory leaks. I added a call to Cache that will avoid freeing all objects in cache.

Test Plan:
I created a script to test the speedup and demonstrate how to use the call: https://phabricator.fb.com/P3864368

Clean shutdown took 7.2 seconds, while fast and dirty one took 6.3 seconds. Unfortunately, the speedup is not that big, but should be bigger with bigger block_cache. I have set up the capacity to 80GB, but the script filled up only ~7GB.

Reviewers: dhruba, haobo, MarkCallaghan, xjin

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15069/Revert `atomic_size_t usage`

Summary:
By disassemble the function, we found that the atomic variables do invoke the `lock` that locks the memory bus.
As a tradeoff, we protect the GetUsage by mutex and leave usage_ as plain size_t.

Test Plan: passed `cache_test`

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14667/Expose usage info for the cache

Summary: This diff will help us to figure out the memory usage for the cache part.

Test Plan: added a new memory usage test for cache

Reviewers: haobo, sdong, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14559/Make Cache::GetCapacity constant

Summary: This will allow us to access constant via `DB::GetOptions().table_cache.GetCapacity()` or `DB::GetOptions().block_cache.GetCapacity()` since GetOptions() is also constant method./"
,,Rocksdb,"[CF] Rethink table cache

Summary:
Adapting table cache to column families is interesting. We want table cache to be global LRU, so if some column families are use not as often as others, we want them to be evicted from cache. However, current TableCache object also constructs tables on its own. If table is not found in the cache, TableCache automatically creates new table. We want each column family to be able to specify different table factory.

To solve the problem, we still have a single LRU, but we provide the LRUCache object to TableCache on construction. We have one TableCache per column family, but the underyling cache is shared by all TableCache objects.

This allows us to have a global LRU, but still be able to support different table factories for different column families. Also, in the future it will also be able to support different directories for different column families.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15915/VersionSet cleanup

Summary:
Removed icmp_ from VersionSet (since it's per-column-family, not per-DB-instance)
Unfriended VersionSet and ColumnFamilyData (yay!)
Removed VersionSet::NumberLevels()
Cleaned up DBImpl

Test Plan: make check

Reviewers: dhruba, haobo, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15819/Make VersionSet::ReduceNumberOfLevels() static

Summary:
A lot of our code implicitly assumes number_levels to be static. ReduceNumberOfLevels() breaks that assumption. For example, after calling ReduceNumberOfLevels(), DBImpl::NumberLevels() will be different from VersionSet::NumberLevels(). This is dangerous. Thankfully, it's not in public headers and is only used from LDB cmd tool. LDB tool is only using it statically, i.e. it never calls it with running DB instance. With this diff, we make it explicitly static. This way, we can assume number_levels to be immutable and not break assumption that lot of our code is relying upon. LDB tool can still use the method.

Also, I removed the method from a separate file since it breaks filename completition. version_se<TAB> now completes to ""version_set."" instead of ""version_set"" (without the dot). I don't see a big reason that the function should be in a different file.

Test Plan: reduce_levels_test

Reviewers: dhruba, haobo, kailiu, sdong

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15303/ColumnFamilySet

Summary:
I created a separate class ColumnFamilySet to keep track of column families. Before we did this in VersionSet and I believe this approach is cleaner.

Let me know if you have any comments. I will commit tomorrow.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15357/[Log dumper broken when merge operator is in log]

Summary: $title

Test Plan:
on my dev box

Revert Plan: OK

Task ID: #

Reviewers: emayanke, dhruba, haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14451/"
,,Rocksdb,"[Performance Branch] HashLinkList to avoid to convert length prefixed string back to internal keys

Summary: Converting from length prefixed buffer back to internal key costs some CPU but it is not necessary. In this patch, internal keys are pass though the functions so that we don't need to convert back to it.

Test Plan: make all check

Reviewers: haobo, kailiu

Reviewed By: kailiu

CC: igor, leveldb

Differential Revision: https://reviews.facebook.net/D15393/Merge branch 'master' into performance

Conflicts:
	Makefile
	db/db_impl.cc
	db/db_impl.h
	db/db_test.cc
	db/memtable.cc
	db/memtable.h
	db/version_edit.h
	db/version_set.cc
	include/rocksdb/options.h
	util/hash_skiplist_rep.cc
	util/options.cc/Remove the unnecessary use of shared_ptr

Summary:
shared_ptr is slower than unique_ptr (which literally comes with no performance cost compare with raw pointers).
In memtable and memtable rep, we use shared_ptr when we'd actually should use unique_ptr.

According to igor's previous work, we are likely to make quite some performance gain from this diff.

Test Plan: make check

Reviewers: dhruba, igor, sdong, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15213/[RocksDB] [Performance Branch] Added dynamic bloom, to be used for memable non-existing key filtering

Summary: as title

Test Plan: dynamic_bloom_test

Reviewers: dhruba, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14385/Merge branch 'master' into performance/[RocksDB][Performance Branch] Make height and branching factor configurable for skiplist implementation

Summary: As title. Especially, HashSkipListRepFactory will be able to specify a relatively small height, to reduce the memory overhead of one skiplist per bucket.

Test Plan: make check and test it on leaf4

Reviewers: dhruba, sdong, kailiu

CC: reconnect.grayhat, leveldb

Differential Revision: https://reviews.facebook.net/D14307/[Only for Performance Branch] A Hacky patch to lazily generate memtable key for prefix-hashed memtables.

Summary:
For prefix mem tables, encoding mem table key may be unnecessary if the prefix doesn't have any key. This patch is a little bit hacky but I want to try out the performance gain of removing this lazy initialization.

In longer term, we might want to revisit the way we abstract mem tables implementations.

Test Plan: make all check

Reviewers: haobo, igor, kailiu

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14265/"
,,Rocksdb,"Fix CompactRange to apply filter to every key

Summary:
When doing CompactRange(), we should first flush the memtable and then calculate max_level_with_files. Also, we want to compact all the levels that have files, including level `max_level_with_files`.

This patch fixed the unit test.

Test Plan: Added a failing unit test and a fix, so it's not failing anymore.

Reviewers: dhruba, haobo, sdong

Reviewed By: haobo

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D14421/"
compression tasks,"Database Management, compression tasks, Thread management",Rocksdb,"[CF] Separate dumping of DBOptions and ColumnFamilyOptions

Summary: When we open a DB, we should dump only DBOptions and then when we create a new column family, we dump ColumnFamilyOptions for each one.

Test Plan: make check, confirm contents of the LOG

Reviewers: dhruba, haobo, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16011/Fix printf format/Merge branch 'master' into columnfamilies/Improve RocksDB ""get"" performance by computing merge result in memtable

Summary:
Added an option (max_successive_merges) that can be used to specify the
maximum number of successive merge operations on a key in the memtable.
This can be used to improve performance of the ""get"" operation. If many
successive merge operations are performed on a key, the performance of ""get""
operations on the key deteriorates, as the value has to be computed for each
""get"" operation by applying all the successive merge operations.

FB Task ID: #3428853

Test Plan:
make all check
db_bench --benchmarks=readrandommergerandom
counter_stress_test

Reviewers: haobo, vamsi, dhruba, sdong

Reviewed By: haobo

CC: zshao

Differential Revision: https://reviews.facebook.net/D14991/[column families] Implement DB::OpenWithColumnFamilies()

Summary:
In addition to implementing OpenWithColumnFamilies, this diff also includes some minor changes:
* Changed all column family names from Slice() to std::string. The performance of column family name handling is not critical, and it's more convenient and cleaner to have names as std::strings
* Implemented ColumnFamilyOptions(const Options&) and DBOptions(const Options&)
* Added ColumnFamilyOptions to VersionSet::ColumnFamilyData. ColumnFamilyOptions are specified on OpenWithColumnFamilies() and CreateColumnFamily()

I will keep the diff in the Phabricator for a day or two and will push to the branch then. Feel free to comment even after the diff has been pushed.

Test Plan: Added a simple unit test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15033/[RocksDB] [Column Family] Interface proposal

Summary:
<This diff is for Column Family branch>

Sharing some of the work I've done so far. This diff compiles and passes the tests.

The biggest change is in options.h - I broke down Options into two parts - DBOptions and ColumnFamilyOptions. DBOptions is DB-specific (env, create_if_missing, block_cache, etc.) and ColumnFamilyOptions is column family-specific (all compaction options, compresion options, etc.). Note that this does not break backwards compatibility at all.

Further, I created DBWithColumnFamily which inherits DB interface and adds new functions with column family support. Clients can transparently switch to DBWithColumnFamily and it will not break their backwards compatibility.
There are few methods worth checking out: ListColumnFamilies(), MultiNewIterator(), MultiGet() and GetSnapshot(). [GetSnapshot() returns the snapshot across all column families for now - I think that's what we agreed on]

Finally, I made small changes to WriteBatch so we are able to atomically insert data across column families.

Please provide feedback.

Test Plan: make check works, the code is backward compatible

Reviewers: dhruba, haobo, sdong, kailiu, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14445/[RocksDB] [Performance Branch] Added dynamic bloom, to be used for memable non-existing key filtering

Summary: as title

Test Plan: dynamic_bloom_test

Reviewers: dhruba, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14385/Add compression options to db_bench

Summary:
This adds 2 options for compression to db_bench:
* universal_compression_size_percent
* compression_level - to set zlib compression level
It also logs compression_size_percent at startup in LOG

Task ID: #

Blame Rev:

Test Plan:
make check, run db_bench

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14439/Clarify CompactionFilter thread safety requirements

Summary: Documenting our discussion

Test Plan: make

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: igor

Differential Revision: https://reviews.facebook.net/D14403/Improve the ""table stats""

Summary:
The primary motivation of the changes is to make it easier to figure out the inside of the tables.

* rename ""table stats"" to ""table properties"" since now we have more than ""integers"" to store in the property block.
* Add filter block size to the basic table properties.
* Whenever a table is built, we'll log the table properties (the sample output is in Test Plan).
* Make an api to expose deleted keys.

Test Plan:
Passed all existing test. and the sample output of table stats:

    ==================================================================
        Basic Properties
    ------------------------------------------------------------------
                  # data blocks: 1
                      # entries: 1

                   raw key size: 9
           raw average key size: 9
                 raw value size: 9
         raw average value size: 0

                data block size: 25
               index block size: 27
              filter block size: 18
         (estimated) table size: 70

                  filter policy: rocksdb.BuiltinBloomFilter
    ==================================================================
        User collected properties: InternalKeyPropertiesCollector
    ------------------------------------------------------------------
                    kDeletedKeys: 1
    ==================================================================

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14187/"
,,Rocksdb,"Expose usage info for the cache

Summary: This diff will help us to figure out the memory usage for the cache part.

Test Plan: added a new memory usage test for cache

Reviewers: haobo, sdong, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14559/"
,,Rocksdb,"[RocksDB] BackupableDB

Summary:
In this diff I present you BackupableDB v1. You can easily use it to backup your DB and it will do incremental snapshots for you.
Let's first describe how you would use BackupableDB. It's inheriting StackableDB interface so you can easily construct it with your DB object -- it will add a method RollTheSnapshot() to the DB object. When you call RollTheSnapshot(), current snapshot of the DB will be stored in the backup dir. To restore, you can just call RestoreDBFromBackup() on a BackupableDB (which is a static method) and it will restore all files from the backup dir. In the next version, it will even support automatic backuping every X minutes.

There are multiple things you can configure:
1. backup_env and db_env can be different, which is awesome because then you can easily backup to HDFS or wherever you feel like.
2. sync - if true, it *guarantees* backup consistency on machine reboot
3. number of snapshots to keep - this will keep last N snapshots around if you want, for some reason, be able to restore from an earlier snapshot. All the backuping is done in incremental fashion - if we already have 00010.sst, we will not copy it again. *IMPORTANT* -- This is based on assumption that 00010.sst never changes - two files named 00010.sst from the same DB will always be exactly the same. Is this true? I always copy manifest, current and log files.
4. You can decide if you want to flush the memtables before you backup, or you're fine with backing up the log files -- either way, you get a complete and consistent view of the database at a time of backup.
5. More things you can find in BackupableDBOptions

Here is the directory structure I use:

   backup_dir/CURRENT_SNAPSHOT - just 4 bytes holding the latest snapshot
               0, 1, 2, ... - files containing serialized version of each snapshot - containing a list of files
               files/*.sst - sst files shared between snapshots - if one snapshot references 00010.sst and another one needs to backup it from the DB, it will just reference the same file
               files/ 0/, 1/, 2/, ... - snapshot directories containing private snapshot files - current, manifest and log files

All the files are ref counted and deleted immediatelly when they get out of scope.

Some other stuff in this diff:
1. Added GetEnv() method to the DB. Discussed with @haobo and we agreed that it seems right thing to do.
2. Fixed StackableDB interface. The way it was set up before, I was not able to implement BackupableDB.

Test Plan:
I have a unittest, but please don't look at this yet. I just hacked it up to help me with debugging. I will write a lot of good tests and update the diff.

Also, `make asan_check`

Reviewers: dhruba, haobo, emayanke

Reviewed By: dhruba

CC: leveldb, haobo

Differential Revision: https://reviews.facebook.net/D14295/"
,,Rocksdb,"Fsync directory after we create a new file

Summary:
@dhruba, I'm not sure where we need to sync the directory. I implemented the function in Env() and added the dir sync just after we close the newly created file in the builder.

Should I also add FsyncDir() to new files that get created by a compaction?

Test Plan: Confirmed that FsyncDir is returning Status::OK()

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D14751/A Simple Plain Table

Summary:
A Simple plain table format. No block structure. When creating the table reader, scanning the full table to create indexes.

Test Plan:Add unit test

Reviewers:haobo,dhruba,kailiu

CC:

Task ID: #

Blame Rev:/make util/env_posix.cc work under mac

Summary: This diff invoves some more complicated issues in the posix environment.

Test Plan: works under mac os. will need to verify dev box.

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14061/"
,,Rocksdb,"[Performance Branch] PlainTable to encode rows with seqID 0, value type using 1 internal byte.

Summary: In PlainTable, use one single byte to represent 8 bytes of internal bytes, if seqID = 0 and it is value type (which should be common for bottom most files). It is to save 7 bytes for uncompressed cases.

Test Plan: make all check

Reviewers: haobo, dhruba, kailiu

Reviewed By: haobo

CC: igor, leveldb

Differential Revision: https://reviews.facebook.net/D15489/"
,,Rocksdb,"Fsync directory after we create a new file

Summary:
@dhruba, I'm not sure where we need to sync the directory. I implemented the function in Env() and added the dir sync just after we close the newly created file in the builder.

Should I also add FsyncDir() to new files that get created by a compaction?

Test Plan: Confirmed that FsyncDir is returning Status::OK()

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D14751/make util/env_posix.cc work under mac

Summary: This diff invoves some more complicated issues in the posix environment.

Test Plan: works under mac os. will need to verify dev box.

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14061/"
,,Rocksdb,"IOError cleanup

Summary: Clean up IOErrors so that it only indicates errors talking to device.

Test Plan: make all check

Reviewers: igor, haobo, dhruba, emayanke

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15831/Avoid malloc in NotFound key status if no message is given.

Summary:
In some places we have NotFound status created with empty message, but it doesn't avoid a malloc. With this patch, the malloc is avoided for that case.

The motivation of it is that I found in db_bench readrandom test when all keys are not existing, about 4% of the total running time is spent on malloc of Status, plus a similar amount of CPU spent on free of them, which is not necessary.

Test Plan: make all check

Reviewers: dhruba, haobo, igor

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14691/"
,,Rocksdb,Initialize sequence number in BatchResult - issue #39/
Memory management,Memory management,Rocksdb,"[CF] Rethinking ColumnFamilyHandle and fix to dropping column families

Summary:
The change to the public behavior:
* When opening a DB or creating new column family client gets a ColumnFamilyHandle.
* As long as column family handle is alive, client can do whatever he wants with it, even drop it
* Dropped column family can still be read from (using the column family handle)
* Added a new call CloseColumnFamily(). Client has to close all column families that he has opened before deleting the DB
* As soon as column family is closed, any calls to DB using that column family handle will fail (also any outstanding calls)

Internally:
* Ref-counting ColumnFamilyData
* New thread-safety for ColumnFamilySet
* Dropped column families are now completely dropped and their memory cleaned-up

Test Plan: added some tests to column_family_test

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16101/[column families] Move memtable and immutable memtable list to column family data

Summary: All memtables and immutable memtables are moved from DBImpl to ColumnFamilyData. For now, they are all referenced from default column family in DBImpl. It shouldn't be hard to get them from custom column family.

Test Plan: make check

Reviewers: dhruba, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15459/Merge branch 'master' into columnfamilies

Conflicts:
	db/db_impl.cc
	db/db_impl.h
	db/db_impl_readonly.h
	db/db_test.cc
	include/rocksdb/db.h
	include/utilities/stackable_db.h/[column families] Support to create and drop column families

Summary:
This diff provides basic implementations of CreateColumnFamily(), DropColumnFamily() and ListColumnFamilies(). It builds on top of https://reviews.facebook.net/D14733

It also includes a bug fix for DBImplReadOnly, where Get implementation would be redirected to DBImpl instead of DBImplReadOnly.

Test Plan: Added unit test

Reviewers: dhruba, haobo, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15021/[RocksDB] [Column Family] Interface proposal

Summary:
<This diff is for Column Family branch>

Sharing some of the work I've done so far. This diff compiles and passes the tests.

The biggest change is in options.h - I broke down Options into two parts - DBOptions and ColumnFamilyOptions. DBOptions is DB-specific (env, create_if_missing, block_cache, etc.) and ColumnFamilyOptions is column family-specific (all compaction options, compresion options, etc.). Note that this does not break backwards compatibility at all.

Further, I created DBWithColumnFamily which inherits DB interface and adds new functions with column family support. Clients can transparently switch to DBWithColumnFamily and it will not break their backwards compatibility.
There are few methods worth checking out: ListColumnFamilies(), MultiNewIterator(), MultiGet() and GetSnapshot(). [GetSnapshot() returns the snapshot across all column families for now - I think that's what we agreed on]

Finally, I made small changes to WriteBatch so we are able to atomically insert data across column families.

Please provide feedback.

Test Plan: make check works, the code is backward compatible

Reviewers: dhruba, haobo, sdong, kailiu, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14445/DB::GetOptions()

Summary: We need access to options for BackupableDB

Test Plan: make check

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, reconnect.grayhat

Differential Revision: https://reviews.facebook.net/D14331/"
,,Rocksdb,"Add a call DisownData() to Cache, which should speed up shutdown

Summary: On a shutdown, freeing memory takes a long time. If we're shutting down, we don't really care about memory leaks. I added a call to Cache that will avoid freeing all objects in cache.

Test Plan:
I created a script to test the speedup and demonstrate how to use the call: https://phabricator.fb.com/P3864368

Clean shutdown took 7.2 seconds, while fast and dirty one took 6.3 seconds. Unfortunately, the speedup is not that big, but should be bigger with bigger block_cache. I have set up the capacity to 80GB, but the script filled up only ~7GB.

Reviewers: dhruba, haobo, MarkCallaghan, xjin

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15069/"
,,Rocksdb,"WriteBatch to provide a way for user to query data size directly and only return constant reference of data in Data()

Summary:
WriteBatch::Data() now is easily to be misuse by users. Also, there is no cheap way for user of WriteBatch to know the data size accumulated. This patch fix the problem by:
(1) return a constant reference to Data() so it's obvious to caller what it means.
(2) add a function to return data size directly

Test Plan: make all check

Reviewers: haobo, igor, kailiu

Reviewed By: kailiu

CC: zshao, leveldb

Differential Revision: https://reviews.facebook.net/D15123/Add column family information to WAL

Summary:
I have added three new value types:
* kTypeColumnFamilyDeletion
* kTypeColumnFamilyValue
* kTypeColumnFamilyMerge
which include column family Varint32 before the data (value, deletion and merge). These values are used only in WAL (not in memtables yet).

This endeavour required changing some WriteBatch internals.

Test Plan: Added a unittest

Reviewers: dhruba, haobo, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15045/[RocksDB] [Column Family] Interface proposal

Summary:
<This diff is for Column Family branch>

Sharing some of the work I've done so far. This diff compiles and passes the tests.

The biggest change is in options.h - I broke down Options into two parts - DBOptions and ColumnFamilyOptions. DBOptions is DB-specific (env, create_if_missing, block_cache, etc.) and ColumnFamilyOptions is column family-specific (all compaction options, compresion options, etc.). Note that this does not break backwards compatibility at all.

Further, I created DBWithColumnFamily which inherits DB interface and adds new functions with column family support. Clients can transparently switch to DBWithColumnFamily and it will not break their backwards compatibility.
There are few methods worth checking out: ListColumnFamilies(), MultiNewIterator(), MultiGet() and GetSnapshot(). [GetSnapshot() returns the snapshot across all column families for now - I think that's what we agreed on]

Finally, I made small changes to WriteBatch so we are able to atomically insert data across column families.

Please provide feedback.

Test Plan: make check works, the code is backward compatible

Reviewers: dhruba, haobo, sdong, kailiu, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14445/"
,,Rocksdb,"Merge branch 'master' into columnfamilies


Summary: By removing some includes form options.h and reply on forward declaration, we can more easily reason the dependencies.

Test Plan: make all check

Reviewers: kailiu, haobo, igor, dhruba

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15411/Add read/modify/write functionality to Put() api

Summary: The application can set a callback function, which is applied on the previous value. And calculates the new value. This new value can be set, either inplace, if the previous value existed in memtable, and new value is smaller than previous value. Otherwise the new value is added normally.

Test Plan: fbmake. Added unit tests. All unit tests pass.

Reviewers: dhruba, haobo

Reviewed By: haobo

CC: sdong, kailiu, xinyaohu, sumeet, leveldb

Differential Revision: https://reviews.facebook.net/D14745/Separate the aligned and unaligned memory allocation

Summary: Use two vectors for different types of memory allocation.

Test Plan: run all unit tests.

Reviewers: haobo, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15027/[RocksDB] [Column Family] Interface proposal

Summary:
<This diff is for Column Family branch>

Sharing some of the work I've done so far. This diff compiles and passes the tests.

The biggest change is in options.h - I broke down Options into two parts - DBOptions and ColumnFamilyOptions. DBOptions is DB-specific (env, create_if_missing, block_cache, etc.) and ColumnFamilyOptions is column family-specific (all compaction options, compresion options, etc.). Note that this does not break backwards compatibility at all.

Further, I created DBWithColumnFamily which inherits DB interface and adds new functions with column family support. Clients can transparently switch to DBWithColumnFamily and it will not break their backwards compatibility.
There are few methods worth checking out: ListColumnFamilies(), MultiNewIterator(), MultiGet() and GetSnapshot(). [GetSnapshot() returns the snapshot across all column families for now - I think that's what we agreed on]

Finally, I made small changes to WriteBatch so we are able to atomically insert data across column families.

Please provide feedback.

Test Plan: make check works, the code is backward compatible

Reviewers: dhruba, haobo, sdong, kailiu, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14445/Improve the ""table stats""

Summary:
The primary motivation of the changes is to make it easier to figure out the inside of the tables.

* rename ""table stats"" to ""table properties"" since now we have more than ""integers"" to store in the property block.
* Add filter block size to the basic table properties.
* Whenever a table is built, we'll log the table properties (the sample output is in Test Plan).
* Make an api to expose deleted keys.

Test Plan:
Passed all existing test. and the sample output of table stats:

    ==================================================================
        Basic Properties
    ------------------------------------------------------------------
                  # data blocks: 1
                      # entries: 1

                   raw key size: 9
           raw average key size: 9
                 raw value size: 9
         raw average value size: 0

                data block size: 25
               index block size: 27
              filter block size: 18
         (estimated) table size: 70

                  filter policy: rocksdb.BuiltinBloomFilter
    ==================================================================
        User collected properties: InternalKeyPropertiesCollector
    ------------------------------------------------------------------
                    kDeletedKeys: 1
    ==================================================================

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14187/"
,Thread management,Rocksdb,C API: add rocksdb_env_set_high_priority_background_threads/C bindings: add a bunch of the newer options/Rename leveldb to rocksdb in C api/
Memory management,Memory management,Rocksdb,"Remove the unnecessary use of shared_ptr

Summary:
shared_ptr is slower than unique_ptr (which literally comes with no performance cost compare with raw pointers).
In memtable and memtable rep, we use shared_ptr when we'd actually should use unique_ptr.

According to igor's previous work, we are likely to make quite some performance gain from this diff.

Test Plan: make check

Reviewers: dhruba, igor, sdong, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15213/[Performance Branch] A Hashed Linked List Based Mem Table

Summary:
Implement a mem table, in which keys are hashed based on prefixes. In each bucket, entries are organized in a sorted linked list. It has the same thread safety guarantee as skip list.

The motivation is to optimize memory usage for the case that prefix hashing is primary way of seeking to the entry. Compared to hash skip list implementation, this implementation is more memory efficient, but inside each bucket, search is always linear. The target scenario is that there are only very limited number of records in each hash bucket.

Test Plan: Add a test case in db_test

Reviewers: haobo, kailiu, dhruba

Reviewed By: haobo

CC: igor, nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D14979/[RocksDB][Performance Branch] Make height and branching factor configurable for skiplist implementation

Summary: As title. Especially, HashSkipListRepFactory will be able to specify a relatively small height, to reduce the memory overhead of one skiplist per bucket.

Test Plan: make check and test it on leaf4

Reviewers: dhruba, sdong, kailiu

CC: reconnect.grayhat, leveldb

Differential Revision: https://reviews.facebook.net/D14307/"
,Thread management,Rocksdb,"Add monitoring for universal compaction and add counters for compaction IO

Summary:
Adds these counters
{ WAL_FILE_SYNCED, ""rocksdb.wal.synced"" }
  number of writes that  a WAL sync
{ WAL_FILE_BYTES, ""rocksdb.wal.bytes"" },
  number of bytes written to the WAL
{ WRITE_DONE_BY_SELF, ""rocksdb.write.self"" },
  number of writes processed by the calling thread
{ WRITE_DONE_BY_OTHER, ""rocksdb.write.other"" },
  number of writes not processed by the calling thread. Instead these were
  processed by the current holder of the write lock
{ WRITE_WITH_WAL, ""rocksdb.write.wal"" },
  number of writes that  WAL logging
{ COMPACT_READ_BYTES, ""rocksdb.compact.read.bytes"" },
  number of bytes read during compaction
{ COMPACT_WRITE_BYTES, ""rocksdb.compact.write.bytes"" },
  number of bytes written during compaction

Per-interval stats output was updated with WAL stats and correct stats for universal compaction
including a correct value for write-amplification. It now looks like:
                               Compactions
Level  Files Size(MB) Score Time(sec)  Read(MB) Write(MB)    Rn(MB)  Rnp1(MB)  Wnew(MB) RW-Amplify Read(MB/s) Write(MB/s)      Rn     Rnp1     Wnp1     NewW    Count  Ln-stall Stall-cnt
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0        7      464  46.4       281      3411      3875      3411         0      3875        2.1      12.1        13.8      621        0      240      240      628       0.0         0
Uptime(secs): 310.8 total, 2.0 interval
Writes cumulative: 9999999 total, 9999999 batches, 1.0 per batch, 1.22 ingest GB
WAL cumulative: 9999999 WAL writes, 9999999 WAL syncs, 1.00 writes per sync, 1.22 GB written
Compaction IO cumulative (GB): 1.22 new, 3.33 read, 3.78 write, 7.12 read+write
Compaction IO cumulative (MB/sec): 4.0 new, 11.0 read, 12.5 write, 23.4 read+write
Amplification cumulative: 4.1 write, 6.8 compaction
Writes interval: 100000 total, 100000 batches, 1.0 per batch, 12.5 ingest MB
WAL interval: 100000 WAL writes, 100000 WAL syncs, 1.00 writes per sync, 0.01 MB written
Compaction IO interval (MB): 12.49 new, 14.98 read, 21.50 write, 36.48 read+write
Compaction IO interval (MB/sec): 6.4 new, 7.6 read, 11.0 write, 18.6 read+write
Amplification interval: 101.7 write, 102.9 compaction
Stalls(secs): 142.924 level0_slowdown, 0.000 level0_numfiles, 0.805 memtable_compaction, 0.000 leveln_slowdown
Stalls(count): 132461 level0_slowdown, 0 level0_numfiles, 3 memtable_compaction, 0 leveln_slowdown

Task ID: #3329644, #3301695

Blame Rev:

Test Plan:
Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14583/Refine the statistics/[rocksdb] statistics counters for memtable hits and misses

Summary:
added counters
rocksdb.memtable.hit - for memtable hit
rocksdb.memtable.miss - for memtable miss

Test Plan: db_bench tests

Reviewers: igor, dhruba, haobo

Reviewed By: dhruba

Differential Revision: https://reviews.facebook.net/D14433/"
,,Rocksdb,"First phase API clean up

Summary:
Addressed all the issues in https://reviews.facebook.net/D15447.
Now most table-related modules are hidden from user land.

Test Plan: make check

Reviewers: sdong, haobo, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15525/Improve the ""table stats""

Summary:
The primary motivation of the changes is to make it easier to figure out the inside of the tables.

* rename ""table stats"" to ""table properties"" since now we have more than ""integers"" to store in the property block.
* Add filter block size to the basic table properties.
* Whenever a table is built, we'll log the table properties (the sample output is in Test Plan).
* Make an api to expose deleted keys.

Test Plan:
Passed all existing test. and the sample output of table stats:

    ==================================================================
        Basic Properties
    ------------------------------------------------------------------
                  # data blocks: 1
                      # entries: 1

                   raw key size: 9
           raw average key size: 9
                 raw value size: 9
         raw average value size: 0

                data block size: 25
               index block size: 27
              filter block size: 18
         (estimated) table size: 70

                  filter policy: rocksdb.BuiltinBloomFilter
    ==================================================================
        User collected properties: InternalKeyPropertiesCollector
    ------------------------------------------------------------------
                    kDeletedKeys: 1
    ==================================================================

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14187/"
,,Rocksdb,"Merge branch 'master' into columnfamilies

Conflicts:
	db/db_impl.cc
	db/db_impl.h
	db/db_impl_readonly.h
	db/db_test.cc
	include/rocksdb/db.h
	include/utilities/stackable_db.h/[RocksDB] [Column Family] Interface proposal

Summary:
<This diff is for Column Family branch>

Sharing some of the work I've done so far. This diff compiles and passes the tests.

The biggest change is in options.h - I broke down Options into two parts - DBOptions and ColumnFamilyOptions. DBOptions is DB-specific (env, create_if_missing, block_cache, etc.) and ColumnFamilyOptions is column family-specific (all compaction options, compresion options, etc.). Note that this does not break backwards compatibility at all.

Further, I created DBWithColumnFamily which inherits DB interface and adds new functions with column family support. Clients can transparently switch to DBWithColumnFamily and it will not break their backwards compatibility.
There are few methods worth checking out: ListColumnFamilies(), MultiNewIterator(), MultiGet() and GetSnapshot(). [GetSnapshot() returns the snapshot across all column families for now - I think that's what we agreed on]

Finally, I made small changes to WriteBatch so we are able to atomically insert data across column families.

Please provide feedback.

Test Plan: make check works, the code is backward compatible

Reviewers: dhruba, haobo, sdong, kailiu, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14445/DB::GetOptions()

Summary: We need access to options for BackupableDB

Test Plan: make check

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, reconnect.grayhat

Differential Revision: https://reviews.facebook.net/D14331/[RocksDB] Interface changes required for BackupableDB

Summary: This is part of https://reviews.facebook.net/D14295 -- smaller diff that is easier to review

Test Plan: make asan_check

Reviewers: dhruba, haobo, emayanke

Reviewed By: emayanke

CC: leveldb, kailiu, reconnect.grayhat

Differential Revision: https://reviews.facebook.net/D14301/"
,,Rocksdb,"[RocksDB] [Column Family] Interface proposal

Summary:
<This diff is for Column Family branch>

Sharing some of the work I've done so far. This diff compiles and passes the tests.

The biggest change is in options.h - I broke down Options into two parts - DBOptions and ColumnFamilyOptions. DBOptions is DB-specific (env, create_if_missing, block_cache, etc.) and ColumnFamilyOptions is column family-specific (all compaction options, compresion options, etc.). Note that this does not break backwards compatibility at all.

Further, I created DBWithColumnFamily which inherits DB interface and adds new functions with column family support. Clients can transparently switch to DBWithColumnFamily and it will not break their backwards compatibility.
There are few methods worth checking out: ListColumnFamilies(), MultiNewIterator(), MultiGet() and GetSnapshot(). [GetSnapshot() returns the snapshot across all column families for now - I think that's what we agreed on]

Finally, I made small changes to WriteBatch so we are able to atomically insert data across column families.

Please provide feedback.

Test Plan: make check works, the code is backward compatible

Reviewers: dhruba, haobo, sdong, kailiu, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14445/Make DBWithTTL more like StackableDB

Summary: Now DBWithTTL takes DB* and can behave more like StackableDB. This saves us a lot of duplicate work by defining interfaces

Test Plan: ttl_test with ASAN - OK

Reviewers: emayanke

Reviewed By: emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14481/"
,,Rocksdb,"[Performance Branch] PlainTable to encode rows with seqID 0, value type using 1 internal byte.

Summary: In PlainTable, use one single byte to represent 8 bytes of internal bytes, if seqID = 0 and it is value type (which should be common for bottom most files). It is to save 7 bytes for uncompressed cases.

Test Plan: make all check

Reviewers: haobo, dhruba, kailiu

Reviewed By: haobo

CC: igor, leveldb

Differential Revision: https://reviews.facebook.net/D15489/Get rid of some shared_ptrs

Summary:
I went through all remaining shared_ptrs and removed the ones that I found not-necessary. Only GenerateCachePrefix() is called fairly often, so don't expect much perf wins.

The ones that are left are accessed infrequently and I think we're fine with keeping them.

Test Plan: make asan_check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14427/Improve the ""table stats""

Summary:
The primary motivation of the changes is to make it easier to figure out the inside of the tables.

* rename ""table stats"" to ""table properties"" since now we have more than ""integers"" to store in the property block.
* Add filter block size to the basic table properties.
* Whenever a table is built, we'll log the table properties (the sample output is in Test Plan).
* Make an api to expose deleted keys.

Test Plan:
Passed all existing test. and the sample output of table stats:

    ==================================================================
        Basic Properties
    ------------------------------------------------------------------
                  # data blocks: 1
                      # entries: 1

                   raw key size: 9
           raw average key size: 9
                 raw value size: 9
         raw average value size: 0

                data block size: 25
               index block size: 27
              filter block size: 18
         (estimated) table size: 70

                  filter policy: rocksdb.BuiltinBloomFilter
    ==================================================================
        User collected properties: InternalKeyPropertiesCollector
    ------------------------------------------------------------------
                    kDeletedKeys: 1
    ==================================================================

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14187/"
,,Rocksdb,"[Performance Branch] PlainTable to encode rows with seqID 0, value type using 1 internal byte.

Summary: In PlainTable, use one single byte to represent 8 bytes of internal bytes, if seqID = 0 and it is value type (which should be common for bottom most files). It is to save 7 bytes for uncompressed cases.

Test Plan: make all check

Reviewers: haobo, dhruba, kailiu

Reviewed By: haobo

CC: igor, leveldb

Differential Revision: https://reviews.facebook.net/D15489/First phase API clean up

Summary:
Addressed all the issues in https://reviews.facebook.net/D15447.
Now most table-related modules are hidden from user land.

Test Plan: make check

Reviewers: sdong, haobo, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15525/[RocksDB] [Performance Branch] Some Changes to PlainTable format

Summary:
Some changes to PlainTable format:
(1) support variable key length
(2) use user defined slice transformer to extract prefixes
(3) Run some test cases against PlainTable in db_test and table_test

Test Plan: test db_test

Reviewers: haobo, kailiu

CC: dhruba, igor, leveldb, nkg-

Differential Revision: https://reviews.facebook.net/D14457/A Simple Plain Table

Summary:
A Simple plain table format. No block structure. When creating the table reader, scanning the full table to create indexes.

Test Plan:Add unit test

Reviewers:haobo,dhruba,kailiu

CC:

Task ID: #

Blame Rev:/Add an option to table_reader_bench to access the table from DB And Iterating non-existing prefix case.

Summary: This patch adds an option to table_reader_bench that queries run against DB level (which has one table). It is useful if user wants to see the extra costs DB level introduces.

Test Plan: Run the benchmark with and without the new parameter

Reviewers: haobo, dhruba, kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D13863/"
,,Rocksdb,"Support for LZ4 compression./[Performance Branch] PlainTable to encode rows with seqID 0, value type using 1 internal byte.

Summary: In PlainTable, use one single byte to represent 8 bytes of internal bytes, if seqID = 0 and it is value type (which should be common for bottom most files). It is to save 7 bytes for uncompressed cases.

Test Plan: make all check

Reviewers: haobo, dhruba, kailiu

Reviewed By: haobo

CC: igor, leveldb

Differential Revision: https://reviews.facebook.net/D15489/Add the property block for the plain table

Summary:
This is the last diff that adds the property block to plain table.
The format resembles that of the block-based table: https://github.com/facebook/rocksdb/wiki/Rocksdb-table-format

  [data block]
  [meta block 1: stats block]
  [meta block 2: future extended block]
  ...
  [meta block K: future extended block]  (we may add more meta blocks in the future)
  [metaindex block]
  [index block: we only have the placeholder here, we can add persistent index block in the future]
  [Footer: contains magic number, handle to metaindex block and index block]
  <end_of_file>

Test Plan: extended existing property block test.

Reviewers: haobo, sdong, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14523/Extract metaindex block from block-based table

Summary: This change will allow other table to reuse the code for meta blocks.

Test Plan: all existing unit tests passed

Reviewers: dhruba, haobo, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14475/Improve the readability of the TableProperties::ToString()/Move flush_block_policy from Options to TableFactory

Summary:
Previously we introduce a `flush_block_policy_factory` in Options, however, that options is strongly releated to Table based tables.
It will make more sense to move it to block based table's own factory class.

Test Plan: make check to pass existing tests

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14211/Improve the ""table stats""

Summary:
The primary motivation of the changes is to make it easier to figure out the inside of the tables.

* rename ""table stats"" to ""table properties"" since now we have more than ""integers"" to store in the property block.
* Add filter block size to the basic table properties.
* Whenever a table is built, we'll log the table properties (the sample output is in Test Plan).
* Make an api to expose deleted keys.

Test Plan:
Passed all existing test. and the sample output of table stats:

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14187/"
,,Rocksdb,"Allow users to profile a query and see bottleneck of the query

Summary:
Provide a framework to profile a query in detail to figure out latency bottleneck. Currently, in Get(), Put() and iterators, 2-3 simple timing is used. We can easily add more profile counters to the framework later.

Test Plan: Enable this profiling in seveal existing tests.

Reviewers: haobo, dhruba, kailiu, emayanke, vamsi, igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14001

Conflicts:
	table/merger.cc/MergingIterator.Seek() to lazily initialize MinHeap

Summary:
For the use cases that prefix filtering is enabled, initializing heaps when doing MergingIterator.Seek() might introduce non-negligible costs. This patch makes it lazily done.

Test Plan: make all check

Reviewers: haobo,dhruba,kailiu

CC:

Task ID: #

Blame Rev:/"
,,Rocksdb,"Make table properties shareable

Summary:
We are going to expose properties of all tables to end users through ""some"" db interface.
However, current design doesn't naturally fit for this need, which is because:

1. If a table presents in table cache, we cannot simply return the reference to its table properties, because the table may be destroy after compaction (and we don't want to hold the ref of the version).
2. Copy table properties is OK, but it's slow.

Thus in this diff, I change the table reader's interface to return a shared pointer (for const table properties), instead a const refernce.

Test Plan: `make check` passed

Reviewers: haobo, sdong, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15999/[Performance Branch] PlainTable to encode rows with seqID 0, value type using 1 internal byte.

Summary: In PlainTable, use one single byte to represent 8 bytes of internal bytes, if seqID = 0 and it is value type (which should be common for bottom most files). It is to save 7 bytes for uncompressed cases.

Test Plan: make all check

Reviewers: haobo, dhruba, kailiu

Reviewed By: haobo

CC: igor, leveldb

Differential Revision: https://reviews.facebook.net/D15489/Temporarily disable caching index/filter blocks

Summary:
Mixing index/filter blocks with data blocks resulted in some known
issues.  To make sure in next release our users won't be affected,
we added a new option in BlockBasedTableFactory::TableOption to
conceal this functionality for now.

This patch also introduced a BlockBasedTableReader::OpenOptions,
which avoids the ""infinite"" growth of parameters in
BlockBasedTableReader::Open().

Test Plan: make check

Reviewers: haobo, sdong, igor, dhruba

Reviewed By: igor

CC: leveldb, tnovak

Differential Revision: https://reviews.facebook.net/D15327/Add the property block for the plain table

Summary:
This is the last diff that adds the property block to plain table.
The format resembles that of the block-based table: https://github.com/facebook/rocksdb/wiki/Rocksdb-table-format

  [data block]
  [meta block 1: stats block]
  [meta block 2: future extended block]
  ...
  [meta block K: future extended block]  (we may add more meta blocks in the future)
  [metaindex block]
  [index block: we only have the placeholder here, we can add persistent index block in the future]
  [Footer: contains magic number, handle to metaindex block and index block]
  <end_of_file>

Test Plan: extended existing property block test.

Reviewers: haobo, sdong, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14523/Get rid of some shared_ptrs

Summary:
I went through all remaining shared_ptrs and removed the ones that I found not-necessary. Only GenerateCachePrefix() is called fairly often, so don't expect much perf wins.

The ones that are left are accessed infrequently and I think we're fine with keeping them.

Test Plan: make asan_check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14427/[RocksDB] Use raw pointer instead of shared pointer when passing Statistics object internally

Summary: liveness of the statistics object is already ensured by the shared pointer in DB options. There's no reason to pass again shared pointer among internal functions. Raw pointer is sufficient and efficient.

Test Plan: make check

Reviewers: dhruba, MarkCallaghan, igor

Reviewed By: dhruba

CC: leveldb, reconnect.grayhat

Differential Revision: https://reviews.facebook.net/D14289/Improve the ""table stats""

Summary:
The primary motivation of the changes is to make it easier to figure out the inside of the tables.

* rename ""table stats"" to ""table properties"" since now we have more than ""integers"" to store in the property block.
* Add filter block size to the basic table properties.
* Whenever a table is built, we'll log the table properties (the sample output is in Test Plan).
* Make an api to expose deleted keys.

Test Plan:
Passed all existing test. and the sample output of table stats:

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14187/Fix bloom filters

Summary: https://reviews.facebook.net/D13167 broke bloom filters. If filter is not in cache, we want to return true (safe thing). Am I right?

Test Plan: when benchmarking https://reviews.facebook.net/D14031 I got different results when using bloom filters vs. when not using them. This fixed the issue. I will also be putting this change to the other diff, but that one will probably be in review for longer time.

Reviewers: kailiu, dhruba, haobo

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14085/"
,,Rocksdb,"Move flush_block_policy from Options to TableFactory

Summary:
Previously we introduce a `flush_block_policy_factory` in Options, however, that options is strongly releated to Table based tables.
It will make more sense to move it to block based table's own factory class.

Test Plan: make check to pass existing tests

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14211/"
Data Conversion,"Data conversion, database management, compression tasks",Rocksdb,"Support for LZ4 compression./Make table properties shareable

Summary:
We are going to expose properties of all tables to end users through ""some"" db interface.
However, current design doesn't naturally fit for this need, which is because:

1. If a table presents in table cache, we cannot simply return the reference to its table properties, because the table may be destroy after compaction (and we don't want to hold the ref of the version).
2. Copy table properties is OK, but it's slow.

Thus in this diff, I change the table reader's interface to return a shared pointer (for const table properties), instead a const refernce.

Test Plan: `make check` passed

Reviewers: haobo, sdong, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15999/Add support for plain table format to sst_dump.

Summary:
This diff enables the command line tool `sst_dump` to work for sst files
under plain table format.  Changes include:
  * In tools/sst_dump.cc:
    - add support for plain table format
    - display prefix_extractor information when --show_properties is on
  * In table/format.cc
    - Now the table magic number of a Footer can be later initialized
      via ReadFooterFromFile().
  * In table/meta_bocks:
    - add function ReadTableMagicNumber() that reads the magic number of
      the specified file.

Minor fixes:
 - remove a duplicate #include in table/table_test.cc
 - fix a commentary typo in include/rocksdb/memtablerep.h
 - fix lint errors.

Test Plan:
Runs sst_dump with both block-based and plain-table format files with
different arguments, specifically those with --show-properties and --from.

* sample output:
  https://reviews.facebook.net/P261

Reviewers: kailiu, sdong, xjin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15903/Generalize footer reading from file

Summary:

Generalizing this process will help us to re-use the code for plain table

Test Plan:

ran ./table_test/"
,,Rocksdb,"First phase API clean up

Summary:
Addressed all the issues in https://reviews.facebook.net/D15447.
Now most table-related modules are hidden from user land.

Test Plan: make check

Reviewers: sdong, haobo, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15525/Temporarily disable caching index/filter blocks

Summary:
Mixing index/filter blocks with data blocks resulted in some known
issues.  To make sure in next release our users won't be affected,
we added a new option in BlockBasedTableFactory::TableOption to
conceal this functionality for now.

This patch also introduced a BlockBasedTableReader::OpenOptions,
which avoids the ""infinite"" growth of parameters in
BlockBasedTableReader::Open().

Test Plan: make check

Reviewers: haobo, sdong, igor, dhruba

Reviewed By: igor

CC: leveldb, tnovak

Differential Revision: https://reviews.facebook.net/D15327/Add TableOptions for BlockBasedTableFactory

We are having more and more options to specify for this table so it makes sense to have a TableOptions for future extension./Move flush_block_policy from Options to TableFactory

Summary:
Previously we introduce a `flush_block_policy_factory` in Options, however, that options is strongly releated to Table based tables.
It will make more sense to move it to block based table's own factory class.

Test Plan: make check to pass existing tests

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14211/"
,,Rocksdb,"Add support for plain table format to sst_dump.

Summary:
This diff enables the command line tool `sst_dump` to work for sst files
under plain table format.  Changes include:
  * In tools/sst_dump.cc:
    - add support for plain table format
    - display prefix_extractor information when --show_properties is on
  * In table/format.cc
    - Now the table magic number of a Footer can be later initialized
      via ReadFooterFromFile().
  * In table/meta_bocks:
    - add function ReadTableMagicNumber() that reads the magic number of
      the specified file.

Minor fixes:
 - remove a duplicate #include in table/table_test.cc
 - fix a commentary typo in include/rocksdb/memtablerep.h
 - fix lint errors.

Test Plan:
Runs sst_dump with both block-based and plain-table format files with
different arguments, specifically those with --show-properties and --from.

* sample output:
  https://reviews.facebook.net/P261

Reviewers: kailiu, sdong, xjin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15903/Introducing the concept of NULL block handle/Parameterize table magic number

Summary:

As we are having different types of tables and they all might share the same structure in block-based table:

[metaindex block]
[index block]
[Footer]

To be able to identify differnt types of tables, we need to parameterize the ""magic number"" in the `Footer`.

Test Plan:

make check/"
,,Rocksdb,"[Performance Branch] PlainTable to encode rows with seqID 0, value type using 1 internal byte.

Summary: In PlainTable, use one single byte to represent 8 bytes of internal bytes, if seqID = 0 and it is value type (which should be common for bottom most files). It is to save 7 bytes for uncompressed cases.

Test Plan: make all check

Reviewers: haobo, dhruba, kailiu

Reviewed By: haobo

CC: igor, leveldb

Differential Revision: https://reviews.facebook.net/D15489/First phase API clean up

Summary:
Addressed all the issues in https://reviews.facebook.net/D15447.
Now most table-related modules are hidden from user land.

Test Plan: make check

Reviewers: sdong, haobo, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15525/Add TableOptions for BlockBasedTableFactory

We are having more and more options to specify for this table so it makes sense to have a TableOptions for future extension./Move flush_block_policy from Options to TableFactory

Summary:
Previously we introduce a `flush_block_policy_factory` in Options, however, that options is strongly releated to Table based tables.
It will make more sense to move it to block based table's own factory class.

Test Plan: make check to pass existing tests

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14211/"
Database Management,"DataBase Management, Compression tasks",Rocksdb,"Support prefix seek in UserCollectedProperties

Summary: We'll need the prefix seek support for property aggregation.

Test Plan: make all check

Reviewers: haobo, sdong, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15963/Support for LZ4 compression./Make table properties shareable

Summary:
We are going to expose properties of all tables to end users through ""some"" db interface.
However, current design doesn't naturally fit for this need, which is because:

1. If a table presents in table cache, we cannot simply return the reference to its table properties, because the table may be destroy after compaction (and we don't want to hold the ref of the version).
2. Copy table properties is OK, but it's slow.

Thus in this diff, I change the table reader's interface to return a shared pointer (for const table properties), instead a const refernce.

Test Plan: `make check` passed

Reviewers: haobo, sdong, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15999/Add support for plain table format to sst_dump.

Summary:
This diff enables the command line tool `sst_dump` to work for sst files
under plain table format.  Changes include:
  * In tools/sst_dump.cc:
    - add support for plain table format
    - display prefix_extractor information when --show_properties is on
  * In table/format.cc
    - Now the table magic number of a Footer can be later initialized
      via ReadFooterFromFile().
  * In table/meta_bocks:
    - add function ReadTableMagicNumber() that reads the magic number of
      the specified file.

Minor fixes:
 - remove a duplicate #include in table/table_test.cc
 - fix a commentary typo in include/rocksdb/memtablerep.h
 - fix lint errors.

Test Plan:
Runs sst_dump with both block-based and plain-table format files with
different arguments, specifically those with --show-properties and --from.

* sample output:
  https://reviews.facebook.net/P261

Reviewers: kailiu, sdong, xjin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15903/[CF] Propagate correct options to WriteBatch::InsertInto

Summary:
WriteBatch can have multiple column families in one batch. Every column family has different options. So we have to add a way for write batch to get options for an arbitrary column family.

This required a bit more acrobatics since lots of interfaces had to be changed.

Test Plan: make check

Reviewers: dhruba, haobo, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15957/[Performance Branch] PlainTable to encode rows with seqID 0, value type using 1 internal byte.

Summary: In PlainTable, use one single byte to represent 8 bytes of internal bytes, if seqID = 0 and it is value type (which should be common for bottom most files). It is to save 7 bytes for uncompressed cases.

Test Plan: make all check

Reviewers: haobo, dhruba, kailiu

Reviewed By: haobo

CC: igor, leveldb

Differential Revision: https://reviews.facebook.net/D15489/First phase API clean up

Summary:
Addressed all the issues in https://reviews.facebook.net/D15447.
Now most table-related modules are hidden from user land.

Test Plan: make check

Reviewers: sdong, haobo, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15525/Merge branch 'master' into performance

Conflicts:
	Makefile
	db/db_impl.cc
	db/db_test.cc
	db/memtable_list.cc
	db/memtable_list.h
	table/block_based_table_reader.cc
	table/table_test.cc
	util/cache.cc
	util/coding.cc/[column families] Move memtable and immutable memtable list to column family data

Summary: All memtables and immutable memtables are moved from DBImpl to ColumnFamilyData. For now, they are all referenced from default column family in DBImpl. It shouldn't be hard to get them from custom column family.

Test Plan: make check

Reviewers: dhruba, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15459/Some refactorings on plain table

Summary:
Plain table has been working well and this is just a nit-picking patch,
which is generated during my coding reading. No real functional changes.
only some changes regarding:

* Improve some comments from the perspective a ""new"" code reader.
* Change some magic number to constant, which can help us to parameterize them
  in the future.
* Did some style, naming, C++ convention changes.
* Fix warnings from new ""arc lint""

Test Plan: make check

Reviewers: sdong, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15429/Re-org the table tests

Summary:
We'll divide the table tests into 3 buckets, plain table test, block-based table test and general table feature test.
This diff does no real change and only does the rename and reorg.

Test Plan: run table_test

Reviewers: sdong, haobo, igor, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15417/Some small refactorings on table_test

Summary:

Just revise some hard-to-read or unnecessarily verbose code.

Test Plan:

make check/Fix one more valgrind error in table_test/Fix the valgrind issues/[RocksDB] [Performance Branch] Some Changes to PlainTable format

Summary:
Some changes to PlainTable format:
(1) support variable key length
(2) use user defined slice transformer to extract prefixes
(3) Run some test cases against PlainTable in db_test and table_test

Test Plan: test db_test

Reviewers: haobo, kailiu

CC: dhruba, igor, leveldb, nkg-

Differential Revision: https://reviews.facebook.net/D14457/Add the property block for the plain table

Summary:
This is the last diff that adds the property block to plain table.
The format resembles that of the block-based table: https://github.com/facebook/rocksdb/wiki/Rocksdb-table-format

  [data block]
  [meta block 1: stats block]
  [meta block 2: future extended block]
  ...
  [meta block K: future extended block]  (we may add more meta blocks in the future)
  [metaindex block]
  [index block: we only have the placeholder here, we can add persistent index block in the future]
  [Footer: contains magic number, handle to metaindex block and index block]
  <end_of_file>

Test Plan: extended existing property block test.

Reviewers: haobo, sdong, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14523/Fix #26 by putting the implementation of CreateDBStatistics() to a cc file/Get rid of some shared_ptrs

Summary:
I went through all remaining shared_ptrs and removed the ones that I found not-necessary. Only GenerateCachePrefix() is called fairly often, so don't expect much perf wins.

The ones that are left are accessed infrequently and I think we're fine with keeping them.

Test Plan: make asan_check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14427/Move flush_block_policy from Options to TableFactory

Summary:
Previously we introduce a `flush_block_policy_factory` in Options, however, that options is strongly releated to Table based tables.
It will make more sense to move it to block based table's own factory class.

Test Plan: make check to pass existing tests

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14211/Improve the ""table stats""

Summary:
The primary motivation of the changes is to make it easier to figure out the inside of the tables.

* rename ""table stats"" to ""table properties"" since now we have more than ""integers"" to store in the property block.
* Add filter block size to the basic table properties.
* Whenever a table is built, we'll log the table properties (the sample output is in Test Plan).
* Make an api to expose deleted keys.

Test Plan:
Passed all existing test. and the sample output of table stats:

    ==================================================================
        Basic Properties
    ------------------------------------------------------------------
                  # data blocks: 1
                      # entries: 1

                   raw key size: 9
           raw average key size: 9
                 raw value size: 9
         raw average value size: 0

                data block size: 25
               index block size: 27
              filter block size: 18
         (estimated) table size: 70

                  filter policy: rocksdb.BuiltinBloomFilter
    ==================================================================
        User collected properties: InternalKeyPropertiesCollector
    ------------------------------------------------------------------
                    kDeletedKeys: 1
    ==================================================================

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14187/"
,,Rocksdb,"IOError cleanup

Summary: Clean up IOErrors so that it only indicates errors talking to device.

Test Plan: make all check

Reviewers: igor, haobo, dhruba, emayanke

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15831/"
Memory management,Memory management,Rocksdb,"Merge branch 'master' into columnfamilies

Conflicts:
	db/db_impl.cc
	db/db_impl.h
	db/memtable_list.cc
	db/memtable_list.h
	db/version_set.cc
	db/version_set.h/[CF] Rethinking ColumnFamilyHandle and fix to dropping column families

Summary:
The change to the public behavior:
* When opening a DB or creating new column family client gets a ColumnFamilyHandle.
* As long as column family handle is alive, client can do whatever he wants with it, even drop it
* Dropped column family can still be read from (using the column family handle)
* Added a new call CloseColumnFamily(). Client has to close all column families that he has opened before deleting the DB
* As soon as column family is closed, any calls to DB using that column family handle will fail (also any outstanding calls)

Internally:
* Ref-counting ColumnFamilyData
* New thread-safety for ColumnFamilySet
* Dropped column families are now completely dropped and their memory cleaned-up

Test Plan: added some tests to column_family_test

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16101/IOError cleanup

Summary: Clean up IOErrors so that it only indicates errors talking to device.

Test Plan: make all check

Reviewers: igor, haobo, dhruba, emayanke

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15831/preload table handle on Recover() when max_open_files == -1

Summary: This covers existing table files before DB open happens and avoids contention on table cache

Test Plan: db_test

Reviewers: haobo, sdong, igor, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16089/[CF] OpenWithColumnFamilies -> Open

Summary: By discussion with @dhruba, overloading Open makes more sense

Test Plan: compiles!

Reviewers: dhruba

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D16017/[CF] Separate dumping of DBOptions and ColumnFamilyOptions

Summary: When we open a DB, we should dump only DBOptions and then when we create a new column family, we dump ColumnFamilyOptions for each one.

Test Plan: make check, confirm contents of the LOG

Reviewers: dhruba, haobo, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16011/Merge branch 'master' into columnfamilies

Conflicts:
	HISTORY.md
	db/db_impl.cc
	db/db_impl.h
	db/db_iter.cc
	db/db_test.cc
	db/dbformat.h
	db/memtable.cc
	db/memtable_list.cc
	db/memtable_list.h
	db/table_cache.cc
	db/table_cache.h
	db/version_edit.h
	db/version_set.cc
	db/version_set.h
	db/write_batch.cc
	db/write_batch_test.cc
	include/rocksdb/options.h
	util/options.cc/Flushes should always go to HIGH priority thread pool

Summary:
This is not column-family related diff. It is in columnfamily branch because the change is significant and we want to push it with next major release (3.0).

It removes the leveldb notion of one thread pool and expands it to two thread pools by default (HIGH and LOW). Flush process is removed from compaction process and all flush threads are executed on HIGH thread pool, since we don't want long-running compactions to influence flush latency.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15987/[CF] Thread-safety guarantees for ColumnFamilySet

Summary: Revised thread-safety guarantees and implemented a way to spinlock the object.

Test Plan: make check

Reviewers: dhruba, haobo, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15975/[CF] Propagate correct options to WriteBatch::InsertInto

Summary:
WriteBatch can have multiple column families in one batch. Every column family has different options. So we have to add a way for write batch to get options for an arbitrary column family.

This required a bit more acrobatics since lots of interfaces had to be changed.

Test Plan: make check

Reviewers: dhruba, haobo, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15957/[CF] Options -> DBOptions

Summary: Replaced most of occurrences of Options with more specific DBOptions. This brings us very close to supporting different configuration options for each column family.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15933/[CF] Move InternalStats to ColumnFamilyData

Summary: InternalStats is a messy thing, keeping both DB data and column family data. However, it's better off living in ColumnFamilyData than in DBImpl. For now, at least.

Test Plan: make check

Reviewers: dhruba, kailiu, haobo, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15879/[CF] Split SanitizeOptions into two

Summary:
There are three SanitizeOption-s now : one for DBOptions, one for ColumnFamilyOptions and one for Options (which just calls the other two)

I have also reshuffled some options -- table_cache options and info_log should live in DBOptions, for example.

Test Plan: make check doesn't complain

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15873/Get rid of DBImpl::user_comparator()

Summary: user_comparator() is a Column Family property, not DBImpl

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15855/[column families] Iterator and MultiGet

Summary: Support for different column families in Iterator and MultiGet code path.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15849/Merge branch 'master' into columnfamilies

Conflicts:
	db/db_impl.cc
	db/db_impl.h
	db/db_impl_readonly.cc/use super_version in NewIterator() and MultiGet() function

Summary:
Use super_version insider NewIterator to avoid Ref() each component
separately under mutex
The new added bench shows NewIterator QPS increases from 515K to 719K
No meaningful improvement for multiget I guess due to its relatively small
cost comparing to 90 keys fetch in the test.

Test Plan: unit test and db_bench

Reviewers: igor, sdong

Reviewed By: igor

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D15609/VersionSet cleanup

Summary:
Removed icmp_ from VersionSet (since it's per-column-family, not per-DB-instance)
Unfriended VersionSet and ColumnFamilyData (yay!)
Removed VersionSet::NumberLevels()
Cleaned up DBImpl

Test Plan: make check

Reviewers: dhruba, haobo, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15819/Compacting column families

Summary: This diff enables non-default column families to get compacted both automatically and also by calling CompactRange()

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15813/Enable flushing memtables from arbitrary column families

Summary: Removed default_cfd_ from all flush code paths. This means we can now flush memtables from arbitrary column families!

Test Plan: Added a new unit test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15789/MakeRoomForWrite() support for column families

Summary: Making room for write will be the hardest part of the column family implementation. For now, I just iterate through all column families and run MakeRoomForWrite() for every one.

Test Plan: make check does not complain

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15597/set bg_error_ when background flush goes wrong

Summary: as title

Test Plan: unit test

Reviewers: haobo, igor, sdong, kailiu, dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15435/Change ColumnFamilyData from struct to class

Summary: ColumnFamilyData grew a lot, there's much more data that it holds now. It makes more sense to encapsulate it better by making it a class.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15579/PurgeObsoleteFiles in DropColumnFamily

Summary: When we drop the column family, we want to delete all the files from that column family.

Test Plan: make check

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15561/Read from and write to different column families

Summary: This one is big. It adds ability to write to and read from different column families (see the unit test). It also supports recovery of different column families from log, which was the hardest part to reason about. We need to make sure to never delete the log file which has unflushed data from any column family. To support that, I added another concept, which is versions_->MinLogNumber()

Test Plan: Added a unit test in column_family_test

Reviewers: dhruba, haobo, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15537/[column families] Removing VersionSet::current()

Summary: Instead of VersionSet::current(), DBImpl uses default_cfd_->current directly.

Test Plan: make check

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15483/LogAndApply to take ColumnFamilyData

Summary: This removes the default implementation of LogAndApply that applied the changed to the default column family by default. It is mostly simple reformatting.

Test Plan: make check

Reviewers: dhruba, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15465/[column families] Move memtable and immutable memtable list to column family data

Summary: All memtables and immutable memtables are moved from DBImpl to ColumnFamilyData. For now, they are all referenced from default column family in DBImpl. It shouldn't be hard to get them from custom column family.

Test Plan: make check

Reviewers: dhruba, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15459/Fsync directory after we create a new file

Summary:
@dhruba, I'm not sure where we need to sync the directory. I implemented the function in Env() and added the dir sync just after we close the newly created file in the builder.

Should I also add FsyncDir() to new files that get created by a compaction?

Test Plan: Confirmed that FsyncDir is returning Status::OK()

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D14751/Fixing iterator cleanup for Tailing iterator

Immutable tailing iterator doesn't set CleanupState::mem, so we don't
have to unref it./MemTableListVersion

Summary:
MemTableListVersion is to MemTableList what Version is to VersionSet. I took almost the same ideas to develop MemTableListVersion. The reason is to have copying std::list done in background, while flushing, rather than in foreground (MultiGet() and NewIterator()) under a mutex! Also, whenever we copied MemTableList, we copied also some MemTableList metadata (flush_requested_, commit_in_progress_, etc.), which was wasteful.

This diff avoids std::list copy under a mutex in both MultiGet() and NewIterator(). I created a small database with some number of immutable memtables, and creating 100.000 iterators in a single-thread (!) decreased from {188739, 215703, 198028} to {154352, 164035, 159817}. A lot of the savings come from code under a mutex, so we should see much higher savings with multiple threads. Creating new iterator is very important to LogDevice team.

I also think this diff will make SuperVersion obsolete for performance reasons. I will try it in the next diff. SuperVersion gave us huge savings on Get() code path, but I think that most of the savings came from copying MemTableList under a mutex. If we had MemTableListVersion, we would never need to copy the entire object (like we still do in NewIterator() and MultiGet())

Test Plan: `make check` works. I will also do `make valgrind_check` before commit

Reviewers: dhruba, haobo, kailiu, sdong, emayanke, tnovak

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15255/Merge branch 'master' into columnfamilies

Conflicts:
	db/db_impl.cc
	db/db_impl.h
	db/db_impl_readonly.h
	db/db_test.cc
	include/rocksdb/db.h
	include/utilities/stackable_db.h/Fix a bug in DBImpl::CreateColumnFamily/CompactRange() to return status

Summary: as title

Test Plan:
make all check
What else tests shall I cover?

Reviewers: igor, haobo

CC:

Differential Revision: https://reviews.facebook.net/D15339/Merge branch 'master' into performance

Conflicts:
	db/db_impl.cc
	db/db_test.cc
	db/memtable.cc
	db/version_set.cc
	include/rocksdb/statistics.h
	util/statistics_imp.h/Tailing iterator

Summary:
This diff implements a special type of iterator that doesn't create a snapshot
(can be used to read newly inserted data) and is optimized for doing sequential
reads.

TailingIterator uses current superversion number to determine whether to
invalidate its internal iterators. If the version hasn't changed, it can often
avoid doing expensive seeks over immutable structures (sst files and immutable
memtables).

Test Plan:
* new unit tests
* running LD with this patch

Reviewers: igor, dhruba, haobo, sdong, kailiu

Reviewed By: sdong

CC: leveldb, lovro, march

Differential Revision: https://reviews.facebook.net/D15285/ColumnFamilySet

Summary:
I created a separate class ColumnFamilySet to keep track of column families. Before we did this in VersionSet and I believe this approach is cleaner.

Let me know if you have any comments. I will commit tomorrow.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15357/Refactor Recover() code

Summary:
This diff does two things:
* Rethinks how we call Recover() with read_only option. Before, we call it with pointer to memtable where we'd like to apply those changes to. This memtable is set in db_impl_readonly.cc and it's actually DBImpl::mem_. Why don't we just apply updates to mem_ right away? It seems more intuitive.
* Changes when we apply updates to manifest. Before, the process is to recover all the logs, flush it to sst files and then do one giant commit that atomically adds all recovered sst files and sets the next log number. This works good enough, but causes some small troubles for my column family approach, since I can't have one VersionEdit apply to more than single column family[1]. The change here is to commit the files recovered from logs right away. Here is the state of the world before the change:
1. Recover log 5, add new sst files to edit
2. Recover log 7, add new sst files to edit
3. Recover log 8, add new sst files to edit
4. Commit all added sst files to manifest and mark log files 5, 7 and 8 as recoverd (via SetLogNumber(9) function)
After the change, we'll do:
1. Recover log 5, commit the new sst files and set log 5 as recovered
2. Recover log 7, commit the new sst files and set log 7 as recovered
3. Recover log 8, commit the new sst files and set log 8 as recovered

The added (small) benefit is that if we fail after (2), the new recovery will only have to recover log 8. In previous case, we'll have to restart the recovery from the beginning. The bigger benefit will be to enable easier integration of multiple column families in Recovery code path.

[1] I'm happy to dicuss this decison, but I believe this is the cleanest way to go. It also makes backward compatibility much easier. We don't have a requirement of adding multiple column families atomically.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15237/Boost access before mutex is unlocked

Summary:
This moves the use of versions_ to before the mutex is unlocked
to avoid a possible race.

Task ID: #

Blame Rev:

Test Plan:
make check

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: haobo, dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15279/Statistics code cleanup

Summary: I'm separating code-cleanup part of https://reviews.facebook.net/D14517. This will make D14517 easier to understand and this diff easier to review.

Test Plan: make check

Reviewers: haobo, kailiu, sdong, dhruba, tnovak

Reviewed By: tnovak

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15099/Fix SlowdownAmount

Summary:
This had a few bugs.
1) bottom and top were reversed. top is for the max value but the callers were passing the max
value to bottom. The result is that the max sleep is used when n >= bottom.
2) one of the callers passed values with type double and these values are frequently between
1.0 and 2.0 so rounding will do some bad things
3) sometimes the function returned 0 when there should be a stall

With this change and one other diff (out for review soon) there are slightly fewer stalls on one workload.

With the fix.
Stalls(secs): 160.166 level0_slowdown, 0.000 level0_numfiles, 0.000 memtable_compaction, 58.495 leveln_slowdown
Stalls(count): 910261 level0_slowdown, 0 level0_numfiles, 0 memtable_compaction, 54526 leveln_slowdown

Without the fix.
Stalls(secs): 172.227 level0_slowdown, 0.000 level0_numfiles, 0.000 memtable_compaction, 56.538 leveln_slowdown
Stalls(count): 160831 level0_slowdown, 0 level0_numfiles, 0 memtable_compaction, 52845 leveln_slowdown

Task ID: #

Blame Rev:

Test Plan:
run db_bench for --benchmarks=overwrite with IO-bound database

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15243/Decrease reliance on VersionSet::NumberLevels()

Summary:
With column families VersionSet will not have a constant number of levels (each CF can have different options), so we'll need to eliminate call to VersionSet::NumberLevels()

This diff decreases number of callsites, but we're not there yet. It associates number of levels with Version (each version is associated with single CF) instead of VersionSet.

I have also slightly changed how VersionSet keeps track of manifest size.

This diff also modifies constructor of Compaction such that it takes input_version and automatically Ref()s it. Before this was done outside of constructor.

In next diffs I will continue to decrease number of callsites of VersionSet::NumberLevels() and also references to current_

Test Plan: make check

Reviewers: haobo, dhruba, kailiu, sdong

Reviewed By: sdong

Differential Revision: https://reviews.facebook.net/D15171/[RocksDB Performance Branch] DBImpl.NewInternalIterator() to reduce works inside mutex

Summary: To reduce mutex contention caused by DBImpl.NewInternalIterator(), in this function, move all the iteration creation works out of mutex, only leaving object ref and get.

Test Plan:
make all check
will run db_stress for a while too to make sure no problem.

Reviewers: haobo, dhruba, kailiu

Reviewed By: haobo

CC: igor, leveldb

Differential Revision: https://reviews.facebook.net/D14589

Conflicts:
	db/db_impl.cc/Fix CompactRange to apply filter to every key

Summary:
When doing CompactRange(), we should first flush the memtable and then calculate max_level_with_files. Also, we want to compact all the levels that have files, including level `max_level_with_files`.

This patch fixed the unit test.

Test Plan: Added a failing unit test and a fix, so it's not failing anymore.

Reviewers: dhruba, haobo, sdong

Reviewed By: haobo

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D14421/VersionEdit not to take NumLevels()

Summary:
I will submit a sequence of diffs that are preparing master branch for column families. There are a lot of implicit assumptions in the code that are making column family implementation hard. If I make the change only in column family branch, it will make merging back to master impossible.

Most of the diffs will be simple code refactorings, so I hope we can have fast turnaround time. Feel free to grab me in person to discuss any of them.

This diff removes number of level check from VersionEdit. It is used only when VersionEdit is read, not written, but has to be set when it is written. I believe it is a right thing to make VersionEdit dumb and check consistency on the caller side. This will also make it much easier to implement Column Families, since different column families can have different number of levels.

Test Plan: make check

Reviewers: dhruba, haobo, sdong, kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15159/BuildBatchGroup -- memcpy outside of lock

Summary: When building batch group, don't actually build a new batch since it requires heavy-weight mem copy and malloc. Only store references to the batches and build the batch group without lock held.

Test Plan:
`make check`

I am also planning to run performance tests. The workload that will benefit from this change is readwhilewriting. I will post the results once I have them.

Reviewers: dhruba, haobo, kailiu

Reviewed By: haobo

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D15063/Use sanitized options while opening db

Summary: We use SanitizeOptions() to set appropriate values for some options, based on other options. So we should use the sanitized options by default. Luckily it hasn't caused a bug yet, but can result in a bug in the fugture.

Test Plan: make check

Reviewers: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14103/[column families] Get rid of VersionSet::current_ and keep current Version for each column family

Summary:
The biggest change here is getting rid of current_ Version and adding a column_family_data->current Version to each column family.

I have also fixed some smaller things in VersionSet that made it easier to implement Column family support.

Test Plan: make check

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15105/[Performance Branch] If options.max_open_files set to be -1, cache table readers in FileMetadata for Get() and NewIterator()

Summary:
In some use cases, table readers for all live files should always be cached. In that case, there will be an opportunity to avoid the table cache look-up while Get() and NewIterator().

We define options.max_open_files = -1 to be the mode that table readers for live files will always be kept. In that mode, table readers are cached in FileMetaData (with a reference count hold in table cache). So that when executing table_cache.Get() and table_cache.newInterator(), LRU cache checking can be by-passed, to reduce latency.

Test Plan: add a test case in db_test

Reviewers: haobo, kailiu

Reviewed By: haobo

CC: dhruba, igor, leveldb

Differential Revision: https://reviews.facebook.net/D15039/StopWatch not to get time if it is created for statistics and it is disabled

Summary: Currently, even if statistics is not enabled, StopWatch only for the stats still gets the time of the day, which is wasteful. This patch adds a new option to StopWatch to disable this get in this case.

Test Plan: make all check

Reviewers: dhruba, haobo, igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14703/Add column family information to WAL

Summary:
I have added three new value types:
* kTypeColumnFamilyDeletion
* kTypeColumnFamilyValue
* kTypeColumnFamilyMerge
which include column family Varint32 before the data (value, deletion and merge). These values are used only in WAL (not in memtables yet).

This endeavour required changing some WriteBatch internals.

Test Plan: Added a unittest

Reviewers: dhruba, haobo, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15045/[column families] Implement DB::OpenWithColumnFamilies()

Summary:
In addition to implementing OpenWithColumnFamilies, this diff also includes some minor changes:
* Changed all column family names from Slice() to std::string. The performance of column family name handling is not critical, and it's more convenient and cleaner to have names as std::strings
* Implemented ColumnFamilyOptions(const Options&) and DBOptions(const Options&)
* Added ColumnFamilyOptions to VersionSet::ColumnFamilyData. ColumnFamilyOptions are specified on OpenWithColumnFamilies() and CreateColumnFamily()

I will keep the diff in the Phabricator for a day or two and will push to the branch then. Feel free to comment even after the diff has been pushed.

Test Plan: Added a simple unit test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15033/Fix a deadlock in CompactRange()

Summary:
The way DBImpl::TEST_CompactRange() throttles down the number of bg compactions
can cause it to deadlock when CompactRange() is called concurrently from
multiple threads. Imagine a following scenario with only two threads
(max_background_compactions is 10 and bg_compaction_scheduled_ is initially 0):

   1. Thread #1 increments bg_compaction_scheduled_ (to LargeNumber), sets
      bg_compaction_scheduled_ to 9 (newvalue), schedules the compaction
      (bg_compaction_scheduled_ is now 10) and waits for it to complete.
   2. Thread #2 calls TEST_CompactRange(), increments bg_compaction_scheduled_
      (now LargeNumber + 10) and waits on a cv for bg_compaction_scheduled_ to
      drop to LargeNumber.
   3. BG thread completes the first manual compaction, decrements
      bg_compaction_scheduled_ and wakes up all threads waiting on bg_cv_.
      Thread #1 runs, increments bg_compaction_scheduled_ by LargeNumber again
      (now 2*LargeNumber + 9). Since that's more than LargeNumber + newvalue,
      thread #2 also goes to sleep (waiting on bg_cv_), without resetting
      bg_compaction_scheduled_.

This diff attempts to address the problem by introducing a new counter
bg_manual_only_ (when positive, MaybeScheduleFlushOrCompaction() will only
schedule manual compactions).

Test Plan:
I could pretty much consistently reproduce the deadlock with a program that
calls CompactRange(nullptr, nullptr) immediately after Write() from multiple
threads. This no longer happens with this patch.

Tests (make check) pass.

Reviewers: dhruba, igor, sdong, haobo

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14799/[column families] Support to create and drop column families

Summary:
This diff provides basic implementations of CreateColumnFamily(), DropColumnFamily() and ListColumnFamilies(). It builds on top of https://reviews.facebook.net/D14733

It also includes a bug fix for DBImplReadOnly, where Get implementation would be redirected to DBImpl instead of DBImplReadOnly.

Test Plan: Added unit test

Reviewers: dhruba, haobo, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15021/Replace vector with autovector

Summary: this diff only replace the cases when we need to frequently create vector with small amount of entries. This diff doesn't aim to improve performance of a specific area, but more like a small scale test for the autovector and see how it works in real life.

Test Plan:
make check

I also ran the performance tests, however there is no performance gain/loss. All performance numbers are pretty much the same before/after the change.

Reviewers: dhruba, haobo, sdong, igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14985/Some minor refactoring on the code

Summary: I made some cleanup while reading the source code in `db`. Most changes are about style, naming or C++ 11 new features.

Test Plan: ran `make check`

Reviewers: haobo, dhruba, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15009/Support multi-threaded DisableFileDeletions() and EnableFileDeletions()

Summary:
We don't want two threads to clash if they concurrently call DisableFileDeletions() and EnableFileDeletions(). I'm adding a counter that will enable file deletions only after all DisableFileDeletions() calls have been negated with EnableFileDeletions().

However, we also don't want to break the old behavior, so I added a parameter force to EnableFileDeletions(). If force is true, we will still enable file deletions after every call to EnableFileDeletions(), which is what is happening now.

Test Plan: make check

Reviewers: dhruba, haobo, sanketh

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14781/[RocksDB] Optimize locking for Get

Summary:
Instead of locking and saving a DB state, we can cache a DB state and update it only when it changes. This change reduces lock contention and speeds up read operations on the DB.

Performance improvements are substantial, although there is some cost in no-read workloads. I ran the regression tests on my devserver and here are the numbers:

  overwrite                    56345  ->   63001
  fillseq                      193730 ->  185296
  readrandom                   771301 -> 1219803 (58% improvement!)
  readrandom_smallblockcache   677609 ->  862850
  readrandom_memtable_sst      710440 -> 1109223
  readrandom_fillunique_random 221589 ->  247869
  memtablefillrandom           105286 ->   92643
  memtablereadrandom           763033 -> 1288862

Test Plan:
make asan_check
I am also running db_stress

Reviewers: dhruba, haobo, sdong, kailiu

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14679/[RocksDB] [Column Family] Interface proposal

Summary:
<This diff is for Column Family branch>

Sharing some of the work I've done so far. This diff compiles an"
DataBase Management,"Thread management, Database Management",Rocksdb,"enable plain table in db_bench

Summary: as title

Test Plan: ran db_bench to gather stats

Reviewers: haobo, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16059/Support for LZ4 compression./Readrandom with tailing iterator

Summary:
Added an option for readrandom benchmark to run with tailing iterator instead of Get. Benefit of tailing iterator is that it doesn't require locking DB mutex on access.

I also have some results when running on my machine. The results highly depend on number of cache shards. With our current benchmark setting of 4 table cache shards and 6 block cache shards, I don't see much improvements of using tailing iterator. In that case, we're probably seeing cache mutex contention.

Here are the results for different number of shards

    cache shards       tailing iterator        get
       6                      1.38M           1.16M
      10                      1.58M           1.15M

As soon as we get rid of cache mutex contention, we're seeing big improvements in using tailing iterator vs. ordinary get.

Test Plan: ran regression test

Reviewers: dhruba, haobo, ljin, kailiu, sding

Reviewed By: haobo

CC: tnovak

Differential Revision: https://reviews.facebook.net/D15867/use super_version in NewIterator() and MultiGet() function

Summary:
Use super_version insider NewIterator to avoid Ref() each component
separately under mutex
The new added bench shows NewIterator QPS increases from 515K to 719K
No meaningful improvement for multiget I guess due to its relatively small
cost comparing to 90 keys fetch in the test.

Test Plan: unit test and db_bench

Reviewers: igor, sdong

Reviewed By: igor

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D15609/Fix printf format/Improve RocksDB ""get"" performance by computing merge result in memtable

Summary:
Added an option (max_successive_merges) that can be used to specify the
maximum number of successive merge operations on a key in the memtable.
This can be used to improve performance of the ""get"" operation. If many
successive merge operations are performed on a key, the performance of ""get""
operations on the key deteriorates, as the value has to be computed for each
""get"" operation by applying all the successive merge operations.

FB Task ID: #3428853

Test Plan:
make all check
db_bench --benchmarks=readrandommergerandom
counter_stress_test

Reviewers: haobo, vamsi, dhruba, sdong

Reviewed By: haobo

CC: zshao

Differential Revision: https://reviews.facebook.net/D14991/Add 'readtocache' test

Summary:
For some tests I want to cache the database prior to running other tests on the same invocation
of db_bench. The readtocache test ignores --threads and --reads so those can be used by other tests
and it will still do a full read of --num rows with one thread. It might be invoked like:
  db_bench --benchmarks=readtocache,readrandom --reads 100 --num 10000 --threads 8

Task ID: #

Blame Rev:

Test Plan:
run db_bench

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14739/Killing Transform Rep

Summary:
Let's get rid of TransformRep and it's children. We have confirmed that HashSkipListRep works better with multifeed, so there is no benefit to keeping this around.

This diff is mostly just deleting references to obsoleted functions. I also have a diff for fbcode that we'll need to push when we switch to new release.

I had to expose HashSkipListRepFactory in the client header files because db_impl.cc needs access to GetTransform() function for SanitizeOptions.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14397/make util/env_posix.cc work under mac

Summary: This diff invoves some more complicated issues in the posix environment.

Test Plan: works under mac os. will need to verify dev box.

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14061/"
,,Rocksdb,"[RocksDB][Performance Branch] Make height and branching factor configurable for skiplist implementation

Summary: As title. Especially, HashSkipListRepFactory will be able to specify a relatively small height, to reduce the memory overhead of one skiplist per bucket.

Test Plan: make check and test it on leaf4

Reviewers: dhruba, sdong, kailiu

CC: reconnect.grayhat, leveldb

Differential Revision: https://reviews.facebook.net/D14307/"
Memory management,Memory management,Rocksdb,"MemTableListVersion

Summary:
MemTableListVersion is to MemTableList what Version is to VersionSet. I took almost the same ideas to develop MemTableListVersion. The reason is to have copying std::list done in background, while flushing, rather than in foreground (MultiGet() and NewIterator()) under a mutex! Also, whenever we copied MemTableList, we copied also some MemTableList metadata (flush_ed_, commit_in_progress_, etc.), which was wasteful.

This diff avoids std::list copy under a mutex in both MultiGet() and NewIterator(). I created a small database with some number of immutable memtables, and creating 100.000 iterators in a single-thread (!) decreased from {188739, 215703, 198028} to {154352, 164035, 159817}. A lot of the savings come from code under a mutex, so we should see much higher savings with multiple threads. Creating new iterator is very important to LogDevice team.

I also think this diff will make SuperVersion obsolete for performance reasons. I will try it in the next diff. SuperVersion gave us huge savings on Get() code path, but I think that most of the savings came from copying MemTableList under a mutex. If we had MemTableListVersion, we would never need to copy the entire object (like we still do in NewIterator() and MultiGet())

Test Plan: `make check` works. I will also do `make valgrind_check` before commit

Reviewers: dhruba, haobo, kailiu, sdong, emayanke, tnovak

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15255/Introduce MergeContext to Lazily Initialize merge operand list

Summary: In get operations, merge_operands is only used in few cases. Lazily initialize it can reduce average latency in some cases

Test Plan: make all check

Reviewers: haobo, kailiu, dhruba

Reviewed By: haobo

CC: igor, nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D14415

Conflicts:
	db/db_impl.cc
	db/memtable.cc/Free obsolete memtables outside the dbmutex.

Summary:
Large memory allocations and frees are costly and best done outside the
db-mutex. The memtables are already allocated outside the db-mutex but
they were being freed while holding the db-mutex.
This patch frees obsolete memtables outside the db-mutex.

Test Plan:
make check
db_stress

Unit tests pass, I am in the process of running stress tests.

Reviewers: haobo, igor, emayanke

Reviewed By: haobo

CC: reconnect.grayhat, leveldb

Differential Revision: https://reviews.facebook.net/D14319/"
,,Rocksdb,"[Performance Branch] PlainTable to encode rows with seqID 0, value type using 1 internal byte.

Summary: In PlainTable, use one single byte to represent 8 bytes of internal bytes, if seqID = 0 and it is value type (which should be common for bottom most files). It is to save 7 bytes for uncompressed cases.

Test Plan: make all check

Reviewers: haobo, dhruba, kailiu

Reviewed By: haobo

CC: igor, leveldb

Differential Revision: https://reviews.facebook.net/D15489/Add column family information to WAL

Summary:
I have added three new value types:
* kTypeColumnFamilyDeletion
* kTypeColumnFamilyValue
* kTypeColumnFamilyMerge
which include column family Varint32 before the data (value, deletion and merge). These values are used only in WAL (not in memtables yet).

This endeavour required changing some WriteBatch internals.

Test Plan: Added a unittest

Reviewers: dhruba, haobo, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15045/"
,,Rocksdb,"Change ColumnFamilyData from struct to class

Summary: ColumnFamilyData grew a lot, there's much more data that it holds now. It makes more sense to encapsulate it better by making it a class.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15579/"
,,Rocksdb,"[CF] Rethink table cache

Summary:
Adapting table cache to column families is interesting. We want table cache to be global LRU, so if some column families are use not as often as others, we want them to be evicted from cache. However, current TableCache object also constructs tables on its own. If table is not found in the cache, TableCache automatically creates new table. We want each column family to be able to specify different table factory.

To solve the problem, we still have a single LRU, but we provide the LRUCache object to TableCache on construction. We have one TableCache per column family, but the underyling cache is shared by all TableCache objects.

This allows us to have a global LRU, but still be able to support different table factories for different column families. Also, in the future it will also be able to support different directories for different column families.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15915/[Performance Branch] PlainTable to encode rows with seqID 0, value type using 1 internal byte.

Summary: In PlainTable, use one single byte to represent 8 bytes of internal bytes, if seqID = 0 and it is value type (which should be common for bottom most files). It is to save 7 bytes for uncompressed cases.

Test Plan: make all check

Reviewers: haobo, dhruba, kailiu

Reviewed By: haobo

CC: igor, leveldb

Differential Revision: https://reviews.facebook.net/D15489/[Performance Branch] If options.max_open_files set to be -1, cache table readers in FileMetadata for Get() and NewIterator()

Summary:
In some use cases, table readers for all live files should always be cached. In that case, there will be an opportunity to avoid the table cache look-up while Get() and NewIterator().

We define options.max_open_files = -1 to be the mode that table readers for live files will always be kept. In that mode, table readers are cached in FileMetaData (with a reference count hold in table cache). So that when executing table_cache.Get() and table_cache.newInterator(), LRU cache checking can be by-passed, to reduce latency.

Test Plan: add a test case in db_test

Reviewers: haobo, kailiu

Reviewed By: haobo

CC: dhruba, igor, leveldb

Differential Revision: https://reviews.facebook.net/D15039/Hotfix the bug in table cache's GetSliceForFileNumber

Forgot to fix this problem in master branch. Already fixed it in performance branch./TableCache.FindTable() to avoid the mem copy of file number

Summary: I'm not sure what's the purpose of encoding file number to a new buffer for looking up the table cache. It seems to be unnecessary to me. With this patch, we point the lookup key to the address of the int64 of the file number.

Test Plan: make all check

Reviewers: dhruba, haobo, igor, kailiu

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14811/[RocksDB] Use raw pointer instead of shared pointer when passing Statistics object internally

Summary: liveness of the statistics object is already ensured by the shared pointer in DB options. There's no reason to pass again shared pointer among internal functions. Raw pointer is sufficient and efficient.

Test Plan: make check

Reviewers: dhruba, MarkCallaghan, igor

Reviewed By: dhruba

CC: leveldb, reconnect.grayhat

Differential Revision: https://reviews.facebook.net/D14289/"
Memory management,Memory management,Rocksdb,"[CF] Rethinking ColumnFamilyHandle and fix to dropping column families

Summary:
The change to the public behavior:
* When opening a DB or creating new column family client gets a ColumnFamilyHandle.
* As long as column family handle is alive, client can do whatever he wants with it, even drop it
* Dropped column family can still be read from (using the column family handle)
* Added a new call CloseColumnFamily(). Client has to close all column families that he has opened before deleting the DB
* As soon as column family is closed, any calls to DB using that column family handle will fail (also any outstanding calls)

Internally:
* Ref-counting ColumnFamilyData
* New thread-safety for ColumnFamilySet
* Dropped column families are now completely dropped and their memory cleaned-up

Test Plan: added some tests to column_family_test

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16101/Merge branch 'master' into columnfamilies

Conflicts:
	HISTORY.md
	db/db_impl.cc
	db/db_impl.h
	db/db_iter.cc
	db/db_test.cc
	db/dbformat.h
	db/memtable.cc
	db/memtable_list.cc
	db/memtable_list.h
	db/table_cache.cc
	db/table_cache.h
	db/version_edit.h
	db/version_set.cc
	db/version_set.h
	db/write_batch.cc
	db/write_batch_test.cc
	include/rocksdb/options.h
	util/options.cc/[CF] Propagate correct options to WriteBatch::InsertInto

Summary:
WriteBatch can have multiple column families in one batch. Every column family has different options. So we have to add a way for write batch to get options for an arbitrary column family.

This required a bit more acrobatics since lots of interfaces had to be changed.

Test Plan: make check

Reviewers: dhruba, haobo, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15957/Read from and write to different column families

Summary: This one is big. It adds ability to write to and read from different column families (see the unit test). It also supports recovery of different column families from log, which was the hardest part to reason about. We need to make sure to never delete the log file which has unflushed data from any column family. To support that, I added another concept, which is versions_->MinLogNumber()

Test Plan: Added a unit test in column_family_test

Reviewers: dhruba, haobo, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15537/Allow callback to change size of existing value. Change return type of the callback function to an enum status to handle 3 cases.

Summary:
This diff fixes 2 hacks:
* The callback function can modify the existing value inplace, if the merged value fits within the existing buffer size. But currently the existing buffer size is not being modified. Now the callback recieves a int* allowing the size to be modified. Since size is encoded as a varint in the internal key for memtable. It might happen that the entire value might have be copied to the new location if the new size varint is smaller than the existing size varint.
* The callback function has 3 functionalities
    1. Modify existing buffer inplace, and update size correspondingly. Now to indicate that, Returns 1.
    2. Generate a new buffer indicating merged value. Returns 2.
    3. Fails to do either of above, based on whatever application logic. Returns 0.

Test Plan: Just make all for now. I'm adding another unit test to test each scenario.

Reviewers: dhruba, haobo

Reviewed By: haobo

CC: leveldb, sdong, kailiu, xinyaohu, sumeet, danguo

Differential Revision: https://reviews.facebook.net/D15195/Add read/modify/write functionality to Put() api

Summary: The application can set a callback function, which is applied on the previous value. And calculates the new value. This new value can be set, either inplace, if the previous value existed in memtable, and new value is smaller than previous value. Otherwise the new value is added normally.

Test Plan: fbmake. Added unit tests. All unit tests pass.

Reviewers: dhruba, haobo

Reviewed By: haobo

CC: sdong, kailiu, xinyaohu, sumeet, leveldb

Differential Revision: https://reviews.facebook.net/D14745/Improve RocksDB ""get"" performance by computing merge result in memtable

Summary:
Added an option (max_successive_merges) that can be used to specify the
maximum number of successive merge operations on a key in the memtable.
This can be used to improve performance of the ""get"" operation. If many
successive merge operations are performed on a key, the performance of ""get""
operations on the key deteriorates, as the value has to be computed for each
""get"" operation by applying all the successive merge operations.

FB Task ID: #3428853

Test Plan:
make all check
db_bench --benchmarks=readrandommergerandom
counter_stress_test

Reviewers: haobo, vamsi, dhruba, sdong

Reviewed By: haobo

CC: zshao

Differential Revision: https://reviews.facebook.net/D14991/Add column family information to WAL

Summary:
I have added three new value types:
* kTypeColumnFamilyDeletion
* kTypeColumnFamilyValue
* kTypeColumnFamilyMerge
which include column family Varint32 before the data (value, deletion and merge). These values are used only in WAL (not in memtables yet).

This endeavour required changing some WriteBatch internals.

Test Plan: Added a unittest

Reviewers: dhruba, haobo, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15045/[RocksDB] [Column Family] Interface proposal

Summary:
<This diff is for Column Family branch>

Sharing some of the work I've done so far. This diff compiles and passes the tests.

The biggest change is in options.h - I broke down Options into two parts - DBOptions and ColumnFamilyOptions. DBOptions is DB-specific (env, create_if_missing, block_cache, etc.) and ColumnFamilyOptions is column family-specific (all compaction options, compresion options, etc.). Note that this does not break backwards compatibility at all.

Further, I created DBWithColumnFamily which inherits DB interface and adds new functions with column family support. Clients can transparently switch to DBWithColumnFamily and it will not break their backwards compatibility.
There are few methods worth checking out: ListColumnFamilies(), MultiNewIterator(), MultiGet() and GetSnapshot(). [GetSnapshot() returns the snapshot across all column families for now - I think that's what we agreed on]

Finally, I made small changes to WriteBatch so we are able to atomically insert data across column families.

Please provide feedback.

Test Plan: make check works, the code is backward compatible

Reviewers: dhruba, haobo, sdong, kailiu, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14445/"
Memory management,Memory management,Rocksdb,"[Performance Branch] A Hashed Linked List Based Mem Table

Summary:
Implement a mem table, in which keys are hashed based on prefixes. In each bucket, entries are organized in a sorted linked list. It has the same thread safety guarantee as skip list.

The motivation is to optimize memory usage for the case that prefix hashing is primary way of seeking to the entry. Compared to hash skip list implementation, this implementation is more memory efficient, but inside each bucket, search is always linear. The target scenario is that there are only very limited number of records in each hash bucket.

Test Plan: Add a test case in db_test

Reviewers: haobo, kailiu, dhruba

Reviewed By: haobo

CC: igor, nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D14979/[RocksDB] [Performance Branch] Added dynamic bloom, to be used for memable non-existing key filtering

Summary: as title

Test Plan: dynamic_bloom_test

Reviewers: dhruba, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14385/[RocksDB][Performance Branch] Make height and branching factor configurable for skiplist implementation

Summary: As title. Especially, HashSkipListRepFactory will be able to specify a relatively small height, to reduce the memory overhead of one skiplist per bucket.

Test Plan: make check and test it on leaf4

Reviewers: dhruba, sdong, kailiu

CC: reconnect.grayhat, leveldb

Differential Revision: https://reviews.facebook.net/D14307/[RocksDB] fix prefix_test

Summary: user comparator needs to work if either input is prefix only.

Test Plan: ./prefix_test --write_buffer_size=100000 --total_prefixes=10000 --items_per_prefix=10

Reviewers: dhruba, igor

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14241/"
,,Rocksdb,C bindings: add a bunch of the newer options/Rename leveldb to rocksdb in C api/
compression tasks,Compression Tasks,Rocksdb,"Don't always compress L0 files written by memtable flush

Summary:
Code was always compressing L0 files written by a memtable flush
when compression was enabled. Now this is done when
min_level_to_compress=0 for leveled compaction and when
universal_compaction_size_percent=-1 for universal compaction.

Task ID: #3416472

Blame Rev:

Test Plan:
ran db_bench with compression options

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: dhruba, igor, sdong

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14757/"
,,Rocksdb,"VersionEdit not to take NumLevels()

Summary:
I will submit a sequence of diffs that are preparing master branch for column families. There are a lot of implicit assumptions in the code that are making column family implementation hard. If I make the change only in column family branch, it will make merging back to master impossible.

Most of the diffs will be simple code refactorings, so I hope we can have fast turnaround time. Feel free to grab me in person to discuss any of them.

This diff removes number of level check from VersionEdit. It is used only when VersionEdit is read, not written, but has to be set when it is written. I believe it is a right thing to make VersionEdit dumb and check consistency on the caller side. This will also make it much easier to implement Column Families, since different column families can have different number of levels.

Test Plan: make check

Reviewers: dhruba, haobo, sdong, kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15159/Some minor refactoring on the code

Summary: I made some cleanup while reading the source code in `db`. Most changes are about style, naming or C++ 11 new features.

Test Plan: ran `make check`

Reviewers: haobo, dhruba, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15009/[RocksDB] Support for column families in manifest

Summary:
<This diff is for Column Family branch>

Added fields in manifest file to support adding and deleting column families.

Pretty simple change, each version edit record can be:
1. add column family
2. drop column family
3. add and delete N files from a single column family (compactions and flushes will generate such records)

Test Plan: make check works, the code is backward compatible

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14733/"
Memory management,Memory management,Rocksdb,"[CF] Rethinking ColumnFamilyHandle and fix to dropping column families

Summary:
The change to the public behavior:
* When opening a DB or creating new column family client gets a ColumnFamilyHandle.
* As long as column family handle is alive, client can do whatever he wants with it, even drop it
* Dropped column family can still be read from (using the column family handle)
* Added a new call CloseColumnFamily(). Client has to close all column families that he has opened before deleting the DB
* As soon as column family is closed, any calls to DB using that column family handle will fail (also any outstanding calls)

Internally:
* Ref-counting ColumnFamilyData
* New thread-safety for ColumnFamilySet
* Dropped column families are now completely dropped and their memory cleaned-up

Test Plan: added some tests to column_family_test

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16101/[column families] Iterator and MultiGet

Summary: Support for different column families in Iterator and MultiGet code path.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15849/Merge branch 'master' into columnfamilies

Conflicts:
	db/db_impl.cc
	db/db_impl.h
	db/db_impl_readonly.h
	db/db_test.cc
	include/rocksdb/db.h
	include/utilities/stackable_db.h/[column families] Support to create and drop column families

Summary:
This diff provides basic implementations of CreateColumnFamily(), DropColumnFamily() and ListColumnFamilies(). It builds on top of https://reviews.facebook.net/D14733

It also includes a bug fix for DBImplReadOnly, where Get implementation would be redirected to DBImpl instead of DBImplReadOnly.

Test Plan: Added unit test

Reviewers: dhruba, haobo, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15021/Support multi-threaded DisableFileDeletions() and EnableFileDeletions()

Summary:
We don't want two threads to clash if they concurrently call DisableFileDeletions() and EnableFileDeletions(). I'm adding a counter that will enable file deletions only after all DisableFileDeletions() calls have been negated with EnableFileDeletions().

However, we also don't want to break the old behavior, so I added a parameter force to EnableFileDeletions(). If force is true, we will still enable file deletions after every call to EnableFileDeletions(), which is what is happening now.

Test Plan: make check

Reviewers: dhruba, haobo, sanketh

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14781/[RocksDB] [Column Family] Interface proposal

Summary:
<This diff is for Column Family branch>

Sharing some of the work I've done so far. This diff compiles and passes the tests.

The biggest change is in options.h - I broke down Options into two parts - DBOptions and ColumnFamilyOptions. DBOptions is DB-specific (env, create_if_missing, block_cache, etc.) and ColumnFamilyOptions is column family-specific (all compaction options, compresion options, etc.). Note that this does not break backwards compatibility at all.

Further, I created DBWithColumnFamily which inherits DB interface and adds new functions with column family support. Clients can transparently switch to DBWithColumnFamily and it will not break their backwards compatibility.
There are few methods worth checking out: ListColumnFamilies(), MultiNewIterator(), MultiGet() and GetSnapshot(). [GetSnapshot() returns the snapshot across all column families for now - I think that's what we agreed on]

Finally, I made small changes to WriteBatch so we are able to atomically insert data across column families.

Please provide feedback.

Test Plan: make check works, the code is backward compatible

Reviewers: dhruba, haobo, sdong, kailiu, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14445/"
,,Rocksdb,"Killing Transform Rep

Summary:
Let's get rid of TransformRep and it's children. We have confirmed that HashSkipListRep works better with multifeed, so there is no benefit to keeping this around.

This diff is mostly just deleting references to obsoleted functions. I also have a diff for fbcode that we'll need to push when we switch to new release.

I had to expose HashSkipListRepFactory in the client header files because db_impl.cc needs access to GetTransform() function for SanitizeOptions.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14397/Allow users to profile a query and see bottleneck of the query

Summary:
Provide a framework to profile a query in detail to figure out latency bottleneck. Currently, in Get(), Put() and iterators, 2-3 simple timing is used. We can easily add more profile counters to the framework later.

Test Plan: Enable this profiling in seveal existing tests.

Reviewers: haobo, dhruba, kailiu, emayanke, vamsi, igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14001

Conflicts:
	table/merger.cc/"
Memory management,Memory management,Rocksdb,"[CF] Rethinking ColumnFamilyHandle and fix to dropping column families

Summary:
The change to the public behavior:
* When opening a DB or creating new column family client gets a ColumnFamilyHandle.
* As long as column family handle is alive, client can do whatever he wants with it, even drop it
* Dropped column family can still be read from (using the column family handle)
* Added a new call CloseColumnFamily(). Client has to close all column families that he has opened before deleting the DB
* As soon as column family is closed, any calls to DB using that column family handle will fail (also any outstanding calls)

Internally:
* Ref-counting ColumnFamilyData
* New thread-safety for ColumnFamilySet
* Dropped column families are now completely dropped and their memory cleaned-up

Test Plan: added some tests to column_family_test

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16101/preload table handle on Recover() when max_open_files == -1

Summary: This covers existing table files before DB open happens and avoids contention on table cache

Test Plan: db_test

Reviewers: haobo, sdong, igor, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16089/[CF] Options -> DBOptions

Summary: Replaced most of occurrences of Options with more specific DBOptions. This brings us very close to supporting different configuration options for each column family.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15933/Get rid of DBImpl::user_comparator()

Summary: user_comparator() is a Column Family property, not DBImpl

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15855/Enable flushing memtables from arbitrary column families

Summary: Removed default_cfd_ from all flush code paths. This means we can now flush memtables from arbitrary column families!

Test Plan: Added a new unit test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15789/MakeRoomForWrite() support for column families

Summary: Making room for write will be the hardest part of the column family implementation. For now, I just iterate through all column families and run MakeRoomForWrite() for every one.

Test Plan: make check does not complain

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15597/Read from and write to different column families

Summary: This one is big. It adds ability to write to and read from different column families (see the unit test). It also supports recovery of different column families from log, which was the hardest part to reason about. We need to make sure to never delete the log file which has unflushed data from any column family. To support that, I added another concept, which is versions_->MinLogNumber()

Test Plan: Added a unit test in column_family_test

Reviewers: dhruba, haobo, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15537/[column families] Move memtable and immutable memtable list to column family data

Summary: All memtables and immutable memtables are moved from DBImpl to ColumnFamilyData. For now, they are all referenced from default column family in DBImpl. It shouldn't be hard to get them from custom column family.

Test Plan: make check

Reviewers: dhruba, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15459/Merge branch 'master' into columnfamilies

Conflicts:
	db/db_impl.cc
	db/db_impl.h
	db/db_impl_readonly.h
	db/db_test.cc
	include/rocksdb/db.h
	include/utilities/stackable_db.h/CompactRange() to return status

Summary: as title

Test Plan:
make all check
What else tests shall I cover?

Reviewers: igor, haobo

CC:

Differential Revision: https://reviews.facebook.net/D15339/Merge branch 'master' into columnfamilies

Conflicts:
	db/db_impl.cc
	db/db_impl.h
	db/db_impl_readonly.cc
	db/version_set.cc/Refactor Recover() code

Summary:
This diff does two things:
* Rethinks how we call Recover() with read_only option. Before, we call it with pointer to memtable where we'd like to apply those changes to. This memtable is set in db_impl_readonly.cc and it's actually DBImpl::mem_. Why don't we just apply updates to mem_ right away? It seems more intuitive.
* Changes when we apply updates to manifest. Before, the process is to recover all the logs, flush it to sst files and then do one giant commit that atomically adds all recovered sst files and sets the next log number. This works good enough, but causes some small troubles for my column family approach, since I can't have one VersionEdit apply to more than single column family[1]. The change here is to commit the files recovered from logs right away. Here is the state of the world before the change:
1. Recover log 5, add new sst files to edit
2. Recover log 7, add new sst files to edit
3. Recover log 8, add new sst files to edit
4. Commit all added sst files to manifest and mark log files 5, 7 and 8 as recoverd (via SetLogNumber(9) function)
After the change, we'll do:
1. Recover log 5, commit the new sst files and set log 5 as recovered
2. Recover log 7, commit the new sst files and set log 7 as recovered
3. Recover log 8, commit the new sst files and set log 8 as recovered

The added (small) benefit is that if we fail after (2), the new recovery will only have to recover log 8. In previous case, we'll have to restart the recovery from the beginning. The bigger benefit will be to enable easier integration of multiple column families in Recovery code path.

[1] I'm happy to dicuss this decison, but I believe this is the cleanest way to go. It also makes backward compatibility much easier. We don't have a requirement of adding multiple column families atomically.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15237/Fix SlowdownAmount

Summary:
This had a few bugs.
1) bottom and top were reversed. top is for the max value but the callers were passing the max
value to bottom. The result is that the max sleep is used when n >= bottom.
2) one of the callers passed values with type double and these values are frequently between
1.0 and 2.0 so rounding will do some bad things
3) sometimes the function returned 0 when there should be a stall

With this change and one other diff (out for review soon) there are slightly fewer stalls on one workload.

With the fix.
Stalls(secs): 160.166 level0_slowdown, 0.000 level0_numfiles, 0.000 memtable_compaction, 58.495 leveln_slowdown
Stalls(count): 910261 level0_slowdown, 0 level0_numfiles, 0 memtable_compaction, 54526 leveln_slowdown

Without the fix.
Stalls(secs): 172.227 level0_slowdown, 0.000 level0_numfiles, 0.000 memtable_compaction, 56.538 leveln_slowdown
Stalls(count): 160831 level0_slowdown, 0 level0_numfiles, 0 memtable_compaction, 52845 leveln_slowdown

Task ID: #

Blame Rev:

Test Plan:
run db_bench for --benchmarks=overwrite with IO-bound database

Revert Plan:

Database Impact:

Memcache Impact:

Other Notes:

EImportant:

- begin *PUBLIC* platform impact section -
Bugzilla: #
- end platform impact -

Reviewers: haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15243/Fix CompactRange to apply filter to every key

Summary:
When doing CompactRange(), we should first flush the memtable and then calculate max_level_with_files. Also, we want to compact all the levels that have files, including level `max_level_with_files`.

This patch fixed the unit test.

Test Plan: Added a failing unit test and a fix, so it's not failing anymore.

Reviewers: dhruba, haobo, sdong

Reviewed By: haobo

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D14421/Fix a deadlock in CompactRange()

Summary:
The way DBImpl::TEST_CompactRange() throttles down the number of bg compactions
can cause it to deadlock when CompactRange() is called concurrently from
multiple threads. Imagine a following scenario with only two threads
(max_background_compactions is 10 and bg_compaction_scheduled_ is initially 0):

   1. Thread #1 increments bg_compaction_scheduled_ (to LargeNumber), sets
      bg_compaction_scheduled_ to 9 (newvalue), schedules the compaction
      (bg_compaction_scheduled_ is now 10) and waits for it to complete.
   2. Thread #2 calls TEST_CompactRange(), increments bg_compaction_scheduled_
      (now LargeNumber + 10) and waits on a cv for bg_compaction_scheduled_ to
      drop to LargeNumber.
   3. BG thread completes the first manual compaction, decrements
      bg_compaction_scheduled_ and wakes up all threads waiting on bg_cv_.
      Thread #1 runs, increments bg_compaction_scheduled_ by LargeNumber again
      (now 2*LargeNumber + 9). Since that's more than LargeNumber + newvalue,
      thread #2 also goes to sleep (waiting on bg_cv_), without resetting
      bg_compaction_scheduled_.

This diff attempts to address the problem by introducing a new counter
bg_manual_only_ (when positive, MaybeScheduleFlushOrCompaction() will only
schedule manual compactions).

Test Plan:
I could pretty much consistently reproduce the deadlock with a program that
calls CompactRange(nullptr, nullptr) immediately after Write() from multiple
threads. This no longer happens with this patch.

Tests (make check) pass.

Reviewers: dhruba, igor, sdong, haobo

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14799/[column families] Support to create and drop column families

Summary:
This diff provides basic implementations of CreateColumnFamily(), DropColumnFamily() and ListColumnFamilies(). It builds on top of https://reviews.facebook.net/D14733

It also includes a bug fix for DBImplReadOnly, where Get implementation would be redirected to DBImpl instead of DBImplReadOnly.

Test Plan: Added unit test

Reviewers: dhruba, haobo, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15021/Support multi-threaded DisableFileDeletions() and EnableFileDeletions()

Summary:
We don't want two threads to clash if they concurrently call DisableFileDeletions() and EnableFileDeletions(). I'm adding a counter that will enable file deletions only after all DisableFileDeletions() calls have been negated with EnableFileDeletions().

However, we also don't want to break the old behavior, so I added a parameter force to EnableFileDeletions(). If force is true, we will still enable file deletions after every call to EnableFileDeletions(), which is what is happening now.

Test Plan: make check

Reviewers: dhruba, haobo, sanketh

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14781/[RocksDB] Optimize locking for Get

Summary:
Instead of locking and saving a DB state, we can cache a DB state and update it only when it changes. This change reduces lock contention and speeds up read operations on the DB.

Performance improvements are substantial, although there is some cost in no-read workloads. I ran the regression tests on my devserver and here are the numbers:

Test Plan:
make asan_check
I am also running db_stress

Reviewers: dhruba, haobo, sdong, kailiu

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14679/[RocksDB] [Column Family] Interface proposal

Summary:
<This diff is for Column Family branch>

Sharing some of the work I've done so far. This diff compiles and passes the tests.

The biggest change is in options.h - I broke down Options into two parts - DBOptions and ColumnFamilyOptions. DBOptions is DB-specific (env, create_if_missing, block_cache, etc.) and ColumnFamilyOptions is column family-specific (all compaction options, compresion options, etc.). Note that this does not break backwards compatibility at all.

Further, I created DBWithColumnFamily which inherits DB interface and adds new functions with column family support. Clients can transparently switch to DBWithColumnFamily and it will not break their backwards compatibility.
There are few methods worth checking out: ListColumnFamilies(), MultiNewIterator(), MultiGet() and GetSnapshot(). [GetSnapshot() returns the snapshot across all column families for now - I think that's what we agreed on]

Finally, I made small changes to WriteBatch so we are able to atomically insert data across column families.

Please provide feedback.

Test Plan: make check works, the code is backward compatible

Reviewers: dhruba, haobo, sdong, kailiu, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14445/[RocksDB] BackupableDB

Summary:
In this diff I present you BackupableDB v1. You can easily use it to backup your DB and it will do incremental snapshots for you.
Let's first describe how you would use BackupableDB. It's inheriting StackableDB interface so you can easily construct it with your DB object -- it will add a method RollTheSnapshot() to the DB object. When you call RollTheSnapshot(), current snapshot of the DB will be stored in the backup dir. To restore, you can just call RestoreDBFromBackup() on a BackupableDB (which is a static method) and it will restore all files from the backup dir. In the next version, it will even support automatic backuping every X minutes.

There are multiple things you can configure:
1. backup_env and db_env can be different, which is awesome because then you can easily backup to HDFS or wherever you feel like.
2. sync - if true, it *guarantees* backup consistency on machine reboot
3. number of snapshots to keep - this will keep last N snapshots around if you want, for some reason, be able to restore from an earlier snapshot. All the backuping is done in incremental fashion - if we already have 00010.sst, we will not copy it again. *IMPORTANT* -- This is based on assumption that 00010.sst never changes - two files named 00010.sst from the same DB will always be exactly the same. Is this true? I always copy manifest, current and log files.
4. You can decide if you want to flush the memtables before you backup, or you're fine with backing up the log files -- either way, you get a complete and consistent view of the database at a time of backup.
5. More things you can find in BackupableDBOptions

Here is the directory structure I use:

   backup_dir/CURRENT_SNAPSHOT - just 4 bytes holding the latest snapshot
               0, 1, 2, ... - files containing serialized version of each snapshot - containing a list of files
               files/*.sst - sst files shared between snapshots - if one snapshot references 00010.sst and another one needs to backup it from the DB, it will just reference the same file
               files/ 0/, 1/, 2/, ... - snapshot directories containing private snapshot files - current, manifest and log files

All the files are ref counted and deleted immediatelly when they get out of scope.

Some other stuff in this diff:
1. Added GetEnv() method to the DB. Discussed with @haobo and we agreed that it seems right thing to do.
2. Fixed StackableDB interface. The way it was set up before, I was not able to implement BackupableDB.

Test Plan:
I have a unittest, but please don't look at this yet. I just hacked it up to help me with debugging. I will write a lot of good tests and update the diff.

Also, `make asan_check`

Reviewers: dhruba, haobo, emayanke

Reviewed By: dhruba

CC: leveldb, haobo

Differential Revision: https://reviews.facebook.net/D14295/Make an API to get database identity from the IDENTITY file

Summary: This would enable rocksdb users to get the db identity without depending on implementation details(storing that in IDENTITY file)

Test Plan: db/db_test (has identity checks)

Reviewers: dhruba, haobo, igor, kailiu

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14463/Free obsolete memtables outside the dbmutex had a memory leak.

Summary:
The commit at 27bbef11802d27c80df7e0b27091876df23b9986 had a memory leak
that was detected by valgrind. The memtable that has a refcount decrement
in MemTableList::InstallMemtableFlushResults was not freed.

Test Plan: valgrind ./db_test --leak-check=full

Reviewers: igor

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14391/DB::GetOptions()

Summary: We need access to options for BackupableDB

Test Plan: make check

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, reconnect.grayhat

Differential Revision: https://reviews.facebook.net/D14331/[RocksDB] Interface changes required for BackupableDB

Summary: This is part of https://reviews.facebook.net/D14295 -- smaller diff that is easier to review

Test Plan: make asan_check

Reviewers: dhruba, haobo, emayanke

Reviewed By: emayanke

CC: leveldb, kailiu, reconnect.grayhat

Differential Revision: https://reviews.facebook.net/D14301/PurgeObsoleteFiles() unittest

Summary:
Created a unittest that verifies that automatic deletion performed by PurgeObsoleteFiles() works correctly.

Also, few small fixes on the logic part -- call version_set_->GetObsoleteFiles() in FindObsoleteFiles() instead of on some arbitrary positions.

Test Plan: Created a unit test

Reviewers: dhruba, haobo, nkg-

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14079/"
,,Rocksdb,"Introduce MergeContext to Lazily Initialize merge operand list

Summary: In get operations, merge_operands is only used in few cases. Lazily initialize it can reduce average latency in some cases

Test Plan: make all check

Reviewers: haobo, kailiu, dhruba

Reviewed By: haobo

CC: igor, nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D14415

Conflicts:
	db/db_impl.cc
	db/memtable.cc/Free obsolete memtables outside the dbmutex.

Summary:
Large memory allocations and frees are costly and best done outside the
db-mutex. The memtables are already allocated outside the db-mutex but
they were being freed while holding the db-mutex.
This patch frees obsolete memtables outside the db-mutex.

Test Plan:
make check
db_stress

Unit tests pass, I am in the process of running stress tests.

Reviewers: haobo, igor, emayanke

Reviewed By: haobo

CC: reconnect.grayhat, leveldb

Differential Revision: https://reviews.facebook.net/D14319/"
,,Rocksdb,"Clean up arena API

Summary:
Easy thing goes first. This patch moves arena to internal dir; based
on which, the coming patch will deal with memtable_rep.

Test Plan: make check

Reviewers: haobo, sdong, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15615/[column families] Move memtable and immutable memtable list to column family data

Summary: All memtables and immutable memtables are moved from DBImpl to ColumnFamilyData. For now, they are all referenced from default column family in DBImpl. It shouldn't be hard to get them from custom column family.

Test Plan: make check

Reviewers: dhruba, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15459/[Performance Branch] HashLinkList to avoid to convert length prefixed string back to internal keys

Summary: Converting from length prefixed buffer back to internal key costs some CPU but it is not necessary. In this patch, internal keys are pass though the functions so that we don't need to convert back to it.

Test Plan: make all check

Reviewers: haobo, kailiu

Reviewed By: kailiu

CC: igor, leveldb

Differential Revision: https://reviews.facebook.net/D15393/"
,,Rocksdb,"Merge branch 'master' into columnfamilies

Conflicts:
	HISTORY.md
	db/db_impl.cc
	db/db_impl.h
	db/db_iter.cc
	db/db_test.cc
	db/dbformat.h
	db/memtable.cc
	db/memtable_list.cc
	db/memtable_list.h
	db/table_cache.cc
	db/table_cache.h
	db/version_edit.h
	db/version_set.cc
	db/version_set.h
	db/write_batch.cc
	db/write_batch_test.cc
	include/rocksdb/options.h
	util/options.cc/VersionEdit not to take NumLevels()

Summary:
I will submit a sequence of diffs that are preparing master branch for column families. There are a lot of implicit assumptions in the code that are making column family implementation hard. If I make the change only in column family branch, it will make merging back to master impossible.

Most of the diffs will be simple code refactorings, so I hope we can have fast turnaround time. Feel free to grab me in person to discuss any of them.

This diff removes number of level check from VersionEdit. It is used only when VersionEdit is read, not written, but has to be set when it is written. I believe it is a right thing to make VersionEdit dumb and check consistency on the caller side. This will also make it much easier to implement Column Families, since different column families can have different number of levels.

Test Plan: make check

Reviewers: dhruba, haobo, sdong, kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15159/Some minor refactoring on the code

Summary: I made some cleanup while reading the source code in `db`. Most changes are about style, naming or C++ 11 new features.

Test Plan: ran `make check`

Reviewers: haobo, dhruba, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15009/[RocksDB] Support for column families in manifest

Summary:
<This diff is for Column Family branch>

Added fields in manifest file to support adding and deleting column families.

Pretty simple change, each version edit record can be:
1. add column family
2. drop column family
3. add and delete N files from a single column family (compactions and flushes will generate such records)

Test Plan: make check works, the code is backward compatible

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14733/"
,,Rocksdb,"[CF] Options -> DBOptions

Summary: Replaced most of occurrences of Options with more specific DBOptions. This brings us very close to supporting different configuration options for each column family.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15933/[CF] Rethink table cache

Summary:
Adapting table cache to column families is interesting. We want table cache to be global LRU, so if some column families are use not as often as others, we want them to be evicted from cache. However, current TableCache object also constructs tables on its own. If table is not found in the cache, TableCache automatically creates new table. We want each column family to be able to specify different table factory.

To solve the problem, we still have a single LRU, but we provide the LRUCache object to TableCache on construction. We have one TableCache per column family, but the underyling cache is shared by all TableCache objects.

This allows us to have a global LRU, but still be able to support different table factories for different column families. Also, in the future it will also be able to support different directories for different column families.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15915/VersionSet cleanup

Summary:
Removed icmp_ from VersionSet (since it's per-column-family, not per-DB-instance)
Unfriended VersionSet and ColumnFamilyData (yay!)
Removed VersionSet::NumberLevels()
Cleaned up DBImpl

Test Plan: make check

Reviewers: dhruba, haobo, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15819/Change ColumnFamilyData from struct to class

Summary: ColumnFamilyData grew a lot, there's much more data that it holds now. It makes more sense to encapsulate it better by making it a class.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15579/Read from and write to different column families

Summary: This one is big. It adds ability to write to and read from different column families (see the unit test). It also supports recovery of different column families from log, which was the hardest part to reason about. We need to make sure to never delete the log file which has unflushed data from any column family. To support that, I added another concept, which is versions_->MinLogNumber()

Test Plan: Added a unit test in column_family_test

Reviewers: dhruba, haobo, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15537/Make VersionSet::ReduceNumberOfLevels() static

Summary:
A lot of our code implicitly assumes number_levels to be static. ReduceNumberOfLevels() breaks that assumption. For example, after calling ReduceNumberOfLevels(), DBImpl::NumberLevels() will be different from VersionSet::NumberLevels(). This is dangerous. Thankfully, it's not in public headers and is only used from LDB cmd tool. LDB tool is only using it statically, i.e. it never calls it with running DB instance. With this diff, we make it explicitly static. This way, we can assume number_levels to be immutable and not break assumption that lot of our code is relying upon. LDB tool can still use the method.

Also, I removed the method from a separate file since it breaks filename completition. version_se<TAB> now completes to ""version_set."" instead of ""version_set"" (without the dot). I don't see a big reason that the function should be in a different file.

Test Plan: reduce_levels_test

Reviewers: dhruba, haobo, kailiu, sdong

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15303/ColumnFamilySet

Summary:
I created a separate class ColumnFamilySet to keep track of column families. Before we did this in VersionSet and I believe this approach is cleaner.

Let me know if you have any comments. I will commit tomorrow.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15357/Move more functions from VersionSet to Version

Summary:
This moves functions:
* VersionSet::Finalize() -> Version::UpdateCompactionStats()
* VersionSet::UpdateFilesBySize() -> Version::UpdateFilesBySize()

The diff depends on D15189, D15183 and D15171

Test Plan: make check

Reviewers: kailiu, sdong, haobo, dhruba

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15201/Decrease reliance on VersionSet::NumberLevels()

Summary:
With column families VersionSet will not have a constant number of levels (each CF can have different options), so we'll need to eliminate call to VersionSet::NumberLevels()

This diff decreases number of callsites, but we're not there yet. It associates number of levels with Version (each version is associated with single CF) instead of VersionSet.

I have also slightly changed how VersionSet keeps track of manifest size.

This diff also modifies constructor of Compaction such that it takes input_version and automatically Ref()s it. Before this was done outside of constructor.

In next diffs I will continue to decrease number of callsites of VersionSet::NumberLevels() and also references to current_

Test Plan: make check

Reviewers: haobo, dhruba, kailiu, sdong

Reviewed By: sdong

Differential Revision: https://reviews.facebook.net/D15171/[column families] Get rid of VersionSet::current_ and keep current Version for each column family

Summary:
The biggest change here is getting rid of current_ Version and adding a column_family_data->current Version to each column family.

I have also fixed some smaller things in VersionSet that made it easier to implement Column family support.

Test Plan: make check

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15105/[column families] Implement DB::OpenWithColumnFamilies()

Summary:
In addition to implementing OpenWithColumnFamilies, this diff also includes some minor changes:
* Changed all column family names from Slice() to std::string. The performance of column family name handling is not critical, and it's more convenient and cleaner to have names as std::strings
* Implemented ColumnFamilyOptions(const Options&) and DBOptions(const Options&)
* Added ColumnFamilyOptions to VersionSet::ColumnFamilyData. ColumnFamilyOptions are specified on OpenWithColumnFamilies() and CreateColumnFamily()

I will keep the diff in the Phabricator for a day or two and will push to the branch then. Feel free to comment even after the diff has been pushed.

Test Plan: Added a simple unit test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15033/[column families] Support to create and drop column families

Summary:
This diff provides basic implementations of CreateColumnFamily(), DropColumnFamily() and ListColumnFamilies(). It builds on top of https://reviews.facebook.net/D14733

It also includes a bug fix for DBImplReadOnly, where Get implementation would be redirected to DBImpl instead of DBImplReadOnly.

Test Plan: Added unit test

Reviewers: dhruba, haobo, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15021/Some minor refactoring on the code

Summary: I made some cleanup while reading the source code in `db`. Most changes are about style, naming or C++ 11 new features.

Test Plan: ran `make check`

Reviewers: haobo, dhruba, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15009/[RocksDB] Support for column families in manifest

Summary:
<This diff is for Column Family branch>

Added fields in manifest file to support adding and deleting column families.

Pretty simple change, each version edit record can be:
1. add column family
2. drop column family
3. add and delete N files from a single column family (compactions and flushes will generate such records)

Test Plan: make check works, the code is backward compatible

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14733/Introduce MergeContext to Lazily Initialize merge operand list

Summary: In get operations, merge_operands is only used in few cases. Lazily initialize it can reduce average latency in some cases

Test Plan: make all check

Reviewers: haobo, kailiu, dhruba

Reviewed By: haobo

CC: igor, nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D14415

Conflicts:
	db/db_impl.cc
	db/memtable.cc/"
,,Rocksdb,"Fix UnmarkEOF for partial blocks

Summary:
Blocks in the transaction log are a fixed size, but the last block in the transaction log file is usually a partial block. When a new record is added after the reader hit the end of the file, a new physical record will be appended to the last block. ReadPhysicalRecord can only read full blocks and assumes that the file position indicator is aligned to the start of a block. If the reader is forced to read further by simply clearing the EOF flag, ReadPhysicalRecord will read a full block starting from somewhere in the middle of a real block, causing it to lose alignment and to have a partial physical record at the end of the read buffer. This will result in length mismatches and checksum failures. When the log file is tailed for replication this will cause the log iterator to become invalid, necessitating the creation of a new iterator which will have to read the log file from scratch.

This diff fixes this issue by reading the remaining portion of the last block we read from. This is done when the reader is forced to read further (UnmarkEOF is called).

Test Plan:
- Added unit tests
- Stress test (with replication). Check dbdir/LOG file for corruptions.
- Test on test tier

Reviewers: emayanke, haobo, dhruba

Reviewed By: haobo

CC: vamsi, sheki, dhruba, kailiu, igor

Differential Revision: https://reviews.facebook.net/D15249/"
,,Rocksdb,"[CF] Options -> DBOptions

Summary: Replaced most of occurrences of Options with more specific DBOptions. This brings us very close to supporting different configuration options for each column family.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15933/[column families] Iterator and MultiGet

Summary: Support for different column families in Iterator and MultiGet code path.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15849/VersionEdit not to take NumLevels()

Summary:
I will submit a sequence of diffs that are preparing master branch for column families. There are a lot of implicit assumptions in the code that are making column family implementation hard. If I make the change only in column family branch, it will make merging back to master impossible.

Most of the diffs will be simple code refactorings, so I hope we can have fast turnaround time. Feel free to grab me in person to discuss any of them.

This diff removes number of level check from VersionEdit. It is used only when VersionEdit is read, not written, but has to be set when it is written. I believe it is a right thing to make VersionEdit dumb and check consistency on the caller side. This will also make it much easier to implement Column Families, since different column families can have different number of levels.

Test Plan: make check

Reviewers: dhruba, haobo, sdong, kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15159/[column families] Support to create and drop column families

Summary:
This diff provides basic implementations of CreateColumnFamily(), DropColumnFamily() and ListColumnFamilies(). It builds on top of https://reviews.facebook.net/D14733

It also includes a bug fix for DBImplReadOnly, where Get implementation would be redirected to DBImpl instead of DBImplReadOnly.

Test Plan: Added unit test

Reviewers: dhruba, haobo, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15021/[RocksDB] [Column Family] Interface proposal

Summary:
<This diff is for Column Family branch>

Sharing some of the work I've done so far. This diff compiles and passes the tests.

The biggest change is in options.h - I broke down Options into two parts - DBOptions and ColumnFamilyOptions. DBOptions is DB-specific (env, create_if_missing, block_cache, etc.) and ColumnFamilyOptions is column family-specific (all compaction options, compresion options, etc.). Note that this does not break backwards compatibility at all.

Further, I created DBWithColumnFamily which inherits DB interface and adds new functions with column family support. Clients can transparently switch to DBWithColumnFamily and it will not break their backwards compatibility.
There are few methods worth checking out: ListColumnFamilies(), MultiNewIterator(), MultiGet() and GetSnapshot(). [GetSnapshot() returns the snapshot across all column families for now - I think that's what we agreed on]

Finally, I made small changes to WriteBatch so we are able to atomically insert data across column families.

Please provide feedback.

Test Plan: make check works, the code is backward compatible

Reviewers: dhruba, haobo, sdong, kailiu, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14445/Introduce MergeContext to Lazily Initialize merge operand list

Summary: In get operations, merge_operands is only used in few cases. Lazily initialize it can reduce average latency in some cases

Test Plan: make all check

Reviewers: haobo, kailiu, dhruba

Reviewed By: haobo

CC: igor, nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D14415

Conflicts:
	db/db_impl.cc
	db/memtable.cc/"
Restructuring the code,"Thread management, Restructuring the code",Rocksdb,"[CF] Rethinking ColumnFamilyHandle and fix to dropping column families

Summary:
The change to the public behavior:
* When opening a DB or creating new column family client gets a ColumnFamilyHandle.
* As long as column family handle is alive, client can do whatever he wants with it, even drop it
* Dropped column family can still be read from (using the column family handle)
* Added a new call CloseColumnFamily(). Client has to close all column families that he has opened before deleting the DB
* As soon as column family is closed, any calls to DB using that column family handle will fail (also any outstanding calls)

Internally:
* Ref-counting ColumnFamilyData
* New thread-safety for ColumnFamilySet
* Dropped column families are now completely dropped and their memory cleaned-up

Test Plan: added some tests to column_family_test

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16101/preload table handle on Recover() when max_open_files == -1

Summary: This covers existing table files before DB open happens and avoids contention on table cache

Test Plan: db_test

Reviewers: haobo, sdong, igor, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16089/Support for LZ4 compression./use super_version in NewIterator() and MultiGet() function

Summary:
Use super_version insider NewIterator to avoid Ref() each component
separately under mutex
The new added bench shows NewIterator QPS increases from 515K to 719K
No meaningful improvement for multiget I guess due to its relatively small
cost comparing to 90 keys fetch in the test.

Test Plan: unit test and db_bench

Reviewers: igor, sdong

Reviewed By: igor

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D15609/VersionSet cleanup

Summary:
Removed icmp_ from VersionSet (since it's per-column-family, not per-DB-instance)
Unfriended VersionSet and ColumnFamilyData (yay!)
Removed VersionSet::NumberLevels()
Cleaned up DBImpl

Test Plan: make check

Reviewers: dhruba, haobo, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15819/LogAndApply to take ColumnFamilyData

Summary: This removes the default implementation of LogAndApply that applied the changed to the default column family by default. It is mostly simple reformatting.

Test Plan: make check

Reviewers: dhruba, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15465/Merge branch 'master' into columnfamilies

Conflicts:
	db/db_impl.cc
	db/db_impl.h
	db/db_impl_readonly.h
	db/db_test.cc
	include/rocksdb/db.h
	include/utilities/stackable_db.h/Tailing iterator

Summary:
This diff implements a special type of iterator that doesn't create a snapshot
(can be used to read newly inserted data) and is optimized for doing sequential
reads.

TailingIterator uses current superversion number to determine whether to
invalidate its internal iterators. If the version hasn't changed, it can often
avoid doing expensive seeks over immutable structures (sst files and immutable
memtables).

Test Plan:
* new unit tests
* running LD with this patch

Reviewers: igor, dhruba, haobo, sdong, kailiu

Reviewed By: sdong

CC: leveldb, lovro, march

Differential Revision: https://reviews.facebook.net/D15285/ColumnFamilySet

Summary:
I created a separate class ColumnFamilySet to keep track of column families. Before we did this in VersionSet and I believe this approach is cleaner.

Let me know if you have any comments. I will commit tomorrow.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15357/Refactor Recover() code

Summary:
This diff does two things:
* Rethinks how we call Recover() with read_only option. Before, we call it with pointer to memtable where we'd like to apply those changes to. This memtable is set in db_impl_readonly.cc and it's actually DBImpl::mem_. Why don't we just apply updates to mem_ right away? It seems more intuitive.
* Changes when we apply updates to manifest. Before, the process is to recover all the logs, flush it to sst files and then do one giant commit that atomically adds all recovered sst files and sets the next log number. This works good enough, but causes some small troubles for my column family approach, since I can't have one VersionEdit apply to more than single column family[1]. The change here is to commit the files recovered from logs right away. Here is the state of the world before the change:
1. Recover log 5, add new sst files to edit
2. Recover log 7, add new sst files to edit
3. Recover log 8, add new sst files to edit
4. Commit all added sst files to manifest and mark log files 5, 7 and 8 as recoverd (via SetLogNumber(9) function)
After the change, we'll do:
1. Recover log 5, commit the new sst files and set log 5 as recovered
2. Recover log 7, commit the new sst files and set log 7 as recovered
3. Recover log 8, commit the new sst files and set log 8 as recovered

The added (small) benefit is that if we fail after (2), the new recovery will only have to recover log 8. In previous case, we'll have to restart the recovery from the beginning. The bigger benefit will be to enable easier integration of multiple column families in Recovery code path.

[1] I'm happy to dicuss this decison, but I believe this is the cleanest way to go. It also makes backward compatibility much easier. We don't have a requirement of adding multiple column families atomically.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15237/An initial implementation of kCompactionStopStyleSimilarSize for universal compaction/Allow callback to change size of existing value. Change return type of the callback function to an enum status to handle 3 cases.

Summary:
This diff fixes 2 hacks:
* The callback function can modify the existing value inplace, if the merged value fits within the existing buffer size. But currently the existing buffer size is not being modified. Now the callback recieves a int* allowing the size to be modified. Since size is encoded as a varint in the internal key for memtable. It might happen that the entire value might have be copied to the new location if the new size varint is smaller than the existing size varint.
* The callback function has 3 functionalities
    1. Modify existing buffer inplace, and update size correspondingly. Now to indicate that, Returns 1.
    2. Generate a new buffer indicating merged value. Returns 2.
    3. Fails to do either of above, based on whatever application logic. Returns 0.

Test Plan: Just make all for now. I'm adding another unit test to test each scenario.

Reviewers: dhruba, haobo

Reviewed By: haobo

CC: leveldb, sdong, kailiu, xinyaohu, sumeet, danguo

Differential Revision: https://reviews.facebook.net/D15195/Fix CompactRange to apply filter to every key

Summary:
When doing CompactRange(), we should first flush the memtable and then calculate max_level_with_files. Also, we want to compact all the levels that have files, including level `max_level_with_files`.

This patch fixed the unit test.

Test Plan: Added a failing unit test and a fix, so it's not failing anymore.

Reviewers: dhruba, haobo, sdong

Reviewed By: haobo

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D14421/VersionEdit not to take NumLevels()

Summary:
I will submit a sequence of diffs that are preparing master branch for column families. There are a lot of implicit assumptions in the code that are making column family implementation hard. If I make the change only in column family branch, it will make merging back to master impossible.

Most of the diffs will be simple code refactorings, so I hope we can have fast turnaround time. Feel free to grab me in person to discuss any of them.

This diff removes number of level check from VersionEdit. It is used only when VersionEdit is read, not written, but has to be set when it is written. I believe it is a right thing to make VersionEdit dumb and check consistency on the caller side. This will also make it much easier to implement Column Families, since different column families can have different number of levels.

Test Plan: make check

Reviewers: dhruba, haobo, sdong, kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15159/BuildBatchGroup -- memcpy outside of lock

Summary: When building batch group, don't actually build a new batch since it requires heavy-weight mem copy and malloc. Only store references to the batches and build the batch group without lock held.

Test Plan:
`make check`

I am also planning to run performance tests. The workload that will benefit from this change is readwhilewriting. I will post the results once I have them.

Reviewers: dhruba, haobo, kailiu

Reviewed By: haobo

CC: leveldb, xjin

Differential Revision: https://reviews.facebook.net/D15063/Add read/modify/write functionality to Put() api

Summary: The application can set a callback function, which is applied on the previous value. And calculates the new value. This new value can be set, either inplace, if the previous value existed in memtable, and new value is smaller than previous value. Otherwise the new value is added normally.

Test Plan: fbmake. Added unit tests. All unit tests pass.

Reviewers: dhruba, haobo

Reviewed By: haobo

CC: sdong, kailiu, xinyaohu, sumeet, leveldb

Differential Revision: https://reviews.facebook.net/D14745/[Performance Branch] If options.max_open_files set to be -1, cache table readers in FileMetadata for Get() and NewIterator()

Summary:
In some use cases, table readers for all live files should always be cached. In that case, there will be an opportunity to avoid the table cache look-up while Get() and NewIterator().

We define options.max_open_files = -1 to be the mode that table readers for live files will always be kept. In that mode, table readers are cached in FileMetaData (with a reference count hold in table cache). So that when executing table_cache.Get() and table_cache.newInterator(), LRU cache checking can be by-passed, to reduce latency.

Test Plan: add a test case in db_test

Reviewers: haobo, kailiu

Reviewed By: haobo

CC: dhruba, igor, leveldb

Differential Revision: https://reviews.facebook.net/D15039/[Performance Branch] A Hashed Linked List Based Mem Table

Summary:
Implement a mem table, in which keys are hashed based on prefixes. In each bucket, entries are organized in a sorted linked list. It has the same thread safety guarantee as skip list.

The motivation is to optimize memory usage for the case that prefix hashing is primary way of seeking to the entry. Compared to hash skip list implementation, this implementation is more memory efficient, but inside each bucket, search is always linear. The target scenario is that there are only very limited number of records in each hash bucket.

Test Plan: Add a test case in db_test

Reviewers: haobo, kailiu, dhruba

Reviewed By: haobo

CC: igor, nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D14979/Fix the valgrind issues/Support multi-threaded DisableFileDeletions() and EnableFileDeletions()

Summary:
We don't want two threads to clash if they concurrently call DisableFileDeletions() and EnableFileDeletions(). I'm adding a counter that will enable file deletions only after all DisableFileDeletions() calls have been negated with EnableFileDeletions().

However, we also don't want to break the old behavior, so I added a parameter force to EnableFileDeletions(). If force is true, we will still enable file deletions after every call to EnableFileDeletions(), which is what is happening now.

Test Plan: make check

Reviewers: dhruba, haobo, sanketh

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14781/Merge branch 'master' into performance

Conflicts:
	db/db_impl.cc
	db/db_test.cc
	db/memtable.cc
	db/version_set.cc
	include/rocksdb/statistics.h/[RocksDB] [Performance Branch] Some Changes to PlainTable format

Summary:
Some changes to PlainTable format:
(1) support variable key length
(2) use user defined slice transformer to extract prefixes
(3) Run some test cases against PlainTable in db_test and table_test

Test Plan: test db_test

Reviewers: haobo, kailiu

CC: dhruba, igor, leveldb, nkg-

Differential Revision: https://reviews.facebook.net/D14457/[RocksDB] [Column Family] Interface proposal

Summary:
<This diff is for Column Family branch>

Sharing some of the work I've done so far. This diff compiles and passes the tests.

The biggest change is in options.h - I broke down Options into two parts - DBOptions and ColumnFamilyOptions. DBOptions is DB-specific (env, create_if_missing, block_cache, etc.) and ColumnFamilyOptions is column family-specific (all compaction options, compresion options, etc.). Note that this does not break backwards compatibility at all.

Further, I created DBWithColumnFamily which inherits DB interface and adds new functions with column family support. Clients can transparently switch to DBWithColumnFamily and it will not break their backwards compatibility.
There are few methods worth checking out: ListColumnFamilies(), MultiNewIterator(), MultiGet() and GetSnapshot(). [GetSnapshot() returns the snapshot across all column families for now - I think that's what we agreed on]

Finally, I made small changes to WriteBatch so we are able to atomically insert data across column families.

Please provide feedback.

Test Plan: make check works, the code is backward compatible

Reviewers: dhruba, haobo, sdong, kailiu, emayanke

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14445/[RocksDB] BackupableDB

Summary:
In this diff I present you BackupableDB v1. You can easily use it to backup your DB and it will do incremental snapshots for you.
Let's first describe how you would use BackupableDB. It's inheriting StackableDB interface so you can easily construct it with your DB object -- it will add a method RollTheSnapshot() to the DB object. When you call RollTheSnapshot(), current snapshot of the DB will be stored in the backup dir. To restore, you can just call RestoreDBFromBackup() on a BackupableDB (which is a static method) and it will restore all files from the backup dir. In the next version, it will even support automatic backuping every X minutes.

There are multiple things you can configure:
1. backup_env and db_env can be different, which is awesome because then you can easily backup to HDFS or wherever you feel like.
2. sync - if true, it *guarantees* backup consistency on machine reboot
3. number of snapshots to keep - this will keep last N snapshots around if you want, for some reason, be able to restore from an earlier snapshot. All the backuping is done in incremental fashion - if we already have 00010.sst, we will not copy it again. *IMPORTANT* -- This is based on assumption that 00010.sst never changes - two files named 00010.sst from the same DB will always be exactly the same. Is this true? I always copy manifest, current and log files.
4. You can decide if you want to flush the memtables before you backup, or you're fine with backing up the log files -- either way, you get a complete and consistent view of the database at a time of backup.
5. More things you can find in BackupableDBOptions

Here is the directory structure I use:

   backup_dir/CURRENT_SNAPSHOT - just 4 bytes holding the latest snapshot
               0, 1, 2, ... - files containing serialized version of each snapshot - containing a list of files
               files/*.sst - sst files shared between snapshots - if one snapshot references 00010.sst and another one needs to backup it from the DB, it will just reference the same file
               files/ 0/, 1/, 2/, ... - snapshot directories containing private snapshot files - current, manifest and log files

All the files are ref counted and deleted immediatelly when they get out of scope.

Some other stuff in this diff:
1. Added GetEnv() method to the DB. Discussed with @haobo and we agreed that it seems right thing to do.
2. Fixed StackableDB interface. The way it was set up before, I was not able to implement BackupableDB.

Test Plan:
I have a unittest, but please don't look at this yet. I just hacked it up to help me with debugging. I will write a lot of good tests and update the diff.

Also, `make asan_check`

Reviewers: dhruba, haobo, emayanke

Reviewed By: dhruba

CC: leveldb, haobo

Differential Revision: https://reviews.facebook.net/D14295/Make GetDbIdentity pure virtual and also implement it for StackableDB, DBWithTTL

Summary: As title

Test Plan: make clean and make

Reviewers: igor

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14469/Killing Transform Rep

Summary:
Let's get rid of TransformRep and it's children. We have confirmed that HashSkipListRep works better with multifeed, so there is no benefit to keeping this around.

This diff is mostly just deleting references to obsoleted functions. I also have a diff for fbcode that we'll need to push when we switch to new release.

I had to expose HashSkipListRepFactory in the client header files because db_impl.cc needs access to GetTransform() function for SanitizeOptions.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14397/Don't do compression tests if we don't have compression libs

Summary: These tests fail if compression libraries are not installed.

Test Plan: Manually disabled snappy, observed tests not ran.

Reviewers: dhruba, kailiu

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14379/DB::GetOptions()

Summary: We need access to options for BackupableDB

Test Plan: make check

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb, reconnect.grayhat

Differential Revision: https://reviews.facebook.net/D14331/[RocksDB] Interface changes required for BackupableDB

Summary: This is part of https://reviews.facebook.net/D14295 -- smaller diff that is easier to review

Test Plan: make asan_check

Reviewers: dhruba, haobo, emayanke

Reviewed By: emayanke

CC: leveldb, kailiu, reconnect.grayhat

Differential Revision: https://reviews.facebook.net/D14301/Allow users to profile a query and see bottleneck of the query

Summary:
Provide a framework to profile a query in detail to figure out latency bottleneck. Currently, in Get(), Put() and iterators, 2-3 simple timing is used. We can easily add more profile counters to the framework later.

Test Plan: Enable this profiling in seveal existing tests.

Reviewers: haobo, dhruba, kailiu, emayanke, vamsi, igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14001

Conflicts:
	table/merger.cc/Upgrading compiler to gcc4.8.1

Summary:
Finally did it - the trick was in using --dynamic-linker option. This is first step to running ASAN.

All of our code seems to compile just fine on 4.8.1. However, I still left fbcode.471.sh in the 'build_tools/' just in case.

Test Plan: make clean; make

Reviewers: dhruba, haobo, kailiu, emayanke, sdong

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14109/"
Memory management,Memory management,Rocksdb,"[CF] Rethinking ColumnFamilyHandle and fix to dropping column families

Summary:
The change to the public behavior:
* When opening a DB or creating new column family client gets a ColumnFamilyHandle.
* As long as column family handle is alive, client can do whatever he wants with it, even drop it
* Dropped column family can still be read from (using the column family handle)
* Added a new call CloseColumnFamily(). Client has to close all column families that he has opened before deleting the DB
* As soon as column family is closed, any calls to DB using that column family handle will fail (also any outstanding calls)

Internally:
* Ref-counting ColumnFamilyData
* New thread-safety for ColumnFamilySet
* Dropped column families are now completely dropped and their memory cleaned-up

Test Plan: added some tests to column_family_test

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16101/preload table handle on Recover() when max_open_files == -1

Summary: This covers existing table files before DB open happens and avoids contention on table cache

Test Plan: db_test

Reviewers: haobo, sdong, igor, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16089/Merge branch 'master' into columnfamilies

Conflicts:
	HISTORY.md
	db/db_impl.cc
	db/db_impl.h
	db/db_iter.cc
	db/db_test.cc
	db/dbformat.h
	db/memtable.cc
	db/memtable_list.cc
	db/memtable_list.h
	db/table_cache.cc
	db/table_cache.h
	db/version_edit.h
	db/version_set.cc
	db/version_set.h
	db/write_batch.cc
	db/write_batch_test.cc
	include/rocksdb/options.h
	util/options.cc/[CF] Options -> DBOptions

Summary: Replaced most of occurrences of Options with more specific DBOptions. This brings us very close to supporting different configuration options for each column family.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15933/[CF] Rethink table cache

Summary:
Adapting table cache to column families is interesting. We want table cache to be global LRU, so if some column families are use not as often as others, we want them to be evicted from cache. However, current TableCache object also constructs tables on its own. If table is not found in the cache, TableCache automatically creates new table. We want each column family to be able to specify different table factory.

To solve the problem, we still have a single LRU, but we provide the LRUCache object to TableCache on construction. We have one TableCache per column family, but the underyling cache is shared by all TableCache objects.

This allows us to have a global LRU, but still be able to support different table factories for different column families. Also, in the future it will also be able to support different directories for different column families.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15915/[CF] Move InternalStats to ColumnFamilyData

Summary: InternalStats is a messy thing, keeping both DB data and column family data. However, it's better off living in ColumnFamilyData than in DBImpl. For now, at least.

Test Plan: make check

Reviewers: dhruba, kailiu, haobo, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15879/VersionSet cleanup

Summary:
Removed icmp_ from VersionSet (since it's per-column-family, not per-DB-instance)
Unfriended VersionSet and ColumnFamilyData (yay!)
Removed VersionSet::NumberLevels()
Cleaned up DBImpl

Test Plan: make check

Reviewers: dhruba, haobo, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15819/[Performance Branch] PlainTable to encode rows with seqID 0, value type using 1 internal byte.

Summary: In PlainTable, use one single byte to represent 8 bytes of internal bytes, if seqID = 0 and it is value type (which should be common for bottom most files). It is to save 7 bytes for uncompressed cases.

Test Plan: make all check

Reviewers: haobo, dhruba, kailiu

Reviewed By: haobo

CC: igor, leveldb

Differential Revision: https://reviews.facebook.net/D15489/Compacting column families

Summary: This diff enables non-default column families to get compacted both automatically and also by calling CompactRange()

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15813/Fix reduce_levels_test/Move compaction picker and internal key comparator to ColumnFamilyData

Summary: Compaction picker and internal key comparator are different for each column family (not global), so they should live in ColumnFamilyData

Test Plan: make check

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15801/Enable flushing memtables from arbitrary column families

Summary: Removed default_cfd_ from all flush code paths. This means we can now flush memtables from arbitrary column families!

Test Plan: Added a new unit test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15789/Change ColumnFamilyData from struct to class

Summary: ColumnFamilyData grew a lot, there's much more data that it holds now. It makes more sense to encapsulate it better by making it a class.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15579/Fsync directory after we create a new file

Summary:
@dhruba, I'm not sure where we need to sync the directory. I implemented the function in Env() and added the dir sync just after we close the newly created file in the builder.

Should I also add FsyncDir() to new files that get created by a compaction?

Test Plan: Confirmed that FsyncDir is returning Status::OK()

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D14751/Merge branch 'master' into columnfamilies

Conflicts:
	db/version_set.cc
	db/version_set_reduce_num_levels.cc
	util/ldb_cmd.cc/Make VersionSet::ReduceNumberOfLevels() static

Summary:
A lot of our code implicitly assumes number_levels to be static. ReduceNumberOfLevels() breaks that assumption. For example, after calling ReduceNumberOfLevels(), DBImpl::NumberLevels() will be different from VersionSet::NumberLevels(). This is dangerous. Thankfully, it's not in public headers and is only used from LDB cmd tool. LDB tool is only using it statically, i.e. it never calls it with running DB instance. With this diff, we make it explicitly static. This way, we can assume number_levels to be immutable and not break assumption that lot of our code is relying upon. LDB tool can still use the method.

Also, I removed the method from a separate file since it breaks filename completition. version_se<TAB> now completes to ""version_set."" instead of ""version_set"" (without the dot). I don't see a big reason that the function should be in a different file.

Test Plan: reduce_levels_test

Reviewers: dhruba, haobo, kailiu, sdong

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15303/Merge branch 'master' into performance

Conflicts:
	db/db_impl.cc
	db/db_test.cc
	db/memtable.cc
	db/version_set.cc
	include/rocksdb/statistics.h
	util/statistics_imp.h/ColumnFamilySet

Summary:
I created a separate class ColumnFamilySet to keep track of column families. Before we did this in VersionSet and I believe this approach is cleaner.

Let me know if you have any comments. I will commit tomorrow.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15357/Merge branch 'master' into columnfamilies

Conflicts:
	db/db_impl.cc
	db/db_impl_readonly.cc
	db/db_test.cc
	db/version_edit.cc
	db/version_edit.h
	db/version_set.cc
	db/version_set.h
	db/version_set_reduce_num_levels.cc/Move more functions from VersionSet to Version

Summary:
This moves functions:
* VersionSet::Finalize() -> Version::UpdateCompactionStats()
* VersionSet::UpdateFilesBySize() -> Version::UpdateFilesBySize()

The diff depends on D15189, D15183 and D15171

Test Plan: make check

Reviewers: kailiu, sdong, haobo, dhruba

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15201/Move functions from VersionSet to Version

Summary:
There were some functions in VersionSet that had no reason to be there instead of Version. Moving them to Version will make column families implementation easier.

The functions moved are:
* NumLevelBytes
* LevelSummary
* LevelFileSummary
* MaxNextLevelOverlappingBytes
* AddLiveFiles (previously AddLiveFilesCurrentVersion())
* NeedSlowdownForNumLevel0Files

The diff continues on (and depends on) D15171

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong, emayanke

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15183/Decrease reliance on VersionSet::NumberLevels()

Summary:
With column families VersionSet will not have a constant number of levels (each CF can have different options), so we'll need to eliminate call to VersionSet::NumberLevels()

This diff decreases number of callsites, but we're not there yet. It associates number of levels with Version (each version is associated with single CF) instead of VersionSet.

I have also slightly changed how VersionSet keeps track of manifest size.

This diff also modifies constructor of Compaction such that it takes input_version and automatically Ref()s it. Before this was done outside of constructor.

In next diffs I will continue to decrease number of callsites of VersionSet::NumberLevels() and also references to current_

Test Plan: make check

Reviewers: haobo, dhruba, kailiu, sdong

Reviewed By: sdong

Differential Revision: https://reviews.facebook.net/D15171/VersionEdit not to take NumLevels()

Summary:
I will submit a sequence of diffs that are preparing master branch for column families. There are a lot of implicit assumptions in the code that are making column family implementation hard. If I make the change only in column family branch, it will make merging back to master impossible.

Most of the diffs will be simple code refactorings, so I hope we can have fast turnaround time. Feel free to grab me in person to discuss any of them.

This diff removes number of level check from VersionEdit. It is used only when VersionEdit is read, not written, but has to be set when it is written. I believe it is a right thing to make VersionEdit dumb and check consistency on the caller side. This will also make it much easier to implement Column Families, since different column families can have different number of levels.

Test Plan: make check

Reviewers: dhruba, haobo, sdong, kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15159/[column families] Get rid of VersionSet::current_ and keep current Version for each column family

Summary:
The biggest change here is getting rid of current_ Version and adding a column_family_data->current Version to each column family.

I have also fixed some smaller things in VersionSet that made it easier to implement Column family support.

Test Plan: make check

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15105/[Performance Branch] If options.max_open_files set to be -1, cache table readers in FileMetadata for Get() and NewIterator()

Summary:
In some use cases, table readers for all live files should always be cached. In that case, there will be an opportunity to avoid the table cache look-up while Get() and NewIterator().

We define options.max_open_files = -1 to be the mode that table readers for live files will always be kept. In that mode, table readers are cached in FileMetaData (with a reference count hold in table cache). So that when executing table_cache.Get() and table_cache.newInterator(), LRU cache checking can be by-passed, to reduce latency.

Test Plan: add a test case in db_test

Reviewers: haobo, kailiu

Reviewed By: haobo

CC: dhruba, igor, leveldb

Differential Revision: https://reviews.facebook.net/D15039/[column families] Implement DB::OpenWithColumnFamilies()

Summary:
In addition to implementing OpenWithColumnFamilies, this diff also includes some minor changes:
* Changed all column family names from Slice() to std::string. The performance of column family name handling is not critical, and it's more convenient and cleaner to have names as std::strings
* Implemented ColumnFamilyOptions(const Options&) and DBOptions(const Options&)
* Added ColumnFamilyOptions to VersionSet::ColumnFamilyData. ColumnFamilyOptions are specified on OpenWithColumnFamilies() and CreateColumnFamily()

I will keep the diff in the Phabricator for a day or two and will push to the branch then. Feel free to comment even after the diff has been pushed.

Test Plan: Added a simple unit test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15033/[column families] Support to create and drop column families

Summary:
This diff provides basic implementations of CreateColumnFamily(), DropColumnFamily() and ListColumnFamilies(). It builds on top of https://reviews.facebook.net/D14733

It also includes a bug fix for DBImplReadOnly, where Get implementation would be redirected to DBImpl instead of DBImplReadOnly.

Test Plan: Added unit test

Reviewers: dhruba, haobo, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15021/Some minor refactoring on the code

Summary: I made some cleanup while reading the source code in `db`. Most changes are about style, naming or C++ 11 new features.

Test Plan: ran `make check`

Reviewers: haobo, dhruba, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15009/Introduce MergeContext to Lazily Initialize merge operand list

Summary: In get operations, merge_operands is only used in few cases. Lazily initialize it can reduce average latency in some cases

Test Plan: make all check

Reviewers: haobo, kailiu, dhruba

Reviewed By: haobo

CC: igor, nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D14415

Conflicts:
	db/db_impl.cc
	db/memtable.cc/[RocksDB Performance Branch] Avoid sorting in Version::Get() by presorting them in VersionSet::Builder::SaveTo()

Summary: Pre-sort files in VersionSet::Builder::SaveTo() so that when getting the value, no need to sort them. It can avoid the costs of vector operations and sorting in Version::Get().

Test Plan: make all check

Reviewers: haobo, kailiu, dhruba

Reviewed By: dhruba

CC: nkg-, igor, leveldb

Differential Revision: https://reviews.facebook.net/D14409/[RocksDB Performance Branch] Introduce MergeContext to Lazily Initialize merge operand list

Summary: In get operations, merge_operands is only used in few cases. Lazily initialize it can reduce average latency in some cases

Test Plan: make all check

Reviewers: haobo, kailiu, dhruba

Reviewed By: haobo

CC: igor, nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D14415/During benchmarking, I see excessive use of vector.reserve().

Summary:
This code path can potentially accumulate multiple important_files for level 0.
But for other levels, it should have only one file in the
important_files, so it is ok not to reserve excessive space, is it not?

Test Plan: make check

Reviewers: haobo

Reviewed By: haobo

CC: reconnect.grayhat, leveldb

Differential Revision: https://reviews.facebook.net/D14349/[RocksDB] Use raw pointer instead of shared pointer when passing Statistics object internally

Summary: liveness of the statistics object is already ensured by the shared pointer in DB options. There's no reason to pass again shared pointer among internal functions. Raw pointer is sufficient and efficient.

Test Plan: make check

Reviewers: dhruba, MarkCallaghan, igor

Reviewed By: dhruba

CC: leveldb, reconnect.grayhat

Differential Revision: https://reviews.facebook.net/D14289/"
,,Rocksdb,"Use ASSERT_EQ() instead of assert() in merge_test/Improve RocksDB ""get"" performance by computing merge result in memtable

Summary:
Added an option (max_successive_merges) that can be used to specify the
maximum number of successive merge operations on a key in the memtable.
This can be used to improve performance of the ""get"" operation. If many
successive merge operations are performed on a key, the performance of ""get""
operations on the key deteriorates, as the value has to be computed for each
""get"" operation by applying all the successive merge operations.

FB Task ID: #3428853

Test Plan:
make all check
db_bench --benchmarks=readrandommergerandom
counter_stress_test

Reviewers: haobo, vamsi, dhruba, sdong

Reviewed By: haobo

CC: zshao

Differential Revision: https://reviews.facebook.net/D14991/"
,,Rocksdb,"CompactionPicker

Summary:
This is a big one. This diff moves all the code related to picking compactions from VersionSet to new class CompactionPicker. Column families' compactions will be completely separate processes, so we need to have multiple CompactionPickers.

To make this easier to review, most of the code change is just copy/paste. There is also a small change not to use VersionSet::current_, but rather to take `Version* version` as a parameter. Most of the other code is exactly the same.

In future diffs, I will also make some improvements to CompactionPickers. I think the most important part will be encapsulating it better. Currently Version, VersionSet, Compaction and CompactionPicker are all friend classes, which makes it harder to change the implementation.

This diff depends on D15171, D15183, D15189 and D15201

Test Plan: `make check`

Reviewers: kailiu, sdong, dhruba, haobo

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15207/Decrease reliance on VersionSet::NumberLevels()

Summary:
With column families VersionSet will not have a constant number of levels (each CF can have different options), so we'll need to eliminate call to VersionSet::NumberLevels()

This diff decreases number of callsites, but we're not there yet. It associates number of levels with Version (each version is associated with single CF) instead of VersionSet.

I have also slightly changed how VersionSet keeps track of manifest size.

This diff also modifies constructor of Compaction such that it takes input_version and automatically Ref()s it. Before this was done outside of constructor.

In next diffs I will continue to decrease number of callsites of VersionSet::NumberLevels() and also references to current_

Test Plan: make check

Reviewers: haobo, dhruba, kailiu, sdong

Reviewed By: sdong

Differential Revision: https://reviews.facebook.net/D15171/VersionEdit not to take NumLevels()

Summary:
I will submit a sequence of diffs that are preparing master branch for column families. There are a lot of implicit assumptions in the code that are making column family implementation hard. If I make the change only in column family branch, it will make merging back to master impossible.

Most of the diffs will be simple code refactorings, so I hope we can have fast turnaround time. Feel free to grab me in person to discuss any of them.

This diff removes number of level check from VersionEdit. It is used only when VersionEdit is read, not written, but has to be set when it is written. I believe it is a right thing to make VersionEdit dumb and check consistency on the caller side. This will also make it much easier to implement Column Families, since different column families can have different number of levels.

Test Plan: make check

Reviewers: dhruba, haobo, sdong, kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15159/"
,,Rocksdb,"[CF] Propagate correct options to WriteBatch::InsertInto

Summary:
WriteBatch can have multiple column families in one batch. Every column family has different options. So we have to add a way for write batch to get options for an arbitrary column family.

This required a bit more acrobatics since lots of interfaces had to be changed.

Test Plan: make check

Reviewers: dhruba, haobo, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15957/"
,,Rocksdb,"Get rid of some shared_ptrs

Summary:
I went through all remaining shared_ptrs and removed the ones that I found not-necessary. Only GenerateCachePrefix() is called fairly often, so don't expect much perf wins.

The ones that are left are accessed infrequently and I think we're fine with keeping them.

Test Plan: make asan_check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14427/Allow users to profile a query and see bottleneck of the query

Summary:
Provide a framework to profile a query in detail to figure out latency bottleneck. Currently, in Get(), Put() and iterators, 2-3 simple timing is used. We can easily add more profile counters to the framework later.

Test Plan: Enable this profiling in seveal existing tests.

Reviewers: haobo, dhruba, kailiu, emayanke, vamsi, igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14001

Conflicts:
	table/merger.cc/Inline a couple of functions and put one save lazily clearing

Summary:
Machine several functions inline.
Also, in DBIter.Seek() make value cleaning up lazily done.
These are for the use case that Seek() are called lots of times but few return values.

Test Plan: make all check

Differential Revision: https://reviews.facebook.net/D14217/"
,,Rocksdb,"[CF] Propagate correct options to WriteBatch::InsertInto

Summary:
WriteBatch can have multiple column families in one batch. Every column family has different options. So we have to add a way for write batch to get options for an arbitrary column family.

This required a bit more acrobatics since lots of interfaces had to be changed.

Test Plan: make check

Reviewers: dhruba, haobo, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15957/[CF] Rethink table cache

Summary:
Adapting table cache to column families is interesting. We want table cache to be global LRU, so if some column families are use not as often as others, we want them to be evicted from cache. However, current TableCache object also constructs tables on its own. If table is not found in the cache, TableCache automatically creates new table. We want each column family to be able to specify different table factory.

To solve the problem, we still have a single LRU, but we provide the LRUCache object to TableCache on construction. We have one TableCache per column family, but the underyling cache is shared by all TableCache objects.

This allows us to have a global LRU, but still be able to support different table factories for different column families. Also, in the future it will also be able to support different directories for different column families.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15915/"
,,Rocksdb,"[CF] Propagate correct options to WriteBatch::InsertInto

Summary:
WriteBatch can have multiple column families in one batch. Every column family has different options. So we have to add a way for write batch to get options for an arbitrary column family.

This required a bit more acrobatics since lots of interfaces had to be changed.

Test Plan: make check

Reviewers: dhruba, haobo, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15957/[column families] Move memtable and immutable memtable list to column family data

Summary: All memtables and immutable memtables are moved from DBImpl to ColumnFamilyData. For now, they are all referenced from default column family in DBImpl. It shouldn't be hard to get them from custom column family.

Test Plan: make check

Reviewers: dhruba, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15459/Add column family information to WAL

Summary:
I have added three new value types:
* kTypeColumnFamilyDeletion
* kTypeColumnFamilyValue
* kTypeColumnFamilyMerge
which include column family Varint32 before the data (value, deletion and merge). These values are used only in WAL (not in memtables yet).

This endeavour required changing some WriteBatch internals.

Test Plan: Added a unittest

Reviewers: dhruba, haobo, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15045/Get rid of some shared_ptrs

Summary:
I went through all remaining shared_ptrs and removed the ones that I found not-necessary. Only GenerateCachePrefix() is called fairly often, so don't expect much perf wins.

The ones that are left are accessed infrequently and I think we're fine with keeping them.

Test Plan: make asan_check

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14427/"
,,Rocksdb,"VersionEdit not to take NumLevels()

Summary:
I will submit a sequence of diffs that are preparing master branch for column families. There are a lot of implicit assumptions in the code that are making column family implementation hard. If I make the change only in column family branch, it will make merging back to master impossible.

Most of the diffs will be simple code refactorings, so I hope we can have fast turnaround time. Feel free to grab me in person to discuss any of them.

This diff removes number of level check from VersionEdit. It is used only when VersionEdit is read, not written, but has to be set when it is written. I believe it is a right thing to make VersionEdit dumb and check consistency on the caller side. This will also make it much easier to implement Column Families, since different column families can have different number of levels.

Test Plan: make check

Reviewers: dhruba, haobo, sdong, kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15159/[RocksDB] Support for column families in manifest

Summary:
<This diff is for Column Family branch>

Added fields in manifest file to support adding and deleting column families.

Pretty simple change, each version edit record can be:
1. add column family
2. drop column family
3. add and delete N files from a single column family (compactions and flushes will generate such records)

Test Plan: make check works, the code is backward compatible

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14733/"
Restructuring the code,"Restructuring the code, thread management",Rocksdb,"Reduce malloc of iterators in Get() code paths

Summary:
This patch optimized Get() code paths by avoiding malloc of iterators. Iterator creation is moved to mem table rep implementations, where a callback is called when any key is found. This is the same practice as what we do in (SST) table readers.

db_bench result for readrandom following a writeseq, with no compression, single thread and tmpfs, we see throughput improved to 144958 from 139027, about 3%.

Test Plan: make all check

Reviewers: dhruba, haobo, igor

Reviewed By: haobo

CC: leveldb, yhchiang

Differential Revision: https://reviews.facebook.net/D14685/Fix some 32-bit compile errors

Summary: RocksDB doesn't compile on 32-bit architecture apparently. This is attempt to fix some of 32-bit errors. They are reported here: https://gist.github.com/paxos/8789697

Test Plan: RocksDB still compiles on 64-bit :)

Reviewers: kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15825/Clean up arena API

Summary:
Easy thing goes first. This patch moves arena to internal dir; based
on which, the coming patch will deal with memtable_rep.

Test Plan: make check

Reviewers: haobo, sdong, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15615/[column families] Move memtable and immutable memtable list to column family data

Summary: All memtables and immutable memtables are moved from DBImpl to ColumnFamilyData. For now, they are all referenced from default column family in DBImpl. It shouldn't be hard to get them from custom column family.

Test Plan: make check

Reviewers: dhruba, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15459/[Performance Branch] HashLinkList to avoid to convert length prefixed string back to internal keys

Summary: Converting from length prefixed buffer back to internal key costs some CPU but it is not necessary. In this patch, internal keys are pass though the functions so that we don't need to convert back to it.

Test Plan: make all check

Reviewers: haobo, kailiu

Reviewed By: kailiu

CC: igor, leveldb

Differential Revision: https://reviews.facebook.net/D15393/[Performance Branch] Fix a bug when merging from master

Summary: Commit ""1304d8c8cefe66be1a3caa5e93413211ba2486f2"" (Merge branch 'master' into performance) removes a line in performance branch by mistake. This patch fixes it.

Test Plan: make all check

Reviewers: haobo, kailiu, igor

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15297/Misc cleanup on performance branch

Summary:

Did some trivial stuffs:

* Add more comments;
* fix compiler's warning messages (uninitialized variables).
* etc

Test Plan:

make check/Allow callback to change size of existing value. Change return type of the callback function to an enum status to handle 3 cases.

Summary:
This diff fixes 2 hacks:
* The callback function can modify the existing value inplace, if the merged value fits within the existing buffer size. But currently the existing buffer size is not being modified. Now the callback recieves a int* allowing the size to be modified. Since size is encoded as a varint in the internal key for memtable. It might happen that the entire value might have be copied to the new location if the new size varint is smaller than the existing size varint.
* The callback function has 3 functionalities
    1. Modify existing buffer inplace, and update size correspondingly. Now to indicate that, Returns 1.
    2. Generate a new buffer indicating merged value. Returns 2.
    3. Fails to do either of above, based on whatever application logic. Returns 0.

Test Plan: Just make all for now. I'm adding another unit test to test each scenario.

Reviewers: dhruba, haobo

Reviewed By: haobo

CC: leveldb, sdong, kailiu, xinyaohu, sumeet, danguo

Differential Revision: https://reviews.facebook.net/D15195/Merge branch 'master' into performance

Conflicts:
	Makefile
	db/db_impl.cc
	db/db_impl.h
	db/db_test.cc
	db/memtable.cc
	db/memtable.h
	db/version_edit.h
	db/version_set.cc
	include/rocksdb/options.h
	util/hash_skiplist_rep.cc
	util/options.cc/Add read/modify/write functionality to Put() api

Summary: The application can set a callback function, which is applied on the previous value. And calculates the new value. This new value can be set, either inplace, if the previous value existed in memtable, and new value is smaller than previous value. Otherwise the new value is added normally.

Test Plan: fbmake. Added unit tests. All unit tests pass.

Reviewers: dhruba, haobo

Reviewed By: haobo

CC: sdong, kailiu, xinyaohu, sumeet, leveldb

Differential Revision: https://reviews.facebook.net/D14745/Improve RocksDB ""get"" performance by computing merge result in memtable

Summary:
Added an option (max_successive_merges) that can be used to specify the
maximum number of successive merge operations on a key in the memtable.
This can be used to improve performance of the ""get"" operation. If many
successive merge operations are performed on a key, the performance of ""get""
operations on the key deteriorates, as the value has to be computed for each
""get"" operation by applying all the successive merge operations.

FB Task ID: #3428853

Test Plan:
make all check
db_bench --benchmarks=readrandommergerandom
counter_stress_test

Reviewers: haobo, vamsi, dhruba, sdong

Reviewed By: haobo

CC: zshao

Differential Revision: https://reviews.facebook.net/D14991/[RocksDB] [Performance Branch] Added dynamic bloom, to be used for memable non-existing key filtering

Summary: as title

Test Plan: dynamic_bloom_test

Reviewers: dhruba, sdong, kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14385/[RocksDB Performance Branch] Introduce MergeContext to Lazily Initialize merge operand list

Summary: In get operations, merge_operands is only used in few cases. Lazily initialize it can reduce average latency in some cases

Test Plan: make all check

Reviewers: haobo, kailiu, dhruba

Reviewed By: haobo

CC: igor, nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D14415/[RocksDB] Use raw pointer instead of shared pointer when passing Statistics object internally

Summary: liveness of the statistics object is already ensured by the shared pointer in DB options. There's no reason to pass again shared pointer among internal functions. Raw pointer is sufficient and efficient.

Test Plan: make check

Reviewers: dhruba, MarkCallaghan, igor

Reviewed By: dhruba

CC: leveldb, reconnect.grayhat

Differential Revision: https://reviews.facebook.net/D14289/Allow users to profile a query and see bottleneck of the query

Summary:
Provide a framework to profile a query in detail to figure out latency bottleneck. Currently, in Get(), Put() and iterators, 2-3 simple timing is used. We can easily add more profile counters to the framework later.

Test Plan: Enable this profiling in seveal existing tests.

Reviewers: haobo, dhruba, kailiu, emayanke, vamsi, igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14001

Conflicts:
	table/merger.cc/"
,,Rocksdb,"Start DeleteFileTest with clean plate

Summary:
Remove all the files from the test dir before the test. The test failed when there were some old files still in the directory, since it checks the file counts.
This is what caused jenkins' test failures. It was running fine on my machine so it was hard to repro.

Test Plan:
1. create an extra 000001.log file in the test directory
2. run a ./deletefile_test - test failes
3. patch ./deletefile_test with this
4. test succeeds

Reviewers: haobo, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14097/PurgeObsoleteFiles() unittest

Summary:
Created a unittest that verifies that automatic deletion performed by PurgeObsoleteFiles() works correctly.

Also, few small fixes on the logic part -- call version_set_->GetObsoleteFiles() in FindObsoleteFiles() instead of on some arbitrary positions.

Test Plan: Created a unit test

Reviewers: dhruba, haobo, nkg-

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D14079/"
Memory management,Memory management,Rocksdb,"[CF] Rethinking ColumnFamilyHandle and fix to dropping column families

Summary:
The change to the public behavior:
* When opening a DB or creating new column family client gets a ColumnFamilyHandle.
* As long as column family handle is alive, client can do whatever he wants with it, even drop it
* Dropped column family can still be read from (using the column family handle)
* Added a new call CloseColumnFamily(). Client has to close all column families that he has opened before deleting the DB
* As soon as column family is closed, any calls to DB using that column family handle will fail (also any outstanding calls)

Internally:
* Ref-counting ColumnFamilyData
* New thread-safety for ColumnFamilySet
* Dropped column families are now completely dropped and their memory cleaned-up

Test Plan: added some tests to column_family_test

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16101/Change ColumnFamilyData from struct to class

Summary: ColumnFamilyData grew a lot, there's much more data that it holds now. It makes more sense to encapsulate it better by making it a class.

Test Plan: make check

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15579/"
,,Rocksdb,Rename leveldb to rocksdb in C api/
compression tasks,Compression tasks,Rocksdb,Support for LZ4 compression./
,,Rocksdb,"Fix SIGFAULT when running sst_dump on v2.6 db

Summary: Fix the sigfault when running sst_dump on v2.6 db.

Test Plan:
    git checkout bba6595b1f3f42cf79bb21c2d5b981ede1cc0063
    make clean
    make db_bench
    ./db_bench --db=/tmp/some/db --benchmarks=fillseq
    arc patch D18039
    make clean
    make sst_dump
    ./sst_dump --file=/tmp/some/db --command=check

Reviewers: igor, haobo, sdong

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18039/Consolidate SliceTransform object ownership

Summary:
(1) Fix SanitizeOptions() to also check HashLinkList. The current
dynamic case just happens to work because the 2 classes have the same
layout.
(2) Do not delete SliceTransform object in HashSkipListFactory and
HashLinkListFactory destructor. Reason: SanitizeOptions() enforces
prefix_extractor and SliceTransform to be the same object when
Hash**Factory is used. This makes the behavior strange: when
Hash**Factory is used, prefix_extractor will be released by RocksDB. If
other memtable factory is used, prefix_extractor should be released by
user.

Test Plan: db_bench && make asan_check

Reviewers: haobo, igor, sdong

Reviewed By: igor

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D16587/"
,Thread management,Rocksdb,"Make RocksDB work with newer gflags

Summary:
Newer gflags switched from `google` namespace to `gflags` namespace. See: https://github.com/facebook/rocksdb/issues/139 and https://github.com/facebook/rocksdb/issues/102

Unfortunately, they don't define any macro with their namespace, so we need to actually try to compile gflags with two different namespace to figure out which one is the correct one.

Test Plan: works in fbcode environemnt. I'll also try in ubutnu with newer gflags

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18537/Initialize verification_failed in db_stress/Improve stability of db_stress

Summary:
Currently, whenever DB Verification fails we bail out by calling `exit(1)`. This is kind of bad since it causes unclean shutdown and spew of error log messages like:

    05:03:27 pthread lock: Invalid argument
    05:03:27 pthread lock: Invalid argument
    05:03:27 pthread lock: Invalid argument
    05:03:27 pthread lock: Invalid argument
    05:03:27 pthread lock: Invalid argument
    05:03:27 pthread lock: Invalid argument
    05:03:27 pthread lock: Invalid argument
    05:03:27 pthread lock: Invalid argument
    05:03:27 pthread lock: Invalid argument

This diff adds a new parameter that is set to true when verification fails. It can then use the parameter to bail out safely.

Test Plan: Casued artificail failure. Verified that exit was clean.

Reviewers: dhruba, haobo, ljin

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18243/No prefix iterator in db_stress

Summary: We're trying to deprecate prefix iterators, so no need to test them in db_stress

Test Plan: ran it

Reviewers: ljin

Reviewed By: ljin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16917/Change WriteBatch interface/Merge branch 'master' into columnfamilies

Conflicts:
	db/db_impl.cc
	db/db_test.cc
	tools/db_stress.cc/DBStress cleanup

Summary:
*) fixed the comment
*) constant 1 was not casted to 64-bit, which (I think) might cause overflow if we shift it too much
*) default prefix size to be 7, like it was before

Test Plan: compiled

Reviewers: ljin

Reviewed By: ljin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16827/make assert based on FLAGS_prefix_size

Summary: as title

Test Plan: running python tools/db_crashtest.py

Reviewers: igor

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16803/fix db_stress test

Summary: Fix the db_stress test, let is run with HashSkipList for real

Test Plan:
python tools/db_crashtest.py
python tools/db_crashtest2.py

Reviewers: igor, haobo

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16773/initialize static const outside of class/[CF] db_stress for column families

Summary:
I had this diff for a while to test column families implementation. Last night, I ran it sucessfully for 10 hours with the command:

It is ready to be committed :)

Test Plan: Ran it for 10 hours

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16797/"
,,Rocksdb,"Make RocksDB work with newer gflags

Summary:
Newer gflags switched from `google` namespace to `gflags` namespace. See: https://github.com/facebook/rocksdb/issues/139 and https://github.com/facebook/rocksdb/issues/102

Unfortunately, they don't define any macro with their namespace, so we need to actually try to compile gflags with two different namespace to figure out which one is the correct one.

Test Plan: works in fbcode environemnt. I'll also try in ubutnu with newer gflags

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18537/Fix bad merge of D16791 and D16767

Summary: A bad Auto-Merge caused log buffer is flushed twice. Remove the unintended one.

Test Plan: Should already be tested (the code looks the same as when I ran unit tests).

Reviewers: haobo, igor

Reviewed By: haobo

CC: ljin, yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D16821/"
,,Rocksdb,"Consolidate SliceTransform object ownership

Summary:
(1) Fix SanitizeOptions() to also check HashLinkList. The current
dynamic case just happens to work because the 2 classes have the same
layout.
(2) Do not delete SliceTransform object in HashSkipListFactory and
HashLinkListFactory destructor. Reason: SanitizeOptions() enforces
prefix_extractor and SliceTransform to be the same object when
Hash**Factory is used. This makes the behavior strange: when
Hash**Factory is used, prefix_extractor will be released by RocksDB. If
other memtable factory is used, prefix_extractor should be released by
user.

Test Plan: db_bench && make asan_check

Reviewers: haobo, igor, sdong

Reviewed By: igor

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D16587/"
,,Rocksdb,"declare kInline size in arena.cc/Arena to inline 2KB of data in it.

Summary:
In order to use arena to a use case that the total allocation size might be small (LogBuffer is already such a case), inline 1KB of data in it, so that it can be mostly in stack or inline in another class.

If always inlining 2KB is a concern, I could make it a template to determine what to inline. However, dependents need to changes. Doesn't go with it for now

Test Plan: make all check.

Reviewers: haobo, igor, yhchiang, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18609/have proprocessor choose correct mmap args/Pass logger to memtable rep and TLB page allocation error logged to info logs

Summary:
TLB page allocation errors are now logged to info logs, instead of stderr.
In order to do that, mem table rep's factory functions take a info logger now.

Test Plan: make all check

Reviewers: haobo, igor, yhchiang

Reviewed By: yhchiang

CC: leveldb, yhchiang, dhruba

Differential Revision: https://reviews.facebook.net/D18471/Revert ""Revert ""Allow allocating dynamic bloom, plain table indexes and hash linked list from huge page TLB""""
And make the default 0 for hash linked list memtable

This reverts commit d69dc64be78a8da3ce661454655966d11ff61bb6./"
,,Rocksdb,"Enable log info with different levels.

Summary:
* Now each Log related function has a variant that takes an additional
  argument indicating its log level, which is one of the following:
 - DEBUG, INFO, WARN, ERROR, FATAL.

* To ensure backward-compatibility, old version Log functions are kept
  unchanged.

* Logger now has a member variable indicating its log level.  Any incoming
  Log  which log level is lower than Logger's log level will not
  be output.

* The output of the newer version Log will be prefixed by its log level.

Test Plan:
Add a LogType test in auto_roll_logger_test.cc

 = Sample log output =
    2014/02/11-00:03:07.683895 7feded179840 [DEBUG] this is the message to be written to the log file!!
    2014/02/11-00:03:07.683898 7feded179840 [INFO] this is the message to be written to the log file!!
    2014/02/11-00:03:07.683900 7feded179840 [WARN] this is the message to be written to the log file!!
    2014/02/11-00:03:07.683903 7feded179840 [ERROR] this is the message to be written to the log file!!
    2014/02/11-00:03:07.683906 7feded179840 [FATAL] this is the message to be written to the log file!!

Reviewers: dhruba, xjin, kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16071/"
,Thread management,Rocksdb,"Print out stack trace in mac, too

Summary: While debugging Mac-only issue with ThreadLocalPtr, this was very useful. Let's print out stack trace in MAC OS, too.

Test Plan: Verified that somewhat useful stack trace was generated on mac. Will run PrintStack() on linux, too.

Reviewers: ljin, haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18189/"
,,Rocksdb,"Refine the checks in InfoLogLevel test.

Summary:
InfoLogLevel test now checks the number of lines of the output log file
instead of the number of bytes in the log file.

This diff fixes the issue that the previous InfoLogLevel test in
auto_roll_logger_test passed in make check but fails when valgrind
is used.

Test Plan: run with make check and valgrind.

Reviewers: kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16407/Make sure logger is safely released in `InfoLogLevel`

Summary: fix the memory leak that was captured by jenkin build.

Test Plan: ran the valgrind test locally

Reviewers: yhchiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16389/Enable log info with different levels.

Summary:
* Now each Log related function has a variant that takes an additional
  argument indicating its log level, which is one of the following:
 - DEBUG, INFO, WARN, ERROR, FATAL.

* To ensure backward-compatibility, old version Log functions are kept
  unchanged.

* Logger now has a member variable indicating its log level.  Any incoming
  Log  which log level is lower than Logger's log level will not
  be output.

* The output of the newer version Log will be prefixed by its log level.

Test Plan:
Add a LogType test in auto_roll_logger_test.cc

 = Sample log output =
    2014/02/11-00:03:07.683895 7feded179840 [DEBUG] this is the message to be written to the log file!!
    2014/02/11-00:03:07.683898 7feded179840 [INFO] this is the message to be written to the log file!!
    2014/02/11-00:03:07.683900 7feded179840 [WARN] this is the message to be written to the log file!!
    2014/02/11-00:03:07.683903 7feded179840 [ERROR] this is the message to be written to the log file!!
    2014/02/11-00:03:07.683906 7feded179840 [FATAL] this is the message to be written to the log file!!

Reviewers: dhruba, xjin, kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16071/"
,,Rocksdb,"Allow user to specify log level for info_log

Summary:
Currently, there is no easy way for user to change log level of info log. Add a parameter in options to specify that.
Also make the default level to INFO level. Removing the [INFO] tag if it is INFO level as I don't want to cause performance regression. (add [LOG] means another mem-copy and string formatting).

Test Plan:
make all check
manual check the levels work as expected.

Reviewers: dhruba, yhchiang

Reviewed By: yhchiang

CC: dhruba, igor, i.am.jin.lei, ljin, haobo, leveldb

Differential Revision: https://reviews.facebook.net/D16563/Enable log info with different levels.

Summary:
* Now each Log related function has a variant that takes an additional
  argument indicating its log level, which is one of the following:
 - DEBUG, INFO, WARN, ERROR, FATAL.

* To ensure backward-compatibility, old version Log functions are kept
  unchanged.

* Logger now has a member variable indicating its log level.  Any incoming
  Log  which log level is lower than Logger's log level will not
  be output.

* The output of the newer version Log will be prefixed by its log level.

Test Plan:
Add a LogType test in auto_roll_logger_test.cc

 = Sample log output =
    2014/02/11-00:03:07.683895 7feded179840 [DEBUG] this is the message to be written to the log file!!
    2014/02/11-00:03:07.683898 7feded179840 [INFO] this is the message to be written to the log file!!
    2014/02/11-00:03:07.683900 7feded179840 [WARN] this is the message to be written to the log file!!
    2014/02/11-00:03:07.683903 7feded179840 [ERROR] this is the message to be written to the log file!!
    2014/02/11-00:03:07.683906 7feded179840 [FATAL] this is the message to be written to the log file!!

Reviewers: dhruba, xjin, kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16071/"
,,Rocksdb,"Revert ""Revert ""Allow allocating dynamic bloom, plain table indexes and hash linked list from huge page TLB""""
And make the default 0 for hash linked list memtable

This reverts commit d69dc64be78a8da3ce661454655966d11ff61bb6./cache friendly blocked bloomfilter

Summary:
By constraining the probes within cache line(s), we can improve the
cache miss rate thus performance. This probably only makes sense for
in-memory workload so defaults the option to off.

Numbers and comparision can be found in wiki:
https://our.intern.facebook.com/intern/wiki/index.php/Ljin/rocksdb_perf/2014_03_17#Bloom_Filter_Study

Test Plan: benchmarked this change substantially. Will run make all check as well

Reviewers: haobo, igor, dhruba, sdong, yhchiang

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17133/"
,,Rocksdb,"autovector::resize

Summary: Resize the autovector!

Test Plan: test

Reviewers: sdong

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18543/"
,,Rocksdb,"Consistency Check Function

Summary: Added a function/command to check the consistency of live files' meta data

Test Plan:
Manual test (size mismatch, file not exist).
Command test script.

Reviewers: haobo

Reviewed By: haobo

CC: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D16935/[CF] Column family support for LDB tool

Summary: Added list_column_family command and also updated dump_manifest

Test Plan: no

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16419/"
,,Rocksdb,"Revert ""Revert ""Allow allocating dynamic bloom, plain table indexes and hash linked list from huge page TLB""""
And make the default 0 for hash linked list memtable

This reverts commit d69dc64be78a8da3ce661454655966d11ff61bb6./make hash_link_list Node's key space consecutively followed at the end

Summary: per sdong's , this will help processor prefetch on n->key case.

Test Plan: make all check

Reviewers: sdong, haobo, igor

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17415/Consolidate SliceTransform object ownership

Summary:
(1) Fix SanitizeOptions() to also check HashLinkList. The current
dynamic case just happens to work because the 2 classes have the same
layout.
(2) Do not delete SliceTransform object in HashSkipListFactory and
HashLinkListFactory destructor. Reason: SanitizeOptions() enforces
prefix_extractor and SliceTransform to be the same object when
Hash**Factory is used. This makes the behavior strange: when
Hash**Factory is used, prefix_extractor will be released by RocksDB. If
other memtable factory is used, prefix_extractor should be released by
user.

Test Plan: db_bench && make asan_check

Reviewers: haobo, igor, sdong

Reviewed By: igor

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D16587/"
,,Rocksdb,"Arena to inline 2KB of data in it.

Summary:
In order to use arena to a use case that the total allocation size might be small (LogBuffer is already such a case), inline 1KB of data in it, so that it can be mostly in stack or inline in another class.

If always inlining 2KB is a concern, I could make it a template to determine what to inline. However, dependents need to changes. Doesn't go with it for now

Test Plan: make all check.

Reviewers: haobo, igor, yhchiang, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18609/"
,Thread management,Rocksdb,"Dynamically choose SSE 4.2

Summary: Otherwise, if we compile on machine with SSE4.2 support and run it on machine without the support, we will fail.

Test Plan: compiles, verified that isSse42() gets called.

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17505/Make RocksDB compile for iOS

Summary:
I had to make number of changes to the code and Makefile:
* Add `make lib`, that will create static library without debug info. We need this to avoid growing binary too much. Currently it's 14MB.
* Remove cpuinfo() function and use __SSE4_2__ macro. We actually used the macro as part of Fast_CRC32() function.
As a result, I also accidentally fixed this issue: https://www.facebook.com/groups/rocksdb.dev/permalink/549700778461774/?stream_ref=2
* Remove __thread locals in OS_MACOSX

Test Plan: `make lib PLATFORM=IOS`

Reviewers: ljin, haobo, dhruba, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17475/crc32: build a whole special Extend function for SSE 4.2.

Disassembling the Extend function shows something that looks
much more healthy now. The SSE 4.2 instructions are right
there in the body of the function.

Intel(R) Core(TM) i7-3540M CPU @ 3.00GHz

Before:

crc32c: 1.305 micros/op 766260 ops/sec; 2993.2 MB/s (4K per op)

After:

crc32c: 0.442 micros/op 2263843 ops/sec; 8843.1 MB/s (4K per op)/"
,,Rocksdb,"ApplyToAllCacheEntries

Summary: Added a method that executes a callback on every cache entry.

Test Plan: added a unit test

Reviewers: haobo

Reviewed By: haobo

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D18441/"
,,Rocksdb,"Fix more gflag namespace issues/Fix some other signed & unsigned comparisons

Summary: Fix some signed and unsigned comparisons to make some other build script happy.

Test Plan: Build and run those changed tests

Reviewers: ljin, igor, haobo

Reviewed By: igor

CC: yhchiang, dhruba, kailiu, leveldb

Differential Revision: https://reviews.facebook.net/D17463/Fix MacOS errors/cache friendly blocked bloomfilter

Summary:
By constraining the probes within cache line(s), we can improve the
cache miss rate thus performance. This probably only makes sense for
in-memory workload so defaults the option to off.

Numbers and comparision can be found in wiki:
https://our.intern.facebook.com/intern/wiki/index.php/Ljin/rocksdb_perf/2014_03_17#Bloom_Filter_Study

Test Plan: benchmarked this change substantially. Will run make all check as well

Reviewers: haobo, igor, dhruba, sdong, yhchiang

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17133/"
,,Rocksdb,"RocksDBLite

Summary:
Introducing RocksDBLite! Removes all the non-essential features and reduces the binary size. This effort should help our adoption on mobile.

Binary size when compiling for IOS (`TARGET_OS=IOS m static_lib`) is down to 9MB from 15MB (without stripping)

Test Plan: compiles :)

Reviewers: dhruba, haobo, ljin, sdong, yhchiang

Reviewed By: yhchiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17835/"
,Thread management,Rocksdb,"Relax env_test::AllocateTest/Turn on -Wmissing-prototypes

Summary: Compiling for iOS has by default turned on -Wmissing-prototypes, which causes rocksdb to fail compiling. This diff turns on -Wmissing-prototypes in our compile options and cleans up all functions with missing prototypes.

Test Plan: compiles

Reviewers: dhruba, haobo, ljin, sdong

Reviewed By: ljin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17649/Fix data corruption by LogBuffer

Summary: LogBuffer::AddLogToBuffer() uses vsnprintf() in the wrong way, which might cause buffer overflow when log line is too line. Fix it.

Test Plan: Add a unit test to cover most LogBuffer's most logic.

Reviewers: igor, haobo, dhruba

Reviewed By: igor

CC: ljin, yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D17103/Make it compile on Debian/GCC 4.7/Env to add a function to allow users to query waiting queue length

Summary: Add a function to Env so that users can query the waiting queue length of each thread pool

Test Plan: add a test in env_test

Reviewers: haobo

Reviewed By: haobo

CC: dhruba, igor, yhchiang, ljin, nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D16755/Truncate unused space on PosixWritableFile::Close()

Summary:
Blocks allocated with fallocate will take extra space on disk even if they are unused and the file is close.

Now we remove the extra blocks at the end of the file by calling `ftruncate`.

Test Plan: added a test to env_test

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16647/Make sure GetUniqueID releated tests run on ""regular"" storage

Summary:
With the use of tmpfs or ramfs, unit tests related to GetUniqueID()
failed because of the failure from ioctl, which doesn't work with these
fancy file systems at all.

I fixed this issue and make sure all related tests run on the ""regular""
storage (disk or flash).

Test Plan: TEST_TMPDIR=/dev/shm make check -j32

Reviewers: igor, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16593/"
Memory management,Memory management,Rocksdb,"Make it easier to start using RocksDB

Summary:
This diff is addressing multiple things with a single goal -- to make RocksDB easier to use:
* Add some functions to Options that make RocksDB easier to tune.
* Add example code for both simple RocksDB and RocksDB with Column Families.
* Rewrite our README.md

Regarding Options, I took a stab at something we talked about for a long time:
* https://www.facebook.com/groups/rocksdb.dev/permalink/563169950448190/

I added functions:
* IncreaseParallelism() -- easy, increases the thread pool and max_background_compactions
* OptimizeLevelStyleCompaction(memtable_memory_budget) -- the easiest way to optimize rocksdb for less stalls with level style compaction. This is very likely not ideal configuration. Feel free to suggest improvements. I used some of Mark's suggestions from here: https://github.com/facebook/rocksdb/issues/54
* OptimizeUniversalStyleCompaction(memtable_memory_budget) -- optimize for universal compaction.

Test Plan: compiled rocksdb. ran examples.

Reviewers: dhruba, MarkCallaghan, haobo, sdong, yhchiang

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18621/Revert ""Revert ""Allow allocating dynamic bloom, plain table indexes and hash linked list from huge page TLB""""
And make the default 0 for hash linked list memtable

This reverts commit d69dc64be78a8da3ce661454655966d11ff61bb6./Flush stale column families

Summary:
Added a new option `max_total_wal_size`. Once the total WAL size goes over that, we make an attempt to flush all column families that still have data in the earliest WAL file.

By default, I calculate `max_total_wal_size` dynamically, that should be good-enough for non-advanced customers.

Test Plan: Added a test

Reviewers: dhruba, haobo, sdong, ljin, yhchiang

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18345/Initialize Options::bloom_locality

Summary: I think this issue was caused by bad merge. We have to initialize bloom_locality, otherwise valgrind complains: ""Use of uninitialised value of size 8""

Test Plan: Run valgrind ./prefix_test

Reviewers: ljin

Reviewed By: ljin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17553/Change default value of some Options

Summary: Since we are optimizing for server workloads, some default values are not optimized any more. We change some of those values that I feel it's less prone to regression bugs.

Test Plan: make all check

Reviewers: dhruba, haobo, ljin, igor, yhchiang

Reviewed By: igor

CC: leveldb, MarkCallaghan

Differential Revision: https://reviews.facebook.net/D16995/allow mmap writes/[rocksdb] new CompactionFilterV2 API

Summary:
This diff adds a new CompactionFilterV2 API that roll up the
decisions of kv pairs during compactions. These kv pairs must share the
same key prefix. They are buffered inside the db.

    typedef std::vector<Slice> SliceVector;
    virtual std::vector<bool> Filter(int level,
                                 const SliceVector& keys,
                                 const SliceVector& existing_values,
                                 std::vector<std::string>* new_values,
                                 std::vector<bool>* values_changed
                                 ) const = 0;

Application can override the Filter() function to operate
on the buffered kv pairs. More details in the inline documentation.

Test Plan:
make check. Added unit tests to make sure Keep, Delete,
Change all works.

Reviewers: haobo

CCs: leveldb

Differential Revision: https://reviews.facebook.net/D15087/Enhance partial merge to support multiple arguments

Summary:
* PartialMerge api now takes a list of operands instead of two operands.
* Add min_pertial_merge_operands to Options, indicating the minimum
  number of operands to trigger partial merge.
* This diff is based on Schalk's previous diff (D14601), but it also
  includes necessary changes such as updating the pure C api for
  partial merge.

Test Plan:
* make check all
* develop tests for cases where partial merge takes more than two
  operands.

TODOs (from Schalk):
* Add test with min_partial_merge_operands > 2.
* Perform benchmarks to measure the performance improvements (can probably
  use results of task #2837810.)
* Add description of problem to doc/index.html.
* Change wiki pages to reflect the interface changes.

Reviewers: haobo, igor, vamsi

Reviewed By: haobo

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D16815/Add option verify_checksums_in_compaction

Summary:
If verify_checksums_in_compaction is true, compaction will verify checksums. This is default.
If it's false, compaction doesn't verify checksums. This is useful for in-memory workloads.

Test Plan: corruption_test

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16695/Merge branch 'master' into columnfamilies

Conflicts:
	db/compaction_picker.cc
	db/compaction_picker.h
	db/db_impl.cc
	db/version_set.cc
	db/version_set.h
	include/rocksdb/options.h
	util/options.cc/"
,,Rocksdb,"ApplyToAllCacheEntries

Summary: Added a method that executes a callback on every cache entry.

Test Plan: added a unit test

Reviewers: haobo

Reviewed By: haobo

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D18441/Turn on -Wmissing-prototypes

Summary: Compiling for iOS has by default turned on -Wmissing-prototypes, which causes rocksdb to fail compiling. This diff turns on -Wmissing-prototypes in our compile options and cleans up all functions with missing prototypes.

Test Plan: compiles

Reviewers: dhruba, haobo, ljin, sdong

Reviewed By: ljin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17649/"
,,Rocksdb,"FixedPrefixTransform to include prefix length in its name

Summary: As title

Test Plan: make all check.

Reviewers: haobo, igor, yhchiang

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18705/"
,,Rocksdb,"Arena to inline 2KB of data in it.

Summary:
In order to use arena to a use case that the total allocation size might be small (LogBuffer is already such a case), inline 1KB of data in it, so that it can be mostly in stack or inline in another class.

If always inlining 2KB is a concern, I could make it a template to determine what to inline. However, dependents need to changes. Doesn't go with it for now

Test Plan: make all check.

Reviewers: haobo, igor, yhchiang, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18609/Revert ""Revert ""Allow allocating dynamic bloom, plain table indexes and hash linked list from huge page TLB""""
And make the default 0 for hash linked list memtable

This reverts commit d69dc64be78a8da3ce661454655966d11ff61bb6./A heuristic way to check if a memtable is full

Summary:
This is is based on https://reviews.facebook.net/D15027. It's not finished but I would like to give a prototype to avoid arena over-allocation while making better use of the already allocated memory blocks.

Instead of check approximate memtable size, we will take a deeper look at the arena, which incorporate essential idea that @sdong suggests: flush when arena has allocated its last and the last is ""almost full""

Test Plan: N/A

Reviewers: haobo, sdong

Reviewed By: sdong

CC: leveldb, sdong

Differential Revision: https://reviews.facebook.net/D15051/"
,,Rocksdb,"macros for perf_context

Summary: This will allow us to disable them completely for iOS or for better performance

Test Plan: will run make all check

Reviewers: igor, haobo, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17511/"
,Thread management,Rocksdb,"Retry FS system calls on EINTR

Summary: EINTR means 'please retry'. We don't do that currenty. We should.

Test Plan: make check, although it doesn't really test the new code. we'll just have to believe in the code!

Reviewers: haobo, ljin

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17349/Optimize fallocation

Summary:
Based on my recent findings (posted in our internal group), if we use fallocate without KEEP_SIZE flag, we get superior performance of fdatasync() in append-only workloads.

This diff provides an option for user to not use KEEP_SIZE flag, thus optimizing his sync performance by up to 2x-3x.

At one point we also just called posix_fallocate instead of fallocate, which isn't very fast: http://code.woboq.org/userspace/glibc/sysdeps/posix/posix_fallocate.c.html (tl;dr it manually writes out zero bytes to allocate storage). This diff also fixes that, by first calling fallocate and then posix_fallocate if fallocate is not supported.

Test Plan: make check

Reviewers: dhruba, sdong, haobo, ljin

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16761/Env to add a function to allow users to query waiting queue length

Summary: Add a function to Env so that users can query the waiting queue length of each thread pool

Test Plan: add a test in env_test

Reviewers: haobo

Reviewed By: haobo

CC: dhruba, igor, yhchiang, ljin, nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D16755/Truncate unused space on PosixWritableFile::Close()

Summary:
Blocks allocated with fallocate will take extra space on disk even if they are unused and the file is close.

Now we remove the extra blocks at the end of the file by calling `ftruncate`.

Test Plan: added a test to env_test

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16647/thread local pointer storage

Summary:
This is not a generic thread local implementation in the sense that it
only takes pointer. But it does support multiple instances per thread
and lets user plugin function to perform cleanup when thread exits or an
instance gets destroyed.

Test Plan: unit test for now

Reviewers: haobo, igor, sdong, dhruba

Reviewed By: igor

CC: leveldb, kailiu

Differential Revision: https://reviews.facebook.net/D16131/"
,,Rocksdb,"Add a hash-index component for block

Summary:
this is the key component extracted from diff: https://reviews.facebook.net/D14271
I separate it to a dedicated patch to make the review easier.

Test Plan: added a unit test and passed it.

Reviewers: haobo, sdong, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16245/"
,Thread management,Rocksdb,"Buffer info logs when picking compactions and write them out after releasing the mutex

Summary: Now while the background thread is picking compactions, it writes out multiple info_logs, especially for universal compaction, which introduces a chance of waiting log writing in mutex, which is bad. To remove this risk, write all those info logs to a buffer and flush it after releasing the mutex.

Test Plan:
make all check
check the log lines while running some tests that trigger compactions.

Reviewers: haobo, igor, dhruba

Reviewed By: dhruba

CC: i.am.jin.lei, dhruba, yhchiang, leveldb, nkg-

Differential Revision: https://reviews.facebook.net/D16515/Allow user to specify log level for info_log

Summary:
Currently, there is no easy way for user to change log level of info log. Add a parameter in options to specify that.
Also make the default level to INFO level. Removing the [INFO] tag if it is INFO level as I don't want to cause performance regression. (add [LOG] means another mem-copy and string formatting).

Test Plan:
make all check
manual check the levels work as expected.

Reviewers: dhruba, yhchiang

Reviewed By: yhchiang

CC: dhruba, igor, i.am.jin.lei, ljin, haobo, leveldb

Differential Revision: https://reviews.facebook.net/D16563/Make Log::Reader more robust

Summary:
This diff does two things:
(1) Log::Reader does not report a corruption when the last record in a log or manifest file is truncated (meaning that log writer died in the middle of the write). Inherited the code from LevelDB: https://code.google.com/p/leveldb/source/detail?r=269fc6ca9416129248db5ca57050cd5d39d177c8#
(2) Turn off mmap writes for all writes to log and manifest files

(2) is necessary because if we use mmap writes, the last record is not truncated, but is actually filled with zeros, making checksum fail. It is hard to recover from checksum failing.

Test Plan:
Added unit tests from LevelDB
Actually recovered a ""corrupted"" MANIFEST file.

Reviewers: dhruba, haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16119/Enable log info with different levels.

Summary:
* Now each Log related function has a variant that takes an additional
  argument indicating its log level, which is one of the following:
 - DEBUG, INFO, WARN, ERROR, FATAL.

* To ensure backward-compatibility, old version Log functions are kept
  unchanged.

* Logger now has a member variable indicating its log level.  Any incoming
  Log  which log level is lower than Logger's log level will not
  be output.

* The output of the newer version Log will be prefixed by its log level.

Test Plan:
Add a LogType test in auto_roll_logger_test.cc

 = Sample log output =
    2014/02/11-00:03:07.683895 7feded179840 [DEBUG] this is the message to be written to the log file!!
    2014/02/11-00:03:07.683898 7feded179840 [INFO] this is the message to be written to the log file!!
    2014/02/11-00:03:07.683900 7feded179840 [WARN] this is the message to be written to the log file!!
    2014/02/11-00:03:07.683903 7feded179840 [ERROR] this is the message to be written to the log file!!
    2014/02/11-00:03:07.683906 7feded179840 [FATAL] this is the message to be written to the log file!!

Reviewers: dhruba, xjin, kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16071/"
,Thread management,Rocksdb,"Optimize fallocation

Summary:
Based on my recent findings (posted in our internal group), if we use fallocate without KEEP_SIZE flag, we get superior performance of fdatasync() in append-only workloads.

This diff provides an option for user to not use KEEP_SIZE flag, thus optimizing his sync performance by up to 2x-3x.

At one point we also just called posix_fallocate instead of fallocate, which isn't very fast: http://code.woboq.org/userspace/glibc/sysdeps/posix/posix_fallocate.c.html (tl;dr it manually writes out zero bytes to allocate storage). This diff also fixes that, by first calling fallocate and then posix_fallocate if fallocate is not supported.

Test Plan: make check

Reviewers: dhruba, sdong, haobo, ljin

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16761/Env to add a function to allow users to query waiting queue length

Summary: Add a function to Env so that users can query the waiting queue length of each thread pool

Test Plan: add a test in env_test

Reviewers: haobo

Reviewed By: haobo

CC: dhruba, igor, yhchiang, ljin, nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D16755/Buffer info logs when picking compactions and write them out after releasing the mutex

Summary: Now while the background thread is picking compactions, it writes out multiple info_logs, especially for universal compaction, which introduces a chance of waiting log writing in mutex, which is bad. To remove this risk, write all those info logs to a buffer and flush it after releasing the mutex.

Test Plan:
make all check
check the log lines while running some tests that trigger compactions.

Reviewers: haobo, igor, dhruba

Reviewed By: dhruba

CC: i.am.jin.lei, dhruba, yhchiang, leveldb, nkg-

Differential Revision: https://reviews.facebook.net/D16515/Allow user to specify log level for info_log

Summary:
Currently, there is no easy way for user to change log level of info log. Add a parameter in options to specify that.
Also make the default level to INFO level. Removing the [INFO] tag if it is INFO level as I don't want to cause performance regression. (add [LOG] means another mem-copy and string formatting).

Test Plan:
make all check
manual check the levels work as expected.

Reviewers: dhruba, yhchiang

Reviewed By: yhchiang

CC: dhruba, igor, i.am.jin.lei, ljin, haobo, leveldb

Differential Revision: https://reviews.facebook.net/D16563/Make Log::Reader more robust

Summary:
This diff does two things:
(1) Log::Reader does not report a corruption when the last record in a log or manifest file is truncated (meaning that log writer died in the middle of the write). Inherited the code from LevelDB: https://code.google.com/p/leveldb/source/detail?r=269fc6ca9416129248db5ca57050cd5d39d177c8#
(2) Turn off mmap writes for all writes to log and manifest files

(2) is necessary because if we use mmap writes, the last record is not truncated, but is actually filled with zeros, making checksum fail. It is hard to recover from checksum failing.

Test Plan:
Added unit tests from LevelDB
Actually recovered a ""corrupted"" MANIFEST file.

Reviewers: dhruba, haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16119/Enable log info with different levels.

Summary:
* Now each Log related function has a variant that takes an additional
  argument indicating its log level, which is one of the following:
 - DEBUG, INFO, WARN, ERROR, FATAL.

* To ensure backward-compatibility, old version Log functions are kept
  unchanged.

* Logger now has a member variable indicating its log level.  Any incoming
  Log  which log level is lower than Logger's log level will not
  be output.

* The output of the newer version Log will be prefixed by its log level.

Test Plan:
Add a LogType test in auto_roll_logger_test.cc

 = Sample log output =
    2014/02/11-00:03:07.683895 7feded179840 [DEBUG] this is the message to be written to the log file!!
    2014/02/11-00:03:07.683898 7feded179840 [INFO] this is the message to be written to the log file!!
    2014/02/11-00:03:07.683900 7feded179840 [WARN] this is the message to be written to the log file!!
    2014/02/11-00:03:07.683903 7feded179840 [ERROR] this is the message to be written to the log file!!
    2014/02/11-00:03:07.683906 7feded179840 [FATAL] this is the message to be written to the log file!!

Reviewers: dhruba, xjin, kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16071/thread local pointer storage

Summary:
This is not a generic thread local implementation in the sense that it
only takes pointer. But it does support multiple instances per thread
and lets user plugin function to perform cleanup when thread exits or an
instance gets destroyed.

Test Plan: unit test for now

Reviewers: haobo, igor, sdong, dhruba

Reviewed By: igor

CC: leveldb, kailiu

Differential Revision: https://reviews.facebook.net/D16131/"
,,Rocksdb,"Fix bad merge of D16791 and D16767

Summary: A bad Auto-Merge caused log buffer is flushed twice. Remove the unintended one.

Test Plan: Should already be tested (the code looks the same as when I ran unit tests).

Reviewers: haobo, igor

Reviewed By: haobo

CC: ljin, yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D16821/"
,,Rocksdb,"Enhance partial merge to support multiple arguments

Summary:
* PartialMerge api now takes a list of operands instead of two operands.
* Add min_pertial_merge_operands to Options, indicating the minimum
  number of operands to trigger partial merge.
* This diff is based on Schalk's previous diff (D14601), but it also
  includes necessary changes such as updating the pure C api for
  partial merge.

Test Plan:
* make check all
* develop tests for cases where partial merge takes more than two
  operands.

TODOs (from Schalk):
* Add test with min_partial_merge_operands > 2.
* Perform benchmarks to measure the performance improvements (can probably
  use results of task #2837810.)
* Add description of problem to doc/index.html.
* Change wiki pages to reflect the interface changes.

Reviewers: haobo, igor, vamsi

Reviewed By: haobo

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D16815/"
,,Rocksdb,"Declare all DB methods virtual so that StackableDB can override them/Expose the table properties to application

Summary: Provide a public API for users to access the table properties for each SSTable.

Test Plan: Added a unit tests to test the function correctness under differnet conditions.

Reviewers: haobo, dhruba, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16083/"
,,Rocksdb,"[CF] Handle failure in WriteBatch::Handler

Summary:
* Add ColumnFamilyHandle::GetID() function. Client needs to know column family's ID to be able to construct WriteBatch
* Handle WriteBatch::Handler failure gracefully. Since WriteBatch is not a very smart function (it takes raw CF id), client can add data to WriteBatch for column family that doesn't exist. In that case, we need to gracefully return failure status from DB::Write(). To do that, I added a return Status to WriteBatch functions PutCF, DeleteCF and MergeCF.

Test Plan: Added test to column_family_test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16323/"
Memory management,Memory management,Rocksdb,"Make it easier to start using RocksDB

Summary:
This diff is addressing multiple things with a single goal -- to make RocksDB easier to use:
* Add some functions to Options that make RocksDB easier to tune.
* Add example code for both simple RocksDB and RocksDB with Column Families.
* Rewrite our README.md

Regarding Options, I took a stab at something we talked about for a long time:
* https://www.facebook.com/groups/rocksdb.dev/permalink/563169950448190/

I added functions:
* IncreaseParallelism() -- easy, increases the thread pool and max_background_compactions
* OptimizeLevelStyleCompaction(memtable_memory_budget) -- the easiest way to optimize rocksdb for less stalls with level style compaction. This is very likely not ideal configuration. Feel free to suggest improvements. I used some of Mark's suggestions from here: https://github.com/facebook/rocksdb/issues/54
* OptimizeUniversalStyleCompaction(memtable_memory_budget) -- optimize for universal compaction.

Test Plan: compiled rocksdb. ran examples.

Reviewers: dhruba, MarkCallaghan, haobo, sdong, yhchiang

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18621/"
,,Rocksdb,"Add rocksdb_open_for_read_only to C API/[C-API] implemented more options/[C-API] added ""rocksdb_options_set_plain_table_factory"" to make it possible to use plain table factory/[C-API] added the possiblity to create a HashSkipList or HashLinkedList to support prefix seeks/A few more C API functions./Fix C API/Enhancements to the API/"
,,Rocksdb,"RocksDBLite

Summary:
Introducing RocksDBLite! Removes all the non-essential features and reduces the binary size. This effort should help our adoption on mobile.

Binary size when compiling for IOS (`TARGET_OS=IOS m static_lib`) is down to 9MB from 15MB (without stripping)

Test Plan: compiles :)

Reviewers: dhruba, haobo, ljin, sdong, yhchiang

Reviewed By: yhchiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17835/Consolidate SliceTransform object ownership

Summary:
(1) Fix SanitizeOptions() to also check HashLinkList. The current
dynamic case just happens to work because the 2 classes have the same
layout.
(2) Do not delete SliceTransform object in HashSkipListFactory and
HashLinkListFactory destructor. Reason: SanitizeOptions() enforces
prefix_extractor and SliceTransform to be the same object when
Hash**Factory is used. This makes the behavior strange: when
Hash**Factory is used, prefix_extractor will be released by RocksDB. If
other memtable factory is used, prefix_extractor should be released by
user.

Test Plan: db_bench && make asan_check

Reviewers: haobo, igor, sdong

Reviewed By: igor

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D16587/"
,Thread management,Rocksdb,"[RocksDB] make sure KSVObsolete does not get accessed as a valid pointer.

Summary: KSVObsolete is no longer nullptr and needs to be checked explicitly. Also did some minor code cleanup and added a stat counter to track superversion cleanups incurred in the foreground.

Test Plan: make check

Reviewers: ljin

Reviewed By: ljin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16701/use CAS when returning SuperVersion to ThreadLocal

Summary:
Add a check at the end of GetImpl to release SuperVersion if it becomes
obsolete. Also do Scrape() inside InstallSuperVersion so it happens more
frequent.

Test Plan:
make all check
running asan_check now

Reviewers: igor, haobo, sdong, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16641/cache SuperVersion in thread local storage to avoid mutex lock

Summary: as title

Test Plan:
asan_check
will post results later

Reviewers: haobo, igor, dhruba, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16257/Expose the table properties to application

Summary: Provide a public API for users to access the table properties for each SSTable.

Test Plan: Added a unit tests to test the function correctness under differnet conditions.

Reviewers: haobo, dhruba, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16083/"
,,Rocksdb,"Revert ""Revert ""Allow allocating dynamic bloom, plain table indexes and hash linked list from huge page TLB""""
And make the default 0 for hash linked list memtable

This reverts commit d69dc64be78a8da3ce661454655966d11ff61bb6./xxHash

Summary:
Originally: https://github.com/facebook/rocksdb//87/files

I'm taking over to apply some finishing touches

Test Plan: will add tests

Reviewers: dhruba, haobo, sdong, yhchiang, ljin

Reviewed By: yhchiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18315/Enable hash index for block-based table

Summary: Based on previous patches, this diff eventually provides the end-to-end mechanism for users to specify the hash-index.

Test Plan: Wrote several new unit tests.

Reviewers: sdong, haobo, dhruba

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16539//"
,Thread management,Rocksdb,"Make RocksDB compile for iOS

Summary:
I had to make number of changes to the code and Makefile:
* Add `make lib`, that will create static library without debug info. We need this to avoid growing binary too much. Currently it's 14MB.
* Remove cpuinfo() function and use __SSE4_2__ macro. We actually used the macro as part of Fast_CRC32() function.
As a result, I also accidentally fixed this issue: https://www.facebook.com/groups/rocksdb.dev/permalink/549700778461774/?stream_ref=2
* Remove __thread locals in OS_MACOSX

Test Plan: `make lib PLATFORM=IOS`

Reviewers: ljin, haobo, dhruba, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17475/"
,,Rocksdb,"[rocksdb] new CompactionFilterV2 API

Summary:
This diff adds a new CompactionFilterV2 API that roll up the
decisions of kv pairs during compactions. These kv pairs must share the
same key prefix. They are buffered inside the db.

    typedef std::vector<Slice> SliceVector;
    virtual std::vector<bool> Filter(int level,
                                 const SliceVector& keys,
                                 const SliceVector& existing_values,
                                 std::vector<std::string>* new_values,
                                 std::vector<bool>* values_changed
                                 ) const = 0;

Application can override the Filter() function to operate
on the buffered kv pairs. More details in the inline documentation.

Test Plan:
make check. Added unit tests to make sure Keep, Delete,
Change all works.

Reviewers: haobo

CCs: leveldb

Differential Revision: https://reviews.facebook.net/D15087/"
,,Rocksdb,"Expose the table properties to application

Summary: Provide a public API for users to access the table properties for each SSTable.

Test Plan: Added a unit tests to test the function correctness under differnet conditions.

Reviewers: haobo, dhruba, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16083/"
,,Rocksdb,"Improve ttl_test

Summary:
Our valgrind tests are failing because ttl_test is kind of flakey. This diff should fix valgrind issue and make ttl_test less flakey and much faster.

Instead of relying on Env::Default() for getting current time, I expose `Env*` to all TTL functions that are interested in time. That way, I can insert a custom test Env which is then used to provide exactly the times we need. That way, we don't need to sleep anymore -- we control the time.

Test Plan: ttl_test in normal and valgrind run

Reviewers: dhruba, haobo, sdong, yhchiang

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18399/Support for column families in TTL DB

Summary:
This will enable people using TTL DB to do so with multiple column families. They can also specify different TTLs for each one.

TODO: Implement CreateColumnFamily() in TTL world.

Test Plan: Added a very simple sanity test.

Reviewers: dhruba, haobo, ljin, sdong, yhchiang

Reviewed By: haobo

CC: leveldb, alberts

Differential Revision: https://reviews.facebook.net/D17859/Compaction Filter V1 to use old context struct to keep backward compatible

Summary: The previous change D15087 changed existing compaction filter, which makes the commonly used class not backward compatible. Revert the older interface. Use a new interface for V2 instead.

Test Plan: make all check

Reviewers: haobo, yhchiang, igor

CC: danguo, dhruba, ljin, igor, leveldb

Differential Revision: https://reviews.facebook.net/D17223/"
,,Rocksdb,"Add share_files_with_cheksum option to BackupEngine

Summary: added a new option to BackupEngine: if share_files_with_checksum is set to true, sst files are stored in shared_checksum/ and are identified by the triple (file name, checksum, file size) instead of just the file name. This option is targeted at distributed databases that want to backup their primary replica.

Test Plan: unit tests and tested backup and restore on a distributed rocksdb

Reviewers: igor

Reviewed By: igor

Differential Revision: https://reviews.facebook.net/D18393/Read-only BackupEngine

Summary: Read-only BackupEngine can connect to the same backup directory that is already running BackupEngine. That enables some interesting use-cases (i.e. restoring replica from primary's backup directory)

Test Plan: added a unit test

Reviewers: dhruba, haobo, ljin

Reviewed By: ljin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18297/Rate limiter for BackupableDB

Summary: Might be useful if client doesn't want to effect running system during backup too much.

Test Plan: added a test case

Reviewers: dhruba, haobo, ljin

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17091/keep_log_files option in BackupableDB

Summary:
Added an option to BackupableDB implementation that allows users to persist in-memory databases. When the restore happens with keep_log_files = true, it will
*) Not delete existing log files in wal_dir
*) Move log files from archive directory to wal_dir, so that DB can replay them if necessary

Test Plan: Added an unit test

Reviewers: dhruba, ljin

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16941/Fix share_table_files bug

Summary: constructor wasn't properly constructing BackupableDBOptions

Test Plan: no test

Reviewers: benj

Reviewed By: benj

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16749/Dump options in backupable DB

Summary: We should dump options in backupable DB log, just like with to with RocksDB. This will aid debugging.

Test Plan: checked the log

Reviewers: ljin

Reviewed By: ljin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16719/Some fixes to BackupableDB

Summary:
(1) Report corruption if backup meta file has tailing data that was not read. This should fix: https://github.com/facebook/rocksdb/issues/81 (also, @sdong reported similar issue)
(2) Don't use OS buffer when copying file to backup directory. We don't need the file in cache since we won't be reading it twice
(3) Don't delete newer backups when somebody tries to backup the diverged DB (restore from older backup, add new data, try to backup). Rather, just fail the new backup.

Test Plan: backupable_db_test

Reviewers: ljin, dhruba, sdong

Reviewed By: ljin

CC: leveldb, sdong

Differential Revision: https://reviews.facebook.net/D16287/"
,,Rocksdb,"Add share_files_with_cheksum option to BackupEngine

Summary: added a new option to BackupEngine: if share_files_with_checksum is set to true, sst files are stored in shared_checksum/ and are identified by the triple (file name, checksum, file size) instead of just the file name. This option is targeted at distributed databases that want to backup their primary replica.

Test Plan: unit tests and tested backup and restore on a distributed rocksdb

Reviewers: igor

Reviewed By: igor

Differential Revision: https://reviews.facebook.net/D18393/More s/us fixes/Read-only BackupEngine

Summary: Read-only BackupEngine can connect to the same backup directory that is already running BackupEngine. That enables some interesting use-cases (i.e. restoring replica from primary's backup directory)

Test Plan: added a unit test

Reviewers: dhruba, haobo, ljin

Reviewed By: ljin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18297/Rate limiter for BackupableDB

Summary: Might be useful if client doesn't want to effect running system during backup too much.

Test Plan: added a test case

Reviewers: dhruba, haobo, ljin

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17091/keep_log_files option in BackupableDB

Summary:
Added an option to BackupableDB implementation that allows users to persist in-memory databases. When the restore happens with keep_log_files = true, it will
*) Not delete existing log files in wal_dir
*) Move log files from archive directory to wal_dir, so that DB can replay them if necessary

Test Plan: Added an unit test

Reviewers: dhruba, ljin

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16941/DeleteLogFiles in FailOverwritingBackups/More bug fixed introduced by code cleanup/CloseDB in BackupableDBTest to make valgrind happy/Some fixes to BackupableDB

Summary:
(1) Report corruption if backup meta file has tailing data that was not read. This should fix: https://github.com/facebook/rocksdb/issues/81 (also, @sdong reported similar issue)
(2) Don't use OS buffer when copying file to backup directory. We don't need the file in cache since we won't be reading it twice
(3) Don't delete newer backups when somebody tries to backup the diverged DB (restore from older backup, add new data, try to backup). Rather, just fail the new backup.

Test Plan: backupable_db_test

Reviewers: ljin, dhruba, sdong

Reviewed By: ljin

CC: leveldb, sdong

Differential Revision: https://reviews.facebook.net/D16287/"
,,Rocksdb,Use new DBWithTTL API in tests/
,,Rocksdb,"Check PrefixMayMatch on Seek()

Summary:
As a follow-up diff for https://reviews.facebook.net/D17805, add
optimization to check PrefixMayMatch on Seek()

Test Plan: make all check

Reviewers: igor, haobo, sdong, yhchiang, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17853/RocksDB 2.8 to be able to read files generated by 2.6

Summary:
From 2.6 to 2.7, property block name is renamed from rocksdb.stats to rocksdb.properties. Older properties were not able to be loaded. In 2.8, we seem to have added some logic that uses property block without checking null pointers, which create segment faults.

In this patch, we fix it by:
(1) try rocksdb.stats if rocksdb.properties is not found
(2) add some null checking before consuming rep->table_properties

Test Plan: make sure a file generated in 2.7 couldn't be opened now can be opened.

Reviewers: haobo, igor, yhchiang

Reviewed By: igor

CC: ljin, xjin, dhruba, kailiu, leveldb

Differential Revision: https://reviews.facebook.net/D17961/Enable hash index for block-based table

Summary: Based on previous patches, this diff eventually provides the end-to-end mechanism for users to specify the hash-index.

Test Plan: Wrote several new unit tests.

Reviewers: sdong, haobo, dhruba

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16539//Disable putting filter block to block cache

Summary: This bug caused server crash issues because the filter block is too big and kept purging out of cache.

Test Plan: Wrote a new unit tests to make sure it works.

Reviewers: dhruba, haobo, igor, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16221/"
,,Rocksdb,"xxHash

Summary:
Originally: https://github.com/facebook/rocksdb//87/files

I'm taking over to apply some finishing touches

Test Plan: will add tests

Reviewers: dhruba, haobo, sdong, yhchiang, ljin

Reviewed By: yhchiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18315/Fix SIGFAULT when running sst_dump on v2.6 db

Summary: Fix the sigfault when running sst_dump on v2.6 db.

Test Plan:
    git checkout bba6595b1f3f42cf79bb21c2d5b981ede1cc0063
    make clean
    make db_bench
    ./db_bench --db=/tmp/some/db --benchmarks=fillseq
    arc patch D18039
    make clean
    make sst_dump
    ./sst_dump --file=/tmp/some/db --command=check

Reviewers: igor, haobo, sdong

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18039/"
,,Rocksdb,"PlainTableBuilder::Add() to use local char array instead of reused std::string as tmp buffer

Summary: Our profile shows that in one of the applications, 5% of the CPU costs of PlainTableBuilder::Add() are spent on std::string stacks. By this simple change, we avoid this global reusable string. Also, we avoid another call of file appending, which probably gives another 2%.

Test Plan: make all check

Reviewers: haobo, ljin

Reviewed By: haobo

CC: igor, yhchiang, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D17601/"
,,Rocksdb,"Enable hash index for block-based table

Summary: Based on previous patches, this diff eventually provides the end-to-end mechanism for users to specify the hash-index.

Test Plan: Wrote several new unit tests.

Reviewers: sdong, haobo, dhruba

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16539/"
,,Rocksdb,"Fix more gflag namespace issues/Benchmark table reader wiht nanoseconds

Summary: nanosecnods gave us better view of the performance, especially when some operations are fast so that micro seconds may only reveal less informative results.

Test Plan:
sample output:

    ./table_reader_bench --plain_table --time_unit=nanosecond
    =======================================================================================================
    InMemoryTableSimpleBenchmark:           PlainTable   num_key1:   4096   num_key2:   512   non_empty
    =======================================================================================================
    Histogram (unit: nanosecond):
    Count: 6291456  Average: 475.3867  StdDev: 556.05
    Min: 135.0000  Median: 400.1817  Max: 33370.0000
    Percentiles: P50: 400.18 P75: 530.02 P99: 887.73 P99.9: 8843.26 P99.99: 9941.21
    ------------------------------------------------------
    [     120,     140 )        2   0.000%   0.000%
    [     140,     160 )      452   0.007%   0.007%
    [     160,     180 )    13683   0.217%   0.225%
    [     180,     200 )    54353   0.864%   1.089%
    [     200,     250 )   101004   1.605%   2.694%
    [     250,     300 )   729791  11.600%  14.294% ##
    [     300,     350 )   616070   9.792%  24.086% ##
    [     350,     400 )  1628021  25.877%  49.963% #####
    [     400,     450 )   647220  10.287%  60.250% ##
    [     450,     500 )   577206   9.174%  69.424% ##
    [     500,     600 )  1168585  18.574%  87.999% ####
    [     600,     700 )   506875   8.057%  96.055% ##
    [     700,     800 )   147878   2.350%  98.406%
    [     800,     900 )    42633   0.678%  99.083%
    [     900,    1000 )    16304   0.259%  99.342%
    [    1000,    1200 )     7811   0.124%  99.466%
    [    1200,    1400 )     1453   0.023%  99.490%
    [    1400,    1600 )      307   0.005%  99.494%
    [    1600,    1800 )       81   0.001%  99.496%
    [    1800,    2000 )       18   0.000%  99.496%
    [    2000,    2500 )        8   0.000%  99.496%
    [    2500,    3000 )        6   0.000%  99.496%
    [    3500,    4000 )        3   0.000%  99.496%
    [    4000,    4500 )      116   0.002%  99.498%
    [    4500,    5000 )     1144   0.018%  99.516%
    [    5000,    6000 )     1087   0.017%  99.534%
    [    6000,    7000 )     2403   0.038%  99.572%
    [    7000,    8000 )     9840   0.156%  99.728%
    [    8000,    9000 )    12820   0.204%  99.932%
    [    9000,   10000 )     3881   0.062%  99.994%
    [   10000,   12000 )      135   0.002%  99.996%
    [   12000,   14000 )      159   0.003%  99.998%
    [   14000,   16000 )       58   0.001%  99.999%
    [   16000,   18000 )       30   0.000% 100.000%
    [   18000,   20000 )       14   0.000% 100.000%
    [   20000,   25000 )        2   0.000% 100.000%
    [   25000,   30000 )        2   0.000% 100.000%
    [   30000,   35000 )        1   0.000% 100.000%

Reviewers: haobo, dhruba, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16113/Fix table_reader_bench and add it to ""make""

Summary: Fix table_reader_bench after some interface changes. Add it to make to avoid future breaking

Test Plan: make table_reader_bench and run it with different options.

Reviewers: kailiu, haobo

Reviewed By: haobo

CC: igor, leveldb

Differential Revision: https://reviews.facebook.net/D16107/"
,Thread management,Rocksdb,"TablePropertiesCollectorFactory

Summary:
This diff addresses task #4296714 and rethinks how users provide us with TablePropertiesCollectors as part of Options.

Here's description of task #4296714:
       I'm debugging #4295529 and noticed that our count of user properties kDeletedKeys is wrong. We're sharing one single InternalKeyPropertiesCollector with all Table Builders. In LOG Files, we're outputting number of kDeletedKeys as connected with a single table, while it's actually the total count of deleted keys since creation of the DB.

       For example, this table has 3155 entries and 1391828 deleted keys.

The problem with current approach that we call methods on a single TablePropertiesCollector for all the tables we create. Even worse, we could do it from multiple threads at the same time and TablePropertiesCollector has no way of knowing which table we're calling it for.

Good part: Looks like nobody inside Facebook is using Options::table_properties_collectors. This means we should be able to painfully change the API.

In this change, I introduce TablePropertiesCollectorFactory. For every table we create, we call `CreateTablePropertiesCollector`, which creates a TablePropertiesCollector for a single table. We then use it sequentially from a single thread, which means it doesn't have to be thread-safe.

Test Plan:
Added a test in table_properties_collector_test that fails on master (build two tables, assert that kDeletedKeys count is correct for the second one).
Also, all other tests

Reviewers: sdong, dhruba, haobo, kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18579/xxHash

Summary:
Originally: https://github.com/facebook/rocksdb//87/files

I'm taking over to apply some finishing touches

Test Plan: will add tests

Reviewers: dhruba, haobo, sdong, yhchiang, ljin

Reviewed By: yhchiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18315/Use a different approach to make sure BlockBasedTableReader can use hash index on older files

Summary:
A recent commit https://github.com/facebook/rocksdb/commit/e37dd216f9384bfdabc6760fa296e8ee28c79d30 makes sure hash index can be used when reading existing files. This patch uses another way to achieve the approach:
(1) Currently, always writing kBinarySearch to files, despite of BlockBasedTableOptions.IndexType setting.
(2) When reading a file, read out the field, and make sure it is kBinarySearch, while always use index type by users.

The reason for doing it is, to reserve kHashSearch property on disk to future. If now we write out binary index for both of kHashSearch and kBinarySearch. We have to use a new flag in the future for hash index on disk, otherwise compatibility would break. Also, we want the real index type and type shown in properties block to be consistent.

Test Plan: make all check

Reviewers: haobo, kailiu

Reviewed By: kailiu

CC: igor, ljin, yhchiang, xjin, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18009/Use shorten index key for hash-index

Summary:
I was wrong about the ""index builder"", right now since we create index
by scanning both whole table and index, there is not need to preserve
the whole key as the index key.

I switch back to original way index which is both space efficient and
able to supprot in-fly construction of hash index.

IN this patch, I made minimal change since I'm not sure if we still need
the ""pluggable index builder"", under current circumstance it is of no use
and kind of over-engineered. But I'm not sure if we can still exploit its
usefulness in the future; otherwise I think I can just burn them with great
vengeance.

Test Plan: unit tests

Reviewers: sdong, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17745/Enable hash index for block-based table

Summary: Based on previous patches, this diff eventually provides the end-to-end mechanism for users to specify the hash-index.

Test Plan: Wrote several new unit tests.

Reviewers: sdong, haobo, dhruba

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16539//Remove the terrible hack in for flush_block_policy_factory

Summary:
Previous code is too convoluted and I must be drunk for letting
such code to be written without a second thought.

Thanks to the discussion with @sdong, I added the `Options` when
generating the flusher, thus avoiding the tricks.

Just FYI: I resisted to add Options in flush_block_policy.h since I
wanted to avoid cyclic dependencies: FlushBlockPolicy dpends on Options
and Options also depends FlushBlockPolicy... While I appreciate my
effort to prevent it, the old design turns out creating more troubles than
it tried to avoid.

Test Plan: ran ./table_test

Reviewers: sdong

Reviewed By: sdong

CC: sdong, leveldb

Differential Revision: https://reviews.facebook.net/D16503/"
,,Rocksdb,"PlainTableFactory::PlainTableFactory() to have huge TLB turned off by default

Summary: PlainTableFactory::PlainTableFactory() now has Huge TLB page feature turned on by default. Although it is not a public API (which we always turn the feature off now), our unit tests, like db_test sometimes uses it directly, which causes wrong coverage of codes. This patch fix it to allow unit tests to run with the correct setting

Test Plan: Run db_test and make sure this feature is not on any more.

Reviewers: igor, haobo

Reviewed By: igor

CC: yhchiang, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18483/Revert ""Revert ""Allow allocating dynamic bloom, plain table indexes and hash linked list from huge page TLB""""
And make the default 0 for hash linked list memtable

This reverts commit d69dc64be78a8da3ce661454655966d11ff61bb6./Add more black-box tests for PlainTable and explicitly support total order mode

Summary:
1. Add some more implementation-aware tests for PlainTable
2. move from a hard-coded one index per 16 rows in one prefix to a configurable number. Also, make hash table ratio = 0  means binary search only. Also fixes some divide 0 risks.
3. Explicitly support total order (only use binary search)
4. some code cleaning up.

Test Plan: make all check

Reviewers: haobo, kailiu

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16023/"
,,Rocksdb,"Turn on -Wmissing-prototypes

Summary: Compiling for iOS has by default turned on -Wmissing-prototypes, which causes rocksdb to fail compiling. This diff turns on -Wmissing-prototypes in our compile options and cleans up all functions with missing prototypes.

Test Plan: compiles

Reviewers: dhruba, haobo, ljin, sdong

Reviewed By: ljin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17649/macros for perf_context

Summary: This will allow us to disable them completely for iOS or for better performance

Test Plan: will run make all check

Reviewers: igor, haobo, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17511/Fix issue with iterator operations in this order: Prev(), Seek(), Prev()

Summary:
Due to a bad merge of D14163 and D14001 before checking in D14001, ""direction_ = kForward;"" in MergeIterator::Seek() was deleted my mistake (in commit b135d01e7bcdf4186ea852a5b4e6d14a3a815d77 ). It will generate wrong results or assert failure after the sequence of Prev() (or SeekToLast()), Seek() and Prev().

Fix it

Test Plan: make all check

Reviewers: igor, haobo, dhruba

Reviewed By: igor

CC: yhchiang, i.am.jin.lei, ljin, leveldb

Differential Revision: https://reviews.facebook.net/D16527/"
,,Rocksdb,"Revert ""Revert ""Allow allocating dynamic bloom, plain table indexes and hash linked list from huge page TLB""""
And make the default 0 for hash linked list memtable

This reverts commit d69dc64be78a8da3ce661454655966d11ff61bb6./Followup code refactor on plain table

Summary:
Fixed most comments in https://reviews.facebook.net/D15429.
Still have some remaining comments left.

Test Plan: make all check

Reviewers: sdong, haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15885/Add more black-box tests for PlainTable and explicitly support total order mode

Summary:
1. Add some more implementation-aware tests for PlainTable
2. move from a hard-coded one index per 16 rows in one prefix to a configurable number. Also, make hash table ratio = 0  means binary search only. Also fixes some divide 0 risks.
3. Explicitly support total order (only use binary search)
4. some code cleaning up.

Test Plan: make all check

Reviewers: haobo, kailiu

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16023/"
,,Rocksdb,"xxHash

Summary:
Originally: https://github.com/facebook/rocksdb//87/files

I'm taking over to apply some finishing touches

Test Plan: will add tests

Reviewers: dhruba, haobo, sdong, yhchiang, ljin

Reviewed By: yhchiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18315/Check PrefixMayMatch on Seek()

Summary:
As a follow-up diff for https://reviews.facebook.net/D17805, add
optimization to check PrefixMayMatch on Seek()

Test Plan: make all check

Reviewers: igor, haobo, sdong, yhchiang, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17853/Fix SIGFAULT when running sst_dump on v2.6 db

Summary: Fix the sigfault when running sst_dump on v2.6 db.

Test Plan:
    git checkout bba6595b1f3f42cf79bb21c2d5b981ede1cc0063
    make clean
    make db_bench
    ./db_bench --db=/tmp/some/db --benchmarks=fillseq
    arc patch D18039
    make clean
    make sst_dump
    ./sst_dump --file=/tmp/some/db --command=check

Reviewers: igor, haobo, sdong

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18039/Use a different approach to make sure BlockBasedTableReader can use hash index on older files

Summary:
A recent commit https://github.com/facebook/rocksdb/commit/e37dd216f9384bfdabc6760fa296e8ee28c79d30 makes sure hash index can be used when reading existing files. This patch uses another way to achieve the approach:
(1) Currently, always writing kBinarySearch to files, despite of BlockBasedTableOptions.IndexType setting.
(2) When reading a file, read out the field, and make sure it is kBinarySearch, while always use index type by users.

The reason for doing it is, to reserve kHashSearch property on disk to future. If now we write out binary index for both of kHashSearch and kBinarySearch. We have to use a new flag in the future for hash index on disk, otherwise compatibility would break. Also, we want the real index type and type shown in properties block to be consistent.

Test Plan: make all check

Reviewers: haobo, kailiu

Reviewed By: kailiu

CC: igor, ljin, yhchiang, xjin, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18009/Index type doesn't have to be persisted

Summary:

With the recent changes, there is no need to check the property block about the index block type.
If user want to use it, they don't really need any disk format change; everything happens in the fly.

Also another team encountered an error while reading the index type from properties.

Test Plan:

ran all the tests

Reviewers: sdong

CC:

Task ID: #

Blame Rev:/RocksDB 2.8 to be able to read files generated by 2.6

Summary:
From 2.6 to 2.7, property block name is renamed from rocksdb.stats to rocksdb.properties. Older properties were not able to be loaded. In 2.8, we seem to have added some logic that uses property block without checking null pointers, which create segment faults.

In this patch, we fix it by:
(1) try rocksdb.stats if rocksdb.properties is not found
(2) add some null checking before consuming rep->table_properties

Test Plan: make sure a file generated in 2.7 couldn't be opened now can be opened.

Reviewers: haobo, igor, yhchiang

Reviewed By: igor

CC: ljin, xjin, dhruba, kailiu, leveldb

Differential Revision: https://reviews.facebook.net/D17961/Enable hash index for block-based table

Summary: Based on previous patches, this diff eventually provides the end-to-end mechanism for users to specify the hash-index.

Test Plan: Wrote several new unit tests.

Reviewers: sdong, haobo, dhruba

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16539/Make the Create() function comform the convention

Summary:

Moved ""Return multiple values"" a more conventional way./Fix the memory leak in table index

Summary:

BinarySearchIndex didn't use unique_ptr to guard the block object nor
delete it in destructor, leading to valgrind failure for ""definite
memory leak"".

Test Plan:
re-ran the failed valgrind test cases/Fix the unit test failure in devbox

Summary:
My last diff was developed in MacOS but in devserver environment error occurs.

I dug into the problem and found the way we calcuate approximate data size is pretty out-of-date. We can use table properties to get more accurate results.

Test Plan: ran ./table_test and passed

Reviewers: igor, dhruba, haobo, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16509//Fix inconsistent code format

Summary:
Found some function follows camel style. When naming funciton, we have two styles:

Trivially expose internal data in readonly mode: `all_lower_case()`
Regular function: `CapitalizeFirstLetter()`

I renames these functions.

Test Plan: make -j32

Reviewers: haobo, sdong, dhruba, igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16383/Disable putting filter block to block cache

Summary: This bug caused server crash issues because the filter block is too big and kept purging out of cache.

Test Plan: Wrote a new unit tests to make sure it works.

Reviewers: dhruba, haobo, igor, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16221/"
,,Rocksdb,"Revert ""Revert ""Allow allocating dynamic bloom, plain table indexes and hash linked list from huge page TLB""""
And make the default 0 for hash linked list memtable

This reverts commit d69dc64be78a8da3ce661454655966d11ff61bb6./kill ReadOptions.prefix and .prefix_seek

Summary:
also add an override option total_order_iteration if you want to use full
iterator with prefix_extractor

Test Plan: make all check

Reviewers: igor, haobo, sdong, yhchiang

Reviewed By: haobo

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D17805/PlainTableReader to expose index size to users

Summary:
This is a temp solution to expose index sizes to users from PlainTableReader before we persistent them to files.
In this patch, the memory consumption of indexes used by PlainTableReader will be reported as two user defined properties, so that users can monitor them.

Test Plan:
Add a unit test.
make all check`

Reviewers: haobo, ljin

Reviewed By: haobo

CC: nkg-, yhchiang, igor, ljin, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18195/PlainTableIterator not to store copied key in std::string

Summary:
Move PlainTableIterator's copied key from std::string local buffer to avoid paying the extra costs in std::string related to sharing. Reuse the same buffer class in DbIter. Move the class to dbformat.h.

This patch improves iterator performance significantly. Running this benchmark:

./table_reader_bench --num_keys2=17 --iterator --plain_table --time_unit=nanosecond

The average latency is improved to about 750 nanoseconds from 1100 nanoseconds.

Test Plan:
Add a unit test.
make all check

Reviewers: haobo, ljin

Reviewed By: haobo

CC: igor, yhchiang, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D17547/PlainTable::Next() should pass the error message from ReadKey()

Summary:
PlainTable::Next() should pass the error message from ReadKey(). Now it would return a wrong error message.
Also improve the messages of status when failing to read

Test Plan: make all check

Reviewers: ljin, kailiu, haobo

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16365/Followup code refactor on plain table

Summary:
Fixed most comments in https://reviews.facebook.net/D15429.
Still have some remaining comments left.

Test Plan: make all check

Reviewers: sdong, haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D15885/Add more black-box tests for PlainTable and explicitly support total order mode

Summary:
1. Add some more implementation-aware tests for PlainTable
2. move from a hard-coded one index per 16 rows in one prefix to a configurable number. Also, make hash table ratio = 0  means binary search only. Also fixes some divide 0 risks.
3. Explicitly support total order (only use binary search)
4. some code cleaning up.

Test Plan: make all check

Reviewers: haobo, kailiu

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16023/"
,,Rocksdb,"kill ReadOptions.prefix and .prefix_seek

Summary:
also add an override option total_order_iteration if you want to use full
iterator with prefix_extractor

Test Plan: make all check

Reviewers: igor, haobo, sdong, yhchiang

Reviewed By: haobo

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D17805/"
,,Rocksdb,"Fix inconsistent code format

Summary:
Found some function follows camel style. When naming funciton, we have two styles:

Trivially expose internal data in readonly mode: `all_lower_case()`
Regular function: `CapitalizeFirstLetter()`

I renames these functions.

Test Plan: make -j32

Reviewers: haobo, sdong, dhruba, igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16383/"
,,Rocksdb,"fix compile warning/xxHash

Summary:
Originally: https://github.com/facebook/rocksdb//87/files

I'm taking over to apply some finishing touches

Test Plan: will add tests

Reviewers: dhruba, haobo, sdong, yhchiang, ljin

Reviewed By: yhchiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18315/macros for perf_context

Summary: This will allow us to disable them completely for iOS or for better performance

Test Plan: will run make all check

Reviewers: igor, haobo, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17511/"
,,Rocksdb,"avoid calling FindFile twice in TwoLevelIterator for PlainTable

Summary:
this is to reclaim the regression introduced in
https://reviews.facebook.net/D17853

Test Plan: make all check

Reviewers: igor, haobo, sdong, dhruba, yhchiang

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17985/Check PrefixMayMatch on Seek()

Summary:
As a follow-up diff for https://reviews.facebook.net/D17805, add
optimization to check PrefixMayMatch on Seek()

Test Plan: make all check

Reviewers: igor, haobo, sdong, yhchiang, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17853/"
,,Rocksdb,"xxHash

Summary:
Originally: https://github.com/facebook/rocksdb//87/files

I'm taking over to apply some finishing touches

Test Plan: will add tests

Reviewers: dhruba, haobo, sdong, yhchiang, ljin

Reviewed By: yhchiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18315/"
,,Rocksdb,"Remove the terrible hack in for flush_block_policy_factory

Summary:
Previous code is too convoluted and I must be drunk for letting
such code to be written without a second thought.

Thanks to the discussion with @sdong, I added the `Options` when
generating the flusher, thus avoiding the tricks.

Just FYI: I resisted to add Options in flush_block_policy.h since I
wanted to avoid cyclic dependencies: FlushBlockPolicy dpends on Options
and Options also depends FlushBlockPolicy... While I appreciate my
effort to prevent it, the old design turns out creating more troubles than
it tried to avoid.

Test Plan: ran ./table_test

Reviewers: sdong

Reviewed By: sdong

CC: sdong, leveldb

Differential Revision: https://reviews.facebook.net/D16503/"
,,Rocksdb,"Revert ""Revert ""Allow allocating dynamic bloom, plain table indexes and hash linked list from huge page TLB""""
And make the default 0 for hash linked list memtable

This reverts commit d69dc64be78a8da3ce661454655966d11ff61bb6./"
,,Rocksdb,"xxHash

Summary:
Originally: https://github.com/facebook/rocksdb//87/files

I'm taking over to apply some finishing touches

Test Plan: will add tests

Reviewers: dhruba, haobo, sdong, yhchiang, ljin

Reviewed By: yhchiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18315/kill ReadOptions.prefix and .prefix_seek

Summary:
also add an override option total_order_iteration if you want to use full
iterator with prefix_extractor

Test Plan: make all check

Reviewers: igor, haobo, sdong, yhchiang

Reviewed By: haobo

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D17805/Enable hash index for block-based table

Summary: Based on previous patches, this diff eventually provides the end-to-end mechanism for users to specify the hash-index.

Test Plan: Wrote several new unit tests.

Reviewers: sdong, haobo, dhruba

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16539//Disable putting filter block to block cache

Summary: This bug caused server crash issues because the filter block is too big and kept purging out of cache.

Test Plan: Wrote a new unit tests to make sure it works.

Reviewers: dhruba, haobo, igor, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16221/Add more black-box tests for PlainTable and explicitly support total order mode

Summary:
1. Add some more implementation-aware tests for PlainTable
2. move from a hard-coded one index per 16 rows in one prefix to a configurable number. Also, make hash table ratio = 0  means binary search only. Also fixes some divide 0 risks.
3. Explicitly support total order (only use binary search)
4. some code cleaning up.

Test Plan: make all check

Reviewers: haobo, kailiu

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16023/"
,Thread management,Rocksdb,"Improve EnvHdfs

Summary: Copy improvements from fbcode's version of EnvHdfs to our open-source version. Some very important bug fixes in there.

Test Plan: compiles

Reviewers: dhruba, haobo, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18711/Env to add a function to allow users to query waiting queue length

Summary: Add a function to Env so that users can query the waiting queue length of each thread pool

Test Plan: add a test in env_test

Reviewers: haobo

Reviewed By: haobo

CC: dhruba, igor, yhchiang, ljin, nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D16755/thread local pointer storage

Summary:
This is not a generic thread local implementation in the sense that it
only takes pointer. But it does support multiple instances per thread
and lets user plugin function to perform cleanup when thread exits or an
instance gets destroyed.

Test Plan: unit test for now

Reviewers: haobo, igor, sdong, dhruba

Reviewed By: igor

CC: leveldb, kailiu

Differential Revision: https://reviews.facebook.net/D16131/"
,,Rocksdb,"Fixed a file-not-found issue when a log file is moved to archive.

Summary:
Fixed a file-not-found issue when a log file is moved to archive
by doing a missing retry.

Test Plan:
make db_test
export ROCKSDB_TEST=TransactionLogIteratorRace
./db_test

Reviewers: sdong, haobo

Reviewed By: sdong

CC: igor, leveldb

Differential Revision: https://reviews.facebook.net/D18669/Fix bad merge of D16791 and D16767

Summary: A bad Auto-Merge caused log buffer is flushed twice. Remove the unintended one.

Test Plan: Should already be tested (the code looks the same as when I ran unit tests).

Reviewers: haobo, igor

Reviewed By: haobo

CC: ljin, yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D16821/Add ReadOptions to TransactionLogIterator.

Summary:
Add an optional input parameter ReadOptions to DB::GetUpdateSince(),
which allows the verification of checksums to be disabled by setting
ReadOptions::verify_checksums to false.

Test Plan: Tests are done off-line and will not be included in the regular unit test.

Reviewers: igor

Reviewed By: igor

CC: leveldb, xjin, dhruba

Differential Revision: https://reviews.facebook.net/D16305/"
,,Rocksdb,"When Options.max_num_files=-1, non level0 files also by pass table cache

Summary:
This is the part that was not finished when doing the Options.max_num_files=-1 feature. For iterating non level0 SST files (which was done using two level iterator), table cache is not bypassed. With this patch, the leftover feature is done.

Test Plan: make all check; change Options.max_num_files=-1 in one of the tests to cover the codes.

Reviewers: haobo, igor, dhruba, ljin, yhchiang

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17001/Expose the table properties to application

Summary: Provide a public API for users to access the table properties for each SSTable.

Test Plan: Added a unit tests to test the function correctness under differnet conditions.

Reviewers: haobo, dhruba, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16083/"
,Thread management,Rocksdb,"TablePropertiesCollectorFactory

Summary:
This diff addresses task #4296714 and rethinks how users provide us with TablePropertiesCollectors as part of Options.

Here's description of task #4296714:
       I'm debugging #4295529 and noticed that our count of user properties kDeletedKeys is wrong. We're sharing one single InternalKeyPropertiesCollector with all Table Builders. In LOG Files, we're outputting number of kDeletedKeys as connected with a single table, while it's actually the total count of deleted keys since creation of the DB.

       For example, this table has 3155 entries and 1391828 deleted keys.

The problem with current approach that we call methods on a single TablePropertiesCollector for all the tables we create. Even worse, we could do it from multiple threads at the same time and TablePropertiesCollector has no way of knowing which table we're calling it for.

Good part: Looks like nobody inside Facebook is using Options::table_properties_collectors. This means we should be able to painfully change the API.

In this change, I introduce TablePropertiesCollectorFactory. For every table we create, we call `CreateTablePropertiesCollector`, which creates a TablePropertiesCollector for a single table. We then use it sequentially from a single thread, which means it doesn't have to be thread-safe.

Test Plan:
Added a test in table_properties_collector_test that fails on master (build two tables, assert that kDeletedKeys count is correct for the second one).
Also, all other tests

Reviewers: sdong, dhruba, haobo, kailiu

Reviewed By: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18579/"
,Thread management,Rocksdb,"Flush stale column families

Summary:
Added a new option `max_total_wal_size`. Once the total WAL size goes over that, we make an attempt to flush all column families that still have data in the earliest WAL file.

By default, I calculate `max_total_wal_size` dynamically, that should be good-enough for non-advanced customers.

Test Plan: Added a test

Reviewers: dhruba, haobo, sdong, ljin, yhchiang

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18345/Add a new mem-table representation based on cuckoo hash.

Summary:
= Major Changes =
* Add a new mem-table representation, HashCuckooRep, which is based cuckoo hash.
  Cuckoo hash uses multiple hash functions.  This allows each key to have multiple
  possible locations in the mem-table.

  - Put: When insert a key, it will try to find whether one of its possible
    locations is vacant and store the key.  If none of its possible
    locations are available, then it will kick out a victim key and
    store at that location.  The kicked-out victim key will then be
    stored at a vacant space of its possible locations or kick-out
    another victim.  In this diff, the kick-out path (known as
    cuckoo-path) is found using BFS, which guarantees to be the shortest.

 - Get: Simply tries all possible locations of a key --- this guarantees
   worst-case constant time complexity.

 - Time complexity: O(1) for Get, and average O(1) for Put if the
   fullness of the mem-table is below 80%.

 - Default using two hash functions, the number of hash functions used
   by the cuckoo-hash may dynamically increase if it fails to find a
   short-enough kick-out path.

 - Currently, HashCuckooRep does not support iteration and snapshots,
   as our current main purpose of this is to optimize point access.

= Minor Changes =
* Add IsSnapshotSupported() to DB to indicate whether the current DB
  supports snapshots.  If it returns false, then DB::GetSnapshot() will
  always return nullptr.

Test Plan:
Run existing tests.  Will develop a test specifically for cuckoo hash in
the next diff.

Reviewers: sdong, haobo

Reviewed By: sdong

CC: leveldb, dhruba, igor

Differential Revision: https://reviews.facebook.net/D16155/Delete superversion and log outside of mutex

Summary: As summary. Add two autovectors that get filled up in MakeRoomForWrite and they get deleted outside of mutex

Test Plan: make check

Reviewers: dhruba, haobo, ljin, sdong

Reviewed By: ljin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18249/Column family logging

Summary:
Now that we have column families involved, we need to add extra context to every log message. They now start with ""[column family name] log message""

Also added some logging that I think would be useful, like level summary after every flush (I often needed that when going through the logs).

Test Plan: make check + ran db_bench to confirm I'm happy with log output

Reviewers: dhruba, haobo, ljin, yhchiang, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18303/When creating a new DB, fail it when wal_dir contains existing log files

Summary: Current behavior of creating new DB is, if there is existing log files, we will go ahead and replay them on top of empty DB. This is a behavior that no user would expect. With this patch, we will fail the creation if a user creates a DB with existing log files.

Test Plan: make all check

Reviewers: haobo, igor, ljin

Reviewed By: haobo

CC: nkg-, yhchiang, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D17817/RocksDBLite

Summary:
Introducing RocksDBLite! Removes all the non-essential features and reduces the binary size. This effort should help our adoption on mobile.

Binary size when compiling for IOS (`TARGET_OS=IOS m static_lib`) is down to 9MB from 15MB (without stripping)

Test Plan: compiles :)

Reviewers: dhruba, haobo, ljin, sdong, yhchiang

Reviewed By: yhchiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17835/Set log_empty to false even when options.sync is off [fix tests]/Don't roll empty logs

Summary:
With multiple column families, especially when manual Flush is executed, we might roll the log file, although the current log file is empty (no data has been written to the log).

After the diff, we won't create new log file if current is empty.

Next, I will write an algorithm that will flush column families that reference old log files (i.e., that weren't flushed in a while)

Test Plan: Added an unit test. Confirmed that unit test failes in master

Reviewers: dhruba, haobo, ljin, sdong

Reviewed By: ljin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17631/Renamed InfoLogLevel::DEBUG to InfoLogLevel::DEBUG_LEVEL

Summary: XCode for some reason injects `#define DEBUG 1` into our code, which makes compile fail because we use `DEBUG` keyword for other stuff. This diff fixes the issue by renaming `DEBUG` to `DEBUG_LEVEL`.

Test Plan: compiles

Reviewers: dhruba, haobo, sdong, yhchiang, ljin

Reviewed By: yhchiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17709/Small speedup of CompactionFilterV2

Summary: ToString() is expensive. Profiling shows that most compaction threads are stuck in jemalloc, allocating a new string. This will help out a litte.

Test Plan: make check

Reviewers: haobo, danguo

Reviewed By: danguo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17583/macros for perf_context

Summary: This will allow us to disable them completely for iOS or for better performance

Test Plan: will run make all check

Reviewers: igor, haobo, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17511/Simplify cleanup of dead (refcount == 0) column families/Make flush part of compaction process

This will enable user to use only 1 background thread./Merge branch 'master' into columnfamilies

Conflicts:
	db/db_impl.cc
	db/db_impl.h
	db/memtable_list.cc
	db/version_set.cc/Options::wal_dir shouldn't end in '/'

Summary: If a client specifies wal_dir with trailing '/', we will fail in deleting obsolete log files. See task #4083746

Test Plan: make check

Reviewers: haobo, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17535/Create log::Writer out of DB Mutex

Summary: Our measurement shows that sometimes new log::Write's constructor can take hundreds of milliseconds. It's unclear why but just simply move it out of DB mutex.

Test Plan: make all check

Reviewers: haobo, ljin, igor

Reviewed By: haobo

CC: nkg-, yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D17487/Move a info logging out of DB Mutex

Summary: As we know, logging can be slow, or even hang for some file systems. Move one more logging out of DB mutex.

Test Plan: make all check

Reviewers: haobo, igor, ljin

Reviewed By: igor

CC: yhchiang, nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D17427/[rocksdb] make init prefix more robust

Summary:
Currently if client uses kNULLString as the prefix, it will confuse
compaction filter v2. This diff added a bool to indicate if the prefix
has been intialized. I also added a unit test to cover this case and
make sure the new code path is hit.

Test Plan: db_test

Reviewers: igor, haobo

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17151/Merge branch 'master' into columnfamilies

Conflicts:
	db/db_impl.cc
	include/rocksdb/options.h
	util/options.cc/[rocksdb] new CompactionFilterV2 API

Summary:
This diff adds a new CompactionFilterV2 API that roll up the
decisions of kv pairs during compactions. These kv pairs must share the
same key prefix. They are buffered inside the db.

    typedef std::vector<Slice> SliceVector;
    virtual std::vector<bool> Filter(int level,
                                 const SliceVector& keys,
                                 const SliceVector& existing_values,
                                 std::vector<std::string>* new_values,
                                 std::vector<bool>* values_changed
                                 ) const = 0;

Application can override the Filter() function to operate
on the buffered kv pairs. More details in the inline documentation.

Test Plan:
make check. Added unit tests to make sure Keep, Delete,
Change all works.

Reviewers: haobo

CCs: leveldb

Differential Revision: https://reviews.facebook.net/D15087/Merge branch 'master' into columnfamilies

Conflicts:
	db/db_impl.cc
	db/db_test.cc/Sanity check on Open

Summary:
Everytime a client opens a DB, we do a sanity check that:
* checks the existance of all the necessary files
* verifies that file sizes are correct

Some of the code was stolen from https://reviews.facebook.net/D16935

Test Plan: added a unit test

Reviewers: dhruba, haobo, sdong

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17097/If paranoid_checks -- Mark DB read-only on any IOError

Summary:
Whenever we get an IOError from GetImpl() or NewIterator(), we should immediatelly mark the DB read-only. The same check already exists in Write() and Compaction().

This should help with clients that are somehow missing a file.

Test Plan: make check

Reviewers: dhruba, haobo, sdong, ljin

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17061/Merge branch 'master' into columnfamilies

Conflicts:
	db/db_impl.cc
	db/internal_stats.cc
	db/internal_stats.h
	db/version_set.cc/Add a DB property to indicate number of background errors encountered

Summary: Add a property to calculate number of background errors encountered to help users build their monitoring

Test Plan: Add a unit test. make all check

Reviewers: haobo, igor, dhruba

Reviewed By: igor

CC: ljin, nkg-, yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D16959/disable the log_number check in Recover()

Summary:
There is a chance that an old MANIFEST is corrupted in 2.7 but just not noticed.
This check would fail them. Change it to log instead of returning a
Corruption status.

Test Plan: make

Reviewers: haobo, igor

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16923/Optimize fallocation

Summary:
Based on my recent findings (posted in our internal group), if we use fallocate without KEEP_SIZE flag, we get superior performance of fdatasync() in append-only workloads.

This diff provides an option for user to not use KEEP_SIZE flag, thus optimizing his sync performance by up to 2x-3x.

At one point we also just called posix_fallocate instead of fallocate, which isn't very fast: http://code.woboq.org/userspace/glibc/sysdeps/posix/posix_fallocate.c.html (tl;dr it manually writes out zero bytes to allocate storage). This diff also fixes that, by first calling fallocate and then posix_fallocate if fallocate is not supported.

Test Plan: make check

Reviewers: dhruba, sdong, haobo, ljin

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16761/Fix race condition in manifest roll

Summary:
When the manifest is getting rolled the following happens:
1) manifest_file_number_ is assigned to a new manifest number (even though the old one is still current)
2) mutex is unlocked
3) SetCurrentFile() creates temporary file manifest_file_number_.dbtmp
4) SetCurrentFile() renames manifest_file_number_.dbtmp to CURRENT
5) mutex is locked

If FindObsoleteFiles happens between (3) and (4) it will:
1) Delete manifest_file_number_.dbtmp (because it's not in pending_outputs_)
2) Delete old manifest (because the manifest_file_number_ already points to a new one)

I introduce the concept of prev_manifest_file_number_ that will avoid the race condition.

However, we should discuss the future of MANIFEST file rolling. We found some race conditions with it last week and who knows how many more are there. Nobody is using it in production because we don't trust the implementation. Should we even support it?

Test Plan: make check

Reviewers: ljin, dhruba, haobo, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16929/Merge branch 'master' into columnfamilies

Conflicts:
	db/db_impl.cc
	db/db_test.cc
	tools/db_stress.cc/Fix extra compaction tasks scheduled after D16767 in some cases

Summary:
With D16767, there is a case compaction tasks are scheduled infinitely:
(1) no flush thread is configured and more than 1 compaction threads
(2) a flush is going on by one compaction hread
(3) the state of SST files is in the state that versions_->current()->NeedsCompaction() will generate a false positive (return true actually there is no work to be done)
In that case, a infinite loop will be formed.

This patch would fix it.

Test Plan: make all check

Reviewers: haobo, igor, ljin

Reviewed By: igor

CC: dhruba, yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D16863/More bug fixed introduced by code cleanup/Bug fixes introduced by code cleanup/[CF] Code cleanup part 1

Summary:
I'm cleaning up some code preparing for the big diff review tomorrow. This is the first part of the cleanup.

Changes are mostly cosmetic. The goal is to decrease amount of code difference between columnfamilies and master branch.

This diff also fixes race condition when dropping column family.

Test Plan: Ran db_stress with variety of parameters

Reviewers: dhruba, haobo

Differential Revision: https://reviews.facebook.net/D16833/Fix data race against logging data structure because of LogBuffer

Summary:
@igor pointed out that there is a potential data race because of the way we use the newly introduced LogBuffer. After ""bg_compaction_scheduled_--"" or ""bg_flush_scheduled_--"", they can both become 0. As soon as the lock is released after that, DBImpl's deconstructor can go ahead and deconstruct all the states inside DB, including the info_log object hold in a shared pointer of the options object it keeps. At that point it is not safe anymore to continue using the info logger to write the delayed logs.

With the patch, lock is released temporarily for log buffer to be flushed before ""bg_compaction_scheduled_--"" or ""bg_flush_scheduled_--"". In order to make sure we don't miss any pending flush or compaction, a new flag bg_schedule_needed_ is added, which is set to be true if there is a pending flush or compaction but not scheduled because of the max thread limit. If the flag is set to be true, the scheduling function will be called before compaction or flush thread finishes.

Thanks @igor for this finding!

Test Plan: make all check

Reviewers: haobo, igor

Reviewed By: haobo

CC: dhruba, ljin, yhchiang, igor, leveldb

Differential Revision: https://reviews.facebook.net/D16767/Merge branch 'master' into columnfamilies

Conflicts:
	db/compaction_picker.cc
	db/db_impl.cc
	db/db_impl.h
	db/tailing_iter.cc
	db/version_set.h
	include/rocksdb/options.h
	util/options.cc/[RocksDB] Minor cleanup of PurgeObsoleteFiles

Summary: as title. also made info log output of file deletion a bit more descriptive.

Test Plan: make check; db_bench and look at LOG output

Reviewers: igor

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16731/[RocksDB] make sure KSVObsolete does not get accessed as a valid pointer.

Summary: KSVObsolete is no longer nullptr and needs to be checked explicitly. Also did some minor code cleanup and added a stat counter to track superversion cleanups incurred in the foreground.

Test Plan: make check

Reviewers: ljin

Reviewed By: ljin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16701/[RocksDB] LogBuffer Cleanup

Summary: Moved LogBuffer class to an internal header. Removed some unneccesary indirection. Enabled log buffer for BackgroundCallFlush. Forced log buffer flush right after Unlock to improve time ordering of info log.

Test Plan: make check; db_bench compare LOG output

Reviewers: sdong

Reviewed By: sdong

CC: leveldb, igor

Differential Revision: https://reviews.facebook.net/D16707/Ignore dropped column families -- don't flush or compact them/[CF] NewIterators

Summary: Adding the last missing function -- NewIterators(). Pretty simple implementation

Test Plan: added a unit test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16689/use CAS when returning SuperVersion to ThreadLocal

Summary:
Add a check at the end of GetImpl to release SuperVersion if it becomes
obsolete. Also do Scrape() inside InstallSuperVersion so it happens more
frequent.

Test Plan:
make all check
running asan_check now

Reviewers: igor, haobo, sdong, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16641/Buffer info logs when picking compactions and write them out after releasing the mutex

Summary: Now while the background thread is picking compactions, it writes out multiple info_logs, especially for universal compaction, which introduces a chance of waiting log writing in mutex, which is bad. To remove this risk, write all those info logs to a buffer and flush it after releasing the mutex.

Test Plan:
make all check
check the log lines while running some tests that trigger compactions.

Reviewers: haobo, igor, dhruba

Reviewed By: dhruba

CC: i.am.jin.lei, dhruba, yhchiang, leveldb, nkg-

Differential Revision: https://reviews.facebook.net/D16515/[CF] Delete SuperVersion in a special function

Summary: Added a function DeleteSuperVersion that can be called in DBImpl destructor before PurgingObsoleteFiles. That way, PurgeObsoleteFiles will be able to delete all files held by alive super versions.

Test Plan: column_family_test with valgrind

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16545/Merge branch 'master' into columnfamilies

Conflicts:
	db/db_impl.cc
	db/db_impl.h
	db/transaction_log_impl.cc
	db/transaction_log_impl.h
	include/rocksdb/options.h
	util/env.cc
	util/options.cc/[CF] Fix CF bugs in WriteBatch

Summary:
This diff fixes two bugs:
* Increase sequence number even if WriteBatch fails. This is important because WriteBatches in WAL logs have implictly increasing sequence number, even if one update in a write batch fails. This caused some writes to get lost in my CF stress testing
* Tolerate 'invalid column family' errors on recovery. When a column family is dropped, processing WAL logs can have some WriteBatches that still refer to the dropped column family. In recovery environment, we want to ignore those errors. In client's Write() code path, however, we want to return the failure to the client if he's trying to add data to invalid column family.

Test Plan: db_stress's verification works now

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16533/Make Log::Reader more robust

Summary:
This diff does two things:
(1) Log::Reader does not report a corruption when the last record in a log or manifest file is truncated (meaning that log writer died in the middle of the write). Inherited the code from LevelDB: https://code.google.com/p/leveldb/source/detail?r=269fc6ca9416129248db5ca57050cd5d39d177c8#
(2) Turn off mmap writes for all writes to log and manifest files

(2) is necessary because if we use mmap writes, the last record is not truncated, but is actually filled with zeros, making checksum fail. It is hard to recover from checksum failing.

Test Plan:
Added unit tests from LevelDB
Actually recovered a ""corrupted"" MANIFEST file.

Reviewers: dhruba, haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16119/Set dropped column family before persisting in the manifest/[CF] CreateColumnFamily fix

Summary:
This fixes few bugs with CreateColumnFamily
* We first have to LogAndApply and then call VersionSet::CreateColumnFamily. Otherwise, WriteSnapshot might be invoked, writing out column family add inside of LogAndApply, even though it's not really committed
* Fix LogAndApplyHelper() to not apply log number to column_family_data, which is in case of column family add, just a dummy (default) column family
* Create SuperVerion when creating column family

Test Plan: column_family_test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16443/[CF] Change flow of CreateColumnFamily

Summary:
Previously, we first wrote to the manifest and then created internal data structure.
Now, we first create internal data structure. That way, we can write out internal comparator to the manifest

Test Plan: column_family_test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16425/cache SuperVersion in thread local storage to avoid mutex lock

Summary: as title

Test Plan:
asan_check
will post results later

Reviewers: haobo, igor, dhruba, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16257/[CF] Handle failure in WriteBatch::Handler

Summary:
* Add ColumnFamilyHandle::GetID() function. Client needs to know column family's ID to be able to construct WriteBatch
* Handle WriteBatch::Handler failure gracefully. Since WriteBatch is not a very smart function (it takes raw CF id), client can add data to WriteBatch for column family that doesn't exist. In that case, we need to gracefully return failure status from DB::Write(). To do that, I added a return Status to WriteBatch functions PutCF, DeleteCF and MergeCF.

Test Plan: Added test to column_family_test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16323/[CF] Log deletion in column families

Summary:
* Added unit test that verifies that obsolete files are deleted.
* Advance log number for empty column family when cutting log file.
* MinLogNumber() bug fix! (caught by the new unit test)

Test Plan: unit test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16311/Schedule flush when waiting on flush

Summary:
This will also help with avoiding the deadlock. If a flush failed and we're waiting for a memtable to be flushed, we should schedule a new flush and hope a new one succeedes.

If paranoid_checks = false, Wait() will still hang on ENOSPC, but at least it will automatically continue when the space frees up. Current behavior both hangs and deadlocks.

Also, I renamed some 'compaction' to 'flush'. 'compaction' was leveldb way of saying things.

Test Plan: make check

Reviewers: dhruba, haobo, ljin

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16281/[CF] Better handling of memtable logs

Summary: DBImpl now keeps a list of alive_log_files_. On every FindObsoleteFiles, it deletes all alive log files that are smaller than versions_->MinLogNumber()

Test Plan:
make check passes
no specific unit tests yet, will add

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16293/DeleteFile should schedule Flush or Compaction

Summary:
More info here: https://github.com/facebook/rocksdb/issues/89
If flush fails because of ENOSPC, we have a deadlock problem. This is a quick fix that will continue the normal operation when user deletes the file and frees up the space on the device.

We need to address the issue more broadly with bg_error_ cleanup.

Test Plan: make check

Reviewers: dhruba, haobo, ljin

Reviewed By: ljin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16275/First Transaction Logs Should Not Skip Storage Options Given

Summary: Currently, the first transaction log file ignore bytes_per_sync and other storage-related options. It is not consistent. Fix it.

Test Plan: make all check. See the options set in GDB.

Reviewers: haobo, kailiu

Reviewed By: haobo

CC: igor, ljin, yhchiang, leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D16215/[CF] DB test to run on non-default column family

Summary:
This is a huge diff and it was hectic, but the idea is actually quite simple. Every operation (Put, Get, etc.) done on default column family in DBTest is now forwarded to non-default (""pikachu""). The good news is that we had zero test failures! Column families look stable so far.

One interesting test that I adapted for column families is MultiThreadedTest. I replaced every Put() with a WriteBatch writing to all column families concurrently. Every Put in the write batch contains unique_id. Instead of Get() I do a multiget across all column families with the same key. If atomicity holds, I expect to see the same unique_id in all column families.

Test Plan: This is a test!

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16149/Expose the table properties to application

Summary: Provide a public API for users to access the table properties for each SSTable.

Test Plan: Added a unit tests to test the function correctness under differnet conditions.

Reviewers: haobo, dhruba, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16083/"
,,Rocksdb,"xxHash

Summary:
Originally: https://github.com/facebook/rocksdb//87/files

I'm taking over to apply some finishing touches

Test Plan: will add tests

Reviewers: dhruba, haobo, sdong, yhchiang, ljin

Reviewed By: yhchiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18315/db_bench: add a mode to operate multiple DBs

Summary: This patch introduces a new parameter num_multi_db in db_bench. When this parameter is larger than 1, multiple DBs will be created. In all benchmarks, any operation applies to a random DB among them. This is to benchmark the performance of similar applications.

Test Plan: run db_bench on both of num_multi_db=0 and more.

Reviewers: haobo, ljin, igor

Reviewed By: igor

CC: igor, yhchiang, dhruba, nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D17769/SeekRandomWhileWriting

Summary: as title

Test Plan: ran it

Reviewers: igor, haobo, yhchiang

Reviewed By: yhchiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17751/db_bench: add IteratorCreationWhileWriting mode and allow prefix_seek

Summary: as title

Test Plan: ran it

Reviewers: igor, haobo, yhchiang

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17655/speed up db_bench filluniquerandom mode

Summary:
filluniquerandom is painfully slow due to the naive bitmap check to find
out if a key has been seen before. Majority of time is spent on searching
the last few keys. Split a giant BitSet to smaller ones so that we can
quickly check if a BitSet is full and thus can skip quickly.

It used to take over one hour to filluniquerandom for 100M keys, now it
takes about 10 mins.

Test Plan:
unit test
also verified correctness in db_bench and make sure all keys are
generated

Reviewers: igor, haobo, yhchiang

Reviewed By: igor

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D17607/db_bench cleanup

Summary:
clean up the db_bench a little bit. also avoid allocating memory for key
in the loop

Test Plan:
I verified a run with filluniquerandom & readrandom. Iterator seek will be used lot
to measure performance. Will fix whatever comes up

Reviewers: haobo, igor, yhchiang

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17559/make hash_link_list Node's key space consecutively followed at the end

Summary: per sdong's , this will help processor prefetch on n->key case.

Test Plan: make all check

Reviewers: sdong, haobo, igor

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17415/cache friendly blocked bloomfilter

Summary:
By constraining the probes within cache line(s), we can improve the
cache miss rate thus performance. This probably only makes sense for
in-memory workload so defaults the option to off.

Numbers and comparision can be found in wiki:
https://our.intern.facebook.com/intern/wiki/index.php/Ljin/rocksdb_perf/2014_03_17#Bloom_Filter_Study

Test Plan: benchmarked this change substantially. Will run make all check as well

Reviewers: haobo, igor, dhruba, sdong, yhchiang

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17133/Consolidate SliceTransform object ownership

Summary:
(1) Fix SanitizeOptions() to also check HashLinkList. The current
dynamic case just happens to work because the 2 classes have the same
layout.
(2) Do not delete SliceTransform object in HashSkipListFactory and
HashLinkListFactory destructor. Reason: SanitizeOptions() enforces
prefix_extractor and SliceTransform to be the same object when
Hash**Factory is used. This makes the behavior strange: when
Hash**Factory is used, prefix_extractor will be released by RocksDB. If
other memtable factory is used, prefix_extractor should be released by
user.

Test Plan: db_bench && make asan_check

Reviewers: haobo, igor, sdong

Reviewed By: igor

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D16587/fix db_bench to use HashSkipList for real

Summary:
For HashSkipList case, DBImpl has sanity check to see if prefix_extractor in
options is the same as the one in memtable factory. If not, it falls
back to SkipList. As result, I was experimenting with SkipList
performance. No wonder it is much worse than LinkedList

Test Plan: ran benchmark

Reviewers: haobo, sdong, igor

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16569/config max_background_flushes in db_bench

Summary: as title

Test Plan: make release

Reviewers: haobo, sdong, igor

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16437/make key evenly distributed between 0 and FLAGS_num

Summary:
The issue is that when FLAGS_num is small, the leading bytes of the key
are padded with 0s. This makes all keys have the same prefix 00000000

Most of the changes are just to make lint happy

Test Plan: ran db_bench

Reviewers: sdong, haobo, igor

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16317/"
,Thread management,Rocksdb,"thread local for tailing iterator

Summary:
replace the super version acquisision in tailing itrator with thread
local

Test Plan: will post results

Reviewers: igor, haobo, sdong, yhchiang, dhruba

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17757/"
,,Rocksdb,"Enable hash index for block-based table

Summary: Based on previous patches, this diff eventually provides the end-to-end mechanism for users to specify the hash-index.

Test Plan: Wrote several new unit tests.

Reviewers: sdong, haobo, dhruba

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16539/Polish IterKey and use it in DBImpl::ProcessKeyValueCompaction()

Summary:
1. Polish IterKey a little bit.
2. Turn to use it in local parameter of current_user_key in DBImpl::ProcessKeyValueCompaction(). Our profile showing that DBImpl::ProcessKeyValueCompaction() has about 14% costs in std::string (the base including reading and writing data but excluding compaction filtering), which is higher than it should be. There are two std::string used in DBImpl::ProcessKeyValueCompaction(), compaction_filter_value and current_user_key and it's hard to distinguish the two.

Test Plan: make all check

Reviewers: haobo, ljin

Reviewed By: haobo

CC: igor, yhchiang, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D17613/PlainTableIterator not to store copied key in std::string

Summary:
Move PlainTableIterator's copied key from std::string local buffer to avoid paying the extra costs in std::string related to sharing. Reuse the same buffer class in DbIter. Move the class to dbformat.h.

This patch improves iterator performance significantly. Running this benchmark:

./table_reader_bench --num_keys2=17 --iterator --plain_table --time_unit=nanosecond

The average latency is improved to about 750 nanoseconds from 1100 nanoseconds.

Test Plan:
Add a unit test.
make all check

Reviewers: haobo, ljin

Reviewed By: haobo

CC: igor, yhchiang, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D17547/"
,,Rocksdb,"Check PrefixMayMatch on Seek()

Summary:
As a follow-up diff for https://reviews.facebook.net/D17805, add
optimization to check PrefixMayMatch on Seek()

Test Plan: make all check

Reviewers: igor, haobo, sdong, yhchiang, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17853/Fix bugs introduced by D17961

Summary:
D17961 has two bugs:
(1) two level iterator fails to populate FileMetaData.table_reader, causing performance regression.
(2) table cache handle the !status.ok() case in the wrong place, causing seg fault which shouldn't happen.

Test Plan: make all check

Reviewers: ljin, igor, haobo

Reviewed By: ljin

CC: yhchiang, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D17991/Minimize accessing multiple objects in Version::Get()

Summary:
One of our profilings shows that Version::Get() sometimes is slow when getting pointer of user comparators or other global objects. In this patch:
(1) we keep pointers of immutable objects in Version to avoid accesses them though option objects or cfd objects
(2) table_reader is directly cached in FileMetaData so that table cache don't have to go through handle first to fetch it
(3) If level 0 has less than 3 files, skip the filtering logic based on SST tables' key range. Smallest and largest key are stored in separated memory locations, which has potential cache misses

Test Plan: make all check

Reviewers: haobo, ljin

Reviewed By: haobo

CC: igor, yhchiang, nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D17739/When Options.max_num_files=-1, non level0 files also by pass table cache

Summary:
This is the part that was not finished when doing the Options.max_num_files=-1 feature. For iterating non level0 SST files (which was done using two level iterator), table cache is not bypassed. With this patch, the leftover feature is done.

Test Plan: make all check; change Options.max_num_files=-1 in one of the tests to cover the codes.

Reviewers: haobo, igor, dhruba, ljin, yhchiang

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17001/Expose the table properties to application

Summary: Provide a public API for users to access the table properties for each SSTable.

Test Plan: Added a unit tests to test the function correctness under differnet conditions.

Reviewers: haobo, dhruba, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16083/"
,,Rocksdb,"Make Log::Reader more robust

Summary:
This diff does two things:
(1) Log::Reader does not report a corruption when the last record in a log or manifest file is truncated (meaning that log writer died in the middle of the write). Inherited the code from LevelDB: https://code.google.com/p/leveldb/source/detail?r=269fc6ca9416129248db5ca57050cd5d39d177c8#
(2) Turn off mmap writes for all writes to log and manifest files

(2) is necessary because if we use mmap writes, the last record is not truncated, but is actually filled with zeros, making checksum fail. It is hard to recover from checksum failing.

Test Plan:
Added unit tests from LevelDB
Actually recovered a ""corrupted"" MANIFEST file.

Reviewers: dhruba, haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16119/"
,,Rocksdb,"Support for column families in TTL DB

Summary:
This will enable people using TTL DB to do so with multiple column families. They can also specify different TTLs for each one.

TODO: Implement CreateColumnFamily() in TTL world.

Test Plan: Added a very simple sanity test.

Reviewers: dhruba, haobo, ljin, sdong, yhchiang

Reviewed By: haobo

CC: leveldb, alberts

Differential Revision: https://reviews.facebook.net/D17859/[CF] Fix CF bugs in WriteBatch

Summary:
This diff fixes two bugs:
* Increase sequence number even if WriteBatch fails. This is important because WriteBatches in WAL logs have implictly increasing sequence number, even if one update in a write batch fails. This caused some writes to get lost in my CF stress testing
* Tolerate 'invalid column family' errors on recovery. When a column family is dropped, processing WAL logs can have some WriteBatches that still refer to the dropped column family. In recovery environment, we want to ignore those errors. In client's Write() code path, however, we want to return the failure to the client if he's trying to add data to invalid column family.

Test Plan: db_stress's verification works now

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16533/[CF] Handle failure in WriteBatch::Handler

Summary:
* Add ColumnFamilyHandle::GetID() function. Client needs to know column family's ID to be able to construct WriteBatch
* Handle WriteBatch::Handler failure gracefully. Since WriteBatch is not a very smart function (it takes raw CF id), client can add data to WriteBatch for column family that doesn't exist. In that case, we need to gracefully return failure status from DB::Write(). To do that, I added a return Status to WriteBatch functions PutCF, DeleteCF and MergeCF.

Test Plan: Added test to column_family_test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16323/"
,Thread management,Rocksdb,"Make RocksDB work with newer gflags

Summary:
Newer gflags switched from `google` namespace to `gflags` namespace. See: https://github.com/facebook/rocksdb/issues/139 and https://github.com/facebook/rocksdb/issues/102

Unfortunately, they don't define any macro with their namespace, so we need to actually try to compile gflags with two different namespace to figure out which one is the correct one.

Test Plan: works in fbcode environemnt. I'll also try in ubutnu with newer gflags

Reviewers: dhruba

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18537/Fix HashSkipList and HashLinkedList SIGSEGV

Summary:
Original Summary:
Yesterday, @ljin and I were debugging various db_stress issues. We suspected one of them happens when we concurrently call NewIterator without prefix_seek on HashSkipList. This test demonstrates it.

Update:
Arena is not thread-safe!! When creating a new full iterator, we *have* to create a new arena, otherwise we're doomed.

Test Plan: SIGSEGV and assertion-throwing test now works!

Reviewers: ljin, haobo, sdong

Reviewed By: sdong

CC: leveldb, ljin

Differential Revision: https://reviews.facebook.net/D16857/Add a test in prefix_test to verify correctness of results

Summary:
Add a test to verify HashLinkList and HashSkipList (mainly for the former one) returns the correct results when inserting the same bucket in the different orders.

Some other changes:
(1) add the test to test list
(2) fix compile error
(3) add header

Test Plan: ./prefix_test

Reviewers: haobo, kailiu

Reviewed By: haobo

CC: igor, yhchiang, i.am.jin.lei, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D16143/"
,,Rocksdb,"Expose number of entries in mem tables to users

Summary: In this patch, two new DB properties are defined: rocksdb.num-immutable-mem-table and rocksdb.num-entries-imm-mem-tables, from where number of entries in mem tables can be exposed to users

Test Plan:
Cover the codes in db_test
make all check

Reviewers: haobo, ljin, igor

Reviewed By: igor

CC: nkg-, igor, yhchiang, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18207/"
,,Rocksdb,"[C-API] implemented more options/[C-API] added ""rocksdb_options_set_plain_table_factory"" to make it possible to use plain table factory/[C-API] added the possiblity to create a HashSkipList or HashLinkedList to support prefix seeks/Enhance partial merge to support multiple arguments

Summary:
* PartialMerge api now takes a list of operands instead of two operands.
* Add min_pertial_merge_operands to Options, indicating the minimum
  number of operands to trigger partial merge.
* This diff is based on Schalk's previous diff (D14601), but it also
  includes necessary changes such as updating the pure C api for
  partial merge.

Test Plan:
* make check all
* develop tests for cases where partial merge takes more than two
  operands.

TODOs (from Schalk):
* Add test with min_partial_merge_operands > 2.
* Perform benchmarks to measure the performance improvements (can probably
  use results of task #2837810.)
* Add description of problem to doc/index.html.
* Change wiki pages to reflect the interface changes.

Reviewers: haobo, igor, vamsi

Reviewed By: haobo

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D16815/A few more C API functions./added a delete method for custom filter policy and merge operator to make it possible to override the cleanup behaviour of the return value/Enhancements to the API/"
,Thread management,Rocksdb,"Column family logging

Summary:
Now that we have column families involved, we need to add extra context to every log message. They now start with ""[column family name] log message""

Also added some logging that I think would be useful, like level summary after every flush (I often needed that when going through the logs).

Test Plan: make check + ran db_bench to confirm I'm happy with log output

Reviewers: dhruba, haobo, ljin, yhchiang, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18303/initialize candidate count/ComputeCompactionScore in CompactionPicker

Summary:
As it turns out, we need the call to ComputeCompactionScore (previously: Finalize) in CompactionPicker.

The issue caused a deadlock in db_stress: http://ci-builds.fb.com/job/rocksdb_crashtest/290/console

The last two lines before a deadlock were:
2014/03/18-22:43:41.481029 7facafbee700 (Original Log Time 2014/03/18-22:43:41.480989) Compaction nothing to do
2014/03/18-22:43:41.481041 7faccf7fc700 wait for fewer level0 files...

""Compaction nothing to do"" and other thread waiting for fewer level0 files. Hm hm.

I moved the pre-sorting to SaveTo, which should fix both the original and the new issue.

Test Plan: make check for now, will run db_stress in jenkins

Reviewers: dhruba, haobo, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17037/Don't compact with zero input files

Summary:
We have an issue with internal service trying to run compaction with zero input files:
2014/02/07-02:26:58.386531 7f79117ec700 Compaction start summary: Base version 1420 Base level 3, seek compaction:0, inputs:[?~^Qy^?],[]
2014/02/07-02:26:58.386539 7f79117ec700 Compacted 0@3 + 0@4 files => 0 bytes

There are two issues:
* inputsummary is printing out junk
* it's constantly retrying (since I guess madeProgress is true), so it prints out a lot of data in the LOG file (40GB in one day).

I read through the Level compaction picker and added some failure condition if input[0] is empty. I think PickCompaction() should not return compaction with zero input files with this change. I'm not confident enough to add an assertion though :)

Test Plan: make check

Reviewers: dhruba, haobo, sdong, kailiu

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16005/[CF] Code cleanup part 1

Summary:
I'm cleaning up some code preparing for the big diff review tomorrow. This is the first part of the cleanup.

Changes are mostly cosmetic. The goal is to decrease amount of code difference between columnfamilies and master branch.

This diff also fixes race condition when dropping column family.

Test Plan: Ran db_stress with variety of parameters

Reviewers: dhruba, haobo

Differential Revision: https://reviews.facebook.net/D16833/Buffer info logs when picking compactions and write them out after releasing the mutex

Summary: Now while the background thread is picking compactions, it writes out multiple info_logs, especially for universal compaction, which introduces a chance of waiting log writing in mutex, which is bad. To remove this risk, write all those info logs to a buffer and flush it after releasing the mutex.

Test Plan:
make all check
check the log lines while running some tests that trigger compactions.

Reviewers: haobo, igor, dhruba

Reviewed By: dhruba

CC: i.am.jin.lei, dhruba, yhchiang, leveldb, nkg-

Differential Revision: https://reviews.facebook.net/D16515/"
Compression tasks,Compression tasks,Rocksdb,"Another change is to only use class
BlockContents for compressed block, and narrow the class Block to only be used for uncompressed
blocks, including blocks in compressed block cache"
,,Rocksdb,"Expose number of entries in mem tables to users

Summary: In this patch, two new DB properties are defined: rocksdb.num-immutable-mem-table and rocksdb.num-entries-imm-mem-tables, from where number of entries in mem tables can be exposed to users

Test Plan:
Cover the codes in db_test
make all check

Reviewers: haobo, ljin, igor

Reviewed By: igor

CC: nkg-, igor, yhchiang, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18207/[RocksDB] Add db property ""rocksdb.cur-size-active-mem-table""

Summary: as title

Test Plan: db_test

Reviewers: sdong

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17217/Merge branch 'master' into columnfamilies

Conflicts:
	db/db_impl.cc
	db/internal_stats.cc
	db/internal_stats.h
	db/version_set.cc/Add a DB property to indicate number of background errors encountered

Summary: Add a property to calculate number of background errors encountered to help users build their monitoring

Test Plan: Add a unit test. make all check

Reviewers: haobo, igor, dhruba

Reviewed By: igor

CC: ljin, nkg-, yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D16959/Several easy-to-add properties related to compaction and flushes

Summary: To partly address the  @nkg- raised, add three easy-to-add properties to compactions and flushes.

Test Plan: run unit tests and add a new unit test to cover new properties.

Reviewers: haobo, dhruba

Reviewed By: dhruba

CC: nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D13677/"
,,Rocksdb,"Clean up compaction logging

Summary: Cleaned up compaction logging a little bit. Now file sizes are easier to read. Also, removed the trailing space.

Test Plan:
verified that i'm happy with logging output:

        files_size[#33(seq=101,sz=98KB,0) #31(seq=81,sz=159KB,0) #26(seq=0,sz=637KB,0)]

Reviewers: sdong, haobo, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18549/"
,,Rocksdb,"disable the log_number check in Recover()

Summary:
There is a chance that an old MANIFEST is corrupted in 2.7 but just not noticed.
This check would fail them. Change it to log instead of returning a
Corruption status.

Test Plan: make

Reviewers: haobo, igor

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16923/Fix bug in VersionEdit::DebugString()/Add MaxColumnFamily to VersionEdit::DebugString()/[CF] Dont reuse dropped column family IDs

Summary:
Column family IDs should be unique, even if column family is dropped. To achieve this, we save max column family in manifest.

Note that the diff is still not ready. I'm only using differential to move the patch to my Mac machine.

Test Plan: added a test to column_family_test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16581/"
,Thread management,Rocksdb,"Flush stale column families

Summary:
Added a new option `max_total_wal_size`. Once the total WAL size goes over that, we make an attempt to flush all column families that still have data in the earliest WAL file.

By default, I calculate `max_total_wal_size` dynamically, that should be good-enough for non-advanced customers.

Test Plan: Added a test

Reviewers: dhruba, haobo, sdong, ljin, yhchiang

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18345/RocksDBLite

Summary:
Introducing RocksDBLite! Removes all the non-essential features and reduces the binary size. This effort should help our adoption on mobile.

Binary size when compiling for IOS (`TARGET_OS=IOS m static_lib`) is down to 9MB from 15MB (without stripping)

Test Plan: compiles :)

Reviewers: dhruba, haobo, ljin, sdong, yhchiang

Reviewed By: yhchiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17835/Fix data race against logging data structure because of LogBuffer

Summary:
@igor pointed out that there is a potential data race because of the way we use the newly introduced LogBuffer. After ""bg_compaction_scheduled_--"" or ""bg_flush_scheduled_--"", they can both become 0. As soon as the lock is released after that, DBImpl's deconstructor can go ahead and deconstruct all the states inside DB, including the info_log object hold in a shared pointer of the options object it keeps. At that point it is not safe anymore to continue using the info logger to write the delayed logs.

With the patch, lock is released temporarily for log buffer to be flushed before ""bg_compaction_scheduled_--"" or ""bg_flush_scheduled_--"". In order to make sure we don't miss any pending flush or compaction, a new flag bg_schedule_needed_ is added, which is set to be true if there is a pending flush or compaction but not scheduled because of the max thread limit. If the flag is set to be true, the scheduling function will be called before compaction or flush thread finishes.

Thanks @igor for this finding!

Test Plan: make all check

Reviewers: haobo, igor

Reviewed By: haobo

CC: dhruba, ljin, yhchiang, igor, leveldb

Differential Revision: https://reviews.facebook.net/D16767/[RocksDB] LogBuffer Cleanup

Summary: Moved LogBuffer class to an internal header. Removed some unneccesary indirection. Enabled log buffer for BackgroundCallFlush. Forced log buffer flush right after Unlock to improve time ordering of info log.

Test Plan: make check; db_bench compare LOG output

Reviewers: sdong

Reviewed By: sdong

CC: leveldb, igor

Differential Revision: https://reviews.facebook.net/D16707/Buffer info logs when picking compactions and write them out after releasing the mutex

Summary: Now while the background thread is picking compactions, it writes out multiple info_logs, especially for universal compaction, which introduces a chance of waiting log writing in mutex, which is bad. To remove this risk, write all those info logs to a buffer and flush it after releasing the mutex.

Test Plan:
make all check
check the log lines while running some tests that trigger compactions.

Reviewers: haobo, igor, dhruba

Reviewed By: dhruba

CC: i.am.jin.lei, dhruba, yhchiang, leveldb, nkg-

Differential Revision: https://reviews.facebook.net/D16515/Add ReadOptions to TransactionLogIterator.

Summary:
Add an optional input parameter ReadOptions to DB::GetUpdateSince(),
which allows the verification of checksums to be disabled by setting
ReadOptions::verify_checksums to false.

Test Plan: Tests are done off-line and will not be included in the regular unit test.

Reviewers: igor

Reviewed By: igor

CC: leveldb, xjin, dhruba

Differential Revision: https://reviews.facebook.net/D16305/Fix table properties

Summary: Adapt table properties to column family world

Test Plan: make check

Reviewers: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16161/[CF] DB test to run on non-default column family

Summary:
This is a huge diff and it was hectic, but the idea is actually quite simple. Every operation (Put, Get, etc.) done on default column family in DBTest is now forwarded to non-default (""pikachu""). The good news is that we had zero test failures! Column families look stable so far.

One interesting test that I adapted for column families is MultiThreadedTest. I replaced every Put() with a WriteBatch writing to all column families concurrently. Every Put in the write batch contains unique_id. Instead of Get() I do a multiget across all column families with the same key. If atomicity holds, I expect to see the same unique_id in all column families.

Test Plan: This is a test!

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16149/Expose the table properties to application

Summary: Provide a public API for users to access the table properties for each SSTable.

Test Plan: Added a unit tests to test the function correctness under differnet conditions.

Reviewers: haobo, dhruba, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16083/"
,,Rocksdb,"Make CompactionInputErrorParanoid less flakey

Summary:
I'm getting lots of e-mails with CompactionInputErrorParanoid failing. Most recent example early morning today was: http://ci-builds.fb.com/job/rocksdb_valgrind/562/consoleFull

I'm putting a stop to these e-mails. I investigated why the test is flakey and it turns out it's because of non-determinsim of compaction scheduling. If there is a compaction after the last flush, CorruptFile will corrupt the compacted file instead of file at level 0 (as it assumes). That makes `Check(9, 9)` fail big time.

I also saw some errors with table file getting outputed to >= 1 levels instead of 0. Also fixed that.

Test Plan: Ran corruption_test 100 times without a failure. Previously it usually failed at 10th occurrence.

Reviewers: dhruba, haobo, ljin

Reviewed By: ljin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18285/Sanity check on Open

Summary:
Everytime a client opens a DB, we do a sanity check that:
* checks the existance of all the necessary files
* verifies that file sizes are correct

Some of the code was stolen from https://reviews.facebook.net/D16935

Test Plan: added a unit test

Reviewers: dhruba, haobo, sdong

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17097/"
,,Rocksdb,"Expose number of entries in mem tables to users

Summary: In this patch, two new DB properties are defined: rocksdb.num-immutable-mem-table and rocksdb.num-entries-imm-mem-tables, from where number of entries in mem tables can be exposed to users

Test Plan:
Cover the codes in db_test
make all check

Reviewers: haobo, ljin, igor

Reviewed By: igor

CC: nkg-, igor, yhchiang, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18207/A heuristic way to check if a memtable is full

Summary:
This is is based on https://reviews.facebook.net/D15027. It's not finished but I would like to give a prototype to avoid arena over-allocation while making better use of the already allocated memory blocks.

Instead of check approximate memtable size, we will take a deeper look at the arena, which incorporate essential idea that @sdong suggests: flush when arena has allocated its last and the last is ""almost full""

Test Plan: N/A

Reviewers: haobo, sdong

Reviewed By: sdong

CC: leveldb, sdong

Differential Revision: https://reviews.facebook.net/D15051/"
,,Rocksdb,"disable the log_number check in Recover()

Summary:
There is a chance that an old MANIFEST is corrupted in 2.7 but just not noticed.
This check would fail them. Change it to log instead of returning a
Corruption status.

Test Plan: make

Reviewers: haobo, igor

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16923/[CF] Dont reuse dropped column family IDs

Summary:
Column family IDs should be unique, even if column family is dropped. To achieve this, we save max column family in manifest.

Note that the diff is still not ready. I'm only using differential to move the patch to my Mac machine.

Test Plan: added a test to column_family_test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16581/[CF] Rething LogAndApply for column families

Summary:
I though I might get away with as little changes to LogAndApply() as possible. It turns out this is not the case.

This diff introduces different behavior of LogAndApply() for three cases:
1. column family add
2. column family drop
3. no-column family manipulation

(1) and (2) don't support group commit yet.

There were a lot of problems with old version od LogAndApply, detected by db_stress. The biggest was non-atomicity of manifest writes and metadata changes (i.e. if column family add is in manifest, it also has to be in in-memory data structure).

Test Plan: db_stress

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16491/"
Memory management,Memory management,Rocksdb,"Check PrefixMayMatch on Seek()

Summary:
As a follow-up diff for https://reviews.facebook.net/D17805, add
optimization to check PrefixMayMatch on Seek()

Test Plan: make all check

Reviewers: igor, haobo, sdong, yhchiang, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17853/Column family support for DB::OpenForReadOnly()

Summary: When opening DB in read-only mode, client can choose to only specify a subset of column families (""default"" column family can't be omitted, though)

Test Plan: added a unit test in column_family_test

Reviewers: haobo, sdong, ljin, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17565/Don't Finalize in CompactionPicker

Summary:
Finalize re-sorts (read: mutates) the files_ in Version* and it is called by CompactionPicker during normal runtime. At the same time, this same Version* lives in the SuperVersion* and is accessed without the mutex in GetImpl() code path.

Mutating the files_ in one thread and reading the same files_ in another thread is a bad idea. It caused this issue: http://ci-builds.fb.com/job/rocksdb_crashtest/285/console

Long-term, we need to be more careful with method contracts and clearly document what state can be mutated when. Now that we are much faster because we don't lock in GetImpl(), we keep running into data races that were not a problem before when we were slower. db_stress has been very helpful in detecting those.

Short-term, I removed Finalize() from CompactionPicker.

Note: I believe this is an issue in current 2.7 version running in production.

Test Plan:
make check
Will also run db_stress to see if issue is gone

Reviewers: sdong, ljin, dhruba, haobo

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16983/Fix race condition in manifest roll

Summary:
When the manifest is getting rolled the following happens:
1) manifest_file_number_ is assigned to a new manifest number (even though the old one is still current)
2) mutex is unlocked
3) SetCurrentFile() creates temporary file manifest_file_number_.dbtmp
4) SetCurrentFile() renames manifest_file_number_.dbtmp to CURRENT
5) mutex is locked

If FindObsoleteFiles happens between (3) and (4) it will:
1) Delete manifest_file_number_.dbtmp (because it's not in pending_outputs_)
2) Delete old manifest (because the manifest_file_number_ already points to a new one)

I introduce the concept of prev_manifest_file_number_ that will avoid the race condition.

However, we should discuss the future of MANIFEST file rolling. We found some race conditions with it last week and who knows how many more are there. Nobody is using it in production because we don't trust the implementation. Should we even support it?

Test Plan: make check

Reviewers: ljin, dhruba, haobo, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16929/Merge branch 'master' into columnfamilies

Conflicts:
	db/compaction_picker.cc
	db/db_impl.cc
	db/db_impl.h
	db/tailing_iter.cc
	db/version_set.h
	include/rocksdb/options.h
	util/options.cc/[CF] Rething LogAndApply for column families

Summary:
I though I might get away with as little changes to LogAndApply() as possible. It turns out this is not the case.

This diff introduces different behavior of LogAndApply() for three cases:
1. column family add
2. column family drop
3. no-column family manipulation

(1) and (2) don't support group commit yet.

There were a lot of problems with old version od LogAndApply, detected by db_stress. The biggest was non-atomicity of manifest writes and metadata changes (i.e. if column family add is in manifest, it also has to be in in-memory data structure).

Test Plan: db_stress

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16491/[CF] Log deletion in column families

Summary:
* Added unit test that verifies that obsolete files are deleted.
* Advance log number for empty column family when cutting log file.
* MinLogNumber() bug fix! (caught by the new unit test)

Test Plan: unit test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16311/"
,,Rocksdb,"[CF] Code cleanup part 1

Summary:
I'm cleaning up some code preparing for the big diff review tomorrow. This is the first part of the cleanup.

Changes are mostly cosmetic. The goal is to decrease amount of code difference between columnfamilies and master branch.

This diff also fixes race condition when dropping column family.

Test Plan: Ran db_stress with variety of parameters

Reviewers: dhruba, haobo

Differential Revision: https://reviews.facebook.net/D16833/"
Memory management,Memory management,Rocksdb,"Compaction with zero outputs

Summary: We had a hypothesis in https://reviews.facebook.net/D18507 that empty-string internal keys might have been caused by compaction filter deleting all the entries. I added a unit test for that case. Unforutnately, everything works as expected.

Test Plan: this is a test

Reviewers: dhruba, haobo, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18519/xxHash

Summary:
Originally: https://github.com/facebook/rocksdb//87/files

I'm taking over to apply some finishing touches

Test Plan: will add tests

Reviewers: dhruba, haobo, sdong, yhchiang, ljin

Reviewed By: yhchiang

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18315/Add a new mem-table representation based on cuckoo hash.

Summary:
= Major Changes =
* Add a new mem-table representation, HashCuckooRep, which is based cuckoo hash.
  Cuckoo hash uses multiple hash functions.  This allows each key to have multiple
  possible locations in the mem-table.

  - Put: When insert a key, it will try to find whether one of its possible
    locations is vacant and store the key.  If none of its possible
    locations are available, then it will kick out a victim key and
    store at that location.  The kicked-out victim key will then be
    stored at a vacant space of its possible locations or kick-out
    another victim.  In this diff, the kick-out path (known as
    cuckoo-path) is found using BFS, which guarantees to be the shortest.

 - Get: Simply tries all possible locations of a key --- this guarantees
   worst-case constant time complexity.

 - Time complexity: O(1) for Get, and average O(1) for Put if the
   fullness of the mem-table is below 80%.

 - Default using two hash functions, the number of hash functions used
   by the cuckoo-hash may dynamically increase if it fails to find a
   short-enough kick-out path.

 - Currently, HashCuckooRep does not support iteration and snapshots,
   as our current main purpose of this is to optimize point access.

= Minor Changes =
* Add IsSnapshotSupported() to DB to indicate whether the current DB
  supports snapshots.  If it returns false, then DB::GetSnapshot() will
  always return nullptr.

Test Plan:
Run existing tests.  Will develop a test specifically for cuckoo hash in
the next diff.

Reviewers: sdong, haobo

Reviewed By: sdong

CC: leveldb, dhruba, igor

Differential Revision: https://reviews.facebook.net/D16155/Cache result of ReadFirstRecord()

Summary:
ReadFirstRecord() reads the actual log file from disk on every call. This diff introduces a cache layer on top of ReadFirstRecord(), which should significantly speed up repeated calls to GetUpdatesSince().

I also cleaned up some stuff, but the whole TransactionLogIterator could use some refactoring, especially if we see increased usage.

Test Plan: make check

Reviewers: haobo, sdong, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18387/avoid calling FindFile twice in TwoLevelIterator for PlainTable

Summary:
this is to reclaim the regression introduced in
https://reviews.facebook.net/D17853

Test Plan: make all check

Reviewers: igor, haobo, sdong, dhruba, yhchiang

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17985/kill ReadOptions.prefix and .prefix_seek

Summary:
also add an override option total_order_iteration if you want to use full
iterator with prefix_extractor

Test Plan: make all check

Reviewers: igor, haobo, sdong, yhchiang

Reviewed By: haobo

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D17805/Revert ""Better port::Mutex::AssertHeld() and AssertNotHeld()""

This reverts commit ddafceb6c2ecb83b7bdf6711ea1c30d97aeb3b8f./Temporarily disable a test case in db_test

Summary:

Root cause is still under investigation. Just Disable the troubling use case for now./Enable hash index for block-based table

Summary: Based on previous patches, this diff eventually provides the end-to-end mechanism for users to specify the hash-index.

Test Plan: Wrote several new unit tests.

Reviewers: sdong, haobo, dhruba

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16539/Turn on -Wmissing-prototypes

Summary: Compiling for iOS has by default turned on -Wmissing-prototypes, which causes rocksdb to fail compiling. This diff turns on -Wmissing-prototypes in our compile options and cleans up all functions with missing prototypes.

Test Plan: compiles

Reviewers: dhruba, haobo, ljin, sdong

Reviewed By: ljin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17649/Column family support for DB::OpenForReadOnly()

Summary: When opening DB in read-only mode, client can choose to only specify a subset of column families (""default"" column family can't be omitted, though)

Test Plan: added a unit test in column_family_test

Reviewers: haobo, sdong, ljin, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17565/Fix GetProperty() test

Summary:
GetProperty test is flakey.
Before this diff: P8635927
After: P8635945

We need to make sure the thread is done before we destruct sleeping tasks. Otherwise, bad things happen.

Test Plan: See summary

Reviewers: ljin, sdong, haobo, dhruba

Reviewed By: ljin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17595/[RocksDB] Fix a race condition in GetSortedWalFiles

Summary: This patch fixed a race condition where a log file is moved to archived dir in the middle of GetSortedWalFiles. Without the fix, the log file would be missed in the result, which leads to transaction log iterator gap. A test utility SyncPoint is added to help reproducing the race condition.

Test Plan: TransactionLogIteratorRace; make check

Reviewers: dhruba, ljin

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17121/DBIter to use static allocated char array for saved_key_ (if it is not too long)

Summary: DBIter now uses a std::string for saved_key. Based on some profiling, it could be more expensive than we though. Optimize it with the same technique as LookupKey -- if it is short, we copy it to a static allocated char. Otherwise, dynamically allocate memory for it.

Test Plan: make all check

Reviewers: haobo, ljin

Reviewed By: haobo

CC: dhruba, igor, yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D17289/Change default value of some Options

Summary: Since we are optimizing for server workloads, some default values are not optimized any more. We change some of those values that I feel it's less prone to regression bugs.

Test Plan: make all check

Reviewers: dhruba, haobo, ljin, igor, yhchiang

Reviewed By: igor

CC: leveldb, MarkCallaghan

Differential Revision: https://reviews.facebook.net/D16995/[rocksdb] make init prefix more robust

Summary:
Currently if client uses kNULLString as the prefix, it will confuse
compaction filter v2. This diff added a bool to indicate if the prefix
has been intialized. I also added a unit test to cover this case and
make sure the new code path is hit.

Test Plan: db_test

Reviewers: igor, haobo

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17151/[rocksdb] new CompactionFilterV2 API

Summary:
This diff adds a new CompactionFilterV2 API that roll up the
decisions of kv pairs during compactions. These kv pairs must share the
same key prefix. They are buffered inside the db.

    typedef std::vector<Slice> SliceVector;
    virtual std::vector<bool> Filter(int level,
                                 const SliceVector& keys,
                                 const SliceVector& existing_values,
                                 std::vector<std::string>* new_values,
                                 std::vector<bool>* values_changed
                                 ) const = 0;

Application can override the Filter() function to operate
on the buffered kv pairs. More details in the inline documentation.

Test Plan:
make check. Added unit tests to make sure Keep, Delete,
Change all works.

Reviewers: haobo

CCs: leveldb

Differential Revision: https://reviews.facebook.net/D15087/Add a unit test to verify compaction filter context

Summary: Add unit tests to make sure CompactionFilterContext::is_manual_compaction_ and CompactionFilterContext::is_full_compaction_ are set correctly.

Test Plan: run the new tests.

Reviewers: haobo, igor, dhruba, yhchiang, ljin

Reviewed By: haobo

CC: nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D17067/Fix compile issue in Mac OS

Summary:
Compile issues are:
* Unused variable env_
* Unused fallocate_with_keep_size_

Test Plan: compiles

Reviewers: dhruba, haobo, sdong

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17043/Add a DB property to indicate number of background errors encountered

Summary: Add a property to calculate number of background errors encountered to help users build their monitoring

Test Plan: Add a unit test. make all check

Reviewers: haobo, igor, dhruba

Reviewed By: igor

CC: ljin, nkg-, yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D16959/Several easy-to-add properties related to compaction and flushes

Summary: To partly address the request @nkg- raised, add three easy-to-add properties to compactions and flushes.

Test Plan: run unit tests and add a new unit test to cover new properties.

Reviewers: haobo, dhruba

Reviewed By: dhruba

CC: nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D13677/Fix a bug that Prev() can hang.

Summary: Prev() now can hang when there is a key with more than max_skipped number of appearance internally but all of them are newer than the sequence ID to seek. Add unit tests to confirm the bug and fix it.

Test Plan: make all check

Reviewers: igor, haobo

Reviewed By: igor

CC: ljin, yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D16899/Change WriteBatch interface/A heuristic way to check if a memtable is full

Summary:
This is is based on https://reviews.facebook.net/D15027. It's not finished but I would like to give a prototype to avoid arena over-allocation while making better use of the already allocated memory blocks.

Instead of check approximate memtable size, we will take a deeper look at the arena, which incorporate essential idea that @sdong suggests: flush when arena has allocated its last and the last is ""almost full""

Test Plan: N/A

Reviewers: haobo, sdong

Reviewed By: sdong

CC: leveldb, sdong

Differential Revision: https://reviews.facebook.net/D15051/Bug fixes introduced by code cleanup/Fix bad merge of D16791 and D16767

Summary: A bad Auto-Merge caused log buffer is flushed twice. Remove the unintended one.

Test Plan: Should already be tested (the code looks the same as when I ran unit tests).

Reviewers: haobo, igor

Reviewed By: haobo

CC: ljin, yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D16821/Consolidate SliceTransform object ownership

Summary:
(1) Fix SanitizeOptions() to also check HashLinkList. The current
dynamic case just happens to work because the 2 classes have the same
layout.
(2) Do not delete SliceTransform object in HashSkipListFactory and
HashLinkListFactory destructor. Reason: SanitizeOptions() enforces
prefix_extractor and SliceTransform to be the same object when
Hash**Factory is used. This makes the behavior strange: when
Hash**Factory is used, prefix_extractor will be released by RocksDB. If
other memtable factory is used, prefix_extractor should be released by
user.

Test Plan: db_bench && make asan_check

Reviewers: haobo, igor, sdong

Reviewed By: igor

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D16587/MergingIterator assertion

Summary: I wrote a test that triggers assertion in MergingIterator. I have not touched that code ever, so I'm looking for somebody with good understanding of the MergingIterator code to fix this. The solution is probably a one-liner. Let me know if you're willing to take a look.

Test Plan: This test fails with an assertion `use_heap_ == false`

Reviewers: dhruba, haobo, sdong, kailiu

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16521/Add ReadOptions to TransactionLogIterator.

Summary:
Add an optional input parameter ReadOptions to DB::GetUpdateSince(),
which allows the verification of checksums to be disabled by setting
ReadOptions::verify_checksums to false.

Test Plan: Tests are done off-line and will not be included in the regular unit test.

Reviewers: igor

Reviewed By: igor

CC: leveldb, xjin, dhruba

Differential Revision: https://reviews.facebook.net/D16305/[CF] Handle failure in WriteBatch::Handler

Summary:
* Add ColumnFamilyHandle::GetID() function. Client needs to know column family's ID to be able to construct WriteBatch
* Handle WriteBatch::Handler failure gracefully. Since WriteBatch is not a very smart function (it takes raw CF id), client can add data to WriteBatch for column family that doesn't exist. In that case, we need to gracefully return failure status from DB::Write(). To do that, I added a return Status to WriteBatch functions PutCF, DeleteCF and MergeCF.

Test Plan: Added test to column_family_test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16323/some improvements to CompressedCache test/Fix table properties

Summary: Adapt table properties to column family world

Test Plan: make check

Reviewers: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16161/[CF] DB test to run on non-default column family

Summary:
This is a huge diff and it was hectic, but the idea is actually quite simple. Every operation (Put, Get, etc.) done on default column family in DBTest is now forwarded to non-default (""pikachu""). The good news is that we had zero test failures! Column families look stable so far.

One interesting test that I adapted for column families is MultiThreadedTest. I replaced every Put() with a WriteBatch writing to all column families concurrently. Every Put in the write batch contains unique_id. Instead of Get() I do a multiget across all column families with the same key. If atomicity holds, I expect to see the same unique_id in all column families.

Test Plan: This is a test!

Reviewers: dhruba, haobo, kailiu, sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16149/Expose the table properties to application

Summary: Provide a public API for users to access the table properties for each SSTable.

Test Plan: Added a unit tests to test the function correctness under differnet conditions.

Reviewers: haobo, dhruba, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16083/"
,,Rocksdb,"Fix the failure of stringappend_test caused by PartialMergeMulti.

Summary:
Fix a bug that PartialMergeMulti will try to merge the first operand
with an empty slice.

Test Plan: run stringappend_test and merge_test.

Reviewers: haobo, igor

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17157/Enhance partial merge to support multiple arguments

Summary:
* PartialMerge api now takes a list of operands instead of two operands.
* Add min_pertial_merge_operands to Options, indicating the minimum
  number of operands to trigger partial merge.
* This diff is based on Schalk's previous diff (D14601), but it also
  includes necessary changes such as updating the pure C api for
  partial merge.

Test Plan:
* make check all
* develop tests for cases where partial merge takes more than two
  operands.

TODOs (from Schalk):
* Add test with min_partial_merge_operands > 2.
* Perform benchmarks to measure the performance improvements (can probably
  use results of task #2837810.)
* Add description of problem to doc/index.html.
* Change wiki pages to reflect the interface changes.

Reviewers: haobo, igor, vamsi

Reviewed By: haobo

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D16815/"
Feature migration,Feature migration,Rocksdb,"Check PrefixMayMatch on Seek()

Summary:
As a follow-up diff for https://reviews.facebook.net/D17805, add
optimization to check PrefixMayMatch on Seek()

Test Plan: make all check

Reviewers: igor, haobo, sdong, yhchiang, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17853/Column family logging

Summary:
Now that we have column families involved, we need to add extra context to every log message. They now start with ""[column family name] log message""

Also added some logging that I think would be useful, like level summary after every flush (I often needed that when going through the logs).

Test Plan: make check + ran db_bench to confirm I'm happy with log output

Reviewers: dhruba, haobo, ljin, yhchiang, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18303/hints for narrowing down FindFile range and avoiding checking unrelevant L0 files

Summary:
The file tree structure in Version is prebuilt and the range of each file is known.
On the Get() code path, we do binary search in FindFile() by comparing
target key with each file's largest key and also check the range for each L0 file.
With some pre-calculated knowledge, each key comparision that has been done can serve
as a hint to narrow down further searches:
(1) If a key falls within a L0 file's range, we can safely skip the next
file if its range does not overlap with the current one.
(2) If a key falls within a file's range in level L0 - Ln-1, we should only
need to binary search in the next level for files that overlap with the current one.

(1) will be able to skip some files depending one the key distribution.
(2) can greatly reduce the range of binary search, especially for bottom
levels, given that one file most likely only overlaps with N files from
the level below (where N is max_bytes_for_level_multiplier). So on level
L, we will only look at ~N files instead of N^L files.

Some inital results: measured with 500M key DB, when write is light (10k/s = 1.2M/s), this
improves QPS ~7% on top of blocked bloom. When write is heavier (80k/s =
9.6M/s), it gives us ~13% improvement.

Test Plan: make all check

Reviewers: haobo, igor, dhruba, sdong, yhchiang

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17205/Minimize accessing multiple objects in Version::Get()

Summary:
One of our profilings shows that Version::Get() sometimes is slow when getting pointer of user comparators or other global objects. In this patch:
(1) we keep pointers of immutable objects in Version to avoid accesses them though option objects or cfd objects
(2) table_reader is directly cached in FileMetaData so that table cache don't have to go through handle first to fetch it
(3) If level 0 has less than 3 files, skip the filtering logic based on SST tables' key range. Smallest and largest key are stored in separated memory locations, which has potential cache misses

Test Plan: make all check

Reviewers: haobo, ljin

Reviewed By: haobo

CC: igor, yhchiang, nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D17739/Column family support for DB::OpenForReadOnly()

Summary: When opening DB in read-only mode, client can choose to only specify a subset of column families (""default"" column family can't be omitted, though)

Test Plan: added a unit test in column_family_test

Reviewers: haobo, sdong, ljin, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17565/Remove env_ from MergingIterator

Summary: env_ is not used. Compiling for iOS complains.

Test Plan: compiles now

Reviewers: ljin, haobo, sdong, dhruba

Reviewed By: ljin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17589/Fix valgrind error caused by FileMetaData as two level iterator's index block handle

Summary: It is a regression valgrind bug caused by using FileMetaData as index block handle. One of the fields of FileMetaData is not initialized after being contructed and copied, but I'm not able to find which one. Also, I realized that it's not a good idea to use FileMetaData as in TwoLevelIterator::InitDataBlock(), a copied FileMetaData can be compared with the one in version set byte by byte, but the refs can be changed. Also comparing such a large structure is slightly more expensive. Use a simpler structure instead

Test Plan:
Run the failing valgrind test (Harness.RandomizedLongDB)
make all check

Reviewers: igor, haobo, ljin

Reviewed By: igor

CC: yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D17409/Don't store version number in MANIFEST

Summary: Talked to <insert internal project name> folks and they found it really scary that they won't be able to roll back once they upgrade to 2.8. We should fix this.

Test Plan: make check

Reviewers: haobo, ljin

Reviewed By: ljin

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17343/[RocksDB] Preallocate new MANIFEST files

Summary: We don't preallocate MANIFEST file, even though we have an option for that. This diff preallocates manifest file every time we create it

Test Plan: make check

Reviewers: dhruba, haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17163/When Options.max_num_files=-1, non level0 files also by pass table cache

Summary:
This is the part that was not finished when doing the Options.max_num_files=-1 feature. For iterating non level0 SST files (which was done using two level iterator), table cache is not bypassed. With this patch, the leftover feature is done.

Test Plan: make all check; change Options.max_num_files=-1 in one of the tests to cover the codes.

Reviewers: haobo, igor, dhruba, ljin, yhchiang

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17001/ComputeCompactionScore in CompactionPicker

Summary:
As it turns out, we need the call to ComputeCompactionScore (previously: Finalize) in CompactionPicker.

The issue caused a deadlock in db_stress: http://ci-builds.fb.com/job/rocksdb_crashtest/290/console

The last two lines before a deadlock were:
2014/03/18-22:43:41.481029 7facafbee700 (Original Log Time 2014/03/18-22:43:41.480989) Compaction nothing to do
2014/03/18-22:43:41.481041 7faccf7fc700 wait for fewer level0 files...

""Compaction nothing to do"" and other thread waiting for fewer level0 files. Hm hm.

I moved the pre-sorting to SaveTo, which should fix both the original and the new issue.

Test Plan: make check for now, will run db_stress in jenkins

Reviewers: dhruba, haobo, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17037/avoid shared_ptr assignment in Version::Get()

Summary:
This is a 500ns operation while the whole Get() call takes only a few
micro!

Test Plan: ran db_bench, for a DB with 50M keys, QPS jumps from 5.2M/s to 7.2M/s

Reviewers: haobo, igor, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17007/Don't Finalize in CompactionPicker

Summary:
Finalize re-sorts (read: mutates) the files_ in Version* and it is called by CompactionPicker during normal runtime. At the same time, this same Version* lives in the SuperVersion* and is accessed without the mutex in GetImpl() code path.

Mutating the files_ in one thread and reading the same files_ in another thread is a bad idea. It caused this issue: http://ci-builds.fb.com/job/rocksdb_crashtest/285/console

Long-term, we need to be more careful with method contracts and clearly document what state can be mutated when. Now that we are much faster because we don't lock in GetImpl(), we keep running into data races that were not a problem before when we were slower. db_stress has been very helpful in detecting those.

Short-term, I removed Finalize() from CompactionPicker.

Note: I believe this is an issue in current 2.7 version running in production.

Test Plan:
make check
Will also run db_stress to see if issue is gone

Reviewers: sdong, ljin, dhruba, haobo

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16983/disable the log_number check in Recover()

Summary:
There is a chance that an old MANIFEST is corrupted in 2.7 but just not noticed.
This check would fail them. Change it to log instead of returning a
Corruption status.

Test Plan: make

Reviewers: haobo, igor

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16923/Finalize version in dumpmanifest/Optimize fallocation

Summary:
Based on my recent findings (posted in our internal group), if we use fallocate without KEEP_SIZE flag, we get superior performance of fdatasync() in append-only workloads.

This diff provides an option for user to not use KEEP_SIZE flag, thus optimizing his sync performance by up to 2x-3x.

At one point we also just called posix_fallocate instead of fallocate, which isn't very fast: http://code.woboq.org/userspace/glibc/sysdeps/posix/posix_fallocate.c.html (tl;dr it manually writes out zero bytes to allocate storage). This diff also fixes that, by first calling fallocate and then posix_fallocate if fallocate is not supported.

Test Plan: make check

Reviewers: dhruba, sdong, haobo, ljin

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16761/Fix race condition in manifest roll

Summary:
When the manifest is getting rolled the following happens:
1) manifest_file_number_ is assigned to a new manifest number (even though the old one is still current)
2) mutex is unlocked
3) SetCurrentFile() creates temporary file manifest_file_number_.dbtmp
4) SetCurrentFile() renames manifest_file_number_.dbtmp to CURRENT
5) mutex is locked

If FindObsoleteFiles happens between (3) and (4) it will:
1) Delete manifest_file_number_.dbtmp (because it's not in pending_outputs_)
2) Delete old manifest (because the manifest_file_number_ already points to a new one)

I introduce the concept of prev_manifest_file_number_ that will avoid the race condition.

However, we should discuss the future of MANIFEST file rolling. We found some race conditions with it last week and who knows how many more are there. Nobody is using it in production because we don't trust the implementation. Should we even support it?

Test Plan: make check

Reviewers: ljin, dhruba, haobo, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16929/journal log_number correctly in MANIFEST

Summary:
Here is what it can cause probelm:
There is one memtable flush and one compaction. Both call LogAndApply(). If both edits are applied in the same batch with flush edit first and the compaction edit followed. LogAndApplyHelper() will assign compaction edit current VersionSet's log number(which should be smaller than the log number from flush edit). It cause log_numbers in MANIFEST to be not monotonic increasing, which violates the assume Recover() makes. What is more is after comitting to MANIFEST file, log_number_ in VersionSet is updated to the log_number from the last edit, which is the compaction one. It ends up not updating the log_number.

Test Plan:
make whitebox_crash_test
got another assertion about iter->valid(), not sure if that is related
to this.

Reviewers: igor, haobo

Reviewed By: igor

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16875/Fix log_number in LogAndApply/CF cleanup part 2/Bug fixes introduced by code cleanup/[CF] Code cleanup part 1

Summary:
I'm cleaning up some code preparing for the big diff review tomorrow. This is the first part of the cleanup.

Changes are mostly cosmetic. The goal is to decrease amount of code difference between columnfamilies and master branch.

This diff also fixes race condition when dropping column family.

Test Plan: Ran db_stress with variety of parameters

Reviewers: dhruba, haobo

Differential Revision: https://reviews.facebook.net/D16833/[CF] Fix column family dropping

Summary: Column family should be dropped after the change has been commited

Test Plan: db stress

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16779/[CF] Dont reuse dropped column family IDs

Summary:
Column family IDs should be unique, even if column family is dropped. To achieve this, we save max column family in manifest.

Note that the diff is still not ready. I'm only using differential to move the patch to my Mac machine.

Test Plan: added a test to column_family_test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16581/Fix a group commit bug in LogAndApply

Summary:
EncodeTo(&record) does not overwrite, it appends to it.

This means that group commit log and apply will look something like:
record1
record1record2
record1record2record3

I'm surprised this didn't show up in production, but I think the reason is that MANIFEST group commit almost never happens.

This bug turned up in column family work, where opening a database failed with ""adding a same column family twice"".

Test Plan: Tested the change in column family branch and observed that the problem is gone (with db_stress)

Reviewers: dhruba, haobo

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16461/Remove the terrible hack in for flush_block_policy_factory

Summary:
Previous code is too convoluted and I must be drunk for letting
such code to be written without a second thought.

Thanks to the discussion with @sdong, I added the `Options` when
generating the flusher, thus avoiding the tricks.

Just FYI: I resisted to add Options in flush_block_policy.h since I
wanted to avoid cyclic dependencies: FlushBlockPolicy dpends on Options
and Options also depends FlushBlockPolicy... While I appreciate my
effort to prevent it, the old design turns out creating more troubles than
it tried to avoid.

Test Plan: ran ./table_test

Reviewers: sdong

Reviewed By: sdong

CC: sdong, leveldb

Differential Revision: https://reviews.facebook.net/D16503/[CF] Rething LogAndApply for column families

Summary:
I though I might get away with as little changes to LogAndApply() as possible. It turns out this is not the case.

This diff introduces different behavior of LogAndApply() for three cases:
1. column family add
2. column family drop
3. no-column family manipulation

(1) and (2) don't support group commit yet.

There were a lot of problems with old version od LogAndApply, detected by db_stress. The biggest was non-atomicity of manifest writes and metadata changes (i.e. if column family add is in manifest, it also has to be in in-memory data structure).

Test Plan: db_stress

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16491/Make Log::Reader more robust

Summary:
This diff does two things:
(1) Log::Reader does not report a corruption when the last record in a log or manifest file is truncated (meaning that log writer died in the middle of the write). Inherited the code from LevelDB: https://code.google.com/p/leveldb/source/detail?r=269fc6ca9416129248db5ca57050cd5d39d177c8#
(2) Turn off mmap writes for all writes to log and manifest files

(2) is necessary because if we use mmap writes, the last record is not truncated, but is actually filled with zeros, making checksum fail. It is hard to recover from checksum failing.

Test Plan:
Added unit tests from LevelDB
Actually recovered a ""corrupted"" MANIFEST file.

Reviewers: dhruba, haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16119/Fix LogAndApply() group commit/Set dropped column family before persisting in the manifest/[CF] Small refactor of Recover() and DumpManifest()/[CF] CreateColumnFamily fix

Summary:
This fixes few bugs with CreateColumnFamily
* We first have to LogAndApply and then call VersionSet::CreateColumnFamily. Otherwise, WriteSnapshot might be invoked, writing out column family add inside of LogAndApply, even though it's not really committed
* Fix LogAndApplyHelper() to not apply log number to column_family_data, which is in case of column family add, just a dummy (default) column family
* Create SuperVerion when creating column family

Test Plan: column_family_test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16443/[CF] Column family support for LDB tool

Summary: Added list_column_family command and also updated dump_manifest

Test Plan: no

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16419/[CF] More tests

Summary: New unit tests for column families

Test Plan: this is a test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16359/Fix table properties

Summary: Adapt table properties to column family world

Test Plan: make check

Reviewers: kailiu

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16161/Expose the table properties to application

Summary: Provide a public API for users to access the table properties for each SSTable.

Test Plan: Added a unit tests to test the function correctness under differnet conditions.

Reviewers: haobo, dhruba, sdong

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16083/"
,,Rocksdb,"[CF] Fix CF bugs in WriteBatch

Summary:
This diff fixes two bugs:
* Increase sequence number even if WriteBatch fails. This is important because WriteBatches in WAL logs have implictly increasing sequence number, even if one update in a write batch fails. This caused some writes to get lost in my CF stress testing
* Tolerate 'invalid column family' errors on recovery. When a column family is dropped, processing WAL logs can have some WriteBatches that still refer to the dropped column family. In recovery environment, we want to ignore those errors. In client's Write() code path, however, we want to return the failure to the client if he's trying to add data to invalid column family.

Test Plan: db_stress's verification works now

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16533/"
,,Rocksdb,"macros for perf_context

Summary: This will allow us to disable them completely for iOS or for better performance

Test Plan: will run make all check

Reviewers: igor, haobo, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17511/DBIter to use static allocated char array for saved_key_ (if it is not too long)

Summary: DBIter now uses a std::string for saved_key. Based on some profiling, it could be more expensive than we though. Optimize it with the same technique as LookupKey -- if it is short, we copy it to a static allocated char. Otherwise, dynamically allocate memory for it.

Test Plan: make all check

Reviewers: haobo, ljin

Reviewed By: haobo

CC: dhruba, igor, yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D17289/"
,,Rocksdb,"Change WriteBatch interface/[CF] Handle failure in WriteBatch::Handler

Summary:
* Add ColumnFamilyHandle::GetID() function. Client needs to know column family's ID to be able to construct WriteBatch
* Handle WriteBatch::Handler failure gracefully. Since WriteBatch is not a very smart function (it takes raw CF id), client can add data to WriteBatch for column family that doesn't exist. In that case, we need to gracefully return failure status from DB::Write(). To do that, I added a return Status to WriteBatch functions PutCF, DeleteCF and MergeCF.

Test Plan: Added test to column_family_test

Reviewers: dhruba, haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16323/"
,,Rocksdb,"Revert ""Revert ""Allow allocating dynamic bloom, plain table indexes and hash linked list from huge page TLB""""
And make the default 0 for hash linked list memtable

This reverts commit d69dc64be78a8da3ce661454655966d11ff61bb6./PlainTableReader to expose index size to users

Summary:
This is a temp solution to expose index sizes to users from PlainTableReader before we persistent them to files.
In this patch, the memory consumption of indexes used by PlainTableReader will be reported as two user defined properties, so that users can monitor them.

Test Plan:
Add a unit test.
make all check`

Reviewers: haobo, ljin

Reviewed By: haobo

CC: nkg-, yhchiang, igor, ljin, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18195/PlainTableIterator not to store copied key in std::string

Summary:
Move PlainTableIterator's copied key from std::string local buffer to avoid paying the extra costs in std::string related to sharing. Reuse the same buffer class in DbIter. Move the class to dbformat.h.

This patch improves iterator performance significantly. Running this benchmark:

./table_reader_bench --num_keys2=17 --iterator --plain_table --time_unit=nanosecond

The average latency is improved to about 750 nanoseconds from 1100 nanoseconds.

Test Plan:
Add a unit test.
make all check

Reviewers: haobo, ljin

Reviewed By: haobo

CC: igor, yhchiang, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D17547/PlainTableIterator::Seek() shouldn't check bloom filter in total order mode

Summary:
In total order mode, iterator's seek() shouldn't check total order.

Also some cleaning up about checking null for shared pointers. I don't know the behavior before it.

Summary:
1. Add some more implementation-aware tests for PlainTable
2. move from a hard-coded one index per 16 rows in one prefix to a configurable number. Also, make hash table ratio = 0  means binary search only. Also fixes some divide 0 risks.
3. Explicitly support total order (only use binary search)
4. some code cleaning up.


Differential Revision: https://reviews.facebook.net/D16023/"
,,Rocksdb,"Revert ""Revert ""Allow allocating dynamic bloom, plain table indexes and hash linked list from huge page TLB""""
And make the default 0 for hash linked list memtable

This reverts commit d69dc64be78a8da3ce661454655966d11ff61bb6./Add a new mem-table representation based on cuckoo hash.

Summary:
= Major Changes =
* Add a new mem-table representation, HashCuckooRep, which is based cuckoo hash.
  Cuckoo hash uses multiple hash functions.  This allows each key to have multiple
  possible locations in the mem-table.

  - Put: When insert a key, it will try to find whether one of its possible
    locations is vacant and store the key.  If none of its possible
    locations are available, then it will kick out a victim key and
    store at that location.  The kicked-out victim key will then be
    stored at a vacant space of its possible locations or kick-out
    another victim.  In this diff, the kick-out path (known as
    cuckoo-path) is found using BFS, which guarantees to be the shortest.

 - Get: Simply tries all possible locations of a key --- this guarantees
   worst-case constant time complexity.

 - Time complexity: O(1) for Get, and average O(1) for Put if the
   fullness of the mem-table is below 80%.

 - Default using two hash functions, the number of hash functions used
   by the cuckoo-hash may dynamically increase if it fails to find a
   short-enough kick-out path.

 - Currently, HashCuckooRep does not support iteration and snapshots,
   as our current main purpose of this is to optimize point access.

= Minor Changes =
* Add IsSnapshotSupported() to DB to indicate whether the current DB
  supports snapshots.  If it returns false, then DB::GetSnapshot() will
  always return nullptr.

Test Plan:
Run existing tests.  Will develop a test specifically for cuckoo hash in
the next diff.

Reviewers: sdong, haobo

Reviewed By: sdong

CC: leveldb, dhruba, igor

Differential Revision: https://reviews.facebook.net/D16155/Expose number of entries in mem tables to users

Summary: In this patch, two new DB properties are defined: rocksdb.num-immutable-mem-table and rocksdb.num-entries-imm-mem-tables, from where number of entries in mem tables can be exposed to users

Test Plan:
Cover the codes in db_test
make all check

Reviewers: haobo, ljin, igor

Reviewed By: igor

CC: nkg-, igor, yhchiang, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18207/macros for perf_context

Summary: This will allow us to disable them completely for iOS or for better performance

Test Plan: will run make all check

Reviewers: igor, haobo, dhruba

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17511/make hash_link_list Node's key space consecutively followed at the end

Summary: per sdong's , this will help processor prefetch on n->key case.

Test Plan: make all check

Reviewers: sdong, haobo, igor

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17415/MemTableIterator not to reference Memtable

Summary: In one of the perf, I shows 10%-25% CPU costs of MemTableIterator.Seek(), when doing dynamic prefix seek, are spent on checking whether we need to do bloom filter check or finding out the prefix extractor. Seems that  more level of pointer checking makes CPU cache miss more likely. This patch makes things slightly simpler by copying pointer of bloom of prefix extractor into the iterator.

Test Plan: make all check

Reviewers: haobo, ljin

Reviewed By: ljin

CC: igor, dhruba, yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D17247/A heuristic way to check if a memtable is full

Summary:
This is is based on https://reviews.facebook.net/D15027. It's not finished but I would like to give a prototype to avoid arena over-allocation while making better use of the already allocated memory blocks.

Instead of check approximate memtable size, we will take a deeper look at the arena, which incorporate essential idea that @sdong suggests: flush when arena has allocated its last and the last is ""almost full""

Test Plan: N/A

Reviewers: haobo, sdong

Reviewed By: sdong

CC: leveldb, sdong

Differential Revision: https://reviews.facebook.net/D15051/Consolidate SliceTransform object ownership

Summary:
(1) Fix SanitizeOptions() to also check HashLinkList. The current
dynamic case just happens to work because the 2 classes have the same
layout.
(2) Do not delete SliceTransform object in HashSkipListFactory and
HashLinkListFactory destructor. Reason: SanitizeOptions() enforces
prefix_extractor and SliceTransform to be the same object when
Hash**Factory is used. This makes the behavior strange: when
Hash**Factory is used, prefix_extractor will be released by RocksDB. If
other memtable factory is used, prefix_extractor should be released by
user.

Test Plan: db_bench && make asan_check

Reviewers: haobo, igor, sdong

Reviewed By: igor

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D16587/Add a hash-index component for block

Summary:
this is the key component extracted from diff: https://reviews.facebook.net/D14271
I separate it to a dedicated patch to make the review easier.

Test Plan: added a unit test and passed it.

Reviewers: haobo, sdong, dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16245/"
,,Rocksdb,"Simplify cleanup of dead (refcount == 0) column families/Fix MacOS errors/[RocksDB] Fix a race condition in GetSortedWalFiles

Summary: This patch fixed a race condition where a log file is moved to archived dir in the middle of GetSortedWalFiles. Without the fix, the log file would be missed in the result, which leads to transaction log iterator gap. A test utility SyncPoint is added to help reproducing the race condition.

Test Plan: TransactionLogIteratorRace; make check

Reviewers: dhruba, ljin

Reviewed By: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D17121/[CF] Adaptation of GetLiveFiles for CF

Summary: Even if user flushes the memtables before getting live files, we still can't guarantee that new data didn't come in (to already-flushed memtables). If we want backups to provide consistent view of the database, we still need to get WAL files.

Test Plan: backupable_db_test

Reviewers: dhruba

CC: leveldb

Differential Revision: https://reviews.facebook.net/D16299/"
,,Rocksdb,[C-API] added the possiblity to create a HashSkipList or HashLinkedList to support prefix seeks/added a test case for custom merge operator/
,,Rocksdb,"[rocksdb] new CompactionFilterV2 API

Summary:
This diff adds a new CompactionFilterV2 API that roll up the
decisions of kv pairs during compactions. These kv pairs must share the
same key prefix. They are buffered inside the db.

    typedef std::vector<Slice> SliceVector;
    virtual std::vector<bool> Filter(int level,
                                 const SliceVector& keys,
                                 const SliceVector& existing_values,
                                 std::vector<std::string>* new_values,
                                 std::vector<bool>* values_changed
                                 ) const = 0;

Application can override the Filter() function to operate
on the buffered kv pairs. More details in the inline documentation.

Test Plan:
make check. Added unit tests to make sure Keep, Delete,
Change all works.

Reviewers: haobo

CCs: leveldb

Differential Revision: https://reviews.facebook.net/D15087/Enhance partial merge to support multiple arguments

Summary:
* PartialMerge api now takes a list of operands instead of two operands.
* Add min_pertial_merge_operands to Options, indicating the minimum
  number of operands to trigger partial merge.
* This diff is based on Schalk's previous diff (D14601), but it also
  includes necessary changes such as updating the pure C api for
  partial merge.

Test Plan:
* make check all
* develop tests for cases where partial merge takes more than two
  operands.

TODOs (from Schalk):
* Add test with min_partial_merge_operands > 2.
* Perform benchmarks to measure the performance improvements (can probably
  use results of task #2837810.)
* Add description of problem to doc/index.html.
* Change wiki pages to reflect the interface changes.

Reviewers: haobo, igor, vamsi

Reviewed By: haobo

CC: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D16815/"
,,Rocksdb,"Make rocksdb work with all versions of lz4

Summary:
There are some projects in fbcode that define lz4 dependency on r108. We, however, defined dependency on r117. That produced some interesting issues and our build system was not happy.

This diff makes rocksdb work with both r108 and r117. Hopefully this will fix our problems.

Test Plan: compiled rocksdb with both r108 and r117 lz4

Reviewers: dhruba, sdong, haobo

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18465/"
,Thread management,Rocksdb,"Install stack trace handlers in unit tests

Summary: Sometimes, our tests fail because of normal `assert` call. It would be helpful to see stack trace in that case, too.

Test Plan: Added `assert(false)` and verified it prints out stack trace

Reviewers: haobo, dhruba, sdong, ljin, yhchiang

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18291/Print out stack trace in mac, too

Summary: While debugging Mac-only issue with ThreadLocalPtr, this was very useful. Let's print out stack trace in MAC OS, too.

Test Plan: Verified that somewhat useful stack trace was generated on mac. Will run PrintStack() on linux, too.

Reviewers: ljin, haobo

Reviewed By: haobo

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18189/"
,,Rocksdb,"sst_dump: still try to print out table properties even if failing to read the file

Summary: Even if the file is corrupted, table properties are usually available to print out. Now sst_dump would just fail without printing table properties. With this patch, table properties are still try to be printed out.

Test Plan: run sst_dump against multiple scenarios

Reviewers: igor, yhchiang, ljin, haobo

Reviewed By: haobo

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18981/sst_dump: Set dummy prefix extractor for binary search index in block based table

Summary: Now sst_dump fails in block based tables if binary search index is used, as it requires a prefix extractor. Add it.

Test Plan: Run it against such a file to make sure it fixes the problem.

Reviewers: yhchiang, kailiu

Reviewed By: kailiu

Subscribers: ljin, igor, dhruba, haobo, leveldb

Differential Revision: https://reviews.facebook.net/D18927/"
,Thread management,Rocksdb,"In tools/db_stress.cc, set proper value in NewHashSkipListRepFactory's bucket_size

Summary:
    Now that the arena is used to allocate space for hashskiplist's bucket. The bucket size
    need to be set small enough to avoid ""should_flush_"" failure in memtable's assertion.

Test Plan:
    make all check

Reviewers: sdong

Reviewed By: sdong

Subscribers: igor

Differential Revision: https://reviews.facebook.net/D19371/db_stress to add an option to periodically change background thread pool size.

Summary: Add an option to indicates the variation of background threads. If the flag is not 0, for every 100 milliseconds, adjust thread pool size to a value within the range.

Test Plan: run db_stress

Reviewers: haobo, dhruba, igor, ljin

Reviewed By: ljin

Subscribers: leveldb, nkg-

Differential Revision: https://reviews.facebook.net/D18759/"
,,Rocksdb,"make statistics forward-able

Summary:
Make StatisticsImpl being able to forward stats to provided statistics
implementation. The main purpose is to allow us to collect internal
stats in the future even when user supplies custom statistics
implementation. It avoids intrumenting 2 sets of stats collection code.
One immediate use case is tuning advisor, which needs to collect some
internal stats, users may not be interested.

Test Plan:
ran db_bench and see stats show up at the end of run
Will run make all check since some tests rely on statistics

Reviewers: yhchiang, sdong, igor

Reviewed By: sdong

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D20145/"
,,Rocksdb,"Add timeout_hint_us to WriteOptions and introduce Status::TimeOut.

Summary:
This diff adds timeout_hint_us to WriteOptions.  If it's non-zero, then
1) writes associated with this options MAY be aborted when it has been
  waiting for longer than the specified time.  If an abortion happens,
  associated writes will return Status::TimeOut.
2) the stall time of the associated write caused by flush or compaction
  will be limited by timeout_hint_us.

The default value of timeout_hint_us is 0 (i.e., OFF.)

The statistics of timeout writes will be recorded in WRITE_TIMEDOUT.

Test Plan:
export ROCKSDB_TESTS=WriteTimeoutAndDelayTest
make db_test
./db_test

Reviewers: igor, ljin, haobo, sdong

Reviewed By: sdong

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18837/"
,,Rocksdb,"Fix a bug causing LOG is not created when max_log_file_size is set.

Summary:
Fix a bug causing LOG is not created when max_log_file_size is set.
This bug is reported in issue #174.

Test Plan:
Add TEST(AutoRollLoggerTest, LogFileExistence).
make auto_roll_logger_test
./auto_roll_logger_test

Reviewers: haobo, sdong, ljin, igor, igor2

Reviewed By: igor2

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D19053/"
,,Rocksdb,"Adding option to save PlainTable index and bloom filter in SST file.

Summary:
Adding option to save PlainTable index and bloom filter in SST file.
If there is no bloom block and/or index block, PlainTableReader builds
new ones. Otherwise PlainTableReader just use these blocks.

Test Plan: make all check

Reviewers: sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19527/use arena to allocate memtable's bloomfilter and hashskiplist's buckets_

Summary:
    Bloomfilter and hashskiplist's buckets_ allocated by memtable's arena
    DynamicBloom: pass arena via constructor, allocate space in SetTotalBits
    HashSkipListRep: allocate space of buckets_ using arena.
       do not delete it in deconstructor because arena would take care of it.
    Several test files are changed.

Test Plan:
    make all check

Reviewers: ljin, haobo, yhchiang, sdong

Reviewed By: sdong

Subscribers: igor, dhruba

Differential Revision: https://reviews.facebook.net/D19335/prefetch bloom filter data block for L0 files

Summary: as title

Test Plan:
db_bench
the initial result is very promising. I will post results of complete
runs

Reviewers: dhruba, haobo, sdong, igor

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D18867/Clean PlainTableReader's variables for better data locality

Summary:
Clean PlainTableReader's data structures:
(1) inline bloom_ (in order to do this, change DynamicBloom to allow lazy initialization)
(2) remove some variables only used when initialization from the class
(3) put variables not used in normal read code paths to the end of the class and reference prefix_extractor directly
(4) make Options a reference.

Test Plan: make all check

Reviewers: haobo, ljin

Reviewed By: ljin

Subscribers: igor, yhchiang, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18891/dynamic_bloom: replace some divide (remainder) operations with shifts in locality mode, and other improvements

Summary:
This patch changes meaning of options.bloom_locality: 0 means disable cache line optimization and any positive number means use CACHE_LINE_SIZE as block size (the previous behavior is the block size will be CACHE_LINE_SIZE*options.bloom_locality). By doing it, the divide operations inside a block can be replaced by a shift.

Performance is improved:
https://reviews.facebook.net/P471

Also, improve the basic algorithm in two ways:
(1) make sure num of blocks is an odd number
(2) rotate bytes after every probe in locality mode. Since the divider is 2^n, unless doing it, we are never able to use all the bits.
Improvements of false positive: https://reviews.facebook.net/P459

Test Plan: make all check

Reviewers: ljin, haobo

Reviewed By: haobo

Subscribers: dhruba, yhchiang, igor, leveldb

Differential Revision: https://reviews.facebook.net/D18843/"
,,Rocksdb,"Change StopWatch interface

Summary: So that we can avoid calling NowSecs() in MakeRoomForWrite twice

Test Plan: make all check

Reviewers: yhchiang, igor, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20529/make statistics forward-able

Summary:
Make StatisticsImpl being able to forward stats to provided statistics
implementation. The main purpose is to allow us to collect internal
stats in the future even when user supplies custom statistics
implementation. It avoids intrumenting 2 sets of stats collection code.
One immediate use case is tuning advisor, which needs to collect some
internal stats, users may not be interested.

Test Plan:
ran db_bench and see stats show up at the end of run
Will run make all check since some tests rely on statistics

Reviewers: yhchiang, sdong, igor

Reviewed By: sdong

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D20145/Add timeout_hint_us to WriteOptions and introduce Status::TimeOut.

Summary:
This diff adds timeout_hint_us to WriteOptions.  If it's non-zero, then
1) writes associated with this options MAY be aborted when it has been
  waiting for longer than the specified time.  If an abortion happens,
  associated writes will return Status::TimeOut.
2) the stall time of the associated write caused by flush or compaction
  will be limited by timeout_hint_us.

The default value of timeout_hint_us is 0 (i.e., OFF.)

The statistics of timeout writes will be recorded in WRITE_TIMEDOUT.

Test Plan:
export ROCKSDB_TESTS=WriteTimeoutAndDelayTest
make db_test
./db_test

Reviewers: igor, ljin, haobo, sdong

Reviewed By: sdong

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18837/"
,,Rocksdb,"- hdfs cleanup; fix to NewDirectory to comply with definition in env.h
- fix compile error with env_test; static casts added/hdfs cleanup and compile test against CDH 4.4./"
,,Rocksdb,"[RocksDB] allow LDB tool to have customized key formatter

Summary: Currently ldb tool dump keys either in ascii format or hex format - neither is ideal if the key has a binary structure and is not readable in ascii. This diff also allows LDB tool to be customized in ways beyond DB options.

Test Plan: verify that key formatter works with some simple db with binary key.

Reviewers: sdong, ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19209/"
,,Rocksdb,"HashLinkList memtable switches a bucket to a skip list to reduce performance outliers

Summary:
In this patch, we enhance HashLinkList memtable to reduce performance outliers when a bucket contains too many entries. We switch to skip list for this case to enable binary search.

Add threshold_use_skiplist parameter to determine when a bucket needs to switch to skip list.

The new data structure is documented in comments in the codes.

Test Plan:
make all check
set threshold_use_skiplist in several tests

Reviewers: yhchiang, haobo, ljin

Reviewed By: yhchiang, ljin

Subscribers: nkg-, xjin, dhruba, yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D19299/"
,,Rocksdb,"Adding option to save PlainTable index and bloom filter in SST file.

Summary:
Adding option to save PlainTable index and bloom filter in SST file.
If there is no bloom block and/or index block, PlainTableReader builds
new ones. Otherwise PlainTableReader just use these blocks.

Test Plan: make all check

Reviewers: sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19527/Clean PlainTableReader's variables for better data locality

Summary:
Clean PlainTableReader's data structures:
(1) inline bloom_ (in order to do this, change DynamicBloom to allow lazy initialization)
(2) remove some variables only used when initialization from the class
(3) put variables not used in normal read code paths to the end of the class and reference prefix_extractor directly
(4) make Options a reference.

Test Plan: make all check

Reviewers: haobo, ljin

Reviewed By: ljin

Subscribers: igor, yhchiang, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18891/dynamic_bloom: replace some divide (remainder) operations with shifts in locality mode, and other improvements

Summary:
This patch changes meaning of options.bloom_locality: 0 means disable cache line optimization and any positive number means use CACHE_LINE_SIZE as block size (the previous behavior is the block size will be CACHE_LINE_SIZE*options.bloom_locality). By doing it, the divide operations inside a block can be replaced by a shift.

Performance is improved:
https://reviews.facebook.net/P471

Also, improve the basic algorithm in two ways:
(1) make sure num of blocks is an odd number
(2) rotate bytes after every probe in locality mode. Since the divider is 2^n, unless doing it, we are never able to use all the bits.
Improvements of false positive: https://reviews.facebook.net/P459

Test Plan: make all check

Reviewers: ljin, haobo

Reviewed By: haobo

Subscribers: dhruba, yhchiang, igor, leveldb

Differential Revision: https://reviews.facebook.net/D18843/"
,,Rocksdb,"use arena to allocate memtable's bloomfilter and hashskiplist's buckets_

Summary:
    Bloomfilter and hashskiplist's buckets_ allocated by memtable's arena
    DynamicBloom: pass arena via constructor, allocate space in SetTotalBits
    HashSkipListRep: allocate space of buckets_ using arena.
       do not delete it in deconstructor because arena would take care of it.
    Several test files are changed.

Test Plan:
    make all check

Reviewers: ljin, haobo, yhchiang, sdong

Reviewed By: sdong

Subscribers: igor, dhruba

Differential Revision: https://reviews.facebook.net/D19335/dynamic_bloom: replace some divide (remainder) operations with shifts in locality mode, and other improvements

Summary:
This patch changes meaning of options.bloom_locality: 0 means disable cache line optimization and any positive number means use CACHE_LINE_SIZE as block size (the previous behavior is the block size will be CACHE_LINE_SIZE*options.bloom_locality). By doing it, the divide operations inside a block can be replaced by a shift.

Performance is improved:
https://reviews.facebook.net/P471

Also, improve the basic algorithm in two ways:
(1) make sure num of blocks is an odd number
(2) rotate bytes after every probe in locality mode. Since the divider is 2^n, unless doing it, we are never able to use all the bits.
Improvements of false positive: https://reviews.facebook.net/P459

Test Plan: make all check

Reviewers: ljin, haobo

Reviewed By: haobo

Subscribers: dhruba, yhchiang, igor, leveldb

Differential Revision: https://reviews.facebook.net/D18843/"
,Thread management,Rocksdb,"ThreadPool to allow decrease number of threads and increase of number of threads is to be instantly scheduled

Summary:
Add a feature to decrease the number of threads in thread pool.
Also instantly schedule more threads if number of threads is increased.

Here is the way it is implemented: each background thread needs its thread ID. After decreasing number of threads, all threads are woken up. The thread with the largest thread ID will terminate. If there are more threads to terminate, the thread will wake up all threads again.

Another change is made so that when number of threads is increased, more threads are created and all previous excessive threads are woken up to do the work.

Test Plan: Add a unit test.

Reviewers: haobo, dhruba

Reviewed By: haobo

CC: yhchiang, igor, nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D18675/"
,,Rocksdb,"print compaction_filter name in Options.Dump

Summary:
Was looking at an issue. All options are the same except
compaction_filter was missed from a newer package. Our option dump does
not capture that

Test Plan: make release

Reviewers: sdong, igor, yhchiang

Reviewed By: yhchiang

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D21765/set bytes_per_sync to 1MB if rate limiter is enabled

Summary: as title

Test Plan: make all check

Reviewers: igor, yhchiang, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D21201/Fixed heap-buffer-overflow issue when Options.num_levels > 7.

Summary:
Currently, when num_levels has been changed to > 7, internally
it will not resize max_bytes_for_level_multiplier_additional.
As a result, max_bytes_for_level_multiplier_additional.size() will
be smaller than num_levels, which causes heap-buffer-overflow.

Test Plan: make all check

Reviewers: haobo, sdong, ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19275/Create Missing Column Families

Summary: Provide an convenience option to create column families if they are missing from the DB. Task #4460490

Test Plan: added unit test. also, stress test for some time

Reviewers: sdong, haobo, dhruba, ljin, yhchiang

Reviewed By: yhchiang

Subscribers: yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D18951/FIFO compaction style

Summary:
Introducing new compaction style -- FIFO.

FIFO compaction style has write amplification of 1 (+1 for WAL) and it deletes the oldest files when the total DB size exceeds pre-configured values.

FIFO compaction style is suited for storing high-frequency event logs.

Test Plan: Added a unit test

Reviewers: dhruba, haobo, sdong

Reviewed By: dhruba

Subscribers: alberts, leveldb

Differential Revision: https://reviews.facebook.net/D18765/"
,Thread management,Rocksdb,"Finer report I/O stats about Flush and Compaction.

Summary:
This diff allows the I/O stats about Flush and Compaction to be reported
in a more accurate way.  Instead of measuring the size of a file, it
measure I/O cost in per read / write basis.

Test Plan: make all check

Reviewers: sdong, igor, ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19383/Fix 32-bit errors

Summary: https://www.facebook.com/groups/rocksdb.dev/permalink/590438347721350/

Test Plan: compiles

Reviewers: sdong, ljin, yhchiang

Reviewed By: yhchiang

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19197/ThreadID printed when Thread terminating in the same format as posix_logger

Summary: https://github.com/facebook/rocksdb/commit/220132b65ec17abb037d3e79d5abf6ca8d797b96 correctly fixed the issue of thread ID printing when terminating a thread. Nothing wrong with it. This diff prints the ID in the same way as in PosixLogger::logv() so that users can be more easily to correlates them.

Test Plan: run env_test and make sure it prints correctly.

Reviewers: igor, haobo, ljin, yhchiang

Reviewed By: yhchiang

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18819/Print pthread_t in a more safe way/"
,Thread management,Rocksdb,"integrate rate limiter into rocksdb

Summary:
Add option and plugin rate limiter for PosixWritableFile. The rate
limiter only applies to flush and compaction. WAL and MANIFEST are
excluded from this enforcement.

Test Plan: db_test

Reviewers: igor, yhchiang, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19425/generic rate limiter

Summary:
A generic rate limiter that can be shared by threads and rocksdb
instances. Will use this to smooth out write traffic generated by
compaction and flush. This will help us get better p99 behavior on flash
storage.

Test Plan:
unit test output
==== Test RateLimiterTest.Rate
 size [1 - 1023], limit 10 KB/sec, actual rate: 10.374969 KB/sec, elapsed 2002265
 size [1 - 2047], limit 20 KB/sec, actual rate: 20.771242 KB/sec, elapsed 2002139
 size [1 - 4095], limit 40 KB/sec, actual rate: 41.285299 KB/sec, elapsed 2202424
 size [1 - 8191], limit 80 KB/sec, actual rate: 81.371605 KB/sec, elapsed 2402558
 size [1 - 16383], limit 160 KB/sec, actual rate: 162.541268 KB/sec, elapsed 3303500

Reviewers: yhchiang, igor, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19359/Add timeout_hint_us to WriteOptions and introduce Status::TimeOut.

Summary:
This diff adds timeout_hint_us to WriteOptions.  If it's non-zero, then
1) writes associated with this options MAY be aborted when it has been
  waiting for longer than the specified time.  If an abortion happens,
  associated writes will return Status::TimeOut.
2) the stall time of the associated write caused by flush or compaction
  will be limited by timeout_hint_us.

The default value of timeout_hint_us is 0 (i.e., OFF.)

The statistics of timeout writes will be recorded in WRITE_TIMEDOUT.

Test Plan:
export ROCKSDB_TESTS=WriteTimeoutAndDelayTest
make db_test
./db_test

Reviewers: igor, ljin, haobo, sdong

Reviewed By: sdong

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18837/"
,,Rocksdb,"Add support for C bindings to the compaction V2 filter mechanism.

Test Plan: make c_test && ./c_test

Some fixes after merge./C API: create missing cf's, cleanup/C API: update options w/ convenience funcs & fifo compaction/Fix valgrind error in c_test

Summary:
External contribution caused some valgrind errors: https://github.com/facebook/rocksdb/commit/1a34aaaef0900785c2de7e55b55d8c48d1201300

This diff fixes them

Test Plan: ran valgrind

Reviewers: sdong, yhchiang, ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19485/C API: Allow setting compaction filter factory/C API: Add support for compaction filter factories (v1)/C API: column family support/C API: support constructing write batch from serialized representation/Add a way to set compaction filter in the C API/Support for compaction filters in the C API/"
,,Rocksdb,"Add histogram for DB_SEEK

Summary: as title

Test Plan: make release

Reviewers: sdong, yhchiang

Reviewed By: yhchiang

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D21717/Finer report I/O stats about Flush and Compaction.

Summary:
This diff allows the I/O stats about Flush and Compaction to be reported
in a more accurate way.  Instead of measuring the size of a file, it
measure I/O cost in per read / write basis.

Test Plan: make all check

Reviewers: sdong, igor, ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19383/Add timeout_hint_us to WriteOptions and introduce Status::TimeOut.

Summary:
This diff adds timeout_hint_us to WriteOptions.  If it's non-zero, then
1) writes associated with this options MAY be aborted when it has been
  waiting for longer than the specified time.  If an abortion happens,
  associated writes will return Status::TimeOut.
2) the stall time of the associated write caused by flush or compaction
  will be limited by timeout_hint_us.

The default value of timeout_hint_us is 0 (i.e., OFF.)

The statistics of timeout writes will be recorded in WRITE_TIMEDOUT.

Test Plan:
export ROCKSDB_TESTS=WriteTimeoutAndDelayTest
make db_test
./db_test

Reviewers: igor, ljin, haobo, sdong

Reviewed By: sdong

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18837/"
,,Rocksdb,"PlainTable to encode to avoid to rewrite prefix when it is the same as the previous key

Summary:
Add a encoding feature of PlainTable to encode PlainTable's keys to save some bytes for the same prefixes.
The data format is documented in table/plain_table_factory.h

Test Plan: Add unit test coverage in plain_table_db_test

Reviewers: yhchiang, igor, dhruba, ljin, haobo

Reviewed By: haobo

Subscribers: nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D18735/Change order of parameters in adaptive table factory

Summary:
This is minor, but if we put the writing talbe factory as the third parameter, when we add a new table format, we'll have a situation:
1) block based factory
2) plain table factory
3) output factory
4) new format factory

I think it makes more sense to have output as the first parameter.

Also, fixed a NewAdaptiveTableFactory() call in unit test

Test Plan: unit test

Reviewers: sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19119/Add a table factory that can read DB with both of PlainTable and BlockBasedTable in it

Summary: The new table factory is used if users want to convert a DB from one table format to the other. A user can use this table to open a DB written using one table format and write new files to another table format.

Test Plan: add a unit test

Reviewers: haobo, igor

Reviewed By: igor

Subscribers: dhruba, ljin, yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D19017/"
,,Rocksdb,"Support Multiple DB paths (without having an interface to expose to users)

Summary:
In this patch, we allow RocksDB to support multiple DB paths internally.
No user interface is supported yet so this patch is silent to users.

Test Plan: make all check

Reviewers: igor, haobo, ljin, yhchiang

Reviewed By: yhchiang

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18921/"
compression tasks,Compression tasks,Rocksdb,"[Java] Add compaction style to options

Summary:
Add compression type to options

make rocksdbjava
make sample

Reviewers: haobo, yhchiang, sdong, dhruba, rsumbaly, zzbennett, swapnilghike
Reviewed By: yhchiang, sdong
CC: leveldb

Differential Revision: https://reviews.facebook.net/D20463/"
API Management,"API Usage, compression tasks",Rocksdb,"[Java] Correct the library loading for zlib in RocksJava.

Summary:
Correct the library loading for zlib in RocksJava: zlib should
be loaded by loadLibrary(""z"") instead of loadLibrary(""zlib"").

Test Plan:
make rocksdbjava
cd java
make db_bench
./jdb_bench.sh --compression_type=zlib

Reviewers: sdong, ljin, ankgup87

Reviewed By: ankgup87

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19341/[Java] Enable compression_ratio option in DbBenchmark.java

Summary:
Enable the random values in Java DB Bench to be generated based
on the compression_ratio specified in the command-line arguments.

Test Plan:
make rocksdbjava
java/jdb_bench.sh

Reviewers: sdong, ankgup87, haobo

Reviewed By: haobo

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19101/"
,,Rocksdb,"[Java] Generalize dis-own native handle and refine dispose framework.

Summary:
1. Move disOwnNativeHandle() function from RocksDB to RocksObject
to allow other RocksObject to use disOwnNativeHandle() when its
ownership of native handle has been transferred.

2. RocksObject now has an abstract implementation of dispose(),
which does the following two things.  First, it checks whether
both isOwningNativeHandle() and isInitialized() return true.
If so, it will call the protected abstract function dispose0(),
which all the subclasses of RocksObject should implement.  Second,
it sets nativeHandle_ = 0.  This redesign ensure all subclasses
of RocksObject have the same dispose behavior.

3. All subclasses of RocksObject now should implement dispose0()
instead of dispose(), and dispose0() will be called only when
isInitialized() returns true.

Test Plan:
make rocksdbjava
make jtest

Reviewers: dhruba, sdong, ankgup87, rsumbaly, swapnilghike, zzbennett, haobo

Reviewed By: haobo

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D18801/"
,,Rocksdb,Caching methodId and fieldId is fine/Class IDs and method IDs should not be cached/
Memory Management,"Memory Management, Thread management",Rocksdb,"remove malloc when create data and index iterator in Get

Summary:
  Define Block::Iter to be an independent class to be used by block_based_table_reader
  When creating data and index iterator, update an existing iterator rather than new one
  Thus malloc and free could be reduced

Benchmark,
Base:
commit 76286ee67ef4b89579a92134b996a681c36a1331
commands:
--db=/dev/shm/rocksdb --num_levels=6 --key_size=20 --prefix_size=20 --keys_per_prefix=0 --value_size=100 --write_buffer_size=134217728 --max_write_buffer_number=2 --target_file_size_base=33554432 --max_bytes_for_level_base=1073741824 --verify_checksum=false --max_background_compactions=4 --use_plain_table=0 --memtablerep=prefix_hash --open_files=-1 --mmap_read=1 --mmap_write=0 --bloom_bits=10 --bloom_locality=1 --memtable_bloom_bits=500000 --compression_type=lz4 --num=2621440 --use_hash_search=1 --block_size=1024 --block_restart_interval=1 --use_existing_db=1 --threads=1 --benchmarks=readrandom —disable_auto_compactions=1

malloc: 3.30% -> 1.42%
free: 3.59%->1.61%

Test Plan:
  make all check
  run db_stress
  valgrind ./db_test ./table_test

Reviewers: ljin, yhchiang, dhruba, igor, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20655/Materialize the hash index

Summary:
Materialize the hash index to avoid the soaring cpu/flash usage
when initializing the database.

Test Plan: existing unit tests passed

Reviewers: sdong, haobo

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18339/"
,,Rocksdb,"Adding option to save PlainTable index and bloom filter in SST file.

Summary:
Adding option to save PlainTable index and bloom filter in SST file.
If there is no bloom block and/or index block, PlainTableReader builds
new ones. Otherwise PlainTableReader just use these blocks.

Test Plan: make all check

Reviewers: sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19527/"
,,Rocksdb,"Adding option to save PlainTable index and bloom filter in SST file.

Summary:
Adding option to save PlainTable index and bloom filter in SST file.
If there is no bloom block and/or index block, PlainTableReader builds
new ones. Otherwise PlainTableReader just use these blocks.

Test Plan: make all check

Reviewers: sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19527/PlainTable to encode to avoid to rewrite prefix when it is the same as the previous key

Summary:
Add a encoding feature of PlainTable to encode PlainTable's keys to save some bytes for the same prefixes.
The data format is documented in table/plain_table_factory.h

Test Plan: Add unit test coverage in plain_table_db_test

Reviewers: yhchiang, igor, dhruba, ljin, haobo

Reviewed By: haobo

Subscribers: nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D18735/"
Memory Management,"Memory Management, Thread management, restructuring the code",Rocksdb,"Add DB property ""rocksdb.estimate-table-readers-mem""

Summary:
Add a DB Property ""rocksdb.estimate-table-readers-mem"" to return estimated memory usage by all loaded table readers, other than allocated from block cache.

Refactor the property codes to allow getting property from a version, with DB mutex not acquired.

Test Plan: Add several checks of this new property in existing codes for various cases.

Reviewers: yhchiang, ljin

Reviewed By: ljin

Subscribers: xjin, igor, leveldb

Differential Revision: https://reviews.facebook.net/D20733/remove malloc when create data and index iterator in Get

Summary:
  Define Block::Iter to be an independent class to be used by block_based_table_reader
  When creating data and index iterator, update an existing iterator rather than new one
  Thus malloc and free could be reduced

Benchmark,
Base:
commit 76286ee67ef4b89579a92134b996a681c36a1331
commands:
--db=/dev/shm/rocksdb --num_levels=6 --key_size=20 --prefix_size=20 --keys_per_prefix=0 --value_size=100 --write_buffer_size=134217728 --max_write_buffer_number=2 --target_file_size_base=33554432 --max_bytes_for_level_base=1073741824 --verify_checksum=false --max_background_compactions=4 --use_plain_table=0 --memtablerep=prefix_hash --open_files=-1 --mmap_read=1 --mmap_write=0 --bloom_bits=10 --bloom_locality=1 --memtable_bloom_bits=500000 --compression_type=lz4 --num=2621440 --use_hash_search=1 --block_size=1024 --block_restart_interval=1 --use_existing_db=1 --threads=1 --benchmarks=readrandom —disable_auto_compactions=1

malloc: 3.30% -> 1.42%
free: 3.59%->1.61%

Test Plan:
  make all check
  run db_stress
  valgrind ./db_test ./table_test

Reviewers: ljin, yhchiang, dhruba, igor, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20655/Use IterKey instead of string in Block::Iter to reduce malloc

Summary:
  Modify a functioin TrimAppend in dbformat.h: IterKey. Write a test for it in dbformat_test
  Use IterKey in block::Iter to replace std::string to reduce malloc.

  Evaluate it using perf record.
  malloc: 4.26% -> 2.91%
  free: 3.61% -> 3.08%

Test Plan:
  make all check
  ./valgrind db_test dbformat_test

Reviewers: ljin, haobo, yhchiang, dhruba, igor, sdong

Reviewed By: sdong

Differential Revision: https://reviews.facebook.net/D20433/[RocksDB] Reduce memory footprint of the blockbased table hash index.

Summary:
Currently, the in-memory hash index of blockbased table uses a precise hash map to track the prefix to block range mapping. In some use cases, especially when prefix itself is big, the memory overhead becomes a problem. This diff introduces a fixed hash bucket array that does not store the prefix and allows prefix collision, which is similar to the plaintable hash index, in order to reduce the memory consumption.
Just a quick draft, still testing and refining.

Test Plan: unit test and shadow testing

Reviewers: dhruba, kailiu, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19047/"
,,Rocksdb,"Fix compressed cache/[RocksDB] Reduce memory footprint of the blockbased table hash index.

Summary:
Currently, the in-memory hash index of blockbased table uses a precise hash map to track the prefix to block range mapping. In some use cases, especially when prefix itself is big, the memory overhead becomes a problem. This diff introduces a fixed hash bucket array that does not store the prefix and allows prefix collision, which is similar to the plaintable hash index, in order to reduce the memory consumption.
Just a quick draft, still testing and refining.

Test Plan: unit test and shadow testing

Reviewers: dhruba, kailiu, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19047/Materialize the hash index

Summary:
Materialize the hash index to avoid the soaring cpu/flash usage
when initializing the database.

Test Plan: existing unit tests passed

Reviewers: sdong, haobo

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18339/"
,,Rocksdb,"Adding option to save PlainTable index and bloom filter in SST file.

Summary:
Adding option to save PlainTable index and bloom filter in SST file.
If there is no bloom block and/or index block, PlainTableReader builds
new ones. Otherwise PlainTableReader just use these blocks.

Test Plan: make all check

Reviewers: sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19527/Add PlainTableOptions

Summary:
Since we have a lot of options for PlainTable, add a struct PlainTableOptions
to manage them

Test Plan: make all check

Reviewers: sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20175/PlainTable to encode to avoid to rewrite prefix when it is the same as the previous key

Summary:
Add a encoding feature of PlainTable to encode PlainTable's keys to save some bytes for the same prefixes.
The data format is documented in table/plain_table_factory.h

Test Plan: Add unit test coverage in plain_table_db_test

Reviewers: yhchiang, igor, dhruba, ljin, haobo

Reviewed By: haobo

Subscribers: nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D18735/"
,,Rocksdb,"Changes to support unity build:
* Script for building the unity.cc file via Makefile
* Unity executable Makefile target for testing builds
* Source code changes to fix compilation of unity build/In DB::NewIterator(), try to allocate the whole iterator tree in an arena

Summary:
In this patch, try to allocate the whole iterator tree starting from DBIter from an arena
1. ArenaWrappedDBIter is created when serves as the entry point of an iterator tree, with an arena in it.
2. Add an option to create iterator from arena for following iterators: DBIter, MergingIterator, MemtableIterator, all mem table's iterators, all table reader's iterators and two level iterator.
3. MergeIteratorBuilder is created to incrementally build the tree of internal iterators. It is passed to mem table list and version set and add iterators to it.

Limitations:
(1) Only DB::NewIterator() without tailing uses the arena. Other cases, including readonly DB and compactions are still from malloc
(2) Two level iterator itself is allocated in arena, but not iterators inside it.

Test Plan: make all check

Reviewers: ljin, haobo

Reviewed By: haobo

Subscribers: leveldb, dhruba, yhchiang, igor

Differential Revision: https://reviews.facebook.net/D18513/"
,,Rocksdb,"Adding option to save PlainTable index and bloom filter in SST file.

Summary:
Adding option to save PlainTable index and bloom filter in SST file.
If there is no bloom block and/or index block, PlainTableReader builds
new ones. Otherwise PlainTableReader just use these blocks.

Test Plan: make all check

Reviewers: sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19527/PlainTable to encode to avoid to rewrite prefix when it is the same as the previous key

Summary:
Add a encoding feature of PlainTable to encode PlainTable's keys to save some bytes for the same prefixes.
The data format is documented in table/plain_table_factory.h

Test Plan: Add unit test coverage in plain_table_db_test

Reviewers: yhchiang, igor, dhruba, ljin, haobo

Reviewed By: haobo

Subscribers: nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D18735/prefetch bloom filter data block for L0 files

Summary: as title

Test Plan:
db_bench
the initial result is very promising. I will post results of complete
runs

Reviewers: dhruba, haobo, sdong, igor

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D18867/Clean PlainTableReader's variables for better data locality

Summary:
Clean PlainTableReader's data structures:
(1) inline bloom_ (in order to do this, change DynamicBloom to allow lazy initialization)
(2) remove some variables only used when initialization from the class
(3) put variables not used in normal read code paths to the end of the class and reference prefix_extractor directly
(4) make Options a reference.

Test Plan: make all check

Reviewers: haobo, ljin

Reviewed By: ljin

Subscribers: igor, yhchiang, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18891/"
,,Rocksdb,"In DB::NewIterator(), try to allocate the whole iterator tree in an arena

Summary:
In this patch, try to allocate the whole iterator tree starting from DBIter from an arena
1. ArenaWrappedDBIter is created when serves as the entry point of an iterator tree, with an arena in it.
2. Add an option to create iterator from arena for following iterators: DBIter, MergingIterator, MemtableIterator, all mem table's iterators, all table reader's iterators and two level iterator.
3. MergeIteratorBuilder is created to incrementally build the tree of internal iterators. It is passed to mem table list and version set and add iterators to it.

Limitations:
(1) Only DB::NewIterator() without tailing uses the arena. Other cases, including readonly DB and compactions are still from malloc
(2) Two level iterator itself is allocated in arena, but not iterators inside it.

Test Plan: make all check

Reviewers: ljin, haobo

Reviewed By: haobo

Subscribers: leveldb, dhruba, yhchiang, igor

Differential Revision: https://reviews.facebook.net/D18513/"
Memory Management,"Memory Management, Thread management, restructuring the code",Rocksdb,"Add DB property ""rocksdb.estimate-table-readers-mem""

Summary:
Add a DB Property ""rocksdb.estimate-table-readers-mem"" to return estimated memory usage by all loaded table readers, other than allocated from block cache.

Refactor the property codes to allow getting property from a version, with DB mutex not acquired.

Test Plan: Add several checks of this new property in existing codes for various cases.

Reviewers: yhchiang, ljin

Reviewed By: ljin

Subscribers: xjin, igor, leveldb

Differential Revision: https://reviews.facebook.net/D20733/remove malloc when create data and index iterator in Get

Summary:
  Define Block::Iter to be an independent class to be used by block_based_table_reader
  When creating data and index iterator, update an existing iterator rather than new one
  Thus malloc and free could be reduced

Benchmark,
Base:
commit 76286ee67ef4b89579a92134b996a681c36a1331
commands:
--db=/dev/shm/rocksdb --num_levels=6 --key_size=20 --prefix_size=20 --keys_per_prefix=0 --value_size=100 --write_buffer_size=134217728 --max_write_buffer_number=2 --target_file_size_base=33554432 --max_bytes_for_level_base=1073741824 --verify_checksum=false --max_background_compactions=4 --use_plain_table=0 --memtablerep=prefix_hash --open_files=-1 --mmap_read=1 --mmap_write=0 --bloom_bits=10 --bloom_locality=1 --memtable_bloom_bits=500000 --compression_type=lz4 --num=2621440 --use_hash_search=1 --block_size=1024 --block_restart_interval=1 --use_existing_db=1 --threads=1 --benchmarks=readrandom —disable_auto_compactions=1

malloc: 3.30% -> 1.42%
free: 3.59%->1.61%

Test Plan:
  make all check
  run db_stress
  valgrind ./db_test ./table_test

Reviewers: ljin, yhchiang, dhruba, igor, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20655/Remove seek compaction

Summary:
As discussed in our internal group, we don't get much use of seek compaction at the moment, while it's making code more complicated and slower in some cases.

This diff removes seek compaction and (hopefully) all code that was introduced to support seek compaction.

There is one test case that relied on didIO information. I'll try to find another way to implement it.

Test Plan: make check

Reviewers: sdong, haobo, yhchiang, ljin, dhruba

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19161/[RocksDB] Make block based table hash index more adaptive

Summary: Currently, RocksDB returns error if a db written with prefix hash index, is later opened without providing a prefix extractor. This is uncessarily harsh. Without a prefix extractor, we could always fallback to the normal binary index.

Test Plan: unit test, also manually veried LOG that fallback did occur.

Reviewers: sdong, ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19191/[RocksDB] Reduce memory footprint of the blockbased table hash index.

Summary:
Currently, the in-memory hash index of blockbased table uses a precise hash map to track the prefix to block range mapping. In some use cases, especially when prefix itself is big, the memory overhead becomes a problem. This diff introduces a fixed hash bucket array that does not store the prefix and allows prefix collision, which is similar to the plaintable hash index, in order to reduce the memory consumption.
Just a quick draft, still testing and refining.

Test Plan: unit test and shadow testing

Reviewers: dhruba, kailiu, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19047/In DB::NewIterator(), try to allocate the whole iterator tree in an arena

Summary:
In this patch, try to allocate the whole iterator tree starting from DBIter from an arena
1. ArenaWrappedDBIter is created when serves as the entry point of an iterator tree, with an arena in it.
2. Add an option to create iterator from arena for following iterators: DBIter, MergingIterator, MemtableIterator, all mem table's iterators, all table reader's iterators and two level iterator.
3. MergeIteratorBuilder is created to incrementally build the tree of internal iterators. It is passed to mem table list and version set and add iterators to it.

Limitations:
(1) Only DB::NewIterator() without tailing uses the arena. Other cases, including readonly DB and compactions are still from malloc
(2) Two level iterator itself is allocated in arena, but not iterators inside it.

Test Plan: make all check

Reviewers: ljin, haobo

Reviewed By: haobo

Subscribers: leveldb, dhruba, yhchiang, igor

Differential Revision: https://reviews.facebook.net/D18513/Materialize the hash index

Summary:
Materialize the hash index to avoid the soaring cpu/flash usage
when initializing the database.

Test Plan: existing unit tests passed

Reviewers: sdong, haobo

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18339/"
,,Rocksdb,"Materialize the hash index

Summary:
Materialize the hash index to avoid the soaring cpu/flash usage
when initializing the database.

Test Plan: existing unit tests passed

Reviewers: sdong, haobo

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18339/"
,,Rocksdb,"Fix db_bench

Summary: Adding check for zero size index

Test Plan: ./build_tools/regression_build_test.sh

Reviewers: yhchiang, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20259/Adding option to save PlainTable index and bloom filter in SST file.

Summary:
Adding option to save PlainTable index and bloom filter in SST file.
If there is no bloom block and/or index block, PlainTableReader builds
new ones. Otherwise PlainTableReader just use these blocks.

Test Plan: make all check

Reviewers: sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19527/PlainTable to encode to avoid to rewrite prefix when it is the same as the previous key

Summary:
Add a encoding feature of PlainTable to encode PlainTable's keys to save some bytes for the same prefixes.
The data format is documented in table/plain_table_factory.h

Test Plan: Add unit test coverage in plain_table_db_test

Reviewers: yhchiang, igor, dhruba, ljin, haobo

Reviewed By: haobo

Subscribers: nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D18735/Clean PlainTableReader's variables for better data locality

Summary:
Clean PlainTableReader's data structures:
(1) inline bloom_ (in order to do this, change DynamicBloom to allow lazy initialization)
(2) remove some variables only used when initialization from the class
(3) put variables not used in normal read code paths to the end of the class and reference prefix_extractor directly
(4) make Options a reference.

Test Plan: make all check

Reviewers: haobo, ljin

Reviewed By: ljin

Subscribers: igor, yhchiang, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18891/In DB::NewIterator(), try to allocate the whole iterator tree in an arena

Summary:
In this patch, try to allocate the whole iterator tree starting from DBIter from an arena
1. ArenaWrappedDBIter is created when serves as the entry point of an iterator tree, with an arena in it.
2. Add an option to create iterator from arena for following iterators: DBIter, MergingIterator, MemtableIterator, all mem table's iterators, all table reader's iterators and two level iterator.
3. MergeIteratorBuilder is created to incrementally build the tree of internal iterators. It is passed to mem table list and version set and add iterators to it.

Limitations:
(1) Only DB::NewIterator() without tailing uses the arena. Other cases, including readonly DB and compactions are still from malloc
(2) Two level iterator itself is allocated in arena, but not iterators inside it.

Test Plan: make all check

Reviewers: ljin, haobo

Reviewed By: haobo

Subscribers: leveldb, dhruba, yhchiang, igor

Differential Revision: https://reviews.facebook.net/D18513/"
,,Rocksdb,"prefetch bloom filter data block for L0 files

Summary: as title

Test Plan:
db_bench
the initial result is very promising. I will post results of complete
runs

Reviewers: dhruba, haobo, sdong, igor

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D18867/"
Memory Management,"Memory Management, Thread management, restructuring the code",Rocksdb,"remove malloc when create data and index iterator in Get

Summary:
  Define Block::Iter to be an independent class to be used by block_based_table_reader
  When creating data and index iterator, update an existing iterator rather than new one
  Thus malloc and free could be reduced

Benchmark,
Base:
commit 76286ee67ef4b89579a92134b996a681c36a1331
commands:
--db=/dev/shm/rocksdb --num_levels=6 --key_size=20 --prefix_size=20 --keys_per_prefix=0 --value_size=100 --write_buffer_size=134217728 --max_write_buffer_number=2 --target_file_size_base=33554432 --max_bytes_for_level_base=1073741824 --verify_checksum=false --max_background_compactions=4 --use_plain_table=0 --memtablerep=prefix_hash --open_files=-1 --mmap_read=1 --mmap_write=0 --bloom_bits=10 --bloom_locality=1 --memtable_bloom_bits=500000 --compression_type=lz4 --num=2621440 --use_hash_search=1 --block_size=1024 --block_restart_interval=1 --use_existing_db=1 --threads=1 --benchmarks=readrandom —disable_auto_compactions=1

malloc: 3.30% -> 1.42%
free: 3.59%->1.61%

Test Plan:
  make all check
  run db_stress
  valgrind ./db_test ./table_test

Reviewers: ljin, yhchiang, dhruba, igor, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20655/"
Memory Management,Memory Management,Rocksdb,"use stack instead of heap memory in ReadBlockContents in some case

Summary:
  When compression is enabled, and blocksize is not too big, use the
  space in stack to hold bytes read from block.

Bencmark:
base version: commit 8f09d53fd11a7debe1e48b73a192de3a458d37bf
  malloc: 1.30% -> 0.98%
  free: 1.49% -> 1.07%

Test Plan:
  make all check

Reviewers: ljin, yhchiang, dhruba, igor, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20679/"
,,Rocksdb,"Adding option to save PlainTable index and bloom filter in SST file.

Summary:
Adding option to save PlainTable index and bloom filter in SST file.
If there is no bloom block and/or index block, PlainTableReader builds
new ones. Otherwise PlainTableReader just use these blocks.

Test Plan: make all check

Reviewers: sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19527/Materialize the hash index

Summary:
Materialize the hash index to avoid the soaring cpu/flash usage
when initializing the database.

Test Plan: existing unit tests passed

Reviewers: sdong, haobo

Reviewed By: sdong

CC: leveldb

Differential Revision: https://reviews.facebook.net/D18339/"
,,Rocksdb,"PlainTable to encode to avoid to rewrite prefix when it is the same as the previous key

Summary:
Add a encoding feature of PlainTable to encode PlainTable's keys to save some bytes for the same prefixes.
The data format is documented in table/plain_table_factory.h

Test Plan: Add unit test coverage in plain_table_db_test

Reviewers: yhchiang, igor, dhruba, ljin, haobo

Reviewed By: haobo

Subscribers: nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D18735/"
,,Rocksdb,"Remove seek compaction

Summary:
As discussed in our internal group, we don't get much use of seek compaction at the moment, while it's making code more complicated and slower in some cases.

This diff removes seek compaction and (hopefully) all code that was introduced to support seek compaction.

There is one test case that relied on didIO information. I'll try to find another way to implement it.

Test Plan: make check

Reviewers: sdong, haobo, yhchiang, ljin, dhruba

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19161/"
,,Rocksdb,"Cache some conditions for DBImpl::MakeRoomForWrite

Summary:
Task 4580155. Some conditions in DBImpl::MakeRoomForWrite can be cached in
ColumnFamilyData, because theirs value can be changed only during compaction,
adding new memtable and/or add recalculation of compaction score.

These conditions are:

cfd->imm()->size() ==  cfd->options()->max_write_buffer_number - 1
cfd->current()->NumLevelFiles(0) >=  cfd->options()->level0_stop_writes_trigger
cfd->options()->soft_rate_limit > 0.0 &&
    (score = cfd->current()->MaxCompactionScore()) >  cfd->options()->soft_rate_limit
cfd->options()->hard_rate_limit > 1.0 &&
    (score = cfd->current()->MaxCompactionScore()) >  cfd->options()->hard_rate_limit

P.S.
As it's my first diff, Siying suggested to add everybody as a reviewers
for this diff. Sorry, if I forgot someone or add someone by mistake.

Test Plan: make all check

Reviewers: haobo, xjin, dhruba, yhchiang, zagfox, ljin, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19311/"
,,Rocksdb,"Flush only one column family

Summary:
Currently DBImpl::Flush() triggers flushes in all column families.
Instead we need to trigger just the column family specified.

Test Plan: make all check

Reviewers: igor, ljin, yhchiang, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20841/Create Missing Column Families

Summary: Provide an convenience option to create column families if they are missing from the DB. Task #4460490

Test Plan: added unit test. also, stress test for some time

Reviewers: sdong, haobo, dhruba, ljin, yhchiang

Reviewed By: yhchiang

Subscribers: yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D18951/"
,,Rocksdb,"Add DB property ""rocksdb.estimate-table-readers-mem""

Summary:
Add a DB Property ""rocksdb.estimate-table-readers-mem"" to return estimated memory usage by all loaded table readers, other than allocated from block cache.

Refactor the property codes to allow getting property from a version, with DB mutex not acquired.

Test Plan: Add several checks of this new property in existing codes for various cases.

Reviewers: yhchiang, ljin

Reviewed By: ljin

Subscribers: xjin, igor, leveldb

Differential Revision: https://reviews.facebook.net/D20733/"
Restructuring the code,Restructuring the code,Rocksdb,"log db path info before open

Summary: 1. write db MANIFEST, CURRENT, IDENTITY, sst files, log files to log before open

Test Plan: run db and check LOG file

Reviewers: ljin, yhchiang, igor, dhruba, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D21459/Flush only one column family

Summary:
Currently DBImpl::Flush() triggers flushes in all column families.
Instead we need to trigger just the column family specified.

Test Plan: make all check

Reviewers: igor, ljin, yhchiang, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20841/Include candidate files under options.db_log_dir in FindObsoleteFiles()

Summary: In FindObsoleteFiles(), we don't scan db_log_dir. Add it.

Test Plan: make all check

Reviewers: ljin, igor, yhchiang

Reviewed By: yhchiang

Subscribers: leveldb, yhchiang

Differential Revision: https://reviews.facebook.net/D21429/Need to schedule compactions when manual compaction finishes

Summary: If there is an outstanding compaction scheduled but at the time a manual compaction is triggered, the manual compaction will preempt. In the end of the manual compaction, we should try to schedule compactions to make sure those preempted ones are not skipped.

Test Plan: make all check

Reviewers: yhchiang, ljin

Reviewed By: ljin

Subscribers: leveldb, dhruba, igor

Differential Revision: https://reviews.facebook.net/D21321/Fix SIGSEGV in travis

Summary:
Travis build was failing a lot. For example see https://travis-ci.org/facebook/rocksdb/builds/31425845

This fixes it.

Also, please don't put any code after SignalAll :)

Test Plan: no more SIGSEGV

Reviewers: yhchiang, sdong, ljin

Reviewed By: ljin

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D21417/Fix valgrind failure caused by recent checked-in.

Summary: Initialize un-initialized parameters

Test Plan: run the failed test (c_test)

Reviewers: yhchiang, ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D21249/Add DB property ""rocksdb.estimate-table-readers-mem""

Summary:
Add a DB Property ""rocksdb.estimate-table-readers-mem"" to return estimated memory usage by all loaded table readers, other than allocated from block cache.

Refactor the property codes to allow getting property from a version, with DB mutex not acquired.

Test Plan: Add several checks of this new property in existing codes for various cases.

Reviewers: yhchiang, ljin

Reviewed By: ljin

Subscribers: xjin, igor, leveldb

Differential Revision: https://reviews.facebook.net/D20733/logging_when_create_and_delete_manifest

Summary:
  1. logging when create and delete manifest file
  2. fix formating in table/format.cc

Test Plan:
  make all check
  run db_bench, track the LOG file.

Reviewers: ljin, yhchiang, igor, yufei.zhu, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D21009/Never CompactRange to level 0 in level compaction

Summary: I was bit by this when developing SpatialDB. In case all files are at level 0, CompactRange() will output the compacted files to level 0. This is not ideal, since read amp. is much better at level 1 and higher.

Test Plan: Compacted data in SpatialDB, read manifest using ldb, verified that files are now at level 1 instead of 0.

Reviewers: sdong, ljin, yhchiang, dhruba

Reviewed By: dhruba

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D20901/Change StopWatch interface

Summary: So that we can avoid calling NowSecs() in MakeRoomForWrite twice

Test Plan: make all check

Reviewers: yhchiang, igor, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20529/make statistics forward-able

Summary:
Make StatisticsImpl being able to forward stats to provided statistics
implementation. The main purpose is to allow us to collect internal
stats in the future even when user supplies custom statistics
implementation. It avoids intrumenting 2 sets of stats collection code.
One immediate use case is tuning advisor, which needs to collect some
internal stats, users may not be interested.

Test Plan:
ran db_bench and see stats show up at the end of run
Will run make all check since some tests rely on statistics

Reviewers: yhchiang, sdong, igor

Reviewed By: sdong

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D20145/Allow user to specify DB path of output file of manual compaction

Summary: Add a parameter path_id to DB::CompactRange(), to indicate where the output file should be placed to.

Test Plan: add a unit test

Reviewers: yhchiang, ljin

Reviewed By: ljin

Subscribers: xjin, igor, dhruba, MarkCallaghan, leveldb

Differential Revision: https://reviews.facebook.net/D20085/make internal stats independent of statistics

Summary:
also make it aware of column family

** DB Stats **
Uptime(secs): 202.1 total, 13.5 interval
Cumulative writes: 6291456 writes, 6291456 batches, 1.0 writes per batch, 4.90 ingest GB
Cumulative WAL: 6291456 writes, 6291456 syncs, 1.00 writes per sync, 4.90 GB written
Interval writes: 1048576 writes, 1048576 batches, 1.0 writes per batch, 836.0 ingest MB
Interval WAL: 1048576 writes, 1048576 syncs, 1.00 writes per sync, 0.82 MB written

Test Plan: ran it

Reviewers: sdong, yhchiang, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19917/Support multiple DB directories in universal compaction style

Summary:
This patch adds a target size parameter in options.db_paths and universal compaction will base it to determine which DB path to place a new file.
Level-style stays the same.

Test Plan: Add new unit tests

Reviewers: ljin, yhchiang

Reviewed By: yhchiang
"
,,Rocksdb,"db_bench: measure the real latency of write/delete

Summary: as title

Test Plan: make release

Reviewers: haobo, sdong, yhchiang

Reviewed By: yhchiang

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19227/add an iterator refresh option for SeekRandom

Summary: One more option to allow iterator refreshing when using normal iterator

Test Plan: ran db_bench

Reviewers: haobo, sdong, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D18849/"
,,Rocksdb,"Remove malloc from FormatFileNumber

Summary: Replace unnecessary malloc with stack allocation

Test Plan: make all check

Reviewers: sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D21771/"
Memory Management,"Memory Management, Thread management",Rocksdb,"store file_indexer info in sequential memory

Summary:
  use arena to allocate space for next_level_index_ and level_rb_
  Thus increasing data locality and make Version::Get faster.

Benchmark detail
Base version: commit d2a727c182338514af955bbcb1d92db4af83b41c


Result:
cpu running percentage:
Version::Get, improved from 7.98% to 7.42%
FileIndexer::GetNextLevelIndex, improved from 1.18% to 0.68%.

Test Plan:
  make all check

Reviewers: ljin, haobo, yhchiang, sdong

Reviewed By: sdong

Subscribers: dhruba, igor

Differential Revision: https://reviews.facebook.net/D19845/"
,,Rocksdb,"Use IterKey instead of string in Block::Iter to reduce malloc

Summary:
  Modify a functioin TrimAppend in dbformat.h: IterKey. Write a test for it in dbformat_test
  Use IterKey in block::Iter to replace std::string to reduce malloc.

  Evaluate it using perf record.
  malloc: 4.26% -> 2.91%
  free: 3.61% -> 3.08%

Test Plan:
  make all check
  ./valgrind db_test dbformat_test

Reviewers: ljin, haobo, yhchiang, dhruba, igor, sdong

Reviewed By: sdong

Differential Revision: https://reviews.facebook.net/D20433/HashLinkList memtable switches a bucket to a skip list to reduce performance outliers

Summary:
In this patch, we enhance HashLinkList memtable to reduce performance outliers when a bucket contains too many entries. We switch to skip list for this case to enable binary search.

Add threshold_use_skiplist parameter to determine when a bucket needs to switch to skip list.

The new data structure is documented in comments in the codes.

Test Plan:
make all check
set threshold_use_skiplist in several tests

Reviewers: yhchiang, haobo, ljin

Reviewed By: yhchiang, ljin

Subscribers: nkg-, xjin, dhruba, yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D19299/forward iterator

Summary:
Forward iterator puts everything together in a flat structure instead of
a hierarchy of nested iterators. this should simplify the code and
provide better performance. It also enables more optimization since all
information are accessiable in one place.
Init evaluation shows about 6% improvement

Test Plan: db_test and db_bench

Reviewers: dhruba, igor, tnovak, sdong, haobo

Reviewed By: haobo

Subscribers: sdong, leveldb

Differential Revision: https://reviews.facebook.net/D18795/"
,,Rocksdb,"Refactor: group metadata needed to open an SST file to a separate copyable struct

Summary:
We added multiple fields to FileMetaData recently and are planning to add more.
This refactoring separate the minimum information for accessing the file. This object is copyable (FileMetaData is not copyable since the ref counter). I hope this refactoring can enable further improvements:

(1) use it to design a more efficient data structure to speed up read queries.
(2) in the future, when we add information of storage level, we can easily do the encoding, instead of enlarge this structure, which might expand memory work set for file meta data.

The definition is same as current EncodedFileMetaData used in two level iterator, so now the logic in two level iterator is easier to understand.

Test Plan: make all check

Reviewers: haobo, igor, ljin

Reviewed By: ljin

Subscribers: leveldb, dhruba, yhchiang

Differential Revision: https://reviews.facebook.net/D18933/"
,,Rocksdb,"Remove unnecessary constructor parameter from ColumnFamilyData

Summary: const string& dbname parameter is not used

Test Plan: make all

Reviewers: sdong, igor

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20703/make internal stats independent of statistics

Summary:
also make it aware of column family
output from db_bench

```
** Compaction Stats [default] **
Level Files Size(MB) Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) RW-Amp W-Amp Rd(MB/s) Wr(MB/s)  Rn(cnt) Rnp1(cnt) Wnp1(cnt) Wnew(cnt)  Comp(sec) Comp(cnt) Avg(sec) Stall(sec) Stall(cnt) Avg(ms)
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  L0    14      956   0.9      0.0     0.0      0.0       2.7      2.7    0.0   0.0      0.0    111.6        0         0         0         0         24        40    0.612      75.20     492387    0.15
  L1    21     2001   2.0      5.7     2.0      3.7       5.3      1.6    5.4   2.6     71.2     65.7       31        43        55        12         82         2   41.242      43.72      41183    1.06
  L2   217    18974   1.9     16.5     2.0     14.4      15.1      0.7   15.6   7.4     70.1     64.3       17       182       185         3        241        16   15.052       0.00          0    0.00
  L3  1641   188245   1.8      9.1     1.1      8.0       8.5      0.5   15.4   7.4     61.3     57.2        9        75        76         1        152         9   16.887       0.00          0    0.00
  L4  4447   449025   0.4     13.4     4.8      8.6       9.1      0.5    4.7   1.9     77.8     52.7       38        79       100        21        176        38    4.639       0.00          0    0.00
 Sum  6340   659201   0.0     44.7    10.0     34.7      40.6      6.0   32.0  15.2     67.7     61.6       95       379       416        37        676       105    6.439     118.91     533570    0.22
 Int     0        0   0.0      1.2     0.4      0.8       1.3      0.5    5.2   2.7     59.1     65.6        3         7         9         2         20        10    2.003       0.00          0    0.00
Stalls(secs): 75.197 level0_slowdown, 0.000 level0_numfiles, 0.000 memtable_compaction, 43.717 leveln_slowdown
Stalls(count): 492387 level0_slowdown, 0 level0_numfiles, 0 memtable_compaction, 41183 leveln_slowdown

** DB Stats **
Uptime(secs): 202.1 total, 13.5 interval
Cumulative writes: 6291456 writes, 6291456 batches, 1.0 writes per batch, 4.90 ingest GB
Cumulative WAL: 6291456 writes, 6291456 syncs, 1.00 writes per sync, 4.90 GB written
Interval writes: 1048576 writes, 1048576 batches, 1.0 writes per batch, 836.0 ingest MB
Interval WAL: 1048576 writes, 1048576 syncs, 1.00 writes per sync, 0.82 MB written

Test Plan: ran it

Reviewers: sdong, yhchiang, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19917/Cache some conditions for DBImpl::MakeRoomForWrite

Summary:
Task 4580155. Some conditions in DBImpl::MakeRoomForWrite can be cached in
ColumnFamilyData, because theirs value can be changed only during compaction,
adding new memtable and/or add recalculation of compaction score.

These conditions are:

cfd->imm()->size() ==  cfd->options()->max_write_buffer_number - 1
cfd->current()->NumLevelFiles(0) >=  cfd->options()->level0_stop_writes_trigger
cfd->options()->soft_rate_limit > 0.0 &&
    (score = cfd->current()->MaxCompactionScore()) >  cfd->options()->soft_rate_limit
cfd->options()->hard_rate_limit > 1.0 &&
    (score = cfd->current()->MaxCompactionScore()) >  cfd->options()->hard_rate_limit

P.S.
As it's my first diff, Siying suggested to add everybody as a reviewers
for this diff. Sorry, if I forgot someone or add someone by mistake.

Test Plan: make all check

Reviewers: haobo, xjin, dhruba, yhchiang, zagfox, ljin, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19311/FIFO compaction style

Summary:
Introducing new compaction style -- FIFO.

FIFO compaction style has write amplification of 1 (+1 for WAL) and it deletes the oldest files when the total DB size exceeds pre-configured values.

FIFO compaction style is suited for storing high-frequency event logs.

Test Plan: Added a unit test

Reviewers: dhruba, haobo, sdong

Reviewed By: dhruba

Subscribers: alberts, leveldb

Differential Revision: https://reviews.facebook.net/D18765/"
,,Rocksdb,"Add support for C bindings to the compaction V2 filter mechanism.

Test Plan: make c_test && ./c_test

Some fixes after merge./Add PlainTableOptions

Summary:
Since we have a lot of options for PlainTable, add a struct PlainTableOptions
to manage them

Test Plan: make all check

Reviewers: sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20175/C API: update options w/ convenience funcs & fifo compaction/Fix valgrind error in c_test

Summary:
External contribution caused some valgrind errors: https://github.com/facebook/rocksdb/commit/1a34aaaef0900785c2de7e55b55d8c48d1201300

This diff fixes them

Test Plan: ran valgrind

Reviewers: sdong, yhchiang, ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19485/C API: Add support for compaction filter factories (v1)/C API: column family support/Support for compaction filters in the C API/"
,,Rocksdb,"Fixed the crash when merge_operator is not properly set after reopen.

Summary:
Fixed the crash when merge_operator is not properly set after reopen
and added two test cases for this.

Test Plan:
make merge_test
./merge_test

Reviewers: igor, ljin, sdong

Reviewed By: sdong

Subscribers: benj, mvikjord, leveldb

Differential Revision: https://reviews.facebook.net/D20793/integrate rate limiter into rocksdb

Summary:
Add option and plugin rate limiter for PosixWritableFile. The rate
limiter only applies to flush and compaction. WAL and MANIFEST are
excluded from this enforcement.

Test Plan: db_test

Reviewers: igor, yhchiang, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19425/Refactor: group metadata needed to open an SST file to a separate copyable struct

Summary:
We added multiple fields to FileMetaData recently and are planning to add more.
This refactoring separate the minimum information for accessing the file. This object is copyable (FileMetaData is not copyable since the ref counter). I hope this refactoring can enable further improvements:

(1) use it to design a more efficient data structure to speed up read queries.
(2) in the future, when we add information of storage level, we can easily do the encoding, instead of enlarge this structure, which might expand memory work set for file meta data.

The definition is same as current EncodedFileMetaData used in two level iterator, so now the logic in two level iterator is easier to understand.

Test Plan: make all check

Reviewers: haobo, igor, ljin

Reviewed By: ljin

Subscribers: leveldb, dhruba, yhchiang

Differential Revision: https://reviews.facebook.net/D18933/"
,,Rocksdb,"Remove malloc from FormatFileNumber

Summary: Replace unnecessary malloc with stack allocation

Test Plan: make all check

Reviewers: sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D21771/Allow user to specify DB path of output file of manual compaction

Summary: Add a parameter path_id to DB::CompactRange(), to indicate where the output file should be placed to.

Test Plan: add a unit test

Reviewers: yhchiang, ljin

Reviewed By: ljin

Subscribers: xjin, igor, dhruba, MarkCallaghan, leveldb

Differential Revision: https://reviews.facebook.net/D20085/Add struct CompactionInputFiles to manage compaction input files.

Summary: Add struct CompactionInputFiles to manage compaction input files.

Test Plan:
export ROCKSDB_TESTS=Compact
make db_test
./db_test

Reviewers: ljin, igor, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20061/Support multiple DB directories in universal compaction style

Summary:
This patch adds a target size parameter in options.db_paths and universal compaction will base it to determine which DB path to place a new file.
Level-style stays the same.

Test Plan: Add new unit tests

Reviewers: ljin, yhchiang

Reviewed By: yhchiang

Subscribers: MarkCallaghan, dhruba, igor, leveldb

Differential Revision: https://reviews.facebook.net/D19869/No need for files_by_size_ in universal compaction

Summary: files_by_size_ is sorted by time in case of universal compaction. However, Version::files_ is also sorted by time. So no need for files_by_size_

Test Plan:
1) make check with the change
2) make check with `assert(last_index == c->input_version_->files_[level].size() - 1);` in compaction picker

Reviewers: dhruba, haobo, yhchiang, sdong, ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19125/In logging format, use PRIu64 instead of casting

Summary: Code cleaning up, since we are already using __STDC_FORMAT_MACROS in printing uint64_t, change other places. Only logging is changed.

Test Plan: make all check

Reviewers: ljin

Reviewed By: ljin

Subscribers: dhruba, yhchiang, haobo, leveldb

Differential Revision: https://reviews.facebook.net/D19113/Allow compaction to reclaim storage more effectively.

Summary:
This diff allows compaction to reclaim storage more effectively.
In the current design, compactions are mainly triggered based on
the file sizes.  However, since deletion entries does not have
value, files which have many deletion entries are less likely
to be compacted.  As a result, it may took a while to make
deletion entries to be compacted.

This diff address issue by compensating the size of deletion
entries during compaction process: the size of each deletion
entry in the compaction process is augmented by 2x average
value size.  The diff applies to both leveled and universal
compacitons.

Test Plan:
develop CompactionDeletionTrigger
make db_test
./db_test

Reviewers: haobo, igor, ljin, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19029/Refactor: group metadata needed to open an SST file to a separate copyable struct

Summary:
We added multiple fields to FileMetaData recently and are planning to add more.
This refactoring separate the minimum information for accessing the file. This object is copyable (FileMetaData is not copyable since the ref counter). I hope this refactoring can enable further improvements:

(1) use it to design a more efficient data structure to speed up read queries.
(2) in the future, when we add information of storage level, we can easily do the encoding, instead of enlarge this structure, which might expand memory work set for file meta data.

The definition is same as current EncodedFileMetaData used in two level iterator, so now the logic in two level iterator is easier to understand.

Test Plan: make all check

Reviewers: haobo, igor, ljin

Reviewed By: ljin

Subscribers: leveldb, dhruba, yhchiang

Differential Revision: https://reviews.facebook.net/D18933/FIFO compaction style

Summary:
Introducing new compaction style -- FIFO.

FIFO compaction style has write amplification of 1 (+1 for WAL) and it deletes the oldest files when the total DB size exceeds pre-configured values.

FIFO compaction style is suited for storing high-frequency event logs.

Test Plan: Added a unit test

Reviewers: dhruba, haobo, sdong

Reviewed By: dhruba

Subscribers: alberts, leveldb

Differential Revision: https://reviews.facebook.net/D18765/"
,,Rocksdb,"Minor: fix a format

Summary: A format fixing

Test Plan: N/A

Reviewers: ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D21255/Add DB property ""rocksdb.estimate-table-readers-mem""

Summary:
Add a DB Property ""rocksdb.estimate-table-readers-mem"" to return estimated memory usage by all loaded table readers, other than allocated from block cache.

Refactor the property codes to allow getting property from a version, with DB mutex not acquired.

Test Plan: Add several checks of this new property in existing codes for various cases.

Reviewers: yhchiang, ljin

Reviewed By: ljin

Subscribers: xjin, igor, leveldb

Differential Revision: https://reviews.facebook.net/D20733/Add DB::GetIntProperty() to return integer properties to be returned as integers

Summary: We have quite some properties that are integers and we are adding more. Add a function to directly return them as an integer, instead of a string

Test Plan: Add several unit test checks

Reviewers: yhchiang, igor, dhruba, haobo, ljin

Reviewed By: ljin

Subscribers: yoshinorim, leveldb

Differential Revision: https://reviews.facebook.net/D20637/Add DB property estimated number of keys

Summary: Add a DB property of estimated number of live keys, by adding number of entries of all mem tables and all files, subtracted by all deletions in all files.

Test Plan: Add the case in unit tests

Reviewers: hobbymanyp, ljin

Reviewed By: ljin

Subscribers: MarkCallaghan, yoshinorim, leveldb, igor, dhruba

Differential Revision: https://reviews.facebook.net/D20631/InternalStats to take cfd on constructor

Summary:
It has one-to-one relationship with CFD. Take a pointer to CFD on
constructor to avoid passing cfd through member functions.

Test Plan: make

Reviewers: sdong, yhchiang, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20565/make internal stats independent of statistics

Summary:
also make it aware of column family
output from db_bench

** DB Stats **
Uptime(secs): 202.1 total, 13.5 interval
Cumulative writes: 6291456 writes, 6291456 batches, 1.0 writes per batch, 4.90 ingest GB
Cumulative WAL: 6291456 writes, 6291456 syncs, 1.00 writes per sync, 4.90 GB written
Interval writes: 1048576 writes, 1048576 batches, 1.0 writes per batch, 836.0 ingest MB
Interval WAL: 1048576 writes, 1048576 syncs, 1.00 writes per sync, 0.82 MB written

Test Plan: ran it

Reviewers: sdong, yhchiang, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19917/BG -> GB/improve InternalStats output

Summary: as title

Reviewers: sdong, yhchiang, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19707/"
,,Rocksdb,"Add DB property ""rocksdb.estimate-table-readers-mem""

Summary:
Add a DB Property ""rocksdb.estimate-table-readers-mem"" to return estimated memory usage by all loaded table readers, other than allocated from block cache.

Refactor the property codes to allow getting property from a version, with DB mutex not acquired.

Test Plan: Add several checks of this new property in existing codes for various cases.

Reviewers: yhchiang, ljin

Reviewed By: ljin

Subscribers: xjin, igor, leveldb

Differential Revision: https://reviews.facebook.net/D20733/Add DB::GetIntProperty() to return integer properties to be returned as integers

Summary: We have quite some properties that are integers and we are adding more. Add a function to directly return them as an integer, instead of a string

Test Plan: Add several unit test checks

Reviewers: yhchiang, igor, dhruba, haobo, ljin

Reviewed By: ljin

Subscribers: yoshinorim, leveldb

Differential Revision: https://reviews.facebook.net/D20637/Add DB property estimated number of keys

Summary: Add a DB property of estimated number of live keys, by adding number of entries of all mem tables and all files, subtracted by all deletions in all files.

Test Plan: Add the case in unit tests

Reviewers: hobbymanyp, ljin

Reviewed By: ljin

Subscribers: MarkCallaghan, yoshinorim, leveldb, igor, dhruba

Differential Revision: https://reviews.facebook.net/D20631/InternalStats to take cfd on constructor

Summary:
It has one-to-one relationship with CFD. Take a pointer to CFD on
constructor to avoid passing cfd through member functions.

Test Plan: make

Reviewers: sdong, yhchiang, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20565/make internal stats independent of statistics

Summary:
also make it aware of column family
output from db_bench

```
** Compaction Stats [default] **
Level Files Size(MB) Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) RW-Amp W-Amp Rd(MB/s) Wr(MB/s)  Rn(cnt) Rnp1(cnt) Wnp1(cnt) Wnew(cnt)  Comp(sec) Comp(cnt) Avg(sec) Stall(sec) Stall(cnt) Avg(ms)
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  L0    14      956   0.9      0.0     0.0      0.0       2.7      2.7    0.0   0.0      0.0    111.6        0         0         0         0         24        40    0.612      75.20     492387    0.15
  L1    21     2001   2.0      5.7     2.0      3.7       5.3      1.6    5.4   2.6     71.2     65.7       31        43        55        12         82         2   41.242      43.72      41183    1.06
  L2   217    18974   1.9     16.5     2.0     14.4      15.1      0.7   15.6   7.4     70.1     64.3       17       182       185         3        241        16   15.052       0.00          0    0.00
  L3  1641   188245   1.8      9.1     1.1      8.0       8.5      0.5   15.4   7.4     61.3     57.2        9        75        76         1        152         9   16.887       0.00          0    0.00
  L4  4447   449025   0.4     13.4     4.8      8.6       9.1      0.5    4.7   1.9     77.8     52.7       38        79       100        21        176        38    4.639       0.00          0    0.00
 Sum  6340   659201   0.0     44.7    10.0     34.7      40.6      6.0   32.0  15.2     67.7     61.6       95       379       416        37        676       105    6.439     118.91     533570    0.22
 Int     0        0   0.0      1.2     0.4      0.8       1.3      0.5    5.2   2.7     59.1     65.6        3         7         9         2         20        10    2.003       0.00          0    0.00
Stalls(secs): 75.197 level0_slowdown, 0.000 level0_numfiles, 0.000 memtable_compaction, 43.717 leveln_slowdown
Stalls(count): 492387 level0_slowdown, 0 level0_numfiles, 0 memtable_compaction, 41183 leveln_slowdown

** DB Stats **
Uptime(secs): 202.1 total, 13.5 interval
Cumulative writes: 6291456 writes, 6291456 batches, 1.0 writes per batch, 4.90 ingest GB
Cumulative WAL: 6291456 writes, 6291456 syncs, 1.00 writes per sync, 4.90 GB written
Interval writes: 1048576 writes, 1048576 batches, 1.0 writes per batch, 836.0 ingest MB
Interval WAL: 1048576 writes, 1048576 syncs, 1.00 writes per sync, 0.82 MB written

Test Plan: ran it

Reviewers: sdong, yhchiang, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19917/improve InternalStats output

Summary: as title

Test Plan:
sampe output:
Level Files Size(MB) Score Read(GB)  Rn(GB) Rnp1(GB) Write(BG) Wnew(GB) RW-Amp W-Amp Rd(MB/s) Wr(MB/s)  Rn(cnt) Rnp1(cnt) Wnp1(cnt) Wnew(cnt)  Comp(sec) Comp(cnt) Avg(sec) Stall(sec) Stall(cnt) Avg(ms)
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  L0    15     1024   1.0      0.0     0.0      0.0       8.2      8.2    0.0   0.0      0.0    111.4        0         0         1         1         75       123    0.612     295.94    1939238    0.15
  L1    23     2118   2.1     20.9     8.3     12.7      20.0      7.3    5.0   2.4     73.2     69.9      124       141       208        67        293         8   36.582      17.05      16100    1.06
  L2   162    15333   1.5     47.0     7.1     40.0      42.6      2.6   12.7   6.0     67.9     61.5       62       457       482        25        709        55   12.898       0.00          0    0.00
  L3   985   108065   1.1     37.8     4.0     33.9      36.9      3.0   18.8   9.3     60.1     58.5       41       338       363        25        645        31   20.812       0.00          0    0.00
  L4  2788   356033   0.3      0.0     0.0      0.0       0.0      0.0    0.0   0.0      0.0      0.0        0         0         0         0          0         0    0.000       0.00          0    0.00
 Sum  3973   482572   0.0    105.8    19.3     86.5     107.7     21.2   11.1   5.6     62.9     64.0      227       936      1054       118       1723       217    7.938     312.99    1955338    0.16

Reviewers: sdong, yhchiang, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19707/"
,,Rocksdb,"Fixed compaction-related errors where number of input levels are hard-coded.

Summary:
Fixed compaction-related errors where number of input levels are hard-coded.
It's a bug found in compaction branch.
This diff will be pushed into master.

Test Plan:
export ROCKSDB_TESTS=Compact
make db_test -j32
./db_test
also passed the tests in compaction branch

Reviewers: igor, sdong, ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20577/Fixed a bug in Compaction.cc where input_levels_ was not properly resized.

Summary:
Fixed a bug in Compaction.cc where input_levels_ was not properly resized.
Without this fix, there would be invalid access in input_levels_ when more
than two levels are involved in one compaction run.

This fix will go to master instead of compaction branch.

Test Plan: tested in compaction branch.

Reviewers: ljin, sdong, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20481/Allow class Compaction to handle input files from multiple levels.

Summary:
Allow class Compaction to handle input files from multiple levels.
This diff is a subset of https://reviews.facebook.net/D19263 where
only db/compaction.cc and db/compaction.h are changed.

Test Plan:
make db_test
export ROCKSDB_TESTS=Compaction
./db_test

Reviewers: igor, sdong, ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19923/Add struct CompactionInputFiles to manage compaction input files.

Summary: Add struct CompactionInputFiles to manage compaction input files.

Test Plan:
export ROCKSDB_TESTS=Compact
make db_test
./db_test

Reviewers: ljin, igor, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20061/use FileLevel in LevelFileNumIterator

Summary:
  Use FileLevel in LevelFileNumIterator, thus use new version of findFile.
  Old version of findFile function is deleted.
  Write a function in version_set.cc to generate FileLevel from files_.
  Add GenerateFileLevelTest in version_set_test.cc

Test Plan:
  make all check

Reviewers: ljin, haobo, yhchiang, sdong

Reviewed By: sdong

Subscribers: igor, dhruba

Differential Revision: https://reviews.facebook.net/D19659/Support Multiple DB paths (without having an interface to expose to users)

Summary:
In this patch, we allow RocksDB to support multiple DB paths internally.
No user interface is supported yet so this patch is silent to users.

Test Plan: make all check

Reviewers: igor, haobo, ljin, yhchiang

Reviewed By: yhchiang

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18921/Centralize compression decision to compaction picker

Summary:
Before this diff, we're deciding enable_compression in CompactionPicker and then we're deciding final compression type in DBImpl. This is kind of confusing.

After the diff, the final compression type will be decided in CompactionPicker.

The reason for this is that I want CompactFiles() to specify output compression type, so that people can mix and match compression styles in their compaction algorithms. This diff makes it much easier to do that.

Test Plan: make check

Reviewers: dhruba, haobo, sdong, yhchiang, ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19137/Refactor: group metadata needed to open an SST file to a separate copyable struct

Summary:
We added multiple fields to FileMetaData recently and are planning to add more.
This refactoring separate the minimum information for accessing the file. This object is copyable (FileMetaData is not copyable since the ref counter). I hope this refactoring can enable further improvements:

(1) use it to design a more efficient data structure to speed up read queries.
(2) in the future, when we add information of storage level, we can easily do the encoding, instead of enlarge this structure, which might expand memory work set for file meta data.

The definition is same as current EncodedFileMetaData used in two level iterator, so now the logic in two level iterator is easier to understand.

Test Plan: make all check

Reviewers: haobo, igor, ljin

Reviewed By: ljin

Subscribers: leveldb, dhruba, yhchiang

Differential Revision: https://reviews.facebook.net/D18933/FIFO compaction style

Summary:
Introducing new compaction style -- FIFO.

FIFO compaction style has write amplification of 1 (+1 for WAL) and it deletes the oldest files when the total DB size exceeds pre-configured values.

FIFO compaction style is suited for storing high-frequency event logs.

Test Plan: Added a unit test

Reviewers: dhruba, haobo, sdong

Reviewed By: dhruba

Subscribers: alberts, leveldb

Differential Revision: https://reviews.facebook.net/D18765/"
Memory management,Memory management,Rocksdb,"Allow user to specify DB path of output file of manual compaction

Summary: Add a parameter path_id to DB::CompactRange(), to indicate where the output file should be placed to.

Test Plan: add a unit test

Reviewers: yhchiang, ljin

Reviewed By: ljin

Subscribers: xjin, igor, dhruba, MarkCallaghan, leveldb

Differential Revision: https://reviews.facebook.net/D20085/store file_indexer info in sequential memory

Summary:
  use arena to allocate space for next_level_index_ and level_rb_
  Thus increasing data locality and make Version::Get faster.

Benchmark detail
Base version: commit d2a727c182338514af955bbcb1d92db4af83b41c

command used:
./db_bench --db=/mnt/db/rocksdb --num_levels=6 --key_size=20 --prefix_size=20 --keys_per_prefix=0 --value_size=100 --block_size=4096 --cache_size=17179869184 --cache_numshardbits=6 --compression_type=none --compression_ratio=1 --min_level_to_compress=-1 --disable_seek_compaction=1 --hard_rate_limit=2 --write_buffer_size=134217728 --max_write_buffer_number=2 --level0_file_num_compaction_trigger=8 --target_file_size_base=2097152 --max_bytes_for_level_base=1073741824 --disable_wal=0 --sync=0 --disable_data_sync=1 --verify_checksum=1 --delete_obsolete_files_period_micros=314572800 --max_grandparent_overlap_factor=10 --max_background_compactions=4 --max_background_flushes=0 --level0_slowdown_writes_trigger=16 --level0_stop_writes_trigger=24 --statistics=0 --stats_per_interval=0 --stats_interval=1048576 --histogram=0 --use_plain_table=1 --open_files=-1 --mmap_read=1 --mmap_write=0 --memtablerep=prefix_hash --bloom_bits=10 --bloom_locality=1 --perf_level=0 --benchmarks=fillseq, readrandom,readrandom,readrandom --use_existing_db=0 --num=52428800 --threads=1

Result:
cpu running percentage:
Version::Get, improved from 7.98% to 7.42%
FileIndexer::GetNextLevelIndex, improved from 1.18% to 0.68%.

Test Plan:
  make all check

Reviewers: ljin, haobo, yhchiang, sdong

Reviewed By: sdong

Subscribers: dhruba, igor

Differential Revision: https://reviews.facebook.net/D19845/"
,,Rocksdb,"Support Multiple DB paths (without having an interface to expose to users)

Summary:
In this patch, we allow RocksDB to support multiple DB paths internally.
No user interface is supported yet so this patch is silent to users.

Test Plan: make all check

Reviewers: igor, haobo, ljin, yhchiang

Reviewed By: yhchiang

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18921/Refactor: group metadata needed to open an SST file to a separate copyable struct

Summary:
We added multiple fields to FileMetaData recently and are planning to add more.
This refactoring separate the minimum information for accessing the file. This object is copyable (FileMetaData is not copyable since the ref counter). I hope this refactoring can enable further improvements:

(1) use it to design a more efficient data structure to speed up read queries.
(2) in the future, when we add information of storage level, we can easily do the encoding, instead of enlarge this structure, which might expand memory work set for file meta data.

The definition is same as current EncodedFileMetaData used in two level iterator, so now the logic in two level iterator is easier to understand.

Test Plan: make all check

Reviewers: haobo, igor, ljin

Reviewed By: ljin

Subscribers: leveldb, dhruba, yhchiang

Differential Revision: https://reviews.facebook.net/D18933/"
,,Rocksdb,"Fix readonly db

Summary:
DBImplReadOnly::CompactRange wasn't override DBImpl::CompactRange;
this can cause problem when using StackableDB inheritors like
DbWithTtl.
P. S. Thanks C++11 for override :)

Test Plan: make all check

Reviewers: igor, sdong

Reviewed By: sdong

Subscribers: yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D20829/NewIterators in read-only mode

Summary: As title.

Test Plan: Added test to column_family_test

Reviewers: ljin, yhchiang, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20523/"
,,Rocksdb,"log db path info before open

Summary: 1. write db MANIFEST, CURRENT, IDENTITY, sst files, log files to log before open

Test Plan: run db and check LOG file

Reviewers: ljin, yhchiang, igor, dhruba, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D21459/Flush only one column family

Summary:
Currently DBImpl::Flush() triggers flushes in all column families.
Instead we need to trigger just the column family specified.

Test Plan: make all check

Reviewers: igor, ljin, yhchiang, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20841/Add DB property ""rocksdb.estimate-table-readers-mem""

Summary:
Add a DB Property ""rocksdb.estimate-table-readers-mem"" to return estimated memory usage by all loaded table readers, other than allocated from block cache.

Refactor the property codes to allow getting property from a version, with DB mutex not acquired.

Test Plan: Add several checks of this new property in existing codes for various cases.

Reviewers: yhchiang, ljin

Reviewed By: ljin

Subscribers: xjin, igor, leveldb

Differential Revision: https://reviews.facebook.net/D20733/Allow user to specify DB path of output file of manual compaction

Summary: Add a parameter path_id to DB::CompactRange(), to indicate where the output file should be placed to.

Test Plan: add a unit test

Reviewers: yhchiang, ljin

Reviewed By: ljin

Subscribers: xjin, igor, dhruba, MarkCallaghan, leveldb

Differential Revision: https://reviews.facebook.net/D20085/make internal stats independent of statistics

Summary:
also make it aware of column family
output from db_bench

** DB Stats **
Uptime(secs): 202.1 total, 13.5 interval
Cumulative writes: 6291456 writes, 6291456 batches, 1.0 writes per batch, 4.90 ingest GB
Cumulative WAL: 6291456 writes, 6291456 syncs, 1.00 writes per sync, 4.90 GB written
Interval writes: 1048576 writes, 1048576 batches, 1.0 writes per batch, 836.0 ingest MB
Interval WAL: 1048576 writes, 1048576 syncs, 1.00 writes per sync, 0.82 MB written

Test Plan: ran it

Reviewers: sdong, yhchiang, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19917/Finer report I/O stats about Flush and Compaction.

Summary:
This diff allows the I/O stats about Flush and Compaction to be reported
in a more accurate way.  Instead of measuring the size of a file, it
measure I/O cost in per read / write basis.

Test Plan: make all check

Reviewers: sdong, igor, ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19383/Write Fast-path for single column family

Summary: We have a perf regression of Write() even with one column family. Make fast path for single column family to avoid the perf regression. See task #4455480

Test Plan: make check

Reviewers: sdong, ljin

Reviewed By: sdong, ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D18963/In DB::NewIterator(), try to allocate the whole iterator tree in an arena

Summary:
In this patch, try to allocate the whole iterator tree starting from DBIter from an arena
1. ArenaWrappedDBIter is created when serves as the entry point of an iterator tree, with an arena in it.
2. Add an option to create iterator from arena for following iterators: DBIter, MergingIterator, MemtableIterator, all mem table's iterators, all table reader's iterators and two level iterator.
3. MergeIteratorBuilder is created to incrementally build the tree of internal iterators. It is passed to mem table list and version set and add iterators to it.

Limitations:
(1) Only DB::NewIterator() without tailing uses the arena. Other cases, including readonly DB and compactions are still from malloc
(2) Two level iterator itself is allocated in arena, but not iterators inside it.

Test Plan: make all check

Reviewers: ljin, haobo

Reviewed By: haobo

Subscribers: leveldb, dhruba, yhchiang, igor

Differential Revision: https://reviews.facebook.net/D18513/"
Memory management,Memory management,Rocksdb,"create compressed_levels_ in Version, allocate its space using arena. Make Version::Get, Version::FindFile faster

Summary:
    Define CompressedFileMetaData that just contains fd, smallest_slice, largest_slice. Create compressed_levels_ in Version, the space is allocated using arena
    Thus increase the file meta data locality, speed up ""Get"" and ""FindFile""

    benchmark with in-memory tmpfs, could have 4% improvement under ""random read"" and 2% improvement under ""read while writing""

benchmark command:
./db_bench --db=/mnt/db/rocksdb --num_levels=6 --key_size=20 --prefix_size=20 --keys_per_prefix=0 --value_size=100 --block_size=4096 --cache_size=17179869184 --cache_numshardbits=6 --compression_type=none --compression_ratio=1 --min_level_to_compress=-1 --disable_seek_compaction=1 --hard_rate_limit=2 --write_buffer_size=134217728 --max_write_buffer_number=2 --level0_file_num_compaction_trigger=8 --target_file_size_base=33554432 --max_bytes_for_level_base=1073741824 --disable_wal=0 --sync=0 --disable_data_sync=1 --verify_checksum=1 --delete_obsolete_files_period_micros=314572800 --max_grandparent_overlap_factor=10 --max_background_compactions=4 --max_background_flushes=0 --level0_slowdown_writes_trigger=16 --level0_stop_writes_trigger=24 --statistics=0 --stats_per_interval=0 --stats_interval=1048576 --histogram=0 --use_plain_table=1 --open_files=-1 --mmap_read=1 --mmap_write=0 --memtablerep=prefix_hash --bloom_bits=10 --bloom_locality=1 --perf_level=0 --benchmarks=readwhilewriting,readwhilewriting,readwhilewriting --use_existing_db=1 --num=52428800 --threads=1 —writes_per_second=81920

Read Random:
From 1.8363 ms/op, improve to 1.7587 ms/op.
Read while writing:
From 2.985 ms/op, improve to 2.924 ms/op.

Test Plan:
    make all check

Reviewers: ljin, haobo, yhchiang, sdong

Reviewed By: sdong

Subscribers: dhruba, igor

Differential Revision: https://reviews.facebook.net/D19419/Support Multiple DB paths (without having an interface to expose to users)

Summary:
In this patch, we allow RocksDB to support multiple DB paths internally.
No user interface is supported yet so this patch is silent to users.

Test Plan: make all check

Reviewers: igor, haobo, ljin, yhchiang

Reviewed By: yhchiang

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18921/Reorder the member variables of FileMetaData to improve cache locality.

Summary:
Move stats related member variables of FileMetaData to the bottom to
improve cache locality of normal DB operations.

Test Plan: make

Reviewers: haobo, ljin, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19287/"
Memory management,Memory management,Rocksdb,"use FileLevel in LevelFileNumIterator

Summary:
  Use FileLevel in LevelFileNumIterator, thus use new version of findFile.
  Old version of findFile function is deleted.
  Write a function in version_set.cc to generate FileLevel from files_.
  Add GenerateFileLevelTest in version_set_test.cc

Test Plan:
  make all check

Reviewers: ljin, haobo, yhchiang, sdong

Reviewed By: sdong

Subscribers: igor, dhruba

Differential Revision: https://reviews.facebook.net/D19659/create compressed_levels_ in Version, allocate its space using arena. Make Version::Get, Version::FindFile faster

Summary:
    Define CompressedFileMetaData that just contains fd, smallest_slice, largest_slice. Create compressed_levels_ in Version, the space is allocated using arena
    Thus increase the file meta data locality, speed up ""Get"" and ""FindFile""

    benchmark with in-memory tmpfs, could have 4% improvement under ""random read"" and 2% improvement under ""read while writing""

benchmark command:
./db_bench --db=/mnt/db/rocksdb --num_levels=6 --key_size=20 --prefix_size=20 --keys_per_prefix=0 --value_size=100 --block_size=4096 --cache_size=17179869184 --cache_numshardbits=6 --compression_type=none --compression_ratio=1 --min_level_to_compress=-1 --disable_seek_compaction=1 --hard_rate_limit=2 --write_buffer_size=134217728 --max_write_buffer_number=2 --level0_file_num_compaction_trigger=8 --target_file_size_base=33554432 --max_bytes_for_level_base=1073741824 --disable_wal=0 --sync=0 --disable_data_sync=1 --verify_checksum=1 --delete_obsolete_files_period_micros=314572800 --max_grandparent_overlap_factor=10 --max_background_compactions=4 --max_background_flushes=0 --level0_slowdown_writes_trigger=16 --level0_stop_writes_trigger=24 --statistics=0 --stats_per_interval=0 --stats_interval=1048576 --histogram=0 --use_plain_table=1 --open_files=-1 --mmap_read=1 --mmap_write=0 --memtablerep=prefix_hash --bloom_bits=10 --bloom_locality=1 --perf_level=0 --benchmarks=readwhilewriting,readwhilewriting,readwhilewriting --use_existing_db=1 --num=52428800 --threads=1 —writes_per_second=81920

Read Random:
From 1.8363 ms/op, improve to 1.7587 ms/op.
Read while writing:
From 2.985 ms/op, improve to 2.924 ms/op.

Test Plan:
    make all check

Reviewers: ljin, haobo, yhchiang, sdong

Reviewed By: sdong

Subscribers: dhruba, igor

Differential Revision: https://reviews.facebook.net/D19419/Support Multiple DB paths (without having an interface to expose to users)

Summary:
In this patch, we allow RocksDB to support multiple DB paths internally.
No user interface is supported yet so this patch is silent to users.

Test Plan: make all check

Reviewers: igor, haobo, ljin, yhchiang

Reviewed By: yhchiang

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18921/Allow compaction to reclaim storage more effectively.

Summary:
This diff allows compaction to reclaim storage more effectively.
In the current design, compactions are mainly triggered based on
the file sizes.  However, since deletion entries does not have
value, files which have many deletion entries are less likely
to be compacted.  As a result, it may took a while to make
deletion entries to be compacted.

This diff address issue by compensating the size of deletion
entries during compaction process: the size of each deletion
entry in the compaction process is augmented by 2x average
value size.  The diff applies to both leveled and universal
compacitons.

Test Plan:
develop CompactionDeletionTrigger
make db_test
./db_test

Reviewers: haobo, igor, ljin, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19029/"
,,Rocksdb,"Fixed a signed and unsigned comparison in Compaction

Summary:
Fixed a signed and unsigned comparison in Compaction

Test Plan:
make db_test
export ROCKSDB_TESTS=Compaction
./db_test/Allow class Compaction to handle input files from multiple levels.

Summary:
Allow class Compaction to handle input files from multiple levels.
This diff is a subset of https://reviews.facebook.net/D19263 where
only db/compaction.cc and db/compaction.h are changed.

Test Plan:
make db_test
export ROCKSDB_TESTS=Compaction
./db_test

Reviewers: igor, sdong, ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19923/Add struct CompactionInputFiles to manage compaction input files.

Summary: Add struct CompactionInputFiles to manage compaction input files.

Test Plan:
export ROCKSDB_TESTS=Compact
make db_test
./db_test

Reviewers: ljin, igor, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20061/use FileLevel in LevelFileNumIterator

Summary:
  Use FileLevel in LevelFileNumIterator, thus use new version of findFile.
  Old version of findFile function is deleted.
  Write a function in version_set.cc to generate FileLevel from files_.
  Add GenerateFileLevelTest in version_set_test.cc

Test Plan:
  make all check

Reviewers: ljin, haobo, yhchiang, sdong

Reviewed By: sdong

Subscribers: igor, dhruba

Differential Revision: https://reviews.facebook.net/D19659/Support Multiple DB paths (without having an interface to expose to users)

Summary:
In this patch, we allow RocksDB to support multiple DB paths internally.
No user interface is supported yet so this patch is silent to users.

Test Plan: make all check

Reviewers: igor, haobo, ljin, yhchiang

Reviewed By: yhchiang

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18921/Centralize compression decision to compaction picker

Summary:
Before this diff, we're deciding enable_compression in CompactionPicker and then we're deciding final compression type in DBImpl. This is kind of confusing.

After the diff, the final compression type will be decided in CompactionPicker.

The reason for this is that I want CompactFiles() to specify output compression type, so that people can mix and match compression styles in their compaction algorithms. This diff makes it much easier to do that.

Test Plan: make check

Reviewers: dhruba, haobo, sdong, yhchiang, ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19137/FIFO compaction style

Summary:
Introducing new compaction style -- FIFO.

FIFO compaction style has write amplification of 1 (+1 for WAL) and it deletes the oldest files when the total DB size exceeds pre-configured values.

FIFO compaction style is suited for storing high-frequency event logs.

Test Plan: Added a unit test

Reviewers: dhruba, haobo, sdong

Reviewed By: dhruba

Subscribers: alberts, leveldb

Differential Revision: https://reviews.facebook.net/D18765/"
Memory management,Memory management,Rocksdb,"use FileLevel in LevelFileNumIterator

Summary:
  Use FileLevel in LevelFileNumIterator, thus use new version of findFile.
  Old version of findFile function is deleted.
  Write a function in version_set.cc to generate FileLevel from files_.
  Add GenerateFileLevelTest in version_set_test.cc

Test Plan:
  make all check

Reviewers: ljin, haobo, yhchiang, sdong

Reviewed By: sdong

Subscribers: igor, dhruba

Differential Revision: https://reviews.facebook.net/D19659/create compressed_levels_ in Version, allocate its space using arena. Make Version::Get, Version::FindFile faster

Summary:
    Define CompressedFileMetaData that just contains fd, smallest_slice, largest_slice. Create compressed_levels_ in Version, the space is allocated using arena
    Thus increase the file meta data locality, speed up ""Get"" and ""FindFile""

    benchmark with in-memory tmpfs, could have 4% improvement under ""random read"" and 2% improvement under ""read while writing""

benchmark command:
./db_bench --db=/mnt/db/rocksdb --num_levels=6 --key_size=20 --prefix_size=20 --keys_per_prefix=0 --value_size=100 --block_size=4096 --cache_size=17179869184 --cache_numshardbits=6 --compression_type=none --compression_ratio=1 --min_level_to_compress=-1 --disable_seek_compaction=1 --hard_rate_limit=2 --write_buffer_size=134217728 --max_write_buffer_number=2 --level0_file_num_compaction_trigger=8 --target_file_size_base=33554432 --max_bytes_for_level_base=1073741824 --disable_wal=0 --sync=0 --disable_data_sync=1 --verify_checksum=1 --delete_obsolete_files_period_micros=314572800 --max_grandparent_overlap_factor=10 --max_background_compactions=4 --max_background_flushes=0 --level0_slowdown_writes_trigger=16 --level0_stop_writes_trigger=24 --statistics=0 --stats_per_interval=0 --stats_interval=1048576 --histogram=0 --use_plain_table=1 --open_files=-1 --mmap_read=1 --mmap_write=0 --memtablerep=prefix_hash --bloom_bits=10 --bloom_locality=1 --perf_level=0 --benchmarks=readwhilewriting,readwhilewriting,readwhilewriting --use_existing_db=1 --num=52428800 --threads=1 —writes_per_second=81920

Read Random:
From 1.8363 ms/op, improve to 1.7587 ms/op.
Read while writing:
From 2.985 ms/op, improve to 2.924 ms/op.

Test Plan:
    make all check

Reviewers: ljin, haobo, yhchiang, sdong

Reviewed By: sdong

Subscribers: dhruba, igor

Differential Revision: https://reviews.facebook.net/D19419/"
,,Rocksdb,"Fixed a signed-unsigned comparison error in db_test

Summary:
Fixed a signed-unsigned comparison error in db_test

Test Plan:
make db_test/Flush only one column family

Summary:
Currently DBImpl::Flush() triggers flushes in all column families.
Instead we need to trigger just the column family specified.

Test Plan: make all check

Reviewers: igor, ljin, yhchiang, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20841/Fix db_test and DBIter

Summary: Fix old issue with DBTest.Randomized with BlockBasedTableWithWholeKeyHashIndex + added printing in DBTest.Randomized.

Test Plan: make all check

Reviewers: zagfox, igor, ljin, yhchiang, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D21003/Add DB property ""rocksdb.estimate-table-readers-mem""

Summary:
Add a DB Property ""rocksdb.estimate-table-readers-mem"" to return estimated memory usage by all loaded table readers, other than allocated from block cache.

Refactor the property codes to allow getting property from a version, with DB mutex not acquired.

Test Plan: Add several checks of this new property in existing codes for various cases.

Reviewers: yhchiang, ljin

Reviewed By: ljin

Subscribers: xjin, igor, leveldb

Differential Revision: https://reviews.facebook.net/D20733/Fixed a warning / error in signed and unsigned comparison

Summary:
Fixed the following compilation error detected in mac:
db/db_test.cc:2524:3: note: in instantiation of function template
  specialization 'rocksdb::test::Tester::IsEq<unsigned long long, int>' ed here
    ASSERT_EQ(int_num, 0);
      ^

Test Plan:
make/Add DB::GetIntProperty() to return integer properties to be returned as integers

Summary: We have quite some properties that are integers and we are adding more. Add a function to directly return them as an integer, instead of a string

Test Plan: Add several unit test checks

Reviewers: yhchiang, igor, dhruba, haobo, ljin

Reviewed By: ljin

Subscribers: yoshinorim, leveldb

Differential Revision: https://reviews.facebook.net/D20637/Add DB property estimated number of keys

Summary: Add a DB property of estimated number of live keys, by adding number of entries of all mem tables and all files, subtracted by all deletions in all files.

Test Plan: Add the case in unit tests

Reviewers: hobbymanyp, ljin

Reviewed By: ljin

Subscribers: MarkCallaghan, yoshinorim, leveldb, igor, dhruba

Differential Revision: https://reviews.facebook.net/D20631/Allow user to specify DB path of output file of manual compaction

Summary: Add a parameter path_id to DB::CompactRange(), to indicate where the output file should be placed to.

Test Plan: add a unit test

Reviewers: yhchiang, ljin

Reviewed By: ljin

Subscribers: xjin, igor, dhruba, MarkCallaghan, leveldb

Differential Revision: https://reviews.facebook.net/D20085/Add Prev() for merge operator

Summary: Implement Prev() with merge operator for DBIterator.  from mongoDB. Task 4673663.

Test Plan: make all check

Reviewers: sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19743/Support multiple DB directories in universal compaction style

Summary:
This patch adds a target size parameter in options.db_paths and universal compaction will base it to determine which DB path to place a new file.
Level-style stays the same.

Test Plan: Add new unit tests

Reviewers: ljin, yhchiang

Reviewed By: yhchiang

Subscribers: MarkCallaghan, dhruba, igor, leveldb

Differential Revision: https://reviews.facebook.net/D19869/ForwardIterator seek bugfix

Summary:
If `NeedToSeekImmutable()` returns false, `SeekInternal()` won't reset the
contents of `immutable_min_heap_`. However, since it calls `UpdateCurrent()`
unconditionally, if `current_` is one of immutable iterators (previously popped
from `immutable_min_heap_`), `UpdateCurrent()` will overwrite it. As a result,
if old `current_` in fact pointed to the smallest entry, forward iterator will
skip some records.

Fix implemented in this diff pushes `current_` back to `immutable_min_heap_`
before calling `UpdateCurrent()`.

Test Plan:
New unit test (courtesy of @lovro):
   $ ROCKSDB_TESTS=TailingIteratorSeekToSame ./db_test

Reviewers: igor, dhruba, haobo, ljin

Reviewed By: ljin

Subscribers: lovro, leveldb

Differential Revision: https://reviews.facebook.net/D19653/ForwardIterator::status() checks all child iterators

Summary:
Forward iterator only checked `status_` and `mutable_iter_->status()`, which is
not sufficient. For example, when reading exclusively from cache
(kBlockCacheTier), `mutable_iter_->status()` may return kOk (e.g. there's
nothing in the memtable), but one of immutable iterators could be in
kIncomplete. In this case, `ForwardIterator::status()` ought to return that
status instead of kOk.

This diff changes `status()` to also check `imm_iters_`, `l0_iters_`, and
`level_iters_`.

Test Plan:
  ROCKSDB_TESTS=TailingIteratorIncomplete ./db_test

Reviewers: ljin, igor

Reviewed By: igor

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D19581/integrate rate limiter into rocksdb

Summary:
Add option and plugin rate limiter for PosixWritableFile. The rate
limiter only applies to flush and compaction. WAL and MANIFEST are
excluded from this enforcement.

Test Plan: db_test

Reviewers: igor, yhchiang, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19425/Improve SimpleWriteTimeoutTest to avoid false alarm.

Summary:
SimpleWriteTimeoutTest has two parts: 1) insert two large key/values
to make memtable full and expect both of them are successful; 2) insert
another key / value and expect it to be timed-out.  Previously we also
set a timeout in the first step, but this might sometimes cause
false alarm.

This diff makes the first two writes run without timeout setting.

Test Plan:
export ROCKSDB_TESTS=Time
make db_test

Reviewers: sdong, ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19461/Add timeout_hint_us to WriteOptions and introduce Status::TimeOut.

Summary:
This diff adds timeout_hint_us to WriteOptions.  If it's non-zero, then
1) writes associated with this options MAY be aborted when it has been
  waiting for longer than the specified time.  If an abortion happens,
  associated writes will return Status::TimeOut.
2) the stall time of the associated write caused by flush or compaction
  will be limited by timeout_hint_us.

The default value of timeout_hint_us is 0 (i.e., OFF.)

The statistics of timeout writes will be recorded in WRITE_TIMEDOUT.

Test Plan:
export ROCKSDB_TESTS=WriteTimeoutAndDelayTest
make db_test
./db_test

Reviewers: igor, ljin, haobo, sdong

Reviewed By: sdong

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18837/Support Multiple DB paths (without having an interface to expose to users)

Summary:
In this patch, we allow RocksDB to support multiple DB paths internally.
No user interface is supported yet so this patch is silent to users.

Test Plan: make all check

Reviewers: igor, haobo, ljin, yhchiang

Reviewed By: yhchiang

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18921/Re-commit the correct part (WalDir) of the revision:

Commit 6634844dba962b9a150646382f4d6531d1f2440b by sdong
Two small fixes in db_test

Summary:
Two fixes:
(1) WalDir to pick a directory under TmpDir to allow two tests running in parallel without impacting each other
(2) kBlockBasedTableWithWholeKeyHashIndex is disabled by mistake (I assume). Enable it.

Test Plan: ./db_test

Reviewers: yhchiang, ljin

Reviewed By: ljin

Subscribers: nkg-, igor, dhruba, haobo, leveldb

Differential Revision: https://reviews.facebook.net/D19389/Revert ""Two small fixes in db_test""

This reverts commit 6634844dba962b9a150646382f4d6531d1f2440b./HashLinkList memtable switches a bucket to a skip list to reduce performance outliers

Summary:
In this patch, we enhance HashLinkList memtable to reduce performance outliers when a bucket contains too many entries. We switch to skip list for this case to enable binary search.

Add threshold_use_skiplist parameter to determine when a bucket needs to switch to skip list.

The new data structure is documented in comments in the codes.

Test Plan:
make all check
set threshold_use_skiplist in several tests

Reviewers: yhchiang, haobo, ljin

Reviewed By: yhchiang, ljin

Subscribers: nkg-, xjin, dhruba, yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D19299/Two small fixes in db_test

Summary:
Two fixes:
(1) WalDir to pick a directory under TmpDir to allow two tests running in parallel without impacting each other
(2) kBlockBasedTableWithWholeKeyHashIndex is disabled by mistake (I assume). Enable it.

Test Plan: ./db_test

Reviewers: yhchiang, ljin

Reviewed By: ljin

Subscribers: nkg-, igor, dhruba, haobo, leveldb

Differential Revision: https://reviews.facebook.net/D19389/use arena to allocate memtable's bloomfilter and hashskiplist's buckets_

Summary:
    Bloomfilter and hashskiplist's buckets_ allocated by memtable's arena
    DynamicBloom: pass arena via constructor, allocate space in SetTotalBits
    HashSkipListRep: allocate space of buckets_ using arena.
       do not delete it in deconstructor because arena would take care of it.
    Several test files are changed.

Test Plan:
    make all check

Reviewers: ljin, haobo, yhchiang, sdong

Reviewed By: sdong

Subscribers: igor, dhruba

Differential Revision: https://reviews.facebook.net/D19335/Allow compaction to reclaim storage more effectively.

Summary:
This diff allows compaction to reclaim storage more effectively.
In the current design, compactions are mainly triggered based on
the file sizes.  However, since deletion entries does not have
value, files which have many deletion entries are less likely
to be compacted.  As a result, it may took a while to make
deletion entries to be compacted.

This diff address issue by compensating the size of deletion
entries during compaction process: the size of each deletion
entry in the compaction process is augmented by 2x average
value size.  The diff applies to both leveled and universal
compacitons.

Test Plan:
develop CompactionDeletionTrigger
make db_test
./db_test

Reviewers: haobo, igor, ljin, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19029/[RocksDB] Make block based table hash index more adaptive

Summary: Currently, RocksDB returns error if a db written with prefix hash index, is later opened without providing a prefix extractor. This is uncessarily harsh. Without a prefix extractor, we could always fallback to the normal binary index.

Test Plan: unit test, also manually veried LOG that fallback did occur.

Reviewers: sdong, ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19191/fix forward iterator bug

Summary: obvious

Test Plan: db_test

Reviewers: sdong, haobo, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D18987/FIFO compaction style

Summary:
Introducing new compaction style -- FIFO.

FIFO compaction style has write amplification of 1 (+1 for WAL) and it deletes the oldest files when the total DB size exceeds pre-configured values.

FIFO compaction style is suited for storing high-frequency event logs.

Test Plan: Added a unit test

Reviewers: dhruba, haobo, sdong

Reviewed By: dhruba

Subscribers: alberts, leveldb

Differential Revision: https://reviews.facebook.net/D18765/"
,,Rocksdb,"integrate rate limiter into rocksdb

Summary:
Add option and plugin rate limiter for PosixWritableFile. The rate
limiter only applies to flush and compaction. WAL and MANIFEST are
excluded from this enforcement.

Test Plan: db_test

Reviewers: igor, yhchiang, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19425/"
,,Rocksdb,"Allow user to specify DB path of output file of manual compaction

Summary: Add a parameter path_id to DB::CompactRange(), to indicate where the output file should be placed to.

Test Plan: add a unit test

Reviewers: yhchiang, ljin

Reviewed By: ljin

Subscribers: xjin, igor, dhruba, MarkCallaghan, leveldb

Differential Revision: https://reviews.facebook.net/D20085/"
Memory management,Memory management,Rocksdb,"Add DB property ""rocksdb.estimate-table-readers-mem""

Summary:
Add a DB Property ""rocksdb.estimate-table-readers-mem"" to return estimated memory usage by all loaded table readers, other than allocated from block cache.

Refactor the property codes to allow getting property from a version, with DB mutex not acquired.

Test Plan: Add several checks of this new property in existing codes for various cases.

Reviewers: yhchiang, ljin

Reviewed By: ljin

Subscribers: xjin, igor, leveldb

Differential Revision: https://reviews.facebook.net/D20733/logging_when_create_and_delete_manifest

Summary:
  1. logging when create and delete manifest file
  2. fix formating in table/format.cc

Test Plan:
  make all check
  run db_bench, track the LOG file.

Reviewers: ljin, yhchiang, igor, yufei.zhu, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D21009/Fixed compaction-related errors where number of input levels are hard-coded.

Summary:
Fixed compaction-related errors where number of input levels are hard-coded.
It's a bug found in compaction branch.
This diff will be pushed into master.

Test Plan:
export ROCKSDB_TESTS=Compact
make db_test -j32
./db_test
also passed the tests in compaction branch

Reviewers: igor, sdong, ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20577/Refactoring Version::Get()

Summary: Refactoring Version::Get() method to move file picker logic to a separate class.

Test Plan: make check all

Reviewers: igor, sdong, ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19713/Support multiple DB directories in universal compaction style

Summary:
This patch adds a target size parameter in options.db_paths and universal compaction will base it to determine which DB path to place a new file.
Level-style stays the same.

Test Plan: Add new unit tests

Reviewers: ljin, yhchiang

Reviewed By: yhchiang

Subscribers: MarkCallaghan, dhruba, igor, leveldb

Differential Revision: https://reviews.facebook.net/D19869/use FileLevel in LevelFileNumIterator

Summary:
  Use FileLevel in LevelFileNumIterator, thus use new version of findFile.
  Old version of findFile function is deleted.
  Write a function in version_set.cc to generate FileLevel from files_.
  Add GenerateFileLevelTest in version_set_test.cc

Test Plan:
  make all check

Reviewers: ljin, haobo, yhchiang, sdong

Reviewed By: sdong

Subscribers: igor, dhruba

Differential Revision: https://reviews.facebook.net/D19659/create compressed_levels_ in Version, allocate its space using arena. Make Version::Get, Version::FindFile faster

Summary:
    Define CompressedFileMetaData that just contains fd, smallest_slice, largest_slice. Create compressed_levels_ in Version, the space is allocated using arena
    Thus increase the file meta data locality, speed up ""Get"" and ""FindFile""

    benchmark with in-memory tmpfs, could have 4% improvement under ""random read"" and 2% improvement under ""read while writing""

benchmark command:
./db_bench --db=/mnt/db/rocksdb --num_levels=6 --key_size=20 --prefix_size=20 --keys_per_prefix=0 --value_size=100 --block_size=4096 --cache_size=17179869184 --cache_numshardbits=6 --compression_type=none --compression_ratio=1 --min_level_to_compress=-1 --disable_seek_compaction=1 --hard_rate_limit=2 --write_buffer_size=134217728 --max_write_buffer_number=2 --level0_file_num_compaction_trigger=8 --target_file_size_base=33554432 --max_bytes_for_level_base=1073741824 --disable_wal=0 --sync=0 --disable_data_sync=1 --verify_checksum=1 --delete_obsolete_files_period_micros=314572800 --max_grandparent_overlap_factor=10 --max_background_compactions=4 --max_background_flushes=0 --level0_slowdown_writes_trigger=16 --level0_stop_writes_trigger=24 --statistics=0 --stats_per_interval=0 --stats_interval=1048576 --histogram=0 --use_plain_table=1 --open_files=-1 --mmap_read=1 --mmap_write=0 --memtablerep=prefix_hash --bloom_bits=10 --bloom_locality=1 --perf_level=0 --benchmarks=readwhilewriting,readwhilewriting,readwhilewriting --use_existing_db=1 --num=52428800 --threads=1 —writes_per_second=81920

Read Random:
From 1.8363 ms/op, improve to 1.7587 ms/op.
Read while writing:
From 2.985 ms/op, improve to 2.924 ms/op.

Test Plan:
    make all check

Reviewers: ljin, haobo, yhchiang, sdong

Reviewed By: sdong

Subscribers: dhruba, igor

Differential Revision: https://reviews.facebook.net/D19419/Support Multiple DB paths (without having an interface to expose to users)

Summary:
In this patch, we allow RocksDB to support multiple DB paths internally.
No user interface is supported yet so this patch is silent to users.

Test Plan: make all check

Reviewers: igor, haobo, ljin, yhchiang

Reviewed By: yhchiang

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18921/Allow compaction to reclaim storage more effectively.

Summary:
This diff allows compaction to reclaim storage more effectively.
In the current design, compactions are mainly triggered based on
the file sizes.  However, since deletion entries does not have
value, files which have many deletion entries are less likely
to be compacted.  As a result, it may took a while to make
deletion entries to be compacted.

This diff address issue by compensating the size of deletion
entries during compaction process: the size of each deletion
entry in the compaction process is augmented by 2x average
value size.  The diff applies to both leveled and universal
compacitons.

Test Plan:
develop CompactionDeletionTrigger
make db_test
./db_test

Reviewers: haobo, igor, ljin, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19029/Refactor: group metadata needed to open an SST file to a separate copyable struct

Summary:
We added multiple fields to FileMetaData recently and are planning to add more.
This refactoring separate the minimum information for accessing the file. This object is copyable (FileMetaData is not copyable since the ref counter). I hope this refactoring can enable further improvements:

(1) use it to design a more efficient data structure to speed up read queries.
(2) in the future, when we add information of storage level, we can easily do the encoding, instead of enlarge this structure, which might expand memory work set for file meta data.

The definition is same as current EncodedFileMetaData used in two level iterator, so now the logic in two level iterator is easier to understand.

Test Plan: make all check

Reviewers: haobo, igor, ljin

Reviewed By: ljin

Subscribers: leveldb, dhruba, yhchiang

Differential Revision: https://reviews.facebook.net/D18933/VersionSet::Get(): Bring back the logic of skipping key range check when there are <=3 level 0 files

Summary:
https://reviews.facebook.net/D17205 removed the logic of skipping file key range check when there are less than 3 level 0 files. This patch brings it back.

Other than that, add another small optimization to avoid to check all the levels if most higher levels don't have any file.

Test Plan: make all check

Reviewers: ljin

Reviewed By: ljin

Subscribers: yhchiang, igor, haobo, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D19035/In DB::NewIterator(), try to allocate the whole iterator tree in an arena

Summary:
In this patch, try to allocate the whole iterator tree starting from DBIter from an arena
1. ArenaWrappedDBIter is created when serves as the entry point of an iterator tree, with an arena in it.
2. Add an option to create iterator from arena for following iterators: DBIter, MergingIterator, MemtableIterator, all mem table's iterators, all table reader's iterators and two level iterator.
3. MergeIteratorBuilder is created to incrementally build the tree of internal iterators. It is passed to mem table list and version set and add iterators to it.

Limitations:
(1) Only DB::NewIterator() without tailing uses the arena. Other cases, including readonly DB and compactions are still from malloc
(2) Two level iterator itself is allocated in arena, but not iterators inside it.

Test Plan: make all check

Reviewers: ljin, haobo

Reviewed By: haobo

Subscribers: leveldb, dhruba, yhchiang, igor

Differential Revision: https://reviews.facebook.net/D18513/"
,,Rocksdb,"Fixed the crash when merge_operator is not properly set after reopen.

Summary:
Fixed the crash when merge_operator is not properly set after reopen
and added two test cases for this.

Test Plan:
make merge_test
./merge_test

Reviewers: igor, ljin, sdong

Reviewed By: sdong

Subscribers: benj, mvikjord, leveldb

Differential Revision: https://reviews.facebook.net/D20793/"
,,Rocksdb,"Changes to support unity build:
* Script for building the unity.cc file via Makefile
* Unity executable Makefile target for testing builds
* Source code changes to fix compilation of unity build/FIFO compaction style

Summary:
Introducing new compaction style -- FIFO.

FIFO compaction style has write amplification of 1 (+1 for WAL) and it deletes the oldest files when the total DB size exceeds pre-configured values.

FIFO compaction style is suited for storing high-frequency event logs.

Test Plan: Added a unit test

Reviewers: dhruba, haobo, sdong

Reviewed By: dhruba

Subscribers: alberts, leveldb

Differential Revision: https://reviews.facebook.net/D18765/"
,,Rocksdb,"Add histogram for DB_SEEK

Summary: as title

Test Plan: make release

Reviewers: sdong, yhchiang

Reviewed By: yhchiang

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D21717/Fix db_test and DBIter

Summary: Fix old issue with DBTest.Randomized with BlockBasedTableWithWholeKeyHashIndex + added printing in DBTest.Randomized.

Test Plan: make all check

Reviewers: zagfox, igor, ljin, yhchiang, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D21003/make statistics forward-able

Summary:
Make StatisticsImpl being able to forward stats to provided statistics
implementation. The main purpose is to allow us to collect internal
stats in the future even when user supplies custom statistics
implementation. It avoids intrumenting 2 sets of stats collection code.
One immediate use case is tuning advisor, which needs to collect some
internal stats, users may not be interested.

Test Plan:
ran db_bench and see stats show up at the end of run
Will run make all check since some tests rely on statistics

Reviewers: yhchiang, sdong, igor

Reviewed By: sdong

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D20145/Add Prev() for merge operator

Summary: Implement Prev() with merge operator for DBIterator.  from mongoDB. Task 4673663.

Test Plan: make all check

Reviewers: sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19743/In DB::NewIterator(), try to allocate the whole iterator tree in an arena

Summary:
In this patch, try to allocate the whole iterator tree starting from DBIter from an arena
1. ArenaWrappedDBIter is created when serves as the entry point of an iterator tree, with an arena in it.
2. Add an option to create iterator from arena for following iterators: DBIter, MergingIterator, MemtableIterator, all mem table's iterators, all table reader's iterators and two level iterator.
3. MergeIteratorBuilder is created to incrementally build the tree of internal iterators. It is passed to mem table list and version set and add iterators to it.

Limitations:
(1) Only DB::NewIterator() without tailing uses the arena. Other cases, including readonly DB and compactions are still from malloc
(2) Two level iterator itself is allocated in arena, but not iterators inside it.

Test Plan: make all check

Reviewers: ljin, haobo

Reviewed By: haobo

Subscribers: leveldb, dhruba, yhchiang, igor

Differential Revision: https://reviews.facebook.net/D18513/"
,,Rocksdb,"Support multiple DB directories in universal compaction style

Summary:
This patch adds a target size parameter in options.db_paths and universal compaction will base it to determine which DB path to place a new file.
Level-style stays the same.

Test Plan: Add new unit tests

Reviewers: ljin, yhchiang

Reviewed By: yhchiang

Subscribers: MarkCallaghan, dhruba, igor, leveldb

Differential Revision: https://reviews.facebook.net/D19869/Support Multiple DB paths (without having an interface to expose to users)

Summary:
In this patch, we allow RocksDB to support multiple DB paths internally.
No user interface is supported yet so this patch is silent to users.

Test Plan: make all check

Reviewers: igor, haobo, ljin, yhchiang

Reviewed By: yhchiang

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18921/In logging format, use PRIu64 instead of casting

Summary: Code cleaning up, since we are already using __STDC_FORMAT_MACROS in printing uint64_t, change other places. Only logging is changed.

Test Plan: make all check

Reviewers: ljin

Reviewed By: ljin

Subscribers: dhruba, yhchiang, haobo, leveldb

Differential Revision: https://reviews.facebook.net/D19113/"
,,Rocksdb,"Adding option to save PlainTable index and bloom filter in SST file.

Summary:
Adding option to save PlainTable index and bloom filter in SST file.
If there is no bloom block and/or index block, PlainTableReader builds
new ones. Otherwise PlainTableReader just use these blocks.

Test Plan: make all check

Reviewers: sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19527/Add PlainTableOptions

Summary:
Since we have a lot of options for PlainTable, add a struct PlainTableOptions
to manage them

Test Plan: make all check

Reviewers: sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D20175/PlainTable to encode to avoid to rewrite prefix when it is the same as the previous key

Summary:
Add a encoding feature of PlainTable to encode PlainTable's keys to save some bytes for the same prefixes.
The data format is documented in table/plain_table_factory.h

Test Plan: Add unit test coverage in plain_table_db_test

Reviewers: yhchiang, igor, dhruba, ljin, haobo

Reviewed By: haobo

Subscribers: nkg-, leveldb

Differential Revision: https://reviews.facebook.net/D18735/Change order of parameters in adaptive table factory

Summary:
This is minor, but if we put the writing talbe factory as the third parameter, when we add a new table format, we'll have a situation:
1) block based factory
2) plain table factory
3) output factory
4) new format factory

I think it makes more sense to have output as the first parameter.

Also, fixed a NewAdaptiveTableFactory() call in unit test

Test Plan: unit test

Reviewers: sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19119/Add a table factory that can read DB with both of PlainTable and BlockBasedTable in it

Summary: The new table factory is used if users want to convert a DB from one table format to the other. A user can use this table to open a DB written using one table format and write new files to another table format.

Test Plan: add a unit test

Reviewers: haobo, igor

Reviewed By: igor

Subscribers: dhruba, ljin, yhchiang, leveldb

Differential Revision: https://reviews.facebook.net/D19017/Clean PlainTableReader's variables for better data locality

Summary:
Clean PlainTableReader's data structures:
(1) inline bloom_ (in order to do this, change DynamicBloom to allow lazy initialization)
(2) remove some variables only used when initialization from the class
(3) put variables not used in normal read code paths to the end of the class and reference prefix_extractor directly
(4) make Options a reference.

Test Plan: make all check

Reviewers: haobo, ljin

Reviewed By: ljin

Subscribers: igor, yhchiang, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D18891/"
,,Rocksdb,"Pass parsed user key to prefix extractor in V2 compaction

Previously, the prefix extractor was being supplied with the RocksDB
key instead of a parsed user key. This makes correct interpretation
by calling application fragile or impossible./Fix leak in c_test/Add support for C bindings to the compaction V2 filter mechanism.

Test Plan: make c_test && ./c_test

Some fixes after merge./C API: Add test for compaction filter factories

Also refactored the compaction filter tests to share some code and ensure that
options were getting reset so future test results aren't confused./C API: column family support/Add a test for using compaction filters via the C API/"
,,Rocksdb,"Use IterKey instead of string in Block::Iter to reduce malloc

Summary:
  Modify a functioin TrimAppend in dbformat.h: IterKey. Write a test for it in dbformat_test
  Use IterKey in block::Iter to replace std::string to reduce malloc.

  Evaluate it using perf record.
  malloc: 4.26% -> 2.91%
  free: 3.61% -> 3.08%

Test Plan:
  make all check
  ./valgrind db_test dbformat_test

Reviewers: ljin, haobo, yhchiang, dhruba, igor, sdong

Reviewed By: sdong

Differential Revision: https://reviews.facebook.net/D20433/"
,,Rocksdb,"Add TimedWait() API to CondVar.

Summary:
Add TimedWait() API to CondVar, which will be used in the future to
support TimedOut Write API and Rate limiter.

Test Plan: make db_test -j32

Reviewers: sdong, ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D19431/"
,,Rocksdb,"Fix lint errors and coding style of ldb related codes.

Summary: Fix lint errors and coding style of ldb related codes.

Test Plan: ./ldb

Reviewers: ljin, sdong, igor

Reviewed By: igor

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D28125/"
,Thread management,Rocksdb,"Fix -Wshadow for tools

Summary: Previously I made `make check` work with -Wshadow, but there are some tools that are not compiled using `make check`.

Test Plan: make all

Reviewers: yhchiang, rven, ljin, sdong

Reviewed By: ljin, sdong

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D28497/Add support for in place update for db_stress

Summary:
Added two flags which operate as follows:
in_place_update: enable in_place_update for default column family
set_in_place_one_in: toggles the value of the option inplace_update_support with a probability of 1/N

Test Plan:
Run db_stress with the two flags above set.
Specifically tried in_place_update set to true and set_in_place_one_in set to 10,000.

Reviewers: ljin, igor, sdong

Reviewed By: sdong

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D28029/db_stress for dynamic options

Summary: Allow SetOptions() during db_stress test

Test Plan: make crash_test

Reviewers: sdong, yhchiang, rven, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D25497/Implement full filter for block based table.

Summary:
1. Make filter_block.h a base class. Derive block_based_filter_block and full_filter_block. The previous one is the traditional filter block. The full_filter_block is newly added. It would generate a filter block that contain all the keys in SST file.

2. When querying a key, table would first check if full_filter is available. If not, it would go to the exact data block and check using block_based filter.

3. User could choose to use full_filter or tradional(block_based_filter). They would be stored in SST file with different meta index name. ""filter.filter_policy"" or ""full_filter.filter_policy"". Then, Table reader is able to know the fllter block type.

4. Some optimizations have been done for full_filter_block, thus it requires a different interface compared to the original one in filter_policy.h.

5. Actual implementation of filter bits coding/decoding is placed in util/bloom_impl.cc

Benchmark: base commit 1d23b5c470844c1208301311f0889eca750431c0
Command:
db_bench --db=/dev/shm/rocksdb --num_levels=6 --key_size=20 --prefix_size=20 --keys_per_prefix=0 --value_size=100 --write_buffer_size=134217728 --max_write_buffer_number=2 --target_file_size_base=33554432 --max_bytes_for_level_base=1073741824 --verify_checksum=false --max_background_compactions=4 --use_plain_table=0 --memtablerep=prefix_hash --open_files=-1 --mmap_read=1 --mmap_write=0 --bloom_bits=10 --bloom_locality=1 --memtable_bloom_bits=500000 --compression_type=lz4 --num=393216000 --use_hash_search=1 --block_size=1024 --block_restart_interval=16 --use_existing_db=1 --threads=1 --benchmarks=readrandom —disable_auto_compactions=1
Read QPS increase for about 30% from 2230002 to 2991411.

Test Plan:
make all check
valgrind db_test
db_stress --use_block_based_filter = 0
./auto_sanity_test.sh

Reviewers: igor, yhchiang, ljin, sdong

Reviewed By: sdong

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D20979/add assert to db Put in db_stress test

Summary:
1. assert db->Put to be true in db_stress
2. begin column family with name ""1"".

Test Plan: 1. ./db_stress

Reviewers: ljin, yhchiang, dhruba, sdong, igor

Reviewed By: sdong, igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D22659/move block based table related options BlockBasedTableOptions

Summary:
I will move compression related options in a separate diff since this
diff is already pretty lengthy.
I guess I will also need to change JNI accordingly :(

Test Plan: make all check

Reviewers: yhchiang, igor, sdong

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D21915/"
,Thread management,Rocksdb,"Implement full filter for block based table.

Summary:
1. Make filter_block.h a base class. Derive block_based_filter_block and full_filter_block. The previous one is the traditional filter block. The full_filter_block is newly added. It would generate a filter block that contain all the keys in SST file.

2. When querying a key, table would first check if full_filter is available. If not, it would go to the exact data block and check using block_based filter.

3. User could choose to use full_filter or tradional(block_based_filter). They would be stored in SST file with different meta index name. ""filter.filter_policy"" or ""full_filter.filter_policy"". Then, Table reader is able to know the fllter block type.

4. Some optimizations have been done for full_filter_block, thus it requires a different interface compared to the original one in filter_policy.h.

5. Actual implementation of filter bits coding/decoding is placed in util/bloom_impl.cc

Benchmark: base commit 1d23b5c470844c1208301311f0889eca750431c0
Command:
db_bench --db=/dev/shm/rocksdb --num_levels=6 --key_size=20 --prefix_size=20 --keys_per_prefix=0 --value_size=100 --write_buffer_size=134217728 --max_write_buffer_number=2 --target_file_size_base=33554432 --max_bytes_for_level_base=1073741824 --verify_checksum=false --max_background_compactions=4 --use_plain_table=0 --memtablerep=prefix_hash --open_files=-1 --mmap_read=1 --mmap_write=0 --bloom_bits=10 --bloom_locality=1 --memtable_bloom_bits=500000 --compression_type=lz4 --num=393216000 --use_hash_search=1 --block_size=1024 --block_restart_interval=16 --use_existing_db=1 --threads=1 --benchmarks=readrandom —disable_auto_compactions=1
Read QPS increase for about 30% from 2230002 to 2991411.

Test Plan:
make all check
valgrind db_test
db_stress --use_block_based_filter = 0
./auto_sanity_test.sh

Reviewers: igor, yhchiang, ljin, sdong

Reviewed By: sdong

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D20979/"
,,Rocksdb,"Adding a user comparator for comparing Uint64 slices.

Summary:
- New Uint64 comparator
- Modify Reader and Builder to take custom user comparators instead of bytewise comparator
- Modify logic for choosing unused user key in builder
- Modify iterator logic in reader
- test changes

Test Plan:
cuckoo_table_{builder,reader,db}_test
make check all

Reviewers: ljin, sdong

Reviewed By: ljin

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D22377/"
,,Rocksdb,"Turn on -Wshorten-64-to-32 and fix all the errors

Summary:
We need to turn on -Wshorten-64-to-32 for mobile. See D1671432 (internal phabricator) for details.

This diff turns on the warning flag and fixes all the errors. There were also some interesting errors that I might call bugs, especially in plain table. Going forward, I think it makes sense to have this flag turned on and be very very careful when converting 64-bit to 32-bit variables.

Test Plan: compiles

Reviewers: ljin, rven, yhchiang, sdong

Reviewed By: yhchiang

Subscribers: bobbaldwin, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D28689/Turn -Wshadow back on

Summary: It turns out that -Wshadow has different rules for gcc than clang. Previous commit fixed clang. This commits fixes the rest of the warnings for gcc.

Test Plan: compiles

Reviewers: ljin, yhchiang, rven, sdong

Reviewed By: sdong

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D28131/"
,,Rocksdb,"Turn on -Wshorten-64-to-32 and fix all the errors

Summary:
We need to turn on -Wshorten-64-to-32 for mobile. See D1671432 (internal phabricator) for details.

This diff turns on the warning flag and fixes all the errors. There were also some interesting errors that I might call bugs, especially in plain table. Going forward, I think it makes sense to have this flag turned on and be very very careful when converting 64-bit to 32-bit variables.

Test Plan: compiles

Reviewers: ljin, rven, yhchiang, sdong

Reviewed By: yhchiang

Subscribers: bobbaldwin, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D28689/Turn on -Wshadow

Summary:
...and fix all the errors :)

Jim suggested turning on -Wshadow because it helped him fix number of critical bugs in fbcode. I think it's a good idea to be -Wshadow clean.

Test Plan: compiles

Reviewers: yhchiang, rven, sdong, ljin

Reviewed By: ljin

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D27711/"
,Thread management,Rocksdb,"Turn on -Wshorten-64-to-32 and fix all the errors

Summary:
We need to turn on -Wshorten-64-to-32 for mobile. See D1671432 (internal phabricator) for details.

This diff turns on the warning flag and fixes all the errors. There were also some interesting errors that I might call bugs, especially in plain table. Going forward, I think it makes sense to have this flag turned on and be very very careful when converting 64-bit to 32-bit variables.

Test Plan: compiles

Reviewers: ljin, rven, yhchiang, sdong

Reviewed By: yhchiang

Subscribers: bobbaldwin, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D28689/Fix #284

Summary: This work on my compiler, but it turns out some compilers don't implicitly add constness, see: https://github.com/facebook/rocksdb/issues/284. This diff adds constness explicitly.

Test Plan: still compiles

Reviewers: sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D23409/Implement full filter for block based table.

Summary:
1. Make filter_block.h a base class. Derive block_based_filter_block and full_filter_block. The previous one is the traditional filter block. The full_filter_block is newly added. It would generate a filter block that contain all the keys in SST file.

2. When querying a key, table would first check if full_filter is available. If not, it would go to the exact data block and check using block_based filter.

3. User could choose to use full_filter or tradional(block_based_filter). They would be stored in SST file with different meta index name. ""filter.filter_policy"" or ""full_filter.filter_policy"". Then, Table reader is able to know the fllter block type.

4. Some optimizations have been done for full_filter_block, thus it requires a different interface compared to the original one in filter_policy.h.

5. Actual implementation of filter bits coding/decoding is placed in util/bloom_impl.cc

Benchmark: base commit 1d23b5c470844c1208301311f0889eca750431c0
Command:
db_bench --db=/dev/shm/rocksdb --num_levels=6 --key_size=20 --prefix_size=20 --keys_per_prefix=0 --value_size=100 --write_buffer_size=134217728 --max_write_buffer_number=2 --target_file_size_base=33554432 --max_bytes_for_level_base=1073741824 --verify_checksum=false --max_background_compactions=4 --use_plain_table=0 --memtablerep=prefix_hash --open_files=-1 --mmap_read=1 --mmap_write=0 --bloom_bits=10 --bloom_locality=1 --memtable_bloom_bits=500000 --compression_type=lz4 --num=393216000 --use_hash_search=1 --block_size=1024 --block_restart_interval=16 --use_existing_db=1 --threads=1 --benchmarks=readrandom —disable_auto_compactions=1
Read QPS increase for about 30% from 2230002 to 2991411.

Test Plan:
make all check
valgrind db_test
db_stress --use_block_based_filter = 0
./auto_sanity_test.sh

Reviewers: igor, yhchiang, ljin, sdong

Reviewed By: sdong

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D20979/"
,,Rocksdb,"Revert ""Fix lint errors and coding style of ldb related codes.""

This reverts commit bc9f36fd5e5f0eae69a5a1b7269bb2623cc0eb1f./Fix lint errors and coding style of ldb related codes.

Summary: Fix lint errors and coding style of ldb related codes.

Test Plan: ./ldb

Reviewers: ljin, sdong, igor

Reviewed By: igor

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D28125/ldb_cmd_execute_result.h: perform init in initialization list

Fix for:

[util/ldb_cmd_execute_result.h:18]: (performance) Variable 'message_'
 is assigned in constructor body. Consider performing initialization
 in initialization list.
[util/ldb_cmd_execute_result.h:23]: (performance) Variable 'message_'
 is assigned in constructor body. Consider performing initialization
 in initialization list.

Signed-off-by: Danny Al-Gaaf <danny.al-gaaf@bisect.de>/"
,,Rocksdb,"Explicitly cast char to signed char in Hash()

Summary:
The compilers we use treat char as signed. However, this is not guarantee of C standard and some compilers (for ARM platform for example), treat char as unsigned. Code that assumes that char is either signed or unsigned is wrong.

This change explicitly casts the char to signed version. This will not break any of our use cases on x86, which, I believe are all of them. In case somebody out there is using RocksDB on ARM AND using bloom filters, they're going to have a bad time. However, it is very unlikely that this is the case.

Test Plan: sanity test with previous commit (with new sanity test)

Reviewers: yhchiang, ljin, sdong

Reviewed By: ljin

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D22767/"
,,Rocksdb,"convert Options from string

Summary: Allow accepting Options as a string of key/value pairs

Test Plan: unit test

Reviewers: yhchiang, sdong, igor

Reviewed By: igor

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D24597/Options conversion function for convenience

Summary: as title

Test Plan: options_test

Reviewers: sdong, yhchiang, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D23283/"
,,Rocksdb,"Apply InfoLogLevel to the logs in util/env_hdfs.cc

Summary: Apply InfoLogLevel to the logs in util/env_hdfs.cc

Test Plan: make

Reviewers: ljin, sdong, igor

Reviewed By: igor

Subscribers: dhruba

Differential Revision: https://reviews.facebook.net/D28011/"
,,Rocksdb,"Revert ""Fix lint errors and coding style of ldb related codes.""

This reverts commit bc9f36fd5e5f0eae69a5a1b7269bb2623cc0eb1f./Fix lint errors and coding style of ldb related codes.

Summary: Fix lint errors and coding style of ldb related codes.

Test Plan: ./ldb

Reviewers: ljin, sdong, igor

Reviewed By: igor

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D28125/ldb: support --fix_prefix_len

Summary:
ldb to support --fix_prefix_len to allow us to verify more cases.
Also fix a small issue that --bloom_bits might not be applied if --block_size is not given.

Test Plan: run ldb tool against an example DB.

Reviewers: ljin, yhchiang, rven, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D24819/"
,,Rocksdb,"Deprecate AtomicPointer

Summary: RocksDB already depends on C++11, so we might as well all the goodness that C++11 provides. This means that we don't need AtomicPointer anymore. The less things in port/, the easier it will be to port to other platforms.

Test Plan: make check + careful visual review verifying that NoBarried got memory_order_relaxed, while Acquire/Release methods got memory_order_acquire and memory_order_release

Reviewers: rven, yhchiang, ljin, sdong

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D27543/"
,,Rocksdb,"Turn on -Wshadow

Summary:
...and fix all the errors :)

Jim suggested turning on -Wshadow because it helped him fix number of critical bugs in fbcode. I think it's a good idea to be -Wshadow clean.

Test Plan: compiles

Reviewers: yhchiang, rven, sdong, ljin

Reviewed By: ljin

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D27711/"
,,Rocksdb,"Revert ""Fix lint errors and coding style of ldb related codes.""

This reverts commit bc9f36fd5e5f0eae69a5a1b7269bb2623cc0eb1f./Fix lint errors and coding style of ldb related codes.

Summary: Fix lint errors and coding style of ldb related codes.

Test Plan: ./ldb

Reviewers: ljin, sdong, igor

Reviewed By: igor

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D28125/ldb: support --fix_prefix_len

Summary:
ldb to support --fix_prefix_len to allow us to verify more cases.
Also fix a small issue that --bloom_bits might not be applied if --block_size is not given.

Test Plan: run ldb tool against an example DB.

Reviewers: ljin, yhchiang, rven, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D24819/util/ldb_cmd.cc: prefer prefix ++operator for non-primitive types

Prefer prefix ++operator for non-primitive types like iterators for
performance reasons. Prefix ++/-- operators avoid creating a temporary
copy.

Signed-off-by: Danny Al-Gaaf <danny.al-gaaf@bisect.de>/Don't leak data returned by opendir/Remove path with arena==nullptr from NewInternalIterator

Summary:
Simply code by removing code path which does not use Arena
from NewInternalIterator

Test Plan:
make all check
make valgrind_check

Reviewers: sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D22395/move block based table related options BlockBasedTableOptions

Summary:
I will move compression related options in a separate diff since this
diff is already pretty lengthy.
I guess I will also need to change JNI accordingly :(

Test Plan: make all check

Reviewers: yhchiang, igor, sdong

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D21915/Eliminate VersionSet memory leak

Summary:
ManifestDumpCommand::DoCommand was allocating a VersionSet and never
freeing it.

Test Plan: make

Reviewers: igor

Reviewed By: igor

Differential Revision: https://reviews.facebook.net/D22221/"
,,Rocksdb,"Turn on -Wshorten-64-to-32 and fix all the errors

Summary:
We need to turn on -Wshorten-64-to-32 for mobile. See D1671432 (internal phabricator) for details.

This diff turns on the warning flag and fixes all the errors. There were also some interesting errors that I might call bugs, especially in plain table. Going forward, I think it makes sense to have this flag turned on and be very very careful when converting 64-bit to 32-bit variables.

Test Plan: compiles

Reviewers: ljin, rven, yhchiang, sdong

Reviewed By: yhchiang

Subscribers: bobbaldwin, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D28689/"
Memory management,Memory management,Rocksdb,"DB::Open() to automatically increase thread pool size if it is smaller than max number of parallel compactions or flushes

Summary:
With the patch, thread pool size will be automatically increased if DB's options ask for more parallelism of compactions or flushes.

Too many users have been confused by the API. Change it to make it harder for users to make mistakes

Test Plan: Add two unit tests to cover the function.

Reviewers: yhchiang, rven, igor, MarkCallaghan, ljin

Reviewed By: ljin

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D27555/introduce TestMemEnv and use it in db_test

Summary:
TestMemEnv simulates all Env APIs using in-memory data structures.
We can use it to speed up db_test run, which is now reduced ~7mins when it is
enabled.
We can also add features to simulate power/disk failures in the next
step
TestMemEnv is derived from helper/mem_env
mem_env can not be used for rocksdb since some of its APIs do not give
the same results as env_posix. And its file read/write is not thread safe

Test Plan:
make all -j32
./db_test
./env_mem_test

Reviewers: sdong, yhchiang, rven, igor

Reviewed By: igor

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D28035/Fix build break because of unsigned/signed mismatch/use fallocate(FALLOC_FL_PUNCH_HOLE) to release unused blocks at the end of file

Summary:
ftruncate does not always free preallocated unused space at the end of file.
In some cases, we pin too much disk space than it should

Test Plan: env_test

Reviewers: sdong, rven, yhchiang, igor

Reviewed By: igor

Subscribers: nkg-, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D25641/Deprecate AtomicPointer

Summary: RocksDB already depends on C++11, so we might as well all the goodness that C++11 provides. This means that we don't need AtomicPointer anymore. The less things in port/, the easier it will be to port to other platforms.

Test Plan: make check + careful visual review verifying that NoBarried got memory_order_relaxed, while Acquire/Release methods got memory_order_acquire and memory_order_release

Reviewers: rven, yhchiang, ljin, sdong

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D27543/Enlarge log size cap when printing file summary

Summary:
Now the file summary is too small for printing. Enlarge it.
To enable it, allow to pass a size to log buffer.

Test Plan:
Add a unit test.
make all check

Reviewers: ljin, yhchiang

Reviewed By: yhchiang

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D21723/Skip AllocateTest if fallocate() is not supported in the file system

Summary: To avoid false positive test failures when the file system doesn't support fallocate. In EnvTest.AllocateTest, we first make a simple fallocate call and check the error codes to rule out the possibility that it is not supported. Skip the test if the error code indicates it is not supported.

Test Plan: Run the test and make sure it passes on file systems supporting and not supporting fallocate

Reviewers: yhchiang, ljin, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D23667/"
,,Rocksdb,"CompactFiles, EventListener and GetDatabaseMetaData

Summary:
This diff adds three sets of APIs to RocksDB.

= GetColumnFamilyMetaData =
* This APIs allow users to obtain the current state of a RocksDB instance on one column family.
* See GetColumnFamilyMetaData in include/rocksdb/db.h

= EventListener =
* A virtual class that allows users to implement a set of
  call-back functions which will be called when specific
  events of a RocksDB instance happens.
* To register EventListener, simply insert an EventListener to ColumnFamilyOptions::listeners

= CompactFiles =
* CompactFiles API inputs a set of file numbers and an output level, and RocksDB
  will try to compact those files into the specified level.

= Example =
* Example code can be found in example/compact_files_example.cc, which implements
  a simple external compactor using EventListener, GetColumnFamilyMetaData, and
  CompactFiles API.

Test Plan:
listener_test
compactor_test
example/compact_files_example
export ROCKSDB_TESTS=CompactFiles
db_test
export ROCKSDB_TESTS=MetaData
db_test

Reviewers: ljin, igor, rven, sdong

Reviewed By: sdong

Subscribers: MarkCallaghan, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D24705/dynamic inplace_update options

Summary:
Make inplace_update_support and inplace_update_num_locks dynamic.
inplace_callback becomes immutable
We are almost free of references to cfd->options() in db_impl

Test Plan: unit test

Reviewers: igor, yhchiang, rven, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D25293/make compaction related options changeable

Summary:
make compaction related options changeable. Most of changes are tedious,
following the same convention: grabs MutableCFOptions at the beginning
of compaction under mutex, then pass it throughout the job and register
it in SuperVersion at the end.

Test Plan: make all check

Reviewers: igor, yhchiang, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D23349/re-enable BlockBasedTable::SetupForCompaction()

Summary:
It was commented out in D22545 by accident. Keep the option in
ImmutableOptions for now. I can make it dynamic in
https://reviews.facebook.net/D23349

Test Plan: make release

Reviewers: sdong, yhchiang, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D23865/change target_file_size_base to uint64_t

Summary: It contrains the file size to be 4G max with int

Test Plan:
tried to grep instance and made sure other related variables are also
uint64

Reviewers: sdong, yhchiang, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D23697/MemTableOptions

Summary: removed reference to options in WriteBatch and DBImpl::Get()

Test Plan: make all check

Reviewers: yhchiang, igor, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D23049/move compaction_filter to immutable_options

Summary:
all shared_ptrs are in immutable_options now. This will also make
options assignment a little cheaper

Test Plan: make release

Reviewers: sdong, yhchiang, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D23001/reduce references to cfd->options() in DBImpl

Summary:
I found it is almost impossible to get rid of this function in a single
batch. I will take a step by step approach

Test Plan: make release

Reviewers: sdong, yhchiang, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D22995/introduce ImmutableOptions

Summary:
As a preparation to support updating some options dynamically, I'd like
to first introduce ImmutableOptions, which is a subset of Options that
cannot be changed during the course of a DB lifetime without restart.

ColumnFamily will keep both Options and ImmutableOptions. Any component
below ColumnFamily should only take ImmutableOptions in their
constructor. Other options should be taken from APIs, which will be
allowed to adjust dynamically.

I am yet to make changes to memtable and other related classes to take
ImmutableOptions in their ctor. That can be done in a seprate diff as
this one is already pretty big.

Test Plan: make all check

Reviewers: yhchiang, igor, sdong

Reviewed By: sdong

Subscribers: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D22545/improve OptimizeForPointLookup()

Summary: also fix HISTORY.md

Test Plan: make all check

Reviewers: sdong, yhchiang, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D22437/print table options

Summary: Add a virtual function in table factory that will print table options

Test Plan: make release

Reviewers: igor, yhchiang, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D22149/"
,,Rocksdb,"Built-in comparator(s) in RocksJava

Extended Built-in comparators with ReverseBytewiseComparator.

Reverse key handling is under certain conditions essential. E.g. while
using timestamp versioned data.

As native-comparators were not available using JAVA-API. Both built-in comparators
were exposed via JNI to be set upon database creation time./"
,Thread management,Rocksdb,"DB::Open() to automatically increase thread pool size if it is smaller than max number of parallel compactions or flushes

Summary:
With the patch, thread pool size will be automatically increased if DB's options ask for more parallelism of compactions or flushes.

Too many users have been confused by the API. Change it to make it harder for users to make mistakes

Test Plan: Add two unit tests to cover the function.

Reviewers: yhchiang, rven, igor, MarkCallaghan, ljin

Reviewed By: ljin

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D27555/use fallocate(FALLOC_FL_PUNCH_HOLE) to release unused blocks at the end of file

Summary:
ftruncate does not always free preallocated unused space at the end of file.
In some cases, we pin too much disk space than it should

Test Plan: env_test

Reviewers: sdong, rven, yhchiang, igor

Reviewed By: igor

Subscribers: nkg-, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D25641/Fix timing/Use chrono for timing

Summary: Since we depend on C++11, we might as well use it for timing, instead of this platform-depended code.

Test Plan: Ran autovector_test, which reports time and confirmed that output is similar to master

Reviewers: ljin, sdong, yhchiang, rven, dhruba

Reviewed By: dhruba

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D25587/"
,,Rocksdb,"Add ComparatorDBTest to test non-default comparators

Summary:
Add some helper functions to make sure DB works well for non-default comparators.
Add a test for SimpleSuffixReverseComparator.

Test Plan: Run the new test

Reviewers: ljin, rven, yhchiang, igor

Reviewed By: igor

Subscribers: leveldb, dhruba

Differential Revision: https://reviews.facebook.net/D27831/"
,Thread management,Rocksdb,"Turn on -Wshorten-64-to-32 and fix all the errors

Summary:
We need to turn on -Wshorten-64-to-32 for mobile. See D1671432 (internal phabricator) for details.

This diff turns on the warning flag and fixes all the errors. There were also some interesting errors that I might call bugs, especially in plain table. Going forward, I think it makes sense to have this flag turned on and be very very careful when converting 64-bit to 32-bit variables.

Test Plan: compiles

Reviewers: ljin, rven, yhchiang, sdong

Reviewed By: yhchiang

Subscribers: bobbaldwin, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D28689/DB::Open() to automatically increase thread pool size if it is smaller than max number of parallel compactions or flushes

Summary:
With the patch, thread pool size will be automatically increased if DB's options ask for more parallelism of compactions or flushes.

Too many users have been confused by the API. Change it to make it harder for users to make mistakes

Test Plan: Add two unit tests to cover the function.

Reviewers: yhchiang, rven, igor, MarkCallaghan, ljin

Reviewed By: ljin

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D27555/Allow env_posix to lower background thread IO priority

Summary: This is a linux-specific system call.

Test Plan: ran db_bench

Reviewers: igor, yhchiang, sdong

Reviewed By: sdong

Subscribers: haobo, leveldb

Differential Revision: https://reviews.facebook.net/D21183/"
,,Rocksdb,"CompactFiles, EventListener and GetDatabaseMetaData

Summary:
This diff adds three sets of APIs to RocksDB.

= GetColumnFamilyMetaData =
* This APIs allow users to obtain the current state of a RocksDB instance on one column family.
* See GetColumnFamilyMetaData in include/rocksdb/db.h

= EventListener =
* A virtual class that allows users to implement a set of
  call-back functions which will be called when specific
  events of a RocksDB instance happens.
* To register EventListener, simply insert an EventListener to ColumnFamilyOptions::listeners

= CompactFiles =
* CompactFiles API inputs a set of file numbers and an output level, and RocksDB
  will try to compact those files into the specified level.

= Example =
* Example code can be found in example/compact_files_example.cc, which implements
  a simple external compactor using EventListener, GetColumnFamilyMetaData, and
  CompactFiles API.

Test Plan:
listener_test
compactor_test
example/compact_files_example
export ROCKSDB_TESTS=CompactFiles
db_test
export ROCKSDB_TESTS=MetaData
db_test

Reviewers: ljin, igor, rven, sdong

Reviewed By: sdong

Subscribers: MarkCallaghan, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D24705/Turn -Wshadow back on

Summary: It turns out that -Wshadow has different rules for gcc than clang. Previous commit fixed clang. This commits fixes the rest of the warnings for gcc.

Test Plan: compiles

Reviewers: ljin, yhchiang, rven, sdong

Reviewed By: sdong

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D28131/"
,,Rocksdb,"CompactFiles, EventListener and GetDatabaseMetaData

Summary:
This diff adds three sets of APIs to RocksDB.

= GetColumnFamilyMetaData =
* This APIs allow users to obtain the current state of a RocksDB instance on one column family.
* See GetColumnFamilyMetaData in include/rocksdb/db.h

= EventListener =
* A virtual class that allows users to implement a set of
  call-back functions which will be called when specific
  events of a RocksDB instance happens.
* To register EventListener, simply insert an EventListener to ColumnFamilyOptions::listeners

= CompactFiles =
* CompactFiles API inputs a set of file numbers and an output level, and RocksDB
  will try to compact those files into the specified level.

= Example =
* Example code can be found in example/compact_files_example.cc, which implements
  a simple external compactor using EventListener, GetColumnFamilyMetaData, and
  CompactFiles API.

Test Plan:
listener_test
compactor_test
example/compact_files_example
export ROCKSDB_TESTS=CompactFiles
db_test
export ROCKSDB_TESTS=MetaData
db_test

Reviewers: ljin, igor, rven, sdong

Reviewed By: sdong

Subscribers: MarkCallaghan, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D24705/SetOptions() to return status and also add it to StackableDB

Summary: as title

Test Plan: ./db_test

Reviewers: sdong, yhchiang, rven, igor

Reviewed By: igor

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D28269/SetOptions() for memtable related options

Summary: as title

Test Plan:
make all check
I will think a way to set up stress test for this

Reviewers: sdong, yhchiang, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D23055/"
,Thread management,Rocksdb,"Ignore missing column families

Summary:
Before this diff, whenever we Write to non-existing column family, Write() would fail.

This diff adds an option to not fail a Write() when WriteBatch points to non-existing column family. MongoDB said this would be useful for them, since they might have a transaction updating an index that was dropped by another thread. This way, they don't have to worry about checking if all indexes are alive on every write. They don't care if they lose writes to dropped index.

Test Plan: added a small unit test

Reviewers: sdong, yhchiang, ljin

Reviewed By: ljin

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D22143/"
,,Rocksdb,"created a new ReadOptions parameter 'iterate_upper_bound'/move block based table related options BlockBasedTableOptions

Summary:
I will move compression related options in a separate diff since this
diff is already pretty lengthy.
I guess I will also need to change JNI accordingly :(

Test Plan: make all check

Reviewers: yhchiang, igor, sdong

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D21915/"
,,Rocksdb,"SetOptions() to return status and also add it to StackableDB

Summary: as title

Test Plan: ./db_test

Reviewers: sdong, yhchiang, rven, igor

Reviewed By: igor

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D28269/"
,,Rocksdb,"call SanitizeDBOptionsByCFOptions() in the right place

Summary: It only covers Open() with default column family right now

Test Plan: make release

Reviewers: igor, yhchiang, sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D22467/"
,,Rocksdb,"32-Bit RocksJava resolution for jlong overflows

Summary:
This   solves the jlong overflow problem on 32-Bit machines as described in https://github.com/facebook/rocksdb/issues/278:

1. There is a new org.rocksdb.test.PlatformRandomHelper to assist in getting random values. For 32 Bit the getLong method is overriden by xpromaches code above. For 64 Bit it behaves as is.
2. The detection should be cross-platform (Windows is supported though it is not ported completely yet).
3. Every JNI method which sets jlong values must check if the value fits into size_t. If it overflows size_t a InvalidArgument Status object will be returned. If its ok a OK Status will be returned.
4. Setters which have this check will throw a RocksDBException if its no OK Status.

Additionally some other parts of code were corrected using the wrong type casts.

Test Plan:
make rocksdbjava
make jtest

Differential Revision: https://reviews.facebook.net/D24531/Add block based table config options/Add rate limiter/[Java] Include WriteBatch into RocksDBSample.java, fix how DbBenchmark.java handles WriteBatch.

Summary:
Include WriteBatch into RocksDBSample.java, fix how DbBenchmark.java handles WriteBatch.
Previously DbBenchmark.java does not use WriteBatch when benchmarks is set to fillbatch.

Test Plan:
make rocksdbjava -j32
make jtest
make jdb_bench
cd java
./jdb_bench.sh --benchmarks=fillbatch

Reviewers: naveenatceg, ljin, sdong, ankgup87

Reviewed By: ankgup87

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D22983/"
,,Rocksdb,"JNI changes corresponding to BlockBasedTableOptions migration

Summary: as title

Test Plan:
tested on my mac
make rocksdbjava
make jtest

Reviewers: sdong, igor, yhchiang

Reviewed By: yhchiang

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D21963/"
,,Rocksdb,"Update RocksDB's Java bindings to support multiple native RocksDB builds in the same Jar file. Cross build RocksDB for linux32 and linux64 using Vagrant. Build a cross-platform fat jar that contains osx, linux32, and linux64 RocksDB static builds./"
,,Rocksdb,"[RocksJava] - Hardening RocksIterator

RocksIterator will sometimes Sigsegv on dispose. Mainly thats related
to dispose order. If the related RocksDB instance is freed beforehand
RocksIterator.dispose() will fail.

Within this commit there is a major change to RocksIterator. RocksIterator
will hold a private reference to the RocksDB instance which created the
RocksIterator. So even if RocksDB is freed in the same GC cycle the
RocksIterator instances will be freed prior to related RocksDB instances.

Another aspect targets the dispose logic if the RocksDB is freed previously
and already gc`ed. On dispose of a RocksIterator the dispose logic will check
if the RocksDB instance points to an initialized DB. If not the dispose logic
will not perform any further action.

The crash can be reproduced by using the related test provided within this
commit.

Related information: This relates to @adamretter`s facebook rocksdb-dev group
post about SigSegv on RocksIterator.dispose()./"
,,Rocksdb,"[RocksJava] Column family support

This commit includes the support for the following functionalities:

 - Single Get/Put operations
 - WriteBatch operations
 - Single iterator functionality
 - Open database with column families
 - Open database with column families Read/Only
 - Create column family
 - Drop column family
 - Properties of column families
 - Listing of column families
 - Fully backwards comptabile implementation
 - Multi Iterator support
 - MultiGet
 - KeyMayExist
 - Option to create missing column families on open

In addition there is are two new Tests:

 - Test of ColumnFamily functionality
 - Test of Read only feature to open subsets of column families
 - Basic test to test the KeyMayExist feature

What is not supported currently using RocksJava:

 - Custom ColumnFamilyOptions

The following targets work as expected:

 - make rocksdbjava
 - make jtest

Test environment: Ubuntu 14.04(LTS, x64), Java 1.7.0_65(OpenJDK IcedTea 2.5.2), g++ 4.8.2, kernel 3.13.0-35-generix/Addressing review comments (adding a env variable to override temp directory)/RocksDB static build
Make file changes to download and build the dependencies
.Load the shared library when RocksDB is initialized/"
,,Rocksdb,"[Java] Add purgeOldBackups API

Summary:
1. Check status of CreateNewBackup. If status is not OK, then throw.
2. Add purgeOldBackups API

Test Plan:
make test
make sample

Reviewers: haobo, sdong, zzbennett, swapnilghike, yhchiang

Reviewed By: yhchiang

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D21753/"
,,Rocksdb,"Iterator support for Write Batches/RocksJava Fix after MutableCFOptions change./[RocksJava] Column family support

This commit includes the support for the following functionalities:

 - Single Get/Put operations
 - WriteBatch operations
 - Single iterator functionality
 - Open database with column families
 - Open database with column families Read/Only
 - Create column family
 - Drop column family
 - Properties of column families
 - Listing of column families
 - Fully backwards comptabile implementation
 - Multi Iterator support
 - MultiGet
 - KeyMayExist
 - Option to create missing column families on open

In addition there is are two new Tests:

 - Test of ColumnFamily functionality
 - Test of Read only feature to open subsets of column families
 - Basic test to test the KeyMayExist feature

What is not supported currently using RocksJava:

 - Custom ColumnFamilyOptions

The following targets work as expected:

 - make rocksdbjava
 - make jtest

Test environment: Ubuntu 14.04(LTS, x64), Java 1.7.0_65(OpenJDK IcedTea 2.5.2), g++ 4.8.2, kernel 3.13.0-35-generix/32-Bit RocksJava resolution for jlong overflows

Summary:
This   solves the jlong overflow problem on 32-Bit machines as described in https://github.com/facebook/rocksdb/issues/278:

1. There is a new org.rocksdb.test.PlatformRandomHelper to assist in getting random values. For 32 Bit the getLong method is overriden by xpromaches code above. For 64 Bit it behaves as is.
2. The detection should be cross-platform (Windows is supported though it is not ported completely yet).
3. Every JNI method which sets jlong values must check if the value fits into size_t. If it overflows size_t a InvalidArgument Status object will be returned. If its ok a OK Status will be returned.
4. Setters which have this check will throw a RocksDBException if its no OK Status.

Additionally some other parts of code were corrected using the wrong type casts.

Test Plan:
make rocksdbjava
make jtest

Differential Revision: https://reviews.facebook.net/D24531/resolution for java build problem introduced by 5ec53f3edf62bec1b690ce12fb21a6c52203f3c8/Fix build/[Java] Fixed 32-bit overflowing issue when converting jlong to size_t

Summary:
Fixed 32-bit overflowing issue when converting jlong to size_t by
capping jlong to std::numeric_limits<size_t>::max().

Test Plan:
make rocksdbjava
make jtest

Reviewers: ankgup87, ljin, sdong, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D23511/fix RocksDB java build

Summary: as title

Test Plan: make rocksdbjava

Reviewers: sdong, yhchiang, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D23193/Remove path with arena==nullptr from NewInternalIterator

Summary:
Simply code by removing code path which does not use Arena
from NewInternalIterator

Test Plan:
make all check
make valgrind_check

Reviewers: sdong

Reviewed By: sdong

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D22395/"
,,Rocksdb,"[RocksJava] Memtables update to 3.6

- Adjusted HashLinkedList to 3.6.0
- Adjusted SkipList to 3.6.0
- Introduced a memtable test/32-Bit RocksJava resolution for jlong overflows

Summary:
This   solves the jlong overflow problem on 32-Bit machines as described in https://github.com/facebook/rocksdb/issues/278:

1. There is a new org.rocksdb.test.PlatformRandomHelper to assist in getting random values. For 32 Bit the getLong method is overriden by xpromaches code above. For 64 Bit it behaves as is.
2. The detection should be cross-platform (Windows is supported though it is not ported completely yet).
3. Every JNI method which sets jlong values must check if the value fits into size_t. If it overflows size_t a InvalidArgument Status object will be returned. If its ok a OK Status will be returned.
4. Setters which have this check will throw a RocksDBException if its no OK Status.

Additionally some other parts of code were corrected using the wrong type casts.

Test Plan:
make rocksdbjava
make jtest

Differential Revision: https://reviews.facebook.net/D24531/[Java] Fixed 32-bit overflowing issue when converting jlong to size_t

Summary:
Fixed 32-bit overflowing issue when converting jlong to size_t by
capping jlong to std::numeric_limits<size_t>::max().

Test Plan:
make rocksdbjava
make jtest

Reviewers: ankgup87, ljin, sdong, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D23511/"
,,Rocksdb,"Turn on -Wshorten-64-to-32 and fix all the errors

Summary:
We need to turn on -Wshorten-64-to-32 for mobile. See D1671432 (internal phabricator) for details.

This diff turns on the warning flag and fixes all the errors. There were also some interesting errors that I might call bugs, especially in plain table. Going forward, I think it makes sense to have this flag turned on and be very very careful when converting 64-bit to 32-bit variables.

Test Plan: compiles

Reviewers: ljin, rven, yhchiang, sdong

Reviewed By: yhchiang

Subscribers: bobbaldwin, dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D28689/[RocksJava] Flush functionality

RocksJava now supports also flush functionality of
RocksDB./[RocksJava] -WShadow improvements

Minor corrections to resolve -WShadow build problems with RocksJava code./[RocksJava] KeyMayExist w/o ColumnFamilies/Merge with ColumnFamilies & Hardening CFHandle

Summary:
ColumnFamilyHandles face the same problem as RocksIterator previously
so used methods were also applied for ColumnFamilyHandles.

Another problem with CF was that Options passed to CFs were
always filled with default values. To enable Merge, all parts
of the database must share the same merge functionality which
is not possible using default values. So from now on every
CF will inherit from db options.

Changes to RocksDB:
- merge can now take also a cfhandle

Changes to MergeTest:
- Corrected formatting
- Included also GC tests
- Extended tests to cover CF related parts
- Corrected paths to cleanup properly within the test process
- Reduced verbosity of the test

Test Plan:
make rocksdbjava
make jtest

Subscribers: dhruba

Differential Revision: https://reviews.facebook.net/D27999/[RocksJava] Support Snapshots

Summary:
Snapshots integration into RocksJava. Added support for the following functionalities:

- getSnapshot
- releaseSnapshot
- ReadOptions support to set a Snapshot
- ReadOptions support to retrieve Snapshot
- SnapshotTest

Test Plan:
make rocksdbjava
make jtest

Differential Revision: https://reviews.facebook.net/D24801/Adding merge functions to RocksDBJava

Summary:
Added support for the merge operation to RocksJava.
You can specify a merge function to be used on the current database.
The merge function can either be one of the functions defined in
utilities/merge_operators.h, which can be specified through its
corresponding name, or a user-created function that needs to be
encapsulated in a JNI object in order to be used. Examples are
provided for both use cases.

Test Plan: There are unit test in MergeTest.java

Reviewers: ankgup87

Subscribers: vladb38

Differential Revision: https://reviews.facebook.net/D24525/[RocksJava] Column family support

In addition there is are two new Tests:

 - Test of ColumnFamily functionality
 - Test of Read only feature to open subsets of column families
 - Basic test to test the KeyMayExist feature

What is not supported currently using RocksJava:

 - Custom ColumnFamilyOptions

The following targets work as expected:

 - make rocksdbjava
 - make jtest

Test environment: Ubuntu 14.04(LTS, x64), Java 1.7.0_65(OpenJDK IcedTea 2.5.2), g++ 4.8.2, kernel 3.13.0-35-generix/Add block based table config options/"
,,Rocksdb,"[RocksJava] -WShadow improvements

Minor corrections to resolve -WShadow build problems with RocksJava code./"
,,Rocksdb,"Filters getting disposed by System.gc before EOL

Previous to this commit Filters passed as parameters to the
BlockTableConfig are disposed before they should be disposed.

Further Smart pointer usage was corrected.

Java holds now the smart pointer to the FilterPolicy correctly
and cares about freeing underlying c++ structures./"
,Thread management,Rocksdb,"[RocksJava] Extend Options with ColumnFamilyOptions implementation ColumnFamilyOptions implementation with tests
[RocksJava] Extended ColumnFamilyTest

Summary: Options Refactoring split part 3

Test Plan:
make rocksdbjava
make jtest

Reviewers: yhchiang, ankgup87

Subscribers: dhruba

Differential Revision: https://reviews.facebook.net/D28023/[RocksJava] Flush functionality

RocksJava now supports also flush functionality of
RocksDB./[RocksJava] Extend Options with DBOptions implementation [RocksJava] Included DBOptionsTest and refactored OptionsTest

Summary: Options refactoring - Split Part2

Test Plan:
make rocksdbjava
make jtest

Reviewers: yhchiang, ankgup87

Subscribers: dhruba

Differential Revision: https://reviews.facebook.net/D28017/[RocksJava] Options Refactoring 3.6

Summary:
Options extends now two interfaces DBOptionsInterface
and ColumnFamilyOptionsInterface. There are also further
improvements to the Options bindings:

Optimize methods were ported to Java. (OptimizeForPointLookup,
OptimizeLevelCompaction, OptimizeUniversalCompaction).

To align BuiltinComparator with every other Enum it was moved to
a separate file.

Test Plan:
make rocksdbjava
make jtest/Fix code style problems identified by lint/Add locking to comparator jni callback methods which must be thread-safe/32-Bit RocksJava resolution for jlong overflows

Summary:
This   solves the jlong overflow problem on 32-Bit machines as described in https://github.com/facebook/rocksdb/issues/278:

1. There is a new org.rocksdb.test.PlatformRandomHelper to assist in getting random values. For 32 Bit the getLong method is overriden by xpromaches code above. For 64 Bit it behaves as is.
2. The detection should be cross-platform (Windows is supported though it is not ported completely yet).
3. Every JNI method which sets jlong values must check if the value fits into size_t. If it overflows size_t a InvalidArgument Status object will be returned. If its ok a OK Status will be returned.
4. Setters which have this check will throw a RocksDBException if its no OK Status.

Additionally some other parts of code were corrected using the wrong type casts.

Test Plan:
make rocksdbjava
make jtest

Differential Revision: https://reviews.facebook.net/D24531/Built-in comparator(s) in RocksJava

Extended Built-in comparators with ReverseBytewiseComparator.

Reverse key handling is under certain conditions essential. E.g. while
using timestamp versioned data.

As native-comparators were not available using JAVA-API. Both built-in comparators
were exposed via JNI to be set upon database creation time./Add rate limiter/[Java] Fixed 32-bit overflowing issue when converting jlong to size_t

Summary:
Fixed 32-bit overflowing issue when converting jlong to size_t by
capping jlong to std::numeric_limits<size_t>::max().

Test Plan:
make rocksdbjava
make jtest

Reviewers: ankgup87, ljin, sdong, igor

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D23511/"
,,Rocksdb,"ttl/ttl_test.cc: prefer prefix ++operator for non-primitive types

Signed-off-by: Danny Al-Gaaf <danny.al-gaaf@bisect.de>/"
,,Rocksdb,"Apply InfoLogLevel to the logs in utilities/ttl/db_ttl_impl.h

Summary: Apply InfoLogLevel to the logs in utilities/ttl/db_ttl_impl.h

Test Plan: make

Reviewers: ljin, sdong, igor

Reviewed By: igor

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D27885/"
,,Rocksdb,"Fix Mac compile/Added a few statistics for BackupableDB

Summary:
Added the following statistics to BackupableDB:

1. Number of successful and failed backups in class BackupStatistics
2. Time taken to do a backup
3. Number of files in a backup

1 is implemented in the BackupStatistics class
2 and 3 are added in the BackupMeta and BackupInfo class

Test Plan:
1 can be tested using BackupStatistics::ToString(),
2 and 3 can be tested in the BackupInfo class

Reviewers: sdong, igor2, ljin, igor

Reviewed By: igor

Differential Revision: https://reviews.facebook.net/D22785/"
,,Rocksdb,"move block based table related options BlockBasedTableOptions

Summary:
I will move compression related options in a separate diff since this
diff is already pretty lengthy.
I guess I will also need to change JNI accordingly :(

Test Plan: make all check

Reviewers: yhchiang, igor, sdong

Reviewed By: igor

Subscribers: leveldb

Differential Revision: https://reviews.facebook.net/D21915/"
,,Rocksdb,"Apply InfoLogLevel to the logs in utilities/merge_operators/uint64add.cc

Summary:
Apply InfoLogLevel to the logs and add missing copy-right information
to  utilities/merge_operators/uint64add.cc.

Test Plan: make

Reviewers: ljin, sdong, igor

Reviewed By: igor

Subscribers: dhruba, leveldb

Differential Revision: https://reviews.facebook.net/D27897/"
,Thread management,Realm,"thread safe HandlerController#emptyAsyncRealmObject & realmObjects  (#2761)

* making HandlerController#emptyAsyncRealmObject & HandlerController#realmObjects thread safe   ,"
,,Realm,"  Give async related vars better names

asyncQueryExecutor and pendingQuery seem to be very confusing since they
are used for async transaction as well.   ,  "
,Thread management,Realm,"thread safe HandlerController#emptyAsyncRealmObject & realmObjects  (#2761)

* making HandlerController#emptyAsyncRealmObject & HandlerController#realmObjects thread safe   ,  "
,,Realm,"Give async related vars better names

asyncQueryExecutor and pendingQuery seem to be very confusing since they
are used for async transaction as well.   ,  "
,,Realm,"Let Mixpanel track the version of sync being used (#161)

* Add support for sync to Mixpanel

Also moves the version and SHA256 of sync in the dependencies.list file
This consolidates with the other repos in the org

* Be aware of whether sync is enabled or not

* Align naming style   ,  "
,Thread management,Realm,"Integrate Object Store [PART2] - SharedRealm (#3031)

This simplified our code base a lot. Basically all APIs we need from
SharedGroup/Group are wrapped in the SharedRealm. So we can just remove
those classes.

But we do need a few APIs from SharedGroup which is not supplied by
SharedRealm because of async queries. Currently we expose those in a
friend class of ShareRealm, see realm/realm-object-store#141

We are still managing Realm caches in Java although there are mechanism
in OS to do the same thing. The major reason is we have method like
deleteRealm needs information from cache to check if all Realm instances
are closed. The ShareRealm is actually a std::shared_ptr. We hold
the pointer to the std::shared_ptr in Java.
Another fundamental change is that we used to have separated
SharedGroups for DynamicRealm and typed Realm in the same thread. But
now they are using different SharedRealm but actually the different
SharedRealms are point to the same SharedGroup.

And some other code cleanup.   , "
,,Realm," Supporting both additive and manual schema modes (#91)

* Adding very thin wrappers for Object Store's ObjectSchema and Property.
* Adding method for building object schema is proxy classes
* Adding rudimentary support for Object Store schemas. Using ObjectStore::update_schema() to update schema for the additive mode.
* Disallowing destructive schema changes in additive mode.   , "
,,Realm," Invalidate schema cache when the schema version of Realm is changed by other process (#3409).

invalidate schema cache when the schema version of Realm is changed by other process.

Now schema cache referred by Realm instance is not shared and global schema cache is introduced instead.   ,  "
,,Realm,"Invalidate schema cache when the schema version of Realm is changed by other process (#3409).

invalidate schema cache when the schema version of Realm is changed by other process.

Now schema cache referred by Realm instance is not shared and global schema cache is introduced instead.   ,  "
,,Realm,"Use createObject(class, primaryKey) in test (#3377)

Since Realm.createObject(class) will be deprecated on master for Classes
with primary key defined, fix the test cases which use it first to avoid
more conflicts when merging.   ,  "
,Thread management,Realm,"add default value instruction support (#3462)

* add default value support to Table class

* Table#isNull() and TableView#isNull()

* use default value feature

* added a test to check if nullified link can be overwritten by default value

* removed duplicate thread check when constructing proxy objects (and small bugfix in setter of the list).

* added thread check

* reflect review comments

* reflect comment in JNI code   ,  Fix unstable test (#3495)   ,  "
Feature migration,Feature migration,Realm,"RealmLog (#3368)

Moved RealmLog to the public API. Routes all log events through it, also from native code.   , "
,,Realm," Upgrade to beta-33 / 2.0.0-rc4 (#90)   ,  "
,Thread management,Realm,"Integrate Object Store [PART2] - SharedRealm (#3031)

This simplified our code base a lot. Basically all APIs we need from
SharedGroup/Group are wrapped in the SharedRealm. So we can just remove
those classes.

But we do need a few APIs from SharedGroup which is not supplied by
SharedRealm because of async queries. Currently we expose those in a
friend class of ShareRealm, see realm/realm-object-store#141

We are still managing Realm caches in Java although there are mechanism
in OS to do the same thing. The major reason is we have method like
deleteRealm needs information from cache to check if all Realm instances
are closed. The ShareRealm is actually a std::shared_ptr. We hold
the pointer to the std::shared_ptr in Java.
Another fundamental change is that we used to have separated
SharedGroups for DynamicRealm and typed Realm in the same thread. But
now they are using different SharedRealm but actually the different
SharedRealms are point to the same SharedGroup.

And some other code cleanup.   ,  "
,,Realm,"DeleteLocalRef when the ref is created in loop (#3366)

Add wrapper class for JNI local reference to delete the local ref after
using it.

This is reported by user on helpscout:
https://secure.helpscout.net/conversation/244053233/6163/?folderId=366141

And some useful explanation can be found:
http://stackoverflow.com/questions/24289724/jni-deletelocalref-clarification

Normally the local ref doesn't have to be deleted since they will be
cleaned up when program returns to Java from native code. Using it in a
loop is obvious a corner case: the size of local ref table is relatively
small (512 on Android). To avoid it, the local ref should be deleted
when using it in a loop.   ,  "
,,Realm,"Add RealmFileException

to replace RealmIOException and IncompatibleLockFileException. Also it
is mapped to the same name exception in ObjectStore to give user a
detailed kind of file exception.   ,  "
,,Realm,"Add cause to RealmMigrationNeededException (#3482)   , "
,,Realm," Add RealmFileException

to replace RealmIOException and IncompatibleLockFileException. Also it
is mapped to the same name exception in ObjectStore to give user a
detailed kind of file exception.   , "
,,Realm," Remove deprecated constructor + add directory() (#3357)

This commit simplifies the RealmConfiguration constructors and also ensures that we always have an Android context. 

It does so by now only having the`RealmConfiguration.Builder(context)` constructor. Custom file locations are now supported through the `directory()` builder method. This also made it possible to simply `assetFile(Context, location)` to only `assetFile(location)`.

Having the Context means that we are now able to access system services and other framework classes without exposing any Android functionality in any potential interface (which will be needed to support Realm on the JVM).   ,  "
,,Realm," Add RealmFileException

to replace RealmIOException and IncompatibleLockFileException. Also it
is mapped to the same name exception in ObjectStore to give user a
detailed kind of file exception.   ,  "
,,Realm,"Allow to specify default value of the field in model's constructor (#3397)

* Allow to call its accessors, and replace its field accesses with accessor calls in model's constructor.

fixes #777
fixes #2536

* use field instead of checking transaction

* fix a bug that acceptDefaultValue is not set correctly

* reject default values when the getter of a model creates other model object

* add simple test for default value

* supports default value of model field

* supports default value of RealmList fields

* add tests for assignment in constructor and setter in constructor

* update javadoc comments of createObject

* always ignores the default value of primary key if the object is managed

* update javadoc

* add a test for default values handling in copyToRealm(). the last assertion of RealmTests.copyToRealm_defaultValuesAreIgnored() is failing now.

* refactor tests

* use isPrimaryKey()

* fix a bug that unexpected realm object is created by default value of RealmModel/RealmList fields

* remove extra ';' from generated code

* add more tests for default value

* fix tests

* fix a bug that creates unexpected objects

* rename internal methods

* update changelog

* update CHANGELOG

* review comments

* update CHANGELOG

* added a description of how proxy object should be created in the Javadoc comment of RealmProcessor   ,  "
,,Realm,"Disallow changing PK after object created (#3418)

Thrown an exception if changing the pk after the object creation.   ,  "
,Thread management,Realm,"Logout and Userstore (#104)   , "
,,Realm," Integrate Object Store [PART4] - OS notifications (#3370)

* Use OS's notification mechanism to notify threads.
* Create RealmNotificer interface for decouple Android related handler
  logic.
* Create AndroidNotifier for the handler notifications.

The major change of this PR is about the timing. The notifications are
not sent immediately after transaction committed. Instead, there is a
dedicated thread monitoring changes and notify others when changes
happen.

The known problem is for every RealmConfiguration, a monitor thread will
be created which is not ideal for app which is using multiple
RealmConfiguration.

There are different implementations for the monitoring thread in OS. For
Android, we can choose from generic which is based on the core's
wait_for_change() and android which is used by dotnet based on the
named pipe.
To align with dotnet, we are using the named pipe for now which also
enables notifications between realm-java and realm-dotnet.   , "
,,Realm,"Allow to specify default value of the field in model's constructor (#3397)

* Allow to call its accessors, and replace its field accesses with accessor calls in model's constructor.

fixes #777
fixes #2536

* use field instead of checking transaction

* fix a bug that acceptDefaultValue is not set correctly

* reject default values when the getter of a model creates other model object

* add simple test for default value

* supports default value of model field

* supports default value of RealmList fields

* add tests for assignment in constructor and setter in constructor

* update javadoc comments of createObject

* always ignores the default value of primary key if the object is managed

* update javadoc

* add a test for default values handling in copyToRealm(). the last assertion of RealmTests.copyToRealm_defaultValuesAreIgnored() is failing now.

* refactor tests

* use isPrimaryKey()

* fix a bug that unexpected realm object is created by default value of RealmModel/RealmList fields

* remove extra ';' from generated code

* add more tests for default value

* fix tests

* fix a bug that creates unexpected objects

* rename internal methods

* update changelog

* update CHANGELOG

* review comments

* update CHANGELOG

* added a description of how proxy object should be created in the Javadoc comment of RealmProcessor   , "
,,Realm,"Fixed leaking unit tests.   ,  "
,,Realm,"Add isManaged() to RealmObject/RealmCollection (#3341)

* isValid() returns true for unmanaged object and collection.
* Add isManaged() to RealmObject and RealmCollection   ,  "
,Thread management,Realm,"add default value instruction support (#3462)

* add default value support to Table class

* Table#isNull() and TableView#isNull()

* use default value feature

* added a test to check if nullified link can be overwritten by default value

* removed duplicate thread check when constructing proxy objects (and small bugfix in setter of the list).

* added thread check

* reflect review comments

* reflect comment in JNI code   ,  "
,,Realm,"Use set_string_unique to set primary key (#3488)

* Migrate PK table when get 1st Realm instance
* migratePrimaryKeyTableIfNeeded will be called when the first time
  Realm instance gets opened.
* Get miss-deleted tests case for pk table back.
* Update Object Store   ,  "
,,Realm,"Fix tests (#194)

* Remove unused tests.
* Fix tests with default port[80].
* One failed test caused by wrong mocked JSON string.   ,  "
,,Realm,"Call SyncManager.notifyErrorHandler   ,  "
,,Realm,"Move classes from objectserver to its super (#113)

 ,  "
,,Realm,"Update core to 2.0.0-rc4 (#3384)

* And with some code cleanup.
* Throw an runtime exception when input java bytes array cannot be read.
* Update Object Store to solve the breaking change caused failure.
   See https://github.com/realm/realm-object-store//158
* Use '-O2' instead '-Os' since it seems a gcc bug hangs encryption releated
   tests with '-Os' enabled in JNI build.   ,  "
,,Realm,"DeleteLocalRef when the ref is created in loop (#3366)

Add wrapper class for JNI local reference to delete the local ref after
using it.

This is reported by user on helpscout:
https://secure.helpscout.net/conversation/244053233/6163/?folderId=366141

And some useful explanation can be found:
http://stackoverflow.com/questions/24289724/jni-deletelocalref-clarification

Normally the local ref doesn't have to be deleted since they will be
cleaned up when program returns to Java from native code. Using it in a
loop is obvious a corner case: the size of local ref table is relatively
small (512 on Android). To avoid it, the local ref should be deleted
when using it in a loop.   ,  RealmLog (#3368)

Moved RealmLog to the public API. Routes all log events through it, also from native code.   ,  "
,,Realm,"RealmLog (#3368)

Moved RealmLog to the public API. Routes all log events through it, also from native code.   ,  "
,,Realm,"Merge remote-tracking branch 'origin/master' into master-sync   ,  "
,,Realm,"Move classes from objectserver to its super (#113)
 ,  RealmLog (#3368)

Moved RealmLog to the public API. Routes all log events through it, also from native code.   ,  "
,Thread management,Realm,"Integrate Object Store [PART4] - OS notifications (#3370)

* Use OS's notification mechanism to notify threads.
* Create RealmNotificer interface for decouple Android related handler
  logic.
* Create AndroidNotifier for the handler notifications.

The major change of this PR is about the timing. The notifications are
not sent immediately after transaction committed. Instead, there is a
dedicated thread monitoring changes and notify others when changes
happen.

The known problem is for every RealmConfiguration, a monitor thread will
be created which is not ideal for app which is using multiple
RealmConfiguration.

There are different implementations for the monitoring thread in OS. For
Android, we can choose from generic which is based on the core's
wait_for_change() and android which is used by dotnet based on the
named pipe.
To align with dotnet, we are using the named pipe for now which also
enables notifications between realm-java and realm-dotnet.   ,  "
,,Realm,"Fix some native compiling warnings (#185)

Close #184   ,  "
,,Realm,"Add RealmFileException

to replace RealmIOException and IncompatibleLockFileException. Also it
is mapped to the same name exception in ObjectStore to give user a
detailed kind of file exception.   ,  RealmLog (#3368)

Moved RealmLog to the public API. Routes all log events through it, also from native code.   ,  "
,,Realm,"Fix some native compiling warnings (#185)

Close #184   ,  Public Sync API (#73)   ,  "
,,Realm,"DeleteLocalRef when the ref is created in loop (#3366)

Add wrapper class for JNI local reference to delete the local ref after
using it.

This is reported by user on helpscout:
https://secure.helpscout.net/conversation/244053233/6163/?folderId=366141

And some useful explanation can be found:
http://stackoverflow.com/questions/24289724/jni-deletelocalref-clarification

Normally the local ref doesn't have to be deleted since they will be
cleaned up when program returns to Java from native code. Using it in a
loop is obvious a corner case: the size of local ref table is relatively
small (512 on Android). To avoid it, the local ref should be deleted
when using it in a loop.   ,  Upgrade to beta-33 / 2.0.0-rc4 (#90)   ,  "
,Thread management,Realm,"Make BaseRealm package protected again (#143)

and move SyncObjectServerFacade to internal/objectserver.   ,  "
,,Realm,"Supporting both additive and manual schema modes (#91)

* Adding very thin wrappers for Object Store's ObjectSchema and Property.
* Adding method for building object schema is proxy classes
* Adding rudimentary support for Object Store schemas. Using ObjectStore::update_schema() to update schema for the additive mode.
* Disallowing destructive schema changes in additive mode.   ,  "
,,Realm,"Merge branch 'master' into cm/merge-globalinit-from-master
  ,  "
,,Realm,"Add cause to RealmMigrationNeededException (#3482)   ,  "
Feature migration,Feature migration,Realm,"Fixed bug in path when getting access tokens for Realms (#157)   ,  "
,,Realm,"Introduce global init (#3457)

Realm now uses a global init function instead of Context on the RealmConfiguration.Builder   ,  fix merge mistakes   ,  "
,,Realm,"Public Sync API (#73)   ,"
,,Realm,"  Invalidate schema cache when the schema version of Realm is changed by other process (#3409).

invalidate schema cache when the schema version of Realm is changed by other process.

Now schema cache referred by Realm instance is not shared and global schema cache is introduced instead.   ,"
,,Realm,"  Upgrade to beta-33 / 2.0.0-rc4 (#90)   ,"
,,Realm,"  Merge branch 'master' into master-sync
   ,  "
,,Realm,"Public Sync API (#73)   ,  "
,,Realm,"Logout and Userstore (#104)   ,  "
,,Realm,"SyncConfiguration Builder now only contains allowed options (#87)   ,  "
,,Realm,"Introduce global init (#3457)

Realm now uses a global init function instead of Context on the RealmConfiguration.Builder   ,  "
,,Realm,"Remove deprecated constructor + add directory() (#3357)

This commit simplifies the RealmConfiguration constructors and also ensures that we always have an Android context. 

It does so by now only having the`RealmConfiguration.Builder(context)` constructor. Custom file locations are now supported through the `directory()` builder method. This also made it possible to simply `assetFile(Context, location)` to only `assetFile(location)`.

Having the Context means that we are now able to access system services and other framework classes without exposing any Android functionality in any potential interface (which will be needed to support Realm on the JVM).   ,  "
,,Realm,"Sync facade to make spliting lib possible (#116)

* Sync facade to make spliting lib possible

* Add class SyncObjectServerFacade which will only exist in the sync
  lib.
* Check if SyncObjectServerFacade exists and create an singleton
  instance.
* Empty implementations for base ObjectServerFacade.
* Add RealmConfiguration.isSyncConfiguration to make the checks faster.
* Other cleanups.

Close #112   ,  "
API Management,API Management,Realm,"Integrate Object Store [PART2] - SharedRealm (#3031)

This simplified our code base a lot. Basically all APIs we need from
SharedGroup/Group are wrapped in the SharedRealm. So we can just remove
those classes.

But we do need a few APIs from SharedGroup which is not supplied by
SharedRealm because of async queries. Currently we expose those in a
friend class of ShareRealm, see realm/realm-object-store#141

We are still managing Realm caches in Java although there are mechanism
in OS to do the same thing. The major reason is we have method like
deleteRealm needs information from cache to check if all Realm instances
are closed. The ShareRealm is actually a std::shared_ptr. We hold
the pointer to the std::shared_ptr in Java.
Another fundamental change is that we used to have separated
SharedGroups for DynamicRealm and typed Realm in the same thread. But
now they are using different SharedRealm but actually the different
SharedRealms are point to the same SharedGroup.

And some other code cleanup.   ,"
,,Realm,"  fix merge mistakes   ,"
,,Realm,"  Supporting both additive and manual schema modes (#91)

* Adding very thin wrappers for Object Store's ObjectSchema and Property.
* Adding method for building object schema is proxy classes
* Adding rudimentary support for Object Store schemas. Using ObjectStore::update_schema() to update schema for the additive mode.
* Disallowing destructive schema changes in additive mode.   ,"
,,Realm,"  Fix unit tests.   ,  "
API Management,API Management,Realm,"Integrate Object Store [PART2] - SharedRealm (#3031)

This simplified our code base a lot. Basically all APIs we need from
SharedGroup/Group are wrapped in the SharedRealm. So we can just remove
those classes.

But we do need a few APIs from SharedGroup which is not supplied by
SharedRealm because of async queries. Currently we expose those in a
friend class of ShareRealm, see realm/realm-object-store#141

We are still managing Realm caches in Java although there are mechanism
in OS to do the same thing. The major reason is we have method like
deleteRealm needs information from cache to check if all Realm instances
are closed. The ShareRealm is actually a std::shared_ptr. We hold
the pointer to the std::shared_ptr in Java.
Another fundamental change is that we used to have separated
SharedGroups for DynamicRealm and typed Realm in the same thread. But
now they are using different SharedRealm but actually the different
SharedRealms are point to the same SharedGroup.

And some other code cleanup.   ,  Invalidate schema cache when the schema version of Realm is changed by other process (#3409).

invalidate schema cache when the schema version of Realm is changed by other process.

Now schema cache referred by Realm instance is not shared and global schema cache is introduced instead.   ,"
,,Realm,"  Supporting both additive and manual schema modes (#91)

* Adding very thin wrappers for Object Store's ObjectSchema and Property.
* Adding method for building object schema is proxy classes
* Adding rudimentary support for Object Store schemas. Using ObjectStore::update_schema() to update schema for the additive mode.
* Disallowing destructive schema changes in additive mode.   ,  "
,,Realm,"Sync facade to make spliting lib possible (#116)

* Sync facade to make spliting lib possible

* Add class SyncObjectServerFacade which will only exist in the sync
  lib.
* Check if SyncObjectServerFacade exists and create an singleton
  instance.
* Empty implementations for base ObjectServerFacade.
* Add RealmConfiguration.isSyncConfiguration to make the checks faster.
* Other cleanups.

Close #112   ,  Move classes from objectserver to its super (#113)

  ,  RealmLog (#3368)

Moved RealmLog to the public API. Routes all log events through it, also from native code.   ,  "
,,Realm,"Moving logging to a stage where response has been parsed (#158)   ,  "
,Thread management,Realm,"Add thread check to methods in DynamicRealmObject (#4259)

* add a simple test case to expose #4258

* update Changelog

* fix issue4258

* fix test failure

* fix test process crash when RealmListTests#add_set_dynamicObjectFromOtherThread() fails   ,  "
,Thread management,Realm,"Add thread check to methods in RealmQuery. (#4257)

* Throw IllegalStateException instead of process crash when any of thread confined methods in RealmQuery is called from wrong thread (#4228).

* fix some bugs in test and remove 'methodParams'

* no need to add 'realm.checkIfValid();' to RealmQuery.isValid()

* PR fixes

* removed section header comments   ,  "
,,Realm,"Fixed element type checking in DynamicRealmOject#setList(). (#4254)

* Fixed element type checking in DynamicRealmOject#setList() (#4252).

* Update CHANGELOG.md

* PR fixes

* PR fixes   ,  "
,,Realm,"Add Supports for Primitive Lists (#5031)

* Extended Annotation Processor to support it in model classes
* Relaxed Generic Constraints
* Support in insert/insertOrUpdate
* Support in copyToRealm/copyToRealmOrUpdate
* Support in copyFromRealm   ,  "
,,Realm,"Support offline client reset (#5297)

* Exposing a `SyncConfiguration` that allows a user to open the backup Realm after the client reset (#4759).   ,  "
,,Realm,"Client reset fixes #4759 (#5159)

* Exposing a `SyncConfiguration` that allows a user to open the backup Realm after the client reset (#4759).   ,  "
,,Realm,"Re-enable breakingSchemaChange_throws (#5317)   ,  "
,,Realm,"Always use object store to create PK table (#5284)

- Add OsObjectStore class to wrap methods in ObjectStore.hpp.
- Use ObjectStore to create meta tables.
- Use ObjectStore to get/set primary key.
- Always create meta tables when open a non-exist, non-readonly Dynamic
  Realm.
- Clean code.   ,  "
,,Realm,"Add support for new error codes (#5308)   ,  "
,,Realm,"Support Partial Sync (#5359)

* Add preview support for partial sync   ,  "
Feature migration,Feature migration,Realm,"Upgrade to Sync-RC21 and ROS 2.0.0-alpha.34  (#5277)   ,  "
,,Realm,"Use NPM to install integration test server (#5249)   ,  "
,,Realm,"Add support for alpha.38 changes (#5313)   ,  "
,,Realm,Message_induce
,,Realm,Updated tutorial/
,,Realm,Added TableBase::nativeToJson() - currently disabled in JNI/
,,Realm,Updated tightdb version. Updated jni - now warning free./
,,Realm,Major overhaul of generic.mk/config.mk and build.sh which shall serve as a uniform front-end for building each part/
,,Realm,Still working on script to build a complete distribution package including all language bindings/
,,Realm,Updated to new Tightdb version (9) which has new file format incompatible with previous versions!!!/
,,Realm,Introduced separate model for the tests - completed (issue #54)./
,,Realm,"Removed "".generated"" suffix in generated sources packages (issue #46)./"
,,Realm,Introduced separate model for the tests (issue #54)./
,,Realm,Refactored (joined) base for table and view tests./
,,Realm,Enable some tests. released dll for B13/
,,Realm,Wrapped new JNI-level methods with high-level methods (issue #53)./
,,Realm,"Added wrapper and test for the ""toJson"" method (issue #53)./Introduced separate model for the tests - completed (issue #54)./"
,,Realm,"Added tests and examples for the ""average"" method (issue #53)./"
,,Realm,Refactored (joined) base for table and view tests./Resolved a compilation error./
,,Realm,Various changes in relation to building on Linux/
,,Realm,Added tests for group introspection operations./
,,Realm,Disabled a few unit tests that were failing for known reasons/
,,Realm,"Added TableBase::nativeToJson() - currently disabled in JNI/Removed 'limit' parameter in aggregate methods. Added TableBase::average().

Added index checks in TableQuery().
Compiles to relative h-files/Enabled some unit tests - they work now./"
,,Realm,Wrapped the query-based statistics with high-level methods (#53)./
,,Realm,Covered the case-sensitive query methods./
,,Realm,Added wrapper and tests for mixed type retrieval method (issue #53)./
,,Realm,"Improved column operations tests to exercise the view columns, too./"
,,Realm,Implemented configurable names of the generated classes (issue #50)./
,,Realm,"Improved (and fixed) the ""field sorting"" mechanism./"
,,Realm,Improved field sorter to support multiple source folders (issue #55)./
,,Realm,Switched from errors to warnings for non-critical problems(issue #49)./
,,Realm,Fixed code generation from empty model class (issue #52)./
,,Realm,"Simplified code generation of table's ""add"" method (issue #37)./"
,,Realm,Completed fix of issue #43./
,,Realm,Better workaround for relative URIs on OS X/
,,Realm,Implemented auto-detection of Eclipse APT environment./
,,Realm,"Fixed the ""null folder"" bug in the code generator./"
,,Realm,Various OS X related generator issues fixed/
,,Realm,"Moved the Table annotation into ""com.tightdb"" package (issue #47)./"
,,Realm,Fixed package name bugs in code generation (issue #43 and issue #44)./
,,Realm,Added check for a valid Java sources path configuration./
,,Realm,small update in error-text for Windows/
,,Realm,Updated error-msg when we can't find the jni lib/
,,Realm,Merge branch 'integration' of github.com:nikuco/
,,Realm,"tightdb_java2 into integration

Conflicts:
	Better Error message when lib can't be loaded./"
,,Realm,Fixed library path configuration (issue #61)./
,,Realm,Separated docs for each classes and adjusted method references./
,,Realm,Added page-per-method reference docs./
,,Realm,"Updated, improved and restructured reference docs and generator./"
,,Realm,"Added TableBase::distinct()

Added TableBase::average()
Added checks for columnType in setIndex()/Prepared and added new methods to table/view interface (issue #53)./"
,,Realm,Order of items in enum ColumnType updated to mach recent changes in the core library/
,,Realm,Fix for: Order of items in enum ColumnType updated to mach recent changes in the core library
,,Realm,Tracking latest error handling improvements in core library/WIP on ReadTransactions/
,,Realm,Fixed create of Group(byte[]). Enabled all GroupTest tests and added new test./
,,Realm,Removed table as parameter to Query methods and TableView. Added TableBase.where() method./
,,Realm,Tracking latest error handling improvements in core library
,,Realm,"Changes due to improved error handling in core library. Group no longer has an is_valid() method. From now on, Group instances are always valid."
,,Realm,Fixes due to changed Group and SharedGroup constructors in core library
,,Realm,Fixed create of Group(byte[]). Enabled all GroupTest tests and added new test.
,,Realm,"Updated testcode to use new add() and insert(). Mixed values can now be literals./"""
,,Realm,"removed some warnings in jni files. Rearranged experiment/example Java files/
,,Realm,Updated to support internal on linux/
,,Realm,Adjusted examples to the inline subtable construction (issue #70)./
,,Realm,Supported float and double types in code generator and typed API./"""
,,Realm,"updated examples to new ""typed"" package from ""lib"". Still a few examples not fully working./"
,,Realm,"String conversion from C++ to Java done, opposite order remains/"
,,Realm,"Correct transcoding from UTF-16 to UTF-8/"""
,,Realm,String conversion in both directions is complete - except for an important FIXME in to_jstring() in 'util.h'/
,,Realm,Tracking changes in core library: Renaming of column type enumeration/
,,Realm,Added experimental table method: nativeFindSortedInt (currently disabled). Add try/catch around WriteToMem() (many more needs this)./
,,Realm,Group::BufferSpec eliminated. Using BinaryData instead./
,,Realm,Fix due to core lib change: Group::get_table_count() -> Group::size()/
,,Realm,Added experimental method: table.moveLastOver() method/
,,Realm,Added experimental method to Table: findSortedLong(). NOTICE it does not return if the value was actually found or not - you have to check that through a get() afterwards for now./
,,Realm,Fixes for: Merge branch 'master' into explicit_string_size
,,Realm,Added experimental method: table.moveLastOver() method
,,Realm,"Added Table::addColumn(), renameColumn(), removeColumn()/"
,,Realm,"Just another FIXME/WIP: Updated with float, double support. Still a crash./"
,,Realm,"WIP: Updated with float, double support. Still a crash./"
,,Realm,Adjusted docs and examples to the refactoring (issue #69 and issue #72)./
,,Realm,minor test added/
,,Realm,REnamed Group::getTableCount() to size()/
,,Realm,"Merge remote-tracking branch 'nikuco/master'



Set on typed tables does not yet fully support subtables (can only set to null)/"
,,Realm,Added testcases for Mixed float and double/
,,Realm,Supported float and double types in code generator and typed API./
,,Realm,refdoc: added dynamic view and query and example for Table
,,Realm,"Added experimental method: table.moveLastOver() method/float: updated floats sum, average to return double instead of float./"
,,Realm,"refdoc: added dynamic view and query and example for Table/Added experimental method to Table: findSortedLong(). NOTICE it does not return if the value was actually found or not - you have to check that through a get() afterwards for now./added a few classes in Java ref-doc/Added experimental method: table.moveLastOver() method/Added Table::addColumn(), renameColumn(), removeColumn()/float: updated floats sum, average to return double instead of float./WIP: Updated with float, double support. Still a crash./"
,,Realm,"float: updated floats sum, average to return double instead of float./"
,,Realm,minor test added/Added set() method to the generated Cursor classes./
,,Realm,"Added experimental method to Table: findSortedLong(). NOTICE it does not return if the value was actually found or not - you have to check that through a get() afterwards for now./"""
,,Realm,"Added Table::addColumn(), renameColumn(), removeColumn()/Added testcases for Mixed float and double/WIP: Updated with float, double support. Still a crash./"
,,Realm,Use java.io.IOException./
,,Realm,Group::mode_Normal has been renamed to Group::mode_ReadWrite in the core library/
,,Realm,"Experimental Table.findSortedLong() replaced by Table.lowerBoundLong() and Table.upperBoundLong() because these provide more flexibility, and it follows similar changes in the core library/"
,,Realm,Added count*() and lookup() methods - currently without tests!/
,,Realm,Update Java Tutorial./
,,Realm,"Now throws exception when calling a method on a closed Group()

Updated tutorial and showcase a bit./"
,,Realm,Added test-case for detection of overwrite when calling Group.writeToFile()/
,,Realm,All uses of '.tdb' replaced by '.tightdb' (according to previously agreed convention)/
,,Realm,Bugfix: mem-leak removed/
,,Realm,"added count(), lookup() metods. Added simple performance test/"
,,Realm,Fixes for: A 'config' step has been introduced into the build procedure./
,,Realm,"A 'config' step has been introduced into the build procedure.
The main reason is that it allows reliable uninstallation, but there are several other benefits too.
Also, support for running in debug mode has been improved./"
,,Realm,Added Javadoc to the typed API classes./
,,Realm,Fixed automatic transaction rollback on close./
,,Realm,Added hasChanged() to SharedGroup/
,,Realm,Added exception handling for Group (untested)/
,,Realm,"support new Query.find() method/Table.close() now private (added private_debug_close() instead.

Added better support for detecting valid View and Query after close of table.  Still not completely tested, also missing core-support./Added last validity checks in Query./"
,,Realm,Added column type check/
,,Realm,Exception handling for Table (not tested)/
,,Realm,"Added equals, toString and toJson/"
,,Realm,Added test cases for all Exceptions that native interface can throw./
,,Realm,"debugging datebug - fixed one/Renamed to reflect c++ renames/Fixed merge bug./Added TableDefinition, to allow dynamic operations on subtable columns/"
,,Realm,"Table.close() now private (added private_debug_close() instead.

Added better support for detecting valid View and Query after close of table.  Still not completely tested, also missing core-support./"
,,Realm,"Added ref-doc of *to_string().

Added TableView.rowToString()/"
,,Realm,"+ Added Table.toString(), Table.toString(maxRows)

+ Added TableView.toString(), TableView.toString(maxRows),
+ Added Mixed.getReadableValue()
+ Remove TightDB.print methods
! Lacking update of ref-doc/Added column type check/"
,,Realm,"Updates due to changes in core library: Table::is_valid() -> Table::is_attached()/Added parameter checks to all Table methods. (lacking testcases!)

Changed Exception for invalid Table form IllegalArgumentException to InvalidStateException./"
,,Realm,Renamed to reflect c++ renames/Added column information methods to views/
,,Realm,Added exception handling to TableView (no tests)/
,,Realm,"+ Added Table.toString(), Table.toString(maxRows)

+ Added TableView.toString(), TableView.toString(maxRows),
+ Added Mixed.getReadableValue()
+ Remove TightDB.print methods
! Lacking update of ref-doc/"
,,Realm,Added column type check/Added parameter checks in tableview./
,,Realm,native getSortedView/renamed all functions to match changes in core c++/
,,Realm,"Added exception handling for Group (untested)/Table.close() now private (added private_debug_close() instead.

Added better support for detecting valid View and Query after close of table.  Still not completely tested, also missing core-support./"
,,Realm,Added column type check/Updates due to changes in core library: Table::is_valid() -> Table::is_attached()/
,,Realm,Added parameter checks in tableview./
,,Realm,"Added parameter checks to all Table methods. (lacking testcases!)

Changed Exception for invalid Table form IllegalArgumentException to InvalidStateException./"
,,Realm,resolved clonflict when merging from brians code/
,,Realm,"Updated test of TableView and Query close() test.

+ other minor updates/Table.close() now private (added private_debug_close() instead.

Added better support for detecting valid View and Query after close of table.  Still not completely tested, also missing core-support./"
,,Realm,hack for renaming/
,,Realm,"Merge branch 'develop' of github.com:Tightdb/tightdb_java2 into Breaking-updates

	tightdb_jni/src/com_tightdb_table.cpp/Moved setIndex() and hasIndex() to the Typed StringColumn, so it only appears for strings. Updated tests./"
,,Realm,method renamed to adjust() + ref doc dyn examples/
,,Realm,Space in public comments added + Renaming of addLong to incrementInColumn (including test cases)/
,,Realm,Indention corrected and test case moved/Table.insert() renamed to addAt(). Test added/
,,Realm,Test method moved to existing test class/
,,Realm,"removal of lock file in test cases/"""
,,Realm,Added test of double rollback() - it works./
,,Realm,Added (disabled) test for double commit() and double rollback()/
,,Realm,"Space in public comments added + Renaming of addLong to incrementInColumn (including test cases)/
,,Realm,test case added/"""
,,Realm,test case column name in subtables/
,,Realm,Fixed type errors/
,,Realm,"Added TableDefinition, to allow dynamic operations on subtable columns/"
,,Realm,typed table equals test case added/more test cases/
,,Realm,test cases/
,,Realm,Added testcase for limit argument/
,,Realm,Changed syntax for Typed Rows. now only traditional getters & setters are allowed/
,,Realm,"Query on views/"""
,,Realm,support new Query.find() method/
,,Realm,Test for thrown exception in queri on wrong column types/
,,Realm,"More comprehensive tests for Aggregates on a Query/
,,Realm,Added column type check/Renamed class util to Util./
,,Realm,renamed exception/"""
,,Realm,add exception. Throw it when wrong type is acessed from Mixed type/
,,Realm,ColumnType.Long renamed to Integer/
,,Realm,"BREAKING CHANGE: Column type enum have been renamed. Is now less verbose and more java like

e.g. ColumnType.ColumnTypeString is now ColumnType.STRING etc/"
,,Realm,"Fixed bug in Group.equals(). Added Table.equals and tests./"""
,,Realm,Test cases added and group.equals modified/
,,Realm,"Updated test of TableView and Query close() test."""
,,Realm,Added finalize to TableQuery/
,,Realm,Outcommented lookup in table and tableview/
,,Realm,Query could leak./
,,Realm,"Updated test of TableView and Query close() test.

+ other minor updates/Renamed class util to Util./"
,,Realm,"Lookup reenabled on typed table. Test case added, Check for null otherwise core crash/debugging datebug - fixed one/merged develop into breaking/"
,,Realm,Renamed to reflect c++ renames/
,,Realm,Renamed class util to Util./
,Thread management,Realm,"Renamed class util to Util./Fixed initialisation to be threadsafe.

Updated load of library file for Windows (must look into this again later - created Asana task)./"
,,Realm,Added findFrom() method to typed query/
,,Realm,Added addEmptyRow() method to typed table/
,,Realm,renamed exception/add exception. Throw it when wrong type is acessed from Mixed type/
,,Realm,Simplified query implementation./
,,Realm,simple pivot on TableView/getSourceRow method added + test cases/
,,Realm,Incremented API version/
,,Realm,"fixed bug in lookup(), which didn't return -1 on 32 bit when it should/"
,,Realm,reflect static methods in jni
,,Realm,added no exception note/added jni bridge to getColumnIndex/simple pivot on TableView/
,,Realm,added jni bridge to getColumnIndex/multiple pivot operations added/initial trial of pivot/
,,Realm,updated check for less than 0 row index macro/
,,Realm,Added internal performance test of get-method/
,,Realm,added jni bridge to getColumnIndex
,,Realm,throw exception when setting null on string + test cases
,,Realm,enabled test for table.lookup. Throws more specific exceptions. Non-breaking as they are subclasses of RuntimeException
,,Realm,changes.txt + better test coverage
,,Realm,narrower lines/subtable sort test case added + try fail added more places/
,,Realm,subtable sort test case added + try fail added more places/
,,Realm,test case added/ simplify data access
,,Realm,Added testcases for unbalanced subtable()/end_subtable()/
,,Realm,added check for if query criteria is null for String and Date + test cases/
,,Realm,"added test cases, currently it will crash the core/"
,,Realm,"implementation of context in Table, View, Group, SharedGroup, ReadTrans and Query/"
,,Realm,moved check to java and changed to illegalArgument exception/
,,Realm,"added parent object to view and query. Now sets ptr to 0 in synchronized block/
,,Realm,close made public and added to TableOrView interface. Initial test cases added for GC case/"
,,Realm,added jni bridge to getColumnIndex/
,,Realm,"removed CloseMutex class
Commented out/added jni bridge to getColumnIndex/pivot native test cases added/"
,,Realm,"throw exception when setting null on string + test cases/missing "".""/"
,,Realm,"check for if group is closed when closing transactions + test case

Will crash core if not detected/"
,,Realm,Support new tableview is_valid() validate. Added testcases in Java./
,,Realm,Updated tutorial to use the dynamic interface and added asserts/Update to java tutorial/
,,Realm,Added maximumDate and minimumDate/
,,Realm,Renamed some aggregating methods in Table and TableView (averageInt() to averageLong() etc. )/
,,Realm,Cleanup test cases/
,,Realm,"Modified code generation to use static column indices. Since the Java VM keeps the fields in a different order than the java compiler, I had to introduce proxy based table generation. removed setter"
,,Realm,getter method for row no RealmObject. row is now accessed directly. Proxy class suffix has been changed to '_PROXY* has been changed to RealmProxy
,,Realm,performance test runs without fail
,,Realm,"New code generation is ""working"". Stills needs to fix the column indices./"
,,Realm,"Modified code generation to use static column indices. Since the Java VM keeps the fields in a different order than the java compiler, I had to introduce proxy based table generation. removed setter/getter method for row no RealmObject. row is now accessed directly. Proxy class suffix has been changed to '_PROXY* has been changed to RealmProxy./Cleanup and extra test conditions in performanceTest. Maintain realmRowIndex./performance test runs without fail./"
Restructuring the code,Restructuring the code,Realm,Refactored json methods to support standalone objects. Added stub methods + begun work on unit tests./
,,Realm,Added copyToRealmOrUpdate method./
,,Realm,RealmObject.toString() method now uses isValid()/
,,Realm,Add test for empty model classes/
,,Realm,Better handling of empty model classes in the code generator/
,,Realm,copyToRealm method implemented + unit tests. Style fixes to unit tests./
,,Realm,Import only minimum number of packages./
,,Realm,copyToRealm method added to proxy classes./
,,Realm,Using getGenericType() is not always a good idea./
,,Realm,"Import statements must be in alphabetic order. Using getGenericType to
simplify code./"
,,Realm,"Only import required classes in the proxy classes in order to avoid


Setting relationship to null throws NullPointerException, because setter method does not exit where it should, trying to access null value instead./"
,,Realm,"PR comment fixes: Cleanup example, method comments has been made better. Regexp for checking date syntax./"
,,Realm,Fixed annotation processor unit test./
,,Realm,Fixed bug in RealmObject.equals()/
,,Realm,Remove rowIndex from RealmObject.toString()/
,,Realm,"Mitigate the file size growing problem

This change does the following:
 * disables the caching of Realm instances in threads without an event loop
 * makes the Realm class implement Closable
 * does reference counting for closing Realm instances
 * checks if the Realm instance has not been closed before any operation/More consistent toString output./toString, equals and hashcode now work properly with cyclic model structures. Refactored proxy class generator to be more readable./Cleanup + fixed merge mistake./Merge branch 'master' into cm-primary-keys


	settings.gradle/"
,,Realm,Preliminary support for primary keys in the binding + Unit tests./Adding thread check on RealmObject. Refactoring unit tests. Updating changelog./
,,Realm,"Merge branch 'master' into cm-primary-keys


	Allow custom constructors but require no arg public constructor as well./Added unit tests and fixes bug found on the way/Preliminary support for primary keys in the binding + Unit tests./"
,,Realm,Renamed ConcurrencyExample to ServiceExample. Added threadExample to distribution examples./
,,Realm,Make ViewHolder static./Fixed wrong usage of ViewHolder pattern. Fixed bad practise when inflating views./
,,Realm,Printing location of error/Move C++ -> Java exception mapping to a function/More debug info when converting a string from to Java fails./Revert the support for lenient UTF conversion/Fix for: Lenient UTF-8 -> UTF-16 transcoding (insert replacement characters)/Lenient UTF-8 -> UTF-16 transcoding (insert replacement characters)/Catching standard exceptions./Adding better error messages when converting to Java string/Use to_jstring() everywhere at the JNI layer./
,,Realm,Moving checks to C++./
,,Realm,Optimizing Realm.allObjectSorted() and refactoring for code reuse./
,,Realm,Using variadic macros instead./
,,Realm,Using to_jstring for better conversion between core and java strings./
,,Realm,"Revert ""Revert ""Merge   #645 from realm/kg-core-0.87.0""""

This reverts commit 6be8edca486f53273f28553584d28bc17c5a0ddf./@bmunkhold suggested a refactoring of the old tracing/logging system./"
,,Realm,"Changing signatures of Realm.allObjects(), RealmQuery.findAll(), and RealmResults.sort()./"
,,Realm,"Realm.allObjects(), RealmResults.sort() and RealmQuery.findAll() extended with multi-field sorting./Move sync_if_needed() to macro/"
,,Realm,Iterating a RealmResult now correctly identifies any changes to the underlying query result./
,,Realm,Use to_jstring() everywhere at the JNI layer./
,,Realm,Moved error checking to JNI./Adding where() to RealmList/Adding where() to RealmList/
,,Realm,Eliminate some inlined std::string constructions/Stylistic updates/Changing to to_jstring()/
,,Realm,"Added missing header file update./Update encryption functionality for the latest core changes/Revert ""Revert ""Merge   #645 from realm/kg-core-0.87.0""""

This reverts commit 6be8edca486f53273f28553584d28bc17c5a0ddf./"
,,Realm,"Realm.allObjects(), RealmResults.sort() and RealmQuery.findAll() extended with multi-field sorting./"
,,Realm,Added test for sorting by child properties./Modify the unit tests to comply to Realm.close()/
,,Realm,Sort with characters/maxDate was not correct./
,,Realm,Modify the unit tests to comply to Realm.close()/
,,Realm,Merge branch 'master' of github.com:realm/
,,Realm,"realm-java into kg-date-issues

Conflicts:
	changelog.txt
	Check for valid date/Added unit tests for detecting cyclic issues in toString(), hashcode(), equals(). Added design document with proposed solution./"
,,Realm,"Extending and adding unit tests of naming convention supported by the
annotation processor./updated/"
,,Realm,"Primary keys are now indexed. Refactored Table/"""
,,Realm,"Row for missing cases + cleaner code. Additional unit tests./
,,Realm,bug fixed and test added/Support for case sensitive searches in equalTo and notEqualTo/
,,Realm,Adding copyOrUpdate(iterable) + unit test./"""
,,Realm,minor fix to error message/
,,Realm,Fail when instantiating a Realm with the wrong key/
,,Realm,Throw an exception if deleting an open Realm file/
,,Realm,Error handling now uses exception thrown from core where it makes sense./
,,Realm,Added check for read only files./Better error reporting for error cases when creating a Realm using the File constructors./
,,Realm,Added support for executing transactions in a closure./
,,Realm,"Removed some code/Fixed bug in equalTo() for date.
Updated testcases/"
,,Realm,Rename compacting related methods/
,,Realm,Change writeCopy signature and improve tests/
,,Realm,Using the transaction (Group) to write a copy/
,,Realm,Added unit test for testing all unicode chars./
,,Realm,Fix RealmTest for Realm.close()/
,,Realm,Add a unit test for reference counting/using RealmQuery boolean/
,,Realm,broke long lines/test contains in query/Catching null pointer in queries with wrong field/
,,Realm,Updated and fixed test/Made separate model for non latin field names/
,,Realm,Guarding JStringAccessor in a try block./
,,Realm,Removed some text in asserts/
,,Realm,Non latin column name and tests/
,,Realm,updated/
,,Realm,Fixed Realm cache not working./
,,Realm,Updates due to PR feedback/
,,Realm,Fix merge mistake + style fixes./Fixed broken unit tests in RealmAnnotationsTest./
,,Realm,It is no longer possible to set the primary key to a column that already contains duplicate values./
,,Realm,"It is no longer possible to manually set 0 or """" in primary key fields. Refactored error checking so it is more maintainable./Primary keys are now indexed. Refactored Table/"
,,Realm,Row for missing cases + cleaner code. Additional unit tests./Preliminary support for primary keys in the binding + Unit tests./
,,Realm,"A cleaner solution/"""
,,Realm,"Thread handler no longer gets removed too soon./Fixed bug causing refresh on closed Realms on background threads with loopers./Modify the unit tests to comply to Realm.close()/
,,Realm,Refactored json methods to support standalone objects. Added stub methods + begun work on unit tests./"""
,,Realm,Added unit tests for RealmList. Throw better errors if not used correctly./
,,Realm,Added managed/non-managed mode to RealmList + a no arg constructor./Added checks to RealmList.move() + added unit tests./More consistent toString output./
,,Realm,"toString, equals and hashcode now work properly with cyclic model structures. Refactored proxy class generator to be more readable./Adding where() to RealmList/"
,,Realm,"Throw NoSuchMethodError when RealmResults.indexOf() is called as the
method is not implemented./Improved error message for child object sorting./"
,,Realm,Changing sort to be in-place./Added support for remove in RealmResults iterators. Additional unit tests for their usage./
,,Realm,Proper iterators implemented for RealmResults./Root cause unit test added./
,,Realm,Adding thread check on RealmObject. Refactoring unit tests. Updating changelog./
,,Realm,Preliminary support for primary keys in the binding + Unit tests./
,,Realm,"Setting same value to a primary field no longer violates the primary key constraint./"""
,,Realm,Primary keys are now indexed. Refactored Table/Row for missing cases + cleaner code. Additional unit tests./
,,Realm,"Preliminary support for primary keys in the binding + Unit tests./
,,Realm,Only check for duplicate values when switching primary key fields./"""
,,Realm,Style fixes./
,,Realm,Setting same value to a primary field no longer violates the primary key constraint./
,,Realm,"Merge branch 'master' into cm-primary-keys
It is no longer possible to manually set 0 or """" in primary key fields. Refactored error checking so it is more maintainable./"
,,Realm,Primary keys are now indexed. Refactored Table/
,,Realm,Row for missing cases + cleaner code. Additional unit tests./
,,Realm,Better error reporting for error cases when creating a Realm using the File constructors./
,,Realm,"Revert ""Revert ""Merge   #645 from realm/kg-core-0.87.0""""

This reverts commit 6be8edca486f53273f28553584d28bc17c5a0ddf./"
,,Realm,"Mitigate the file size growing problem

This change does the following:
 * disables the caching of Realm instances in threads without an event loop
 * makes the Realm class implement Closable
 * does reference counting for closing Realm instances
 * checks if the Realm instance has not been closed before any operation/"
,,Realm,"Remove deprecated sorting methods/Rename the sorting find methods and deprecate the old ones/"""
,,Realm,"Rename internal parameter to a better name./
,,Realm,Fix subquery scope not working properly./"""
,,Realm,Fixed bug when querying a RealmList. Added unit test. Improved error message for wrong query types./
,,Realm,Adding sort functionality to findAll()./Update due to PR feedback/
,,Realm,"Catching null pointer in queries with wrong field/Adding where() to RealmList/
Restructuring the code,Restructuring the code,Realm,Refactored json methods to support standalone objects. Added stub methods + begun work on unit tests./"""
,,Realm,Remove deprecated sorting methods/Added tests for reading iOS realms. Added option for custom schemas./
,,Realm,Added check for read only files./
,,Realm,Call to close() replaced by warning in Realm finalizer./
,,Realm,Optimized getHandler() method./
,,Realm,Thread handler no longer gets removed too soon./
,,Realm,Using id to compare Realms will also support in-memory Realms./
,,Realm,Fixed reference counting bug./
,,Realm,Handlers are now properly removed when a Realm is closed./
,,Realm,"Allow to add/remove RealmChangeListeners in RealmChangeListeners

We create a defensive copy of the list before iterating it/Moved methods/Make the reference counter depend on the realm path/"
,,Realm,"Only commit if needed during instantiation and re-enable caching/Mitigate the file size growing problem

This change does the following:
 * disables the caching of Realm instances in threads without an event loop
 * makes the Realm class implement Closable
 * does reference counting for closing Realm instances
 * checks if the Realm instance has not been closed before any operation/"
,,Realm,Fixed build warnings + Realm cache lookup./
,,Realm,Adding distinct() to Realm/
,,Realm,Adds RealmPath to RealmMigrationNeedException/
,,Realm,Reverted getCanonicalPath to getPath/Fixed annotation processor unit tests./
,,Realm,Fixed annotation processor unit test./copyToRealm now correctly handles nested objects already part of the Realm./
,,Realm,Proxy classes now uses constants for column indexes for getters/setters./
,,Realm,Column indices moved to proxy classes to minimise number of lookups./
,,Realm,Updated annotation processor unit tests./
,,Realm,Encapsulates columnIndicies in it's own class. The internal map now uses Class as key instead of Strings/
,,Realm,Fix annotation-processor unit tests./
,,Realm,Reverted getCanonicalPath to getPath/
,,Realm,Using copyToRealmOrUpdate with a Null primary key now throws a proper exception instead of crashing./
,,Realm,Fixing expected output/
,,Realm,Renaming hasIndex() to hasSearchIndex() and setIndex() to addSearchIndex()./
,,Realm,Fixed annotation processor unit tests./
,,Realm,copyToRealm now correctly handles nested objects already part of the Realm./
,,Realm,validateTable now also verifies that primary key and index is set correctly./
,,Realm,Added RealmObject and RealmList to annotation processor all types unit test./
,,Realm,Fixed issues with Null when using JSONObject./
,,Realm,Adds RealmPath to RealmMigrationNeedException/Reverted getCanonicalPath to getPath/
,,Realm,Fix wrong String comparison/
,,Realm,"Merge branch 'master' into cm-realm-modules
 Fixed generics./"
,,Realm,Cyclic data structures no longer crash copyToRealm/
,,Realm,Don’t use String.join() since it’s Java8 only/
,,Realm,Now properly propagates update or copy flag + fixed unit tests./
,Thread management,Realm,Made Thread example more resilient to monkey test events./
,,Realm,Realm Core has change namespace from tightdb to realm./Fixed unit test./
,,Realm,Primary key table migration added./
,,Realm,Adding Table.unsetIndex() to remove search index./
,,Realm,Annotation processor now fails if a RealmObject contains no fields./
,,Realm,Changing formatting to support both 32 and 64 bit systems when logging./
,,Realm,Moved setPrimaryKey to JNI./
,,Realm,"Release unique_ptrs and let Java maintain ownership of native pointers./"""
,,Realm,Realm Core has change namespace from tightdb to realm./
,,Realm,Only checks parameters in C++. Renaming C++ functions to match Java methods' names/
,,Realm,Moving parameter checking to C++. Adding tracing to LinkView and Row C++ methods./
,,Realm,Fixed unit test./
,,Realm,Adding RealmQuery.isNull() and RealmQuery.isNotNull()./
,,Realm,Added compact() method from core also for encrypted Realms./
,,Realm,Jason streams now only update fields actually present./
,,Realm,Added unit tests./
,,Realm,Disable breaking unit test./
,,Realm,Potential tmp fix for unit test./
,,Realm,Realms are now only cached if correctly opened./
,,Realm,Add a couple of unit tests/
,,Realm,Properly check for key equality while using a cached Realm instance/
,,Realm,Removed warnings./
,,Realm,OnRealmChanged is now called on thread doing the commit./
,,Realm,Queries on links can be case sensitive./
,,Realm,Fixed bug where copyToRealm() crashed when copying objects with primary key data./Moved setPrimaryKey to JNI./
,,Realm,Remove columnIndices getter./
,,Realm,"Merge branch 'master' into cm-bug-migration

"
,,Realm,"Merge branch 'master' into cm-realm-modules


Added test cases for RealmConfiguration + minor refactoring./"
,,Realm,Streamline new constructors and deprecate old constructors + other utility methods that now uses RealmConfiguration./
,,Realm,Added support for RealmModules + Reintroduced RealmProxyMediator/
,,Realm,"Handle null value for String in Json when updating
Fix #1344
Update the object's String field to empty string when the corresponding
field in Json is null./"
,,Realm,"Improve unit test for class with boolean fields in the AP/"""
,,Realm,"Tighter check on table validation involving RealmList<> fields./
,,Realm,Handle null value for String in Json when updating"""
,,Realm,"add support for 'findFirst' & 'findAllSorted*', update UnitTests/"
,,Realm,Readdded AllTypesRealmProxy unit test./
,,Realm,realm-java into kg-bug-migrate-linkview/
,,Realm,"Handle null value for String in Json when updating

Fix #1344
Update the object's String field to empty string when the corresponding
field in Json is null./"
,,Realm,"Support search indexing for column int, bool, date

1. Enable the search index annotation on byte, short, int, long,
   boolean, and Date.
2. Enable add/remove search index in java.
3. Annotation processor test to support better detailed test cases.
4. Modify JNI test cases.
5. Update doc.
6. Add AnnotationIndexTypes to avoid polluting other test cases.

This is the first PR for #1039. Implicit index to int primary keys will
be handled in another PR./add support for 'findFirst' & 'findAllSorted*', update UnitTests/"
,,Realm,Add support for Findbugs and fix the issues found/
,,Realm,Better error messages/
,,Realm,Add analytics on annotation/Add check on fields of type RealmList/
,,Realm,"Add analytics on annotation/Index fields with @PrimaryKey annotation

And update test cases to adapt this change./"
,,Realm,"Support search indexing for column int, bool, date

1. Enable the search index annotation on byte, short, int, long,
   boolean, and Date.
2. Enable add/remove search index in java.
3. Annotation processor test to support better detailed test cases.
4. Modify JNI test cases.
5. Update doc.
6. Add AnnotationIndexTypes to avoid polluting other test cases.

This is the first PR for #1039. Implicit index to int primary keys will
be handled in another PR./"
,,Realm,Simple dynamic API added./
,,Realm,Add check on fields of type RealmList/
,,Realm,"Populate after generating the metadata/"""
,,Realm,Avoid wildcard imports and execute in the background/
,,Realm,Add analytics on annotation/
,,Realm,"Support search indexing for column int
1. Enable the search index annotation on byte, short, int, long
   boolean, and Date.
2. Enable add/remove search index in java.
3. Annotation processor test to support better detailed test cases.
4. Modify JNI test cases.
5. Update doc.
6. Add AnnotationIndexTypes to avoid polluting other test cases.

This is the first PR for #1039. Implicit index to int primary keys will
be handled in another PR./"""""""
,,Realm,Updated examples with RealmConfiguration/
,,Realm,Split row into checked and unchecked variant./
,,Realm,Support 'UnreachableVersionException' from Core/Realm will now throw a RealmError when Realm Core enters an unrecoverable error condition./
,,Realm,Realm will now throw a RealmError when Realm Core enters an unrecoverable error condition./
,,Realm,"Changing logic to handover the query from caller to background thread, handle different failure points (begin_read, import_handover), update UnitTests/"
,,Realm,add retry policy + concurrency tests + perf improvement to the query/
,,Realm,"add logic to release handover resources, update threading example with asyc transaction/"
,,Realm,using  std::unique_ptr for handover + remove unnecessary begin_read in a spearate JNI call + latest fixes from 'fsa_handover_demo' (untyped query etc.)/
,,Realm,Support 'UnreachableVersionException' from Core/
,,Realm,"POC async query, using Core 'fsa_handover_demo' branch/"
Memory Management,Memory Management,Realm,"Add support for in-memory Realm

1. Add durability to createNativeWithImplicitTransactions.
2. Add inMemory to RealmConfiguration.
3. Support passing durability to SharedGroup constructor.
4. Add new static method getInMemoryRealm to Realm class.
5. Add test cases./"
,,Realm,LinkView mutable methods now correctly require a write transaction + added unit tests for all mutable public methods./
,,Realm,"Added a check for null values in io.realm.internal.Table.findFirst{String,Date}()./"
,,Realm,"Index fields with @PrimaryKey annotation

And update test cases to adapt this change./"
Restructuring the code,Restructuring the code,Realm,Removed deprecated methods from Realm. Bumped dev version to 0.83.0/
,,Realm,"Clean up native row accessor from the same thread that created them/add support for 'findFirst' & 'findAllSorted*', update UnitTests/"
,,Realm,"Throw exception when close called on a diff thread.

Without checking the thread in close, some weird logic could be
triggered which is really hard to think about./"
,,Realm,refactor the test to detect a decrease in references (rather than trying to achieve 0 reference left)/
,,Realm,fix memory leak issue with the FinalizerRunnable/
,,Realm,Cleaned up wall of text./
,,Realm,"Ignore dup entries when addChangeListener

Since there should not have lots of listeners in on realm instance,
looping should be an acceptable way to do this./"
,,Realm,Merge remote-tracking branch 'origin/master' into nh-async-query/
,,Realm,Clean up native row accessor from the same thread that created them/
,,Realm,Fix a few lint warning reported by the Android Linter/
,,Realm,PR comment fixes./Split row into checked and unchecked variant./
,,Realm,update PR as per feedback/
,,Realm,"add support for 'findFirst' & 'findAllSorted*', update UnitTests/add retry policy + concurrency tests + perf improvement to the query/"
,,Realm,"RealmConfiguration now used as cache key/Do remove the RealmChangeListener weak reference

Weak references on a same object are not same./Merge branch 'master' into cm-configuration-builder


	Fix a memory leak in RealmBaseAdapter

Fixes #1109
We now use WeakReferences to store the change listeners.
This allows the Garbage Collector to clean up the adapters./"
Data conversion,Data conversion,Realm,"Suppress useless cast and raw type warnings in generated proxy classes.

This warnings are reported when `-Xlint:all` is added to the compilar args.

```/reorder parameters of ColumnInfo class/fix for #1611

Problem:
  The Proxy class of each model holds column indices in static fields.
  These indices are good only when all Realm databases have the same column index in every column of the model class.
  If Realm databases have different column indices, the getter/setter of Proxy class reads/writes wrong column.

Solution:
  This commit introduces `ColumnInfo` object that holds column indices information per Realm instance which shares the same database file./Add Nullable suport

* Add boxed type support.
* Add @Required annotation.
* isNull and isNotNull support all nullable types now.
* equalTo query can take null as input param for nullable fields.
* Add functions for nullable related migration.
* JSON converter behavior changes for null value.
* Update test cases./"
Data conversion,Data conversion,Realm,"Suppress useless cast and raw type warnings in generated proxy classes.

This warnings are reported when `-Xlint:all` is added to the compilar args.

Example configuration of `build.gradle` is:

```
allprojects {
    gradle.projectsEvaluated {
        tasks.withType(JavaCompile) {
            options.compilerArgs << '-Xlint:all'
            options.compilerArgs << '-Werror'
        }
    }
}
```/reorder parameters of ColumnInfo class/fix for #1611

Problem:
  The Proxy class of each model holds column indices in static fields.
  These indices are good only when all Realm databases have the same column index in every column of the model class.
  If Realm databases have different column indices, the getter/setter of Proxy class reads/"
,,Realm,"writes wrong column.

Solution:
  This commit introduces `ColumnInfo` object that holds column indices information per Realm instance which shares the same database file./"
,,Realm,Added missing thread confinement checks/
,,Realm,"Reference equality for empty RealmLists, fix Async tests/"
,,Realm,"fix javadoc, revert getByIndex to Realm.get, remove ArgumentsHolder fron public package/"
,,Realm,"Add Nullable suport

* Add boxed type support.
* Add @Required annotation.
* isNull and isNotNull support all nullable types now.
* equalTo query can take null as input param for nullable fields.
* Add functions for nullable related migration.
* JSON converter behavior changes for null value.
* Update test cases./update branch as per PR feedback/Milestone 2: findAllAsync & findFirst working with retries sratetgies/"
Data conversion,Data conversion,Realm,"Suppress useless cast and raw type warnings in generated proxy classes.

This warnings are reported when `-Xlint:all` is added to the compilar args.

Example configuration of `build.gradle` is:

```
allprojects {
    gradle.projectsEvaluated {
        tasks.withType(JavaCompile) {
            options.compilerArgs << '-Xlint:all'
            options.compilerArgs << '-Werror'
        }
    }
}
```/reorder parameters of ColumnInfo class/fix for #1611

Problem:
  The Proxy class of each model holds column indices in static fields.
  These indices are good only when all Realm databases have the same column index in every column of the model class.
  If Realm databases have different column indices, the getter/setter of Proxy class reads/writes wrong column.

Solution:
  This commit introduces `ColumnInfo` object that holds column indices information per Realm instance which shares the same database file./"
,,Realm,"Reference equality for empty RealmLists, fix Async tests/typo, fix style, & remove layout header for threadExample/"
,,Realm,"Add Nullable suport

* Add boxed type support.
* Add @Required annotation.
* isNull and isNotNull support all nullable types now.
* equalTo query can take null as input param for nullable fields.
* Add functions for nullable related migration.
* JSON converter behavior changes for null value.
* Update test cases./"
,,Realm,update branch as per PR feedback/
,,Realm,Milestone 2: findAllAsync & findFirst working with retries sratetgies/
,,Realm,fix tests/Milestone 4: before merging origin/master/
,,Realm,Primary keys now compatible with primary keys from the Object Store/
,,Realm,"Adding Realm.distinct() method. Closes #960/Add Nullable suport

* Add boxed type support.
* Add @Required annotation.
* isNull and isNotNull support all nullable types now.
* equalTo query can take null as input param for nullable fields.
* Add functions for nullable related migration.
* JSON converter behavior changes for null value.
* Update test cases./Milestone 2: findAllAsync & findFirst working with retries sratetgies/"
,,Realm,"Change to is_not_null for links

Since it is supported by core now./add distinctAsync/fix code style/Added RealmQuery.isEmpty()/optimising TableView construction/small code style fix/using TableRef/remove dynamic allocation for Handover TableView/now throwing Exception if we try to add/remove listener for unmanaged RealmObject/RealmResults/update as per round 2 feedback/Add Nullable suport

* Add boxed type support.
* Add @Required annotation.
* isNull and isNotNull support all nullable types now.
* equalTo query can take null as input param for nullable fields.
* Add functions for nullable related migration.
* JSON converter behavior changes for null value.
* Update test cases./update branch as per PR feedback/Milestone 4: before merging origin/master/Milestone 3: findAllSorted, findFirst & asyncTransaction working/Milestone 2: findAllAsync & findFirst working with retries sratetgies/"
,,Realm,Milesonte1/
,,Realm,"Added missing thread confinement checks/Add Realm.isInWriteTransaction() to indicate if it is in a write transaction.

See also:
https://realm.io/docs/objc/latest/api/Classes/RLMRealm.html#//api/name/inWriteTransaction

Closes #1549/Reference equality for empty RealmLists, fix Async tests/update branch as per PR feedback/"
,,Realm,add fix for #1466/
,,Realm,Schemas are now only validated once across threads./
,,Realm,"Add Nullable suport

* Add boxed type support.
* Add @Required annotation.
* isNull and isNotNull support all nullable types now.
* equalTo query can take null as input param for nullable fields.
* Add functions for nullable related migration.
* JSON converter behavior changes for null value.
* Update test cases./Simply unit test/Unit test to expose timeout in finalizer/"
,,Realm,"Handler is now package protected. Refactored unit test to test actual bug./"""
,,Realm,fix for #1467/
,,Realm,"Milestone 4: before merging origin/master/
,,Realm,Introduced RealmBase and SharedGroupManager/
,,Realm,now throwing Exception if we try to add/remove listener for unmanaged RealmObject/RealmResults/"
,,Realm,update as per round 2 feedback/Milestone 4: before merging origin/master/
,,Realm,"Milestone 2: findAllAsync & findFirst working with retries sratetgies/
,,Realm,Update core to v0.89.8"
,,Realm,fix typos/
,,Realm,Merge remote-tracking branch 'origin/master' into nh-async-query/update branch as per PR feedback/
,,Realm,"Adapt core changes in 0.92.0

 * Remove insert_xxx methods from Table.
 * Table.where() changed to user a Ref instead of a raw ptr.
 * makeWriteLogCollector to make_client_history
 * advance_read/promote_to_write/rollback_and_continue_as_read need a
   replication ptr as param now.
 * Remove add_int method.
 * A bug fix in linkview jni.
 * adjust is removed from TableOrView.
 * Other test cases fix.

 ** testMoveUp & testRemove don't pass right now./"
,,Realm,"write transaction -> transaction to avoid confusing users.

- write transaction -> transaction. Some write transactions with
  read transactinos remain.
- Realm.isInWriteTransaction -> Realm.isInTrasaction./Add Realm.isInWriteTransaction() to indicate if it is in a write transaction.

See also:
https://realm.io/docs/objc/latest/api/Classes/RLMRealm.html#//api/name/inWriteTransaction

Closes #1549/"
,,Realm,Merge remote-tracking branch 'origin/master' into nh-async-query/Milestone 4: before merging origin/master/Cleanup Group.java and SharedGroup.java/
,,Realm,"Add Nullable suport

* Add boxed type support.
* Add @Required annotation.
* isNull and isNotNull support all nullable types now.
* equalTo query can take null as input param for nullable fields.
* Add functions for nullable related migration.
* JSON converter behavior changes for null value.
* Update test cases./Merge branch 'master' into nh-async-query
before merging origin/master/Milestone 3: findAllSorted, findFirst & asyncTransaction working/"
,,Realm,Milestone 2: findAllAsync & findFirst working with retries sratetgies/Milesonte1/
,,Realm,"fix for #1611

Problem:
  The Proxy class of each model holds column indices in static fields.
  These indices are good only when all Realm databases have the same column index in every column of the model class.
  If Realm databases have different column indices, the getter/setter of Proxy class reads/writes wrong column.

Solution:
  This commit introduces `ColumnInfo` object that holds column indices information per Realm instance which shares the same database file./"
,,Realm,change the return type of RealmProxyMediator#getModelClasses() to Set in order to guarantee its contents unordered and unique./
,,Realm,Added fix for column indices/Introduced RealmBase and SharedGroupManager/
,,Realm,Milestone 4: before merging origin/master/
,,Realm,Removed ConcurrentSet/Fixed unit tests./
,,Realm,Use Set instead of Map/
,,Realm,"First working version.

TODO:
 * Make the kotlin example work with the plugin
   (and re-enable it in the settings.grade file)
 * Make the plugin make sure it’s applied after 
   the android plugin (app or library)
 * Modify the model classes in the unit tests of 
   the realm-library project
 * Write unit tests for the Gradle plugin/"
,,Realm,Fixed wrong key in son string/
,,Realm,New Migration and Dynamic API. New Sort/Case enums. New RealmCache/
,,Realm,"More fixes for the AP/Fix the AP unit tests/Fix the unit tests of the annotations processor.

Also remove one unit test which makes no sense now that users can
write model classes in any way they like./"
,,Realm,update unit tests for annotation-processor/
,,Realm,Added Realm.copyFromRealm()/
,,Realm,New Migration and Dynamic API. New Sort/
,,Realm,Case enums. New RealmCache/
,,Realm,More fixes for the AP/Fix the AP unit tests/
,,Realm,"Fix the unit tests of the annotations processor.

Also remove one unit test which makes no sense now that users can
write model classes in any way they like./"
,,Realm,update unit tests for annotation-processor/Added Realm.copyFromRealm()/
,,Realm,Realm.createOrUpdateObjectFromJson() now works correctly if the RealmObject class contains a primary key/
,,Realm,Case enums. New RealmCache/Add a same origin check/
,,Realm,fix typo/
,,Realm,Now setters for RealmObject and RealmList have a check if the value is a valid object or not (#1749)./
,,Realm,More fixes for the AP/
,,Realm,Fix the AP unit tests/
,,Realm,"Fix the unit tests of the annotations processor.

Also remove one unit test which makes no sense now that users can
write model classes in any way they like./update unit tests for annotation-processor/"
,,Realm,Disallow transient and volatile fields/Disallow final fields/
,,Realm,"Remove reflection from the generated proxy code./"""
,,Realm,Address the first wave of review comments/
,,Realm,"Realm.createOrUpdateObjectFromJson() now works correctly if the RealmObject class contains a primary key/
,,Realm,Remove reflection from the generated proxy code./"
,,Realm,Make the examples build again/Added Realm.copyFromRealm()/
,,Realm,Add a same origin check/
,,Realm,"Now setters for RealmObject and RealmList have a check if the value is a valid object or not (#1749)./
,,Realm,Remove reflection from the generated proxy code./"""
,,Realm,Disallow transient and volatile fields/
,,Realm,Disallow final fields/
,,Realm,Don’t override equals/
,,Realm,hashCode/
,,Realm,toString if already implemented/
,,Realm,Fix support for ignored fields/
,,Realm,Remove reflection from the generated proxy code./
,,Realm,"Add support for detached objects/New Migration and Dynamic API. New Sort/"""
,,Realm,"Case enums. New RealmCache/
,,Realm,Converted RealmTests to JUnit4 + Fixed Findbugs issues/"""
,,Realm,switch to JUnit4/
,,Realm,"enhancing notifications, adding Async queries for DynamicRealmObject/"
,,Realm,"Checking to see if a transaction is currently in progress for sync and async transaction pathways.
Also adding logging.
Added a test helper to assist with log assertion.
Fixes #1682

Fixing typo.

Removing logger after testing

Removing extra line

Fixing tests and adding change log message.

Adding breaking change message

Small cosmetic changes

Adding punctuation

formatting

Fixing unit test./"
,,Realm,"Adding jni layer interop code to utilize find method.

Testing

Adding header file

Checking to ensure we're on the right Realm before performing the contains check.

Adding changlog

Adding table view code and changing return types to jlong so we can use them in the indexOf

Adding multi-realm test

Removing code that should not have been in the commit

Changing LinkView.cpp method to use commonly used Macro and udpated null checks.

Changing LinkView.cpp method to use commonly used Macro and udpated null checks and moved contains method to override instead of overload.

Using instance of instead of assignable from and using long value instead of returning a boolean.

Checking for row and index validity in the cpp layer. Also removing the not found constant in order to the use the constant in TableOrView

Checking for managed mode. Falling back to default impl. Updating changelog.

Reverting back to to_jlong_or_not_found

Fixing a few compilation issues as well as applying code review updates.

Fixing managed mode bug that I accidentally introduced.

Updates as per @beeender recommended, issues listed below.
After changing cpp code to return -1 tests started failing - items were not found. I'm on the fence if this is expected or not. I'm not to familiar wit the underlying core to know if -1 should be needed.

Also added tests to check to see if a query or list contains an item that is in the same Realm, but from another query/result. This test creates a native crash. I tried to identify the root cause by ing the tombstone and performing `ndk-stack` and `addr2line` on it but I could not properly identify the symbols and where they might be.

Inspiration and HOWTO to debug NDK from here: http://bytesthink.com/blog/?p=133

Adding nativePtr to JNI call. Also added isLoaded() call. Not contained test failing on out of bounds exception on row indexes.

Changing TableOrView#find() name to TableOrView#sourceRowIndex() as per recommendation of @beeender

Adding check for parent table and re-enabled the other jni targets and fixed incorrect test.
Received help with this from @beeender and @cmelchior./"
,,Realm,Using separate interfaces for async transaction also fixes #2130/
,,Realm,Cleanup unit test naming and looper thread usage./
,,Realm,Fix crash when closing a Realm in listener #1900/
,,Realm,add batch update for async queries to fix #1851/
,,Realm,"Access to RealmResults based on deleted RealmList

* When the original RealmList is deleted, for most methods of
  RealmResults should just work without crash by just treat it like an
  empty RealmResults.
* RealmResults.where() throws IllegalStateExecption in this case.
* RealmResults.isValid() returns false in this case.

This is a temp fix, check https://github.com/realm/realm-core//1434
for more details.

Close #1945/"
,,Realm,Convert IOSRealmTests and JNITransaction tests/New Migration and Dynamic API. New Sort/Case enums. New RealmCache/
,,Realm,Convert migration and cache tests to JUNIT4/New Migration and Dynamic API. New Sort/Case enums. New RealmCache/
,,Realm,switch to JUnit4/New Migration and Dynamic API. New Sort/Case enums. New RealmCache/
,,Realm,"Add a check to prevent removing a RealmChangeListener from a non-Looper thread/Change modifier, remove unused var, and use new JUnit in RealmTest.
 * createAndTestFilename public -> private
 * remove unused variable get_data
 * Use assertEquals(expected, actual, delta) to compare
   floating-point numbers. assertEquals(expected, actual) for
   floating-pointer numbers is deprecated in JUnit.
   See also: http://junit.org/apidocs/org/junit/Assert.html#assertEquals(double, double)/"
,,Realm,"Use JUNIT4 for RealmQueryTest

* Clean up RealmQueryTest.
* No NP_NULL_PARAM_DEREF_ALL_TARGETS_DANGEROUS and DM_GC for tests./Support for RealmQuery.isNotEmpty() added/Add tests of aggregation methods in ReqlmQuery for nullable column/"
,,Realm,"Added RealmQuery.distinct(), RealmResults.distinct()/"
,,Realm,"Allow byte code weaving during unit testing/NotificationTests converted to use TestRealmConfigurationFactory/switch to JUnit4/enhancing notifications, adding Async queries for DynamicRealmObject/"
,,Realm,"Fix flaky tests related with async transaction

* Background realm needs to be closed before notifying other threads
  in async transaction.
* Latch needs to be called after Realm closed./Add RealmList.removeAllFromRealm and Realm.clear

* Add LinkView.removeAllTargetRows.
* Add Realm.clear to remove all objects from Realm.
* Add RealmList.removeAllFromRealm().
* Javadoc & test case update.

Close #1560/Introduce basic PMD rules/Converted DynamicRealmTests to JUnit4. Added JUnit test guidelines./Revert ""Merge   #2084 from realm/ez/pmd""

This reverts commit d6d351f7e55a5598e6abbceda92f0a5bba33156a, reversing
changes made to a73cb7969185ea10649ccf2bc73241d6908b861d./Add a check to prevent removing a RealmChangeListener from a non-Looper thread/fix 1884 listener trigger/enhancing notifications, adding Async queries for DynamicRealmObject/New Migration and Dynamic API. New Sort/Case enums. New RealmCache/Listeners are strong reference now/Checking for transaction in Realm.refresh and throwing if inside of Transaction
as the Realm is always kept up to date.

Fixes per code reviews.

Adding update to changelog

Fixing indentation.

Updating change log to show correct exception/"
,,Realm,"RealmList.set() now correctly returns the old item. RealmList unit tests migrated to JUnit 4./Applying code review recommendations and adding removed from Realm tests.
Review recommendations are from @beeender and @clemchoir and @kneth./Adding jni layer interop code to utilize find method.

Testing

Adding header file

Checking to ensure we're on the right Realm before performing the contains check.

Adding changlog

Adding table view code and changing return types to jlong so we can use them in the indexOf

Adding multi-realm test

Removing code that should not have been in the commit

Changing LinkView.cpp method to use commonly used Macro and udpated null checks.

Changing LinkView.cpp method to use commonly used Macro and udpated null checks and moved contains method to override instead of overload.

Using instance of instead of assignable from and using long value instead of returning a boolean.

Checking for row and index validity in the cpp layer. Also removing the not found constant in order to the use the constant in TableOrView

Checking for managed mode. Falling back to default impl. Updating changelog.

Reverting back to to_jlong_or_not_found

Fixing a few compilation issues as well as applying code review updates.

Fixing managed mode bug that I accidentally introduced.

Updates as per @beeender recommended, issues listed below.
After changing cpp code to return -1 tests started failing - items were not found. I'm on the fence if this is expected or not. I'm not to familiar wit the underlying core to know if -1 should be needed.

Also added tests to check to see if a query or list contains an item that is in the same Realm, but from another query/result. This test creates a native crash. I tried to identify the root cause by ing the tombstone and performing `ndk-stack` and `addr2line` on it but I could not properly identify the symbols and where they might be.

Inspiration and HOWTO to debug NDK from here: http://bytesthink.com/blog/?p=133

Adding nativePtr to JNI call. Also added isLoaded() call. Not contained test failing on out of bounds exception on row indexes.

Changing TableOrView#find() name to TableOrView#sourceRowIndex() as per recommendation of @beeender

Adding check for parent table and re-enabled the other jni targets and fixed incorrect test.
Received help with this from @beeender and @cmelchior./New Migration and Dynamic API. New Sort/Case enums. New RealmCache/"
,,Realm,Add RxJava support/Deprecate RealmConfiguration.getSchemaMediator() and add RealmConfiguration.getRealmObjectClasses() which returns the unmodifiable set of model classes that make up the schema./
,,Realm,"Access to RealmResults based on deleted RealmList

* When the original RealmList is deleted, for most methods of
  RealmResults should just work without crash by just treat it like an
  empty RealmResults.
* RealmResults.where() throws IllegalStateExecption in this case.
* RealmResults.isValid() returns false in this case.

This is a temp fix, check https://github.com/realm/realm-core//1434
for more details.

Close #1945/Added RealmQuery.distinct(), RealmResults.distinct()/Fix PMD error. Nested if/Applying code review recommendations and adding removed from Realm tests.
Review recommendations are from @beeender and @clemchoir and @kneth./Adding jni layer interop code to utilize find method.

Testing

Adding header file

Checking to ensure we're on the right Realm before performing the contains check.

Adding changlog

Adding table view code and changing return types to jlong so we can use them in the indexOf

Adding multi-realm test

Removing code that should not have been in the commit

Changing LinkView.cpp method to use commonly used Macro and udpated null checks.

Changing LinkView.cpp method to use commonly used Macro and udpated null checks and moved contains method to override instead of overload.

Using instance of instead of assignable from and using long value instead of returning a boolean.

Checking for row and index validity in the cpp layer. Also removing the not found constant in order to the use the constant in TableOrView

Checking for managed mode. Falling back to default impl. Updating changelog.

Reverting back to to_jlong_or_not_found

Fixing a few compilation issues as well as applying code review updates.

Fixing managed mode bug that I accidentally introduced.

Updates as per @beeender recommended, issues listed below.
After changing cpp code to return -1 tests started failing - items were not found. I'm on the fence if this is expected or not. I'm not to familiar wit the underlying core to know if -1 should be needed.

Also added tests to check to see if a query or list contains an item that is in the same Realm, but from another query/result. This test creates a native crash. I tried to identify the root cause by ing the tombstone and performing `ndk-stack` and `addr2line` on it but I could not properly identify the symbols and where they might be.

Inspiration and HOWTO to debug NDK from here: http://bytesthink.com/blog/?p=133

Adding nativePtr to JNI call. Also added isLoaded() call. Not contained test failing on out of bounds exception on row indexes.

Changing TableOrView#find() name to TableOrView#sourceRowIndex() as per recommendation of @beeender

Adding check for parent table and re-enabled the other jni targets and fixed incorrect test.
Received help with this from @beeender and @cmelchior./Add RxJava support/fix 1894 changelistener not called/enhancing notifications, adding Async queries for DynamicRealmObject/New Migration and Dynamic API. New Sort/Case enums. New RealmCache/"
,,Realm,"Access to RealmResults based on deleted RealmList

* When the original RealmList is deleted, for most methods of
  RealmResults should just work without crash by just treat it like an
  empty RealmResults.
* RealmResults.where() throws IllegalStateExecption in this case.
* RealmResults.isValid() returns false in this case.

This is a temp fix, check https://github.com/realm/realm-core//1434
for more details.

Close #1945/New Migration and Dynamic API. New Sort/Case enums. New RealmCache/"
,,Realm,"Fix RealmQuery.isNotEmpty().

It is fix for #2025. Fix the native method + Renaming./Support for RealmQuery.isNotEmpty() added/New Migration and Dynamic API. New Sort/Case enums. New RealmCache/"
,,Realm,"Added RealmQuery.distinct(), RealmResults.distinct()/Cleanup internal classes/"
,,Realm,"Release LinkView native pointers

Fix #1285

* Add abstract method to NativeObjectReference for native resource
  releasing.
* Implement NativeObjectReference.cleanup for UncheckRow and LinkView/"
,,Realm,"Notify listeners of async RealmObject even if the result is empty/enhancing notifications, adding Async queries for DynamicRealmObject/"
,,Realm,"Cleanup internal classes/enhancing notifications, adding Async queries for DynamicRealmObject/New Migration and Dynamic API. New Sort/Case enums. New RealmCache/"
,,Realm,"enhancing notifications, adding Async queries for DynamicRealmObject/New Migration and Dynamic API. New Sort/Case enums. New RealmCache/"
,,Realm,Moving actual loading to RealmCore/
,,Realm,"New Migration and Dynamic API. New Sort/Case enums. New RealmCache/Release LinkView native pointers

Fix #1285

* Add abstract method to NativeObjectReference for native resource
  releasing.
* Implement NativeObjectReference.cleanup for UncheckRow and LinkView/"
,,Realm,"Add RealmList.removeAllFromRealm and Realm.clear

* Add LinkView.removeAllTargetRows.
* Add Realm.clear to remove all objects from Realm.
* Add RealmList.removeAllFromRealm().
* Javadoc & test case update.

Close #1560/Applying code review recommendations and adding removed from Realm tests.
Review recommendations are from @beeender and @clemchoir and @kneth./Adding jni layer interop code to utilize find method.

Testing

Adding header file

Checking to ensure we're on the right Realm before performing the contains check.

Adding changlog

Adding table view code and changing return types to jlong so we can use them in the indexOf

Adding multi-realm test

Removing code that should not have been in the commit

Changing LinkView.cpp method to use commonly used Macro and udpated null checks.

Changing LinkView.cpp method to use commonly used Macro and udpated null checks and moved contains method to override instead of overload.

Using instance of instead of assignable from and using long value instead of returning a boolean.

Checking for row and index validity in the cpp layer. Also removing the not found constant in order to the use the constant in TableOrView

Checking for managed mode. Falling back to default impl. Updating changelog.

Reverting back to to_jlong_or_not_found

Fixing a few compilation issues as well as applying code review updates.

Fixing managed mode bug that I accidentally introduced.

Updates as per @beeender recommended, issues listed below.
After changing cpp code to return -1 tests started failing - items were not found. I'm on the fence if this is expected or not. I'm not to familiar wit the underlying core to know if -1 should be needed.

Also added tests to check to see if a query or list contains an item that is in the same Realm, but from another query/result. This test creates a native crash. I tried to identify the root cause by ing the tombstone and performing `ndk-stack` and `addr2line` on it but I could not properly identify the symbols and where they might be.

Inspiration and HOWTO to debug NDK from here: http://bytesthink.com/blog/?p=133

Adding nativePtr to JNI call. Also added isLoaded() call. Not contained test failing on out of bounds exception on row indexes.

Changing TableOrView#find() name to TableOrView#sourceRowIndex() as per recommendation of @beeender

Adding check for parent table and re-enabled the other jni targets and fixed incorrect test.
Received help with this from @beeender and @cmelchior./Cleanup internal classes/Release LinkView native pointers

Fix #1285

* Add abstract method to NativeObjectReference for native resource
  releasing.
* Implement NativeObjectReference.cleanup for UncheckRow and LinkView/"
,,Realm,Added Realm.copyFromRealm()/Deprecate RealmConfiguration.getSchemaMediator() and add RealmConfiguration.getRealmObjectClasses() which returns the unmodifiable set of model classes that make up the schema./
,,Realm,"Notify listeners of async RealmObject even if the result is empty/Add RxJava support/fix 1894 changelistener not called/enhancing notifications, adding Async queries for DynamicRealmObject/New Migration and Dynamic API. New Sort/Case enums. New RealmCache/"
,,Realm,"Better exception message when sort on sub object

Close #2034/Fix RealmQuery.isNotEmpty().

It is fix for #2025. Fix the native method + Renaming./Support for RealmQuery.isNotEmpty() added/Always close shared group first before async msg/fix 1884 listener trigger/enhancing notifications, adding Async queries for DynamicRealmObject/New Migration and Dynamic API. New Sort/Case enums. New RealmCache/"
,,Realm,"Using separate interfaces for async transaction also fixes #2130/Added RealmQuery.distinct(), RealmResults.distinct()/Fix flaky tests related with async transaction

* Background realm needs to be closed before notifying other threads
  in async transaction.
* Latch needs to be called after Realm closed./fix 1884 listener trigger/Fix crash when closing a Realm in listener #1900/Added Realm.copyFromRealm()/Realm.createOrUpdateObjectFromJson() now works correctly if the RealmObject class contains a primary key/New Migration and Dynamic API. New Sort/Case enums. New RealmCache/Checking to see if a transaction is currently in progress for sync and async transaction pathways.
Also adding logging.
Added a test helper to assist with log assertion.
Fixes #1682

Fixing typo.

Removing logger after testing

Removing extra line

Fixing tests and adding change log message.

Adding breaking change message

Small cosmetic changes

Adding punctuation

formatting

Fixing unit test./"
,,Realm,"Revert ""Merge   #2084 from realm/ez/pmd""

This reverts commit d6d351f7e55a5598e6abbceda92f0a5bba33156a, reversing
changes made to a73cb7969185ea10649ccf2bc73241d6908b861d./enhancing notifications, adding Async queries for DynamicRealmObject/"
,,Realm,"Removed explicit GC call when Realm changes/fix 1884 listener trigger/Fix crash when closing a Realm in listener #1900/enhancing notifications, adding Async queries for DynamicRealmObject/"
,,Realm,"Add RealmList.removeAllFromRealm and Realm.clear

* Add LinkView.removeAllTargetRows.
* Add Realm.clear to remove all objects from Realm.
* Add RealmList.removeAllFromRealm().
* Javadoc & test case update.

Close #1560/Adding jni layer interop code to utilize find method.

Testing

Adding header file

Checking to ensure we're on the right Realm before performing the contains check.

Adding changlog

Adding table view code and changing return types to jlong so we can use them in the indexOf

Adding multi-realm test

Removing code that should not have been in the commit

Changing LinkView.cpp method to use commonly used Macro and udpated null checks.

Changing LinkView.cpp method to use commonly used Macro and udpated null checks and moved contains method to override instead of overload.

Using instance of instead of assignable from and using long value instead of returning a boolean.

Checking for row and index validity in the cpp layer. Also removing the not found constant in order to the use the constant in TableOrView

Checking for managed mode. Falling back to default impl. Updating changelog.

Reverting back to to_jlong_or_not_found

Fixing a few compilation issues as well as applying code review updates.

Fixing managed mode bug that I accidentally introduced.

Updates as per @beeender recommended, issues listed below.
After changing cpp code to return -1 tests started failing - items were not found. I'm on the fence if this is expected or not. I'm not to familiar wit the underlying core to know if -1 should be needed.

Also added tests to check to see if a query or list contains an item that is in the same Realm, but from another query/result. This test creates a native crash. I tried to identify the root cause by ing the tombstone and performing `ndk-stack` and `addr2line` on it but I could not properly identify the symbols and where they might be.

Inspiration and HOWTO to debug NDK from here: http://bytesthink.com/blog/?p=133

Adding nativePtr to JNI call. Also added isLoaded() call. Not contained test failing on out of bounds exception on row indexes.

Changing TableOrView#find() name to TableOrView#sourceRowIndex() as per recommendation of @beeender

Adding check for parent table and re-enabled the other jni targets and fixed incorrect test.
Received help with this from @beeender and @cmelchior./"
,,Realm,"Buffer overwritten caused PK migration failure

Close #1775
A String or Binary pointer retrieved from Realm is invalidated when
set_string called./"
,,Realm,"Added RealmQuery.distinct(), RealmResults.distinct()/Adding jni layer interop code to utilize find method.

Testing

Adding header file

Checking to ensure we're on the right Realm before performing the contains check.

Adding changlog

Adding table view code and changing return types to jlong so we can use them in the indexOf

Adding multi-realm test

Removing code that should not have been in the commit

Changing LinkView.cpp method to use commonly used Macro and udpated null checks.

Changing LinkView.cpp method to use commonly used Macro and udpated null checks and moved contains method to override instead of overload.

Using instance of instead of assignable from and using long value instead of returning a boolean.

Checking for row and index validity in the cpp layer. Also removing the not found constant in order to the use the constant in TableOrView

Checking for managed mode. Falling back to default impl. Updating changelog.

Reverting back to to_jlong_or_not_found

Fixing a few compilation issues as well as applying code review updates.

Fixing managed mode bug that I accidentally introduced.

Updates as per @beeender recommended, issues listed below.
After changing cpp code to return -1 tests started failing - items were not found. I'm on the fence if this is expected or not. I'm not to familiar wit the underlying core to know if -1 should be needed.

Also added tests to check to see if a query or list contains an item that is in the same Realm, but from another query/result. This test creates a native crash. I tried to identify the root cause by ing the tombstone and performing `ndk-stack` and `addr2line` on it but I could not properly identify the symbols and where they might be.

Inspiration and HOWTO to debug NDK from here: http://bytesthink.com/blog/?p=133

Adding nativePtr to JNI call. Also added isLoaded() call. Not contained test failing on out of bounds exception on row indexes.

Changing TableOrView#find() name to TableOrView#sourceRowIndex() as per recommendation of @beeender

Adding check for parent table and re-enabled the other jni targets and fixed incorrect test.
Received help with this from @beeender and @cmelchior./"
Memory Management,Memory Management,Realm,Support for RealmQuery.isNotEmpty() added/add batch update for async queries to fix #1851/fix potential memory leak in jni code and skip copying back to java array if native array is not modified./
,,Realm,Support for RealmQuery.isNotEmpty() added/Add RxJava support/add batch update for async queries to fix #1851/
,,Realm,"Access to RealmResults based on deleted RealmList

* When the original RealmList is deleted, for most methods of
  RealmResults should just work without crash by just treat it like an
  empty RealmResults.
* RealmResults.where() throws IllegalStateExecption in this case.
* RealmResults.isValid() returns false in this case.

This is a temp fix, check https://github.com/realm/realm-core//1434
for more details.

Close #1945/enhancing notifications, adding Async queries for DynamicRealmObject/"
,,Realm,"Add RealmList.removeAllFromRealm and Realm.clear

* Add LinkView.removeAllTargetRows.
* Add Realm.clear to remove all objects from Realm.
* Add RealmList.removeAllFromRealm().
* Javadoc & test case update.

Close #1560/"
Memory Management,Memory Management,Realm,"Access to RealmResults based on deleted RealmList

* When the original RealmList is deleted, for most methods of
  RealmResults should just work without crash by just treat it like an
  empty RealmResults.
* RealmResults.where() throws IllegalStateExecption in this case.
* RealmResults.isValid() returns false in this case.

This is a temp fix, check https://github.com/realm/realm-core//1434
for more details.

Close #1945/New Migration and Dynamic API. New Sort/Case enums. New RealmCache/fix potential memory leak in jni code and skip copying back to java array if native array is not modified./"
,,Realm,Fixed lint warnings in RxJava example/
,,Realm,"Added RealmCollection API's/Use accessors for Person in the intro example.

Keep public fields in Cat and Dog./Deprecate Realm.getInstance(Context)

 * Deprecate Realm.getInstance(Context).
 * Replease `Realm.getInstance(Context)` in examples./Use public fields in the intro example and lombok in the json example/"
,,Realm,"Fix warnings in AdapterExample (#2580)

* Fix warnings

* Add missing newline

* Add missing newline

* Update AndroidManifest.xml/Added RealmCollection API's/Deprecate Realm.getInstance(Context)

 * Deprecate Realm.getInstance(Context).
 * Replease `Realm.getInstance(Context)` in examples./"
,,Realm,"Fix warnings in AdapterExample (#2580)

* Fix warnings

* Add missing newline

* Add missing newline

* Update AndroidManifest.xml/Added RealmCollection API's/"
,,Realm,fix lint rawnings in unitTestExample/
,,Realm,Added RealmCollection API's/Fix unit test example mocks/
,,Realm,"Added RealmCollection API's/Deprecate Realm.getInstance(Context)

 * Deprecate Realm.getInstance(Context).
 * Replease `Realm.getInstance(Context)` in examples./"
,,Realm,"Deprecate Realm.getInstance(Context)

 * Deprecate Realm.getInstance(Context).
 * Replease `Realm.getInstance(Context)` in examples./"
Data conversion,Data conversion,Realm,"#1594 RealmChangeListener should provide the changes object/realm/collection as well (#2705)

RealmChangeListener should provide the changes object/realm/collection as well/Interface as supplement to extending RealmObject (#2599)

* Backup

* Add RealmModel

* Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests

* fixing examples

* WIP

* fix Annotation processor

* clean up

* fix javadoc

* add transformer tests & fix DynamicRealm tests

* fixing tests

* adding tests

* adding tests

* fixing tests for the realm-annotations-processor

* add tests to cover more UC for RealmList

* renamed POJO to RealmModel

* reduce method count by using proxyState only in RealmObjectProxy

* fix examples, no need to exclude RealmObject using Gson https://realm.io/docs/java/latest/#gson

* making RealmList final, remove unused method & fixing APT test/Thread check for copyToRealm/copyToRealmOrUpdate/Fix the cast in proxy generator for CacheData

Close #2354/allow 'realm' and 'row' as a field name of model class. This fixes #2255/"
Data conversion,Data conversion,Realm,"#1594 RealmChangeListener should provide the changes object/realm/collection as well (#2705)

RealmChangeListener should provide the changes object/realm/collection as well/@PrimaryKey in String, Byte, Short, Integer, and Long types can be null. (#2634)

@PrimaryKey is allowed to be null for String and Boxed primitive types.
Features part of this commit are

1. A @PrimaryKey annotated field in java.lang.String/Byte/Short/Integer/Long type can be null.
2. The object with nil primary key can be updated.
3. Migration checks if existing nullable type @PrimaryKey is set to be nullable in existing file. If not, throws RealmMigrationNeeded.
4. Duplicated null as PrimaryKey throws RealmPrimaryKeyConstraintException.
5. When a new empty row is added, the default value checked for duplicated is still the same empty string ("""") at Table.addEmptyRow()
6. Since there isn't equivalent term for Optional in Java 7 yet, nullable PK types are configured as following in this PR.

                       | String PK | Number PK | Primitive PK |
------------------------------------------------------------------------
Table Column | Nullable   | Nullable	       | Not Nullable |/Interface as supplement to extending RealmObject (#2599)

* Backup

* Add RealmModel

* Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests

* fixing examples

* WIP

* fix Annotation processor

* clean up

* fix javadoc

* add transformer tests & fix DynamicRealm tests

* fixing tests

* adding tests

* adding tests

* fixing tests for the realm-annotations-processor

* add tests to cover more UC for RealmList

* renamed POJO to RealmModel

* reduce method count by using proxyState only in RealmObjectProxy

* fix examples, no need to exclude RealmObject using Gson https://realm.io/docs/java/latest/#gson

* making RealmList final, remove unused method & fixing APT test/Thread check for copyToRealm/copyToRealmOrUpdate/Fix the cast in proxy generator for CacheData

Close #2354/allow 'realm' and 'row' as a field name of model class. This fixes #2255/"
,,Realm,"Interface as supplement to extending RealmObject (#2599)

* Backup

* Add RealmModel

* Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests

* fixing examples

* WIP

* fix Annotation processor

* clean up

* fix javadoc

* add transformer tests & fix DynamicRealm tests

* fixing tests

* adding tests

* adding tests

* fixing tests for the realm-annotations-processor

* add tests to cover more UC for RealmList

* renamed POJO to RealmModel

* reduce method count by using proxyState only in RealmObjectProxy

* fix examples, no need to exclude RealmObject using Gson https://realm.io/docs/java/latest/#gson

* making RealmList final, remove unused method & fixing APT test/"
,,Realm,"Fix using RealmModel as a field (#2666)

* Modify and rename isRealmObject to isRealmModel.
* Modify annotation processor to allow RealmModel as a field.

Fix #2654/Interface as supplement to extending RealmObject (#2599)

* Backup

* Add RealmModel

* Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests

* fixing examples

* WIP

* fix Annotation processor

* clean up

* fix javadoc

* add transformer tests & fix DynamicRealm tests

* fixing tests

* adding tests

* adding tests

* fixing tests for the realm-annotations-processor

* add tests to cover more UC for RealmList

* renamed POJO to RealmModel

* reduce method count by using proxyState only in RealmObjectProxy

* fix examples, no need to exclude RealmObject using Gson https://realm.io/docs/java/latest/#gson

* making RealmList final, remove unused method & fixing APT test/allow 'realm' and 'row' as a field name of model class. This fixes #2255/"
Data conversion,Data conversion,Realm,"#1594 RealmChangeListener should provide the changes object/realm/collection as well (#2705)

RealmChangeListener should provide the changes object/realm/collection as well/Fix using RealmModel as a field (#2666)

* Modify and rename isRealmObject to isRealmModel.
* Modify annotation processor to allow RealmModel as a field.

Fix #2654/A combination of @PrimaryKey and @Required annotation now generates appropriate type cast when a primary key field type is String

When RealmProxyClassGenerator.emitCopyOrUpdateMethod() is generating code for checking @PrimaryKey value for not-nullable types (i.e. primitive types or marked with @Required), it now generates appropriate type casts according to the field types.

* Required PrimaryKey tests are parameterized./@PrimaryKey in String, Byte, Short, Integer, and Long types can be null. (#2634)

@PrimaryKey is allowed to be null for String and Boxed primitive types.
Features part of this commit are

1. A @PrimaryKey annotated field in java.lang.String/Byte/Short/Integer/Long type can be null.
2. The object with nil primary key can be updated.
3. Migration checks if existing nullable type @PrimaryKey is set to be nullable in existing file. If not, throws RealmMigrationNeeded.
4. Duplicated null as PrimaryKey throws RealmPrimaryKeyConstraintException.
5. When a new empty row is added, the default value checked for duplicated is still the same empty string ("""") at Table.addEmptyRow()
6. Since there isn't equivalent term for Optional in Java 7 yet, nullable PK types are configured as following in this PR.

                       | String PK | Number PK | Primitive PK |
------------------------------------------------------------------------
Table Column | Nullable   | Nullable	       | Not Nullable |/Interface as supplement to extending RealmObject (#2599)

* Backup

* Add RealmModel

* Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests

* fixing examples

* WIP

* fix Annotation processor

* clean up

* fix javadoc

* add transformer tests & fix DynamicRealm tests

* fixing tests

* adding tests

* adding tests

* fixing tests for the realm-annotations-processor

* add tests to cover more UC for RealmList

* renamed POJO to RealmModel

* reduce method count by using proxyState only in RealmObjectProxy

* fix examples, no need to exclude RealmObject using Gson https://realm.io/docs/java/latest/#gson

* making RealmList final, remove unused method & fixing APT test/Thread check for copyToRealm/copyToRealmOrUpdate/Fix the cast in proxy generator for CacheData

Close #2354/allow 'realm' and 'row' as a field name of model class. This fixes #2255/"
,,Realm,"Fix using RealmModel as a field (#2666)

* Modify and rename isRealmObject to isRealmModel.
* Modify annotation processor to allow RealmModel as a field.

Fix #2654/Interface as supplement to extending RealmObject (#2599)

* Backup

* Add RealmModel

* Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests

* fixing examples

* WIP

* fix Annotation processor

* clean up

* fix javadoc

* add transformer tests & fix DynamicRealm tests

* fixing tests

* adding tests

* adding tests

* fixing tests for the realm-annotations-processor

* add tests to cover more UC for RealmList

* renamed POJO to RealmModel

* reduce method count by using proxyState only in RealmObjectProxy

* fix examples, no need to exclude RealmObject using Gson https://realm.io/docs/java/latest/#gson

* making RealmList final, remove unused method & fixing APT test/"
,,Realm,"#1594 RealmChangeListener should provide the changes object/realm/collection as well (#2705)

RealmChangeListener should provide the changes object/realm/collection as well/Interface as supplement to extending RealmObject (#2599)

* Backup

* Add RealmModel

* Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests

* fixing examples

* WIP

* fix Annotation processor

* clean up

* fix javadoc

* add transformer tests & fix DynamicRealm tests

* fixing tests

* adding tests

* adding tests

* fixing tests for the realm-annotations-processor

* add tests to cover more UC for RealmList

* renamed POJO to RealmModel

* reduce method count by using proxyState only in RealmObjectProxy

* fix examples, no need to exclude RealmObject using Gson https://realm.io/docs/java/latest/#gson

* making RealmList final, remove unused method & fixing APT test/Thread local notifactions are now triggered using the Looper instead of immediately/Simplify ReamAsyncQueryTests + added new test util classes/"
,,Realm,"@PrimaryKey in String, Byte, Short, Integer, and Long types can be null. (#2634)

@PrimaryKey is allowed to be null for String and Boxed primitive types.
Features part of this commit are

1. A @PrimaryKey annotated field in java.lang.String/Byte/Shor"
,,Realm,"Handle PK when calling RealmSchema's remove/rename (#2663)

* Handle PK when calling RealmSchema's remove/rename

* RealmSchema.remove() should remove the field from PK table.
* RealmSchema.rename() should change the corresponding row in the PK
  table.

Fix #2555/"
,,Realm,"Add to realmObjects when addChangeListener called (#2723)

* Only add the RealmObject to HandlerController.realmObjects when
  addChangeListener called. This will avoid we have too many objects in
  the map.
* addToRealmObjects missed reference queue which would lead to the map
  grows always without cleaning.

Close #2569, potentially a fix to #2686 ./Increased RealmObjectTests coverage with testing on unmanaged objects and removing null listener./Custom equals(), toString() and hashCode() work correctly./Added RealmCollection API's/allow 'realm' and 'row' as a field name of model class. This fixes #2255/"
,,Realm,"Give async related vars better names

asyncQueryExecutor and pendingQuery seem to be very confusing since they
are used for async transaction as well./#1594 RealmChangeListener should provide the changes object/realm/collection as well (#2705)

RealmChangeListener should provide the changes object/realm/collection as well/Added RealmCollection API's/Fixed race condition causing BadVersionException/Merge remote-tracking branch 'origin/release/0.87' into mc/merge/0.87_e5bd0080
fixes #2115 IllegalStateException Caller thread behind the worker thread/Thread local notifactions are now triggered using the Looper instead of immediately/Simplify ReamAsyncQueryTests + added new test util classes/"
,,Realm,"fix the String from DynamicRealmObject.toString().

Now DynamicRealmObject.toString() shows null value as ""null"".
""class_"" prefix is removed from type name in that String.
Align the format to the String from typed RealmObject./Correct checking in DynamicRealmObject.setList

* Fix #2368
* Check equality of Realm in DynamicRealmObject setters to avoid risks
  mentioned 2) in #2382/"
,,Realm,Add unit tests for compatibility with iOS NSDate/
,,Realm,RealmResults and RealmObjects are no longer accidentally GC'ed while their Observable is still alive./Added RealmCollection API's/Realm Observables now holds a Realm instance until unsubscribed/Thread local notifactions are now triggered using the Looper instead of immediately/
,,Realm,Thread local notifactions are now triggered using the Looper instead of immediately/
,,Realm,Added RealmCollection API's/optimize imports/
,,Realm,"Realm Observables now holds a Realm instance until unsubscribed/Take latches for RunInLooperThread.testComplete

And fix more test cases./"
,,Realm,Convert RealmObjectSchemaTests to JUNIT4/
,,Realm,Added RealmCollection API's/
,,Realm,"Implementation of SharedGroup::wait_for_change() method. (#2386)

Goal
1. Add BaseRealm.waitForChange() which blocks until a change is available, then advance the Realm and continue without triggering RealmChangeListeners.
2. Add BaseRealm.stopWaitForChange() that makes any current waitForChange() return false immediately.
3. Deprecated BaseRealm.refresh().

Behaviors
1. waitForChange() will throw IllegalStateException within a transaction.
2. BaseRealm cannot wait in a thread with Looper. IllegalStateException will be thrown.
3. Once stopWaitForChange is called, all future calls to waitForChange will immediately return false.

For detailed discussions, please refer https://github.com/realm/realm-wiki/wiki/Java-Multi-threaded-wait_for_change/#1594 RealmChangeListener should provide the changes object/realm/collection as well (#2705)

RealmChangeListener should provide the changes object/realm/collection as well/Make Realm.createObject(Class,PrimaryKey) public (#2622)

This method is a way to solve the problem that people might have data with a primary key that is is the default value for that type, e.g a long primary key with the value 0.

Once this object is saved in Realm any calls to the normal Realm.createObject() would throw an exception as it would try to assign the default value (zero) to the object before the user could set it to something else. With this method that is no longer a problem.

Internally we already use this method and it is already public in the Dynamic API. The reason for not making this public earlier was due to fear of mis-use since there is no type-safe guarantee at compile time. However since it is already public in the Dynamic API and we haven't seen any indication of that being misused we feel it should be safe to make this public. Also our Query API is also semi-threadsafe so there is precedence there as well./Only throw RealmException if absolutely necessary (#2618)

We should avoid wrapping lower level exceptions in RealmExceptions unless there is a good reason for it. We had multiple support issues where people were confused about the RealmException and didn't understand they had to dig through the stack trace for the original exception.

This change alters the behaviour of all our JSON methods so they now only convert JSONException to RealmException in order to prevent checked exceptions to reach the user. All other exceptions should be thrown directly./RealmCollection iterators are now stable (#2124)

This commit changes the semantics of how live RealmResults really are. Before this commit
RealmResults where live _all the time_, which meant that code like the below didn't work as expected (only half the elements would be deleted):

```
for (int i = 0; i < results.size(); i++) {
   results.get(i).deleteFromRealm();
}

```

The rather unintuitive work-around was counting backwards:

```
for (int i = results.size() -1; i >=0; i--) {
   results.get(i).deleteFromRealm();
}
```

This commit changes the behaviour of RealmResults, so they no longer are automatically up to date. They are instead refreshed using Looper events just like changes from other threads.

RealmList iterators are not live in the same way that RealmResults are but act more like standard ArrayLists, so they don't suffer from the same liveness issues.

The high-level arguments for doing this change was:

Pros:
* All iterators now work as you would normally expect (ease of use).
* The liveness of Realm objects are now all controlled through the Looper (simpler internal code / easier to document Realm behaviour).

Cons:
* There is a small chance that people can accidentally reference a deleted RealmObject in a RealmResult. (Can be checked through use of `RealmObject.isValid()`)/@PrimaryKey in String, Byte, Short, Integer, and Long types can be null. (#2634)

@PrimaryKey is allowed to be null for String and Boxed primitive types.
Features part of this commit are

1. A @PrimaryKey annotated field in java.lang.String/Byte/Short/Integer/Long type can be null.
2. The object with nil primary key can be updated.
3. Migration checks if existing nullable type @PrimaryKey is set to be nullable in existing file. If not, throws RealmMigrationNeeded.
4. Duplicated null as PrimaryKey throws RealmPrimaryKeyConstraintException.
5. When a new empty row is added, the default value checked for duplicated is still the same empty string ("""") at Table.addEmptyRow()
6. Since there isn't equivalent term for Optional in Java 7 yet, nullable PK types are configured as following in this PR.

                       | String PK | Number PK | Primitive PK |
------------------------------------------------------------------------
Table Column | Nullable   | Nullable	       | Not Nullable |/Added RealmCollection API's/Thread check for copyToRealm/copyToRealmOrUpdate/Rename clear and removeAllFromRealm to conform to new RealmCollection API/Add test case for getInstance(Context)/"
,,Realm,"RealmCollection iterators are now stable (#2124)

This commit changes the semantics of how live RealmResults really are. Before this commit
RealmResults where live _all the time_, which meant that code like the below didn't work as expected (only half the elements would be deleted):

```
for (int i = 0; i < results.size(); i++) {
   results.get(i).deleteFromRealm();
}

```

The rather unintuitive work-around was counting backwards:

```
for (int i = results.size() -1; i >=0; i--) {
   results.get(i).deleteFromRealm();
}
```

This commit changes the behaviour of RealmResults, so they no longer are automatically up to date. They are instead refreshed using Looper events just like changes from other threads.

RealmList iterators are not live in the same way that RealmResults are but act more like standard ArrayLists, so they don't suffer from the same liveness issues.

The high-level arguments for doing this change was:

Pros:
* All iterators now work as you would normally expect (ease of use).
* The liveness of Realm objects are now all controlled through the Looper (simpler internal code / easier to document Realm behaviour).

Cons:
* There is a small chance that people can accidentally reference a deleted RealmObject in a RealmResult. (Can be checked through use of `RealmObject.isValid()`)/Added RealmCollection API's/Merge remote-tracking branch 'origin/release/0.87' into mc/merge/0.87_4c218573
RealmResults.distinctAsync() support./"
,,Realm,"Added RealmCollection API's/More checkings when modify RealmList

Fix #2381

* New method LinkView.getTargetTable().
* Proper checking to add DynamicRealmObject to RealmList.
* Disallow modifying with RealmObject belongs to another Realm instance./"
,,Realm,optimize imports/Added RealmCollection API's/
,,Realm,"Multi-arguments distinct(...) for Realm, DynamicRealm, RealmQuery, and RealmResults/"
,,Realm,"migrateRealm throws when db doesn't exsit

Close #2316 ./"
,,Realm,"#1594 RealmChangeListener should provide the changes object/realm/collection as well (#2705)

RealmChangeListener should provide the changes object/realm/collection as well/RealmCollection iterators are now stable (#2124)

This commit changes the semantics of how live RealmResults really are. Before this commit
RealmResults where live _all the time_, which meant that code like the below didn't work as expected (only half the elements would be deleted):

```
for (int i = 0; i < results.size(); i++) {
   results.get(i).deleteFromRealm();
}

```

The rather unintuitive work-around was counting backwards:

```
for (int i = results.size() -1; i >=0; i--) {
   results.get(i).deleteFromRealm();
}
```

This commit changes the behaviour of RealmResults, so they no longer are automatically up to date. They are instead refreshed using Looper events just like changes from other threads.

RealmList iterators are not live in the same way that RealmResults are but act more like standard ArrayLists, so they don't suffer from the same liveness issues.

The high-level arguments for doing this change was:

Pros:
* All iterators now work as you would normally expect (ease of use).
* The liveness of Realm objects are now all controlled through the Looper (simpler internal code / easier to document Realm behaviour).

Cons:
* There is a small chance that people can accidentally reference a deleted RealmObject in a RealmResult. (Can be checked through use of `RealmObject.isValid()`)/One more flaky test/Thread local notifactions are now triggered using the Looper instead of immediately/"
,,Realm,"Make all methods on RealmObject and all classes in public API final (#2675)


We should mark all methods that should not be overridden final.
We should mark all classes that should not be inherited final./"
,,Realm,"Implementation of SharedGroup::wait_for_change() method. (#2386)

Goal
1. Add BaseRealm.waitForChange() which blocks until a change is available, then advance the Realm and continue without triggering RealmChangeListeners.
2. Add BaseRealm.stopWaitForChange() that makes any current waitForChange() return false immediately.
3. Deprecated BaseRealm.refresh().

Behaviors
1. waitForChange() will throw IllegalStateException within a transaction.
2. BaseRealm cannot wait in a thread with Looper. IllegalStateException will be thrown.
3. Once stopWaitForChange is called, all future calls to waitForChange will immediately return false.

For detailed discussions, please refer https://github.com/realm/realm-wiki/wiki/Java-Multi-threaded-wait_for_change/#1594 RealmChangeListener should provide the changes object/realm/collection as well (#2705)

RealmChangeListener should provide the changes object/realm/collection as well/RealmCollection iterators are now stable (#2124)

This commit changes the semantics of how live RealmResults really are. Before this commit
RealmResults where live _all the time_, which meant that code like the below didn't work as expected (only half the elements would be deleted):

```
for (int i = 0; i < results.size(); i++) {
   results.get(i).deleteFromRealm();
}

```

The rather unintuitive work-around was counting backwards:

```
for (int i = results.size() -1; i >=0; i--) {
   results.get(i).deleteFromRealm();
}
```

This commit changes the behaviour of RealmResults, so they no longer are automatically up to date. They are instead refreshed using Looper events just like changes from other threads.

RealmList iterators are not live in the same way that RealmResults are but act more like standard ArrayLists, so they don't suffer from the same liveness issues.

The high-level arguments for doing this change was:

Pros:
* All iterators now work as you would normally expect (ease of use).
* The liveness of Realm objects are now all controlled through the Looper (simpler internal code / easier to document Realm behaviour).

Cons:
* There is a small chance that people can accidentally reference a deleted RealmObject in a RealmResult. (Can be checked through use of `RealmObject.isValid()`)/Interface as supplement to extending RealmObject (#2599)

* Backup

* Add RealmModel

* Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests

* fixing examples

* WIP

* fix Annotation processor

* clean up

* fix javadoc

* add transformer tests & fix DynamicRealm tests

* fixing tests

* adding tests

* adding tests

* fixing tests for the realm-annotations-processor

* add tests to cover more UC for RealmList

* renamed POJO to RealmModel

* reduce method count by using proxyState only in RealmObjectProxy

* fix examples, no need to exclude RealmObject using Gson https://realm.io/docs/java/latest/#gson

* making RealmList final, remove unused method & fixing APT test/migrateRealm throws when db doesn't exsit

Close #2316 ./Thread local notifactions are now triggered using the Looper instead of immediately/"
,,Realm,"RealmCollection iterators are now stable (#2124)

This commit changes the semantics of how live RealmResults really are. Before this commit
RealmResults where live _all the time_, which meant that code like the below didn't work as expected (only half the elements would be deleted):

```
for (int i = 0; i < results.size(); i++) {
   results.get(i).deleteFromRealm();
}

```

The rather unintuitive work-around was counting backwards:

```
for (int i = results.size() -1; i >=0; i--) {
   results.get(i).deleteFromRealm();
}
```

This commit changes the behaviour of RealmResults, so they no longer are automatically up to date. They are instead refreshed using Looper events just like changes from other threads.

RealmList iterators are not live in the same way that RealmResults are but act more like standard ArrayLists, so they don't suffer from the same liveness issues.

The high-level arguments for doing this change was:

Pros:
* All iterators now work as you would normally expect (ease of use).
* The liveness of Realm objects are now all controlled through the Looper (simpler internal code / easier to document Realm behaviour).

Cons:
* There is a small chance that people can accidentally reference a deleted RealmObject in a RealmResult. (Can be checked through use of `RealmObject.isValid()`)/Interface as supplement to extending RealmObject (#2599)

* Backup

* Add RealmModel

* Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests

* fixing examples

* WIP

* fix Annotation processor

* clean up

* fix javadoc

* add transformer tests & fix DynamicRealm tests

* fixing tests

* adding tests

* adding tests

* fixing tests for the realm-annotations-processor

* add tests to cover more UC for RealmList

* renamed POJO to RealmModel

* reduce method count by using proxyState only in RealmObjectProxy

* fix examples, no need to exclude RealmObject using Gson https://realm.io/docs/java/latest/#gson

* making RealmList final, remove unused method & fixing APT test/Added RealmCollection API's/More checkings when modify RealmList

Fix #2381

* New method LinkView.getTargetTable().
* Proper checking to add DynamicRealmObject to RealmList.
* Disallow modifying with RealmObject belongs to another Realm instance./"
,,Realm,"Replace setModules() with modules() (#2621)

This commit deprecates RealmConfiguration.setModules() in favour of RealmConfiguration.modules(). The reason being using the setter terminology is not common in builders. See Effective Java item 2./Interface as supplement to extending RealmObject (#2599)

* Backup

* Add RealmModel

* Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests

* fixing examples

* WIP

* fix Annotation processor

* clean up

* fix javadoc

* add transformer tests & fix DynamicRealm tests

* fixing tests

* adding tests

* adding tests

* fixing tests for the realm-annotations-processor

* add tests to cover more UC for RealmList

* renamed POJO to RealmModel

* reduce method count by using proxyState only in RealmObjectProxy

* fix examples, no need to exclude RealmObject using Gson https://realm.io/docs/java/latest/#gson

* making RealmList final, remove unused method & fixing APT test/move RealmConfiguration.getSchemaMediator() to package private./"
,,Realm,"RealmCollection iterators are now stable (#2124)

This commit changes the semantics of how live RealmResults really are. Before this commit
RealmResults where live _all the time_, which meant that code like the below didn't work as expected (only half the elements would be deleted):

```
for (int i = 0; i < results.size(); i++) {
   results.get(i).deleteFromRealm();
}

```

The rather unintuitive work-around was counting backwards:

```
for (int i = results.size() -1; i >=0; i--) {
   results.get(i).deleteFromRealm();
}
```

This commit changes the behaviour of RealmResults, so they no longer are automatically up to date. They are instead refreshed using Looper events just like changes from other threads.

RealmList iterators are not live in the same way that RealmResults are but act more like standard ArrayLists, so they don't suffer from the same liveness issues.

The high-level arguments for doing this change was:

Pros:
* All iterators now work as you would normally expect (ease of use).
* The liveness of Realm objects are now all controlled through the Looper (simpler internal code / easier to document Realm behaviour).

Cons:
* There is a small chance that people can accidentally reference a deleted RealmObject in a RealmResult. (Can be checked through use of `RealmObject.isValid()`)/Interface as supplement to extending RealmObject (#2599)

* Backup

* Add RealmModel

* Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests

* fixing examples

* WIP

* fix Annotation processor

* clean up

* fix javadoc

* add transformer tests & fix DynamicRealm tests

* fixing tests

* adding tests

* adding tests

* fixing tests for the realm-annotations-processor

* add tests to cover more UC for RealmList

* renamed POJO to RealmModel

* reduce method count by using proxyState only in RealmObjectProxy

* fix examples, no need to exclude RealmObject using Gson https://realm.io/docs/java/latest/#gson

* making RealmList final, remove unused method & fixing APT test/Added RealmCollection API's/Adapt core fix for the delete RealmList

Update core to 0.96.1.

No more exception will be thrown when access RealmResults if the base
RealmList has be deleted. Instead, it will be treated as an empty
RealmResults forever./"
,,Realm,"Handle PK when calling RealmSchema's remove/rename (#2663)

* Handle PK when calling RealmSchema's remove/rename

* RealmSchema.remove() should remove the field from PK table.
* RealmSchema.rename() should change the corresponding row in the PK
  table.

Fix #2555/Interface as supplement to extending RealmObject (#2599)

* Backup

* Add RealmModel

* Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests

* fixing examples

* WIP

* fix Annotation processor

* clean up

* fix javadoc

* add transformer tests & fix DynamicRealm tests

* fixing tests

* adding tests

* adding tests

* fixing tests for the realm-annotations-processor

* add tests to cover more UC for RealmList

* renamed POJO to RealmModel

* reduce method count by using proxyState only in RealmObjectProxy

* fix examples, no need to exclude RealmObject using Gson https://realm.io/docs/java/latest/#gson

* making RealmList final, remove unused method & fixing APT test/More checkings when modify RealmList

Fix #2381

* New method LinkView.getTargetTable().
* Proper checking to add DynamicRealmObject to RealmList.
* Disallow modifying with RealmObject belongs to another Realm instance./"
,,Realm,"Adapt core fix for the delete RealmList

Update core to 0.96.1.

No more exception will be thrown when access RealmResults if the base
RealmList has be deleted. Instead, it will be treated as an empty
RealmResults forever./"
Feature migration,Feature migration,Realm,"Expanding time resolution into millisecs (#2679)

Expanding time resolution into milliseconds but using core's new column type Timestamp. An upgrade to core version 0.100.0 is required, and some minor updates are coming from some changes in the core API./Upgrading to Realm Core 0.97.0/"
Feature migration,Feature migration,Realm,"Expanding time resolution into millisecs (#2679)

Expanding time resolution into milliseconds but using core's new column type Timestamp. An upgrade to core version 0.100.0 is required, and some minor updates are coming from some changes in the core API./@PrimaryKey in String, Byte, Short, Integer, and Long types can be null. (#2634)

@PrimaryKey is allowed to be null for String and Boxed primitive types.
Features part of this commit are

1. A @PrimaryKey annotated field in java.lang.String/Byte/Short/Integer/Long type can be null.
2. The object with nil primary key can be updated.
3. Migration checks if existing nullable type @PrimaryKey is set to be nullable in existing file. If not, throws RealmMigrationNeeded.
4. Duplicated null as PrimaryKey throws RealmPrimaryKeyConstraintException.
5. When a new empty row is added, the default value checked for duplicated is still the same empty string ("""") at Table.addEmptyRow()
6. Since there isn't equivalent term for Optional in Java 7 yet, nullable PK types are configured as following in this PR.

                       | String PK | Number PK | Primitive PK |
------------------------------------------------------------------------
Table Column | Nullable   | Nullable	       | Not Nullable |/fix the String from DynamicRealmObject.toString().

Now DynamicRealmObject.toString() shows null value as ""null"".
""class_"" prefix is removed from type name in that String.
Align the format to the String from typed RealmObject./"
,,Realm,optimize imports/
,,Realm,Upgrading to Realm Core 0.97.0/
,,Realm,"Interface as supplement to extending RealmObject (#2599)

* Backup

* Add RealmModel

* Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests

* fixing examples

* WIP

* fix Annotation processor

* clean up

* fix javadoc

* add transformer tests & fix DynamicRealm tests

* fixing tests

* adding tests

* adding tests

* fixing tests for the realm-annotations-processor

* add tests to cover more UC for RealmList

* renamed POJO to RealmModel

* reduce method count by using proxyState only in RealmObjectProxy

* fix examples, no need to exclude RealmObject using Gson https://realm.io/docs/java/latest/#gson

* making RealmList final, remove unused method & fixing APT test/Add a check whether transformer was applied or not.

This commit adds default implementation of `transformerApplied()` to `RealmProxyMediator` class which returns `false`.
And then Realm transformer adds overriding method to its subclasses and those methods returns `true`.

If Transformer was applied, `RealmProxyMediator.transformerApplied()` returns `true` and
if not, `RealmProxyMediator.transformerApplied()` returns `false`.
That enable us to detect Realm transformer was applied or not at runtime./"
,,Realm,"#1594 RealmChangeListener should provide the changes object/realm/collection as well (#2705)

RealmChangeListener should provide the changes object/realm/collection as well/Interface as supplement to extending RealmObject (#2599)

* Backup

* Add RealmModel

* Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests

* fixing examples

* WIP

* fix Annotation processor

* clean up

* fix javadoc

* add transformer tests & fix DynamicRealm tests

* fixing tests

* adding tests

* adding tests

* fixing tests for the realm-annotations-processor

* add tests to cover more UC for RealmList

* renamed POJO to RealmModel

* reduce method count by using proxyState only in RealmObjectProxy

* fix examples, no need to exclude RealmObject using Gson https://realm.io/docs/java/latest/#gson

* making RealmList final, remove unused method & fixing APT test/Added RealmCollection API's/Only load RealmObservableFactory when RxJava exist

Close #1990.

* Check RxJava existence only once.
* Don't load RealmObservableFactory if RxJava doesn't exist.

NOTE: When reflection called on the RealmObject/RealmResults, crash will
still happen because of Observable doesn't exist in the class path. In
such a case, a dummy rx.Observable is still needed as a workaround./allow 'realm' and 'row' as a field name of model class. This fixes #2255/follow lint warnings/"
,,Realm,"Implementation of SharedGroup::wait_for_change() method. (#2386)

Goal
1. Add BaseRealm.waitForChange() which blocks until a change is available, then advance the Realm and continue without triggering RealmChangeListeners.
2. Add BaseRealm.stopWaitForChange() that makes any current waitForChange() return false immediately.
3. Deprecated BaseRealm.refresh().

Behaviors
1. waitForChange() will throw IllegalStateException within a transaction.
2. BaseRealm cannot wait in a thread with Looper. IllegalStateException will be thrown.
3. Once stopWaitForChange is called, all future calls to waitForChange will immediately return false.

For detailed discussions, please refer https://github.com/realm/realm-wiki/wiki/Java-Multi-threaded-wait_for_change/"
,,Realm,"Make all methods on RealmObject and all classes in public API final (#2675)


We should mark all methods that should not be overridden final.
We should mark all classes that should not be inherited final./Add getPrimaryKey() method to RealmObjectSchema (#2648)

Without this method it was necessary to iterator over all fields manually to find the primary key using isPrimaryKey(field)./@PrimaryKey in String, Byte, Short, Integer, and Long types can be null. (#2634)

@PrimaryKey is allowed to be null for String and Boxed primitive types.
Features part of this commit are

1. A @PrimaryKey annotated field in java.lang.String/Byte/Short/Integer/Long type can be null.
2. The object with nil primary key can be updated.
3. Migration checks if existing nullable type @PrimaryKey is set to be nullable in existing file. If not, throws RealmMigrationNeeded.
4. Duplicated null as PrimaryKey throws RealmPrimaryKeyConstraintException.
5. When a new empty row is added, the default value checked for duplicated is still the same empty string ("""") at Table.addEmptyRow()
6. Since there isn't equivalent term for Optional in Java 7 yet, nullable PK types are configured as following in this PR.

                       | String PK | Number PK | Primitive PK |
------------------------------------------------------------------------
Table Column | Nullable   | Nullable	       | Not Nullable |/Fixed wrong method name in exception

Fixed wrong method name in exception/Added RealmCollection API's/Add RealmObjectSchema.isPrimaryKey()

* Add RealmObjectSchema.isPrimaryKey(). RealmObjectSchema.getPrimaryKey()
  is not added since it doesn't seem to be necessary.
* isNullable & isRequired didn't throw which is wrong.
* Add throws doc to hasIndex.

Close #2440/"
,,Realm,"Give async related vars better names

asyncQueryExecutor and pendingQuery seem to be very confusing since they
are used for async transaction as well./Deprecated 3 field sort in Realm and RealmQuery (#2619)/RealmCollection iterators are now stable (#2124)

This commit changes the semantics of how live RealmResults really are. Before this commit
RealmResults where live _all the time_, which meant that code like the below didn't work as expected (only half the elements would be deleted):

```
for (int i = 0; i < results.size(); i++) {
   results.get(i).deleteFromRealm();
}

```

The rather unintuitive work-around was counting backwards:

```
for (int i = results.size() -1; i >=0; i--) {
   results.get(i).deleteFromRealm();
}
```

This commit changes the behaviour of RealmResults, so they no longer are automatically up to date. They are instead refreshed using Looper events just like changes from other threads.

RealmList iterators are not live in the same way that RealmResults are but act more like standard ArrayLists, so they don't suffer from the same liveness issues.

The high-level arguments for doing this change was:

Pros:
* All iterators now work as you would normally expect (ease of use).
* The liveness of Realm objects are now all controlled through the Looper (simpler internal code / easier to document Realm behaviour).

Cons:
* There is a small chance that people can accidentally reference a deleted RealmObject in a RealmResult. (Can be checked through use of `RealmObject.isValid()`)/Interface as supplement to extending RealmObject (#2599)

* Backup

* Add RealmModel

* Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests

* fixing examples

* WIP

* fix Annotation processor

* clean up

* fix javadoc

* add transformer tests & fix DynamicRealm tests

* fixing tests

* adding tests

* adding tests

* fixing tests for the realm-annotations-processor

* add tests to cover more UC for RealmList

* renamed POJO to RealmModel

* reduce method count by using proxyState only in RealmObjectProxy

* fix examples, no need to exclude RealmObject using Gson https://realm.io/docs/java/latest/#gson

* making RealmList final, remove unused method & fixing APT test/RealmQuery.distinctAsync(), RealmResults.distinctAsync() support./"
,,Realm,"Move all query methods to RealmQuery (#2620)

Currently we have the following helper query methods on Realm/DynamicRealm: allObjects() and distinct()

However we have to be mindful about the number of methods in our API and from the projects we have seen so far, these methods does not seem to have been used much.

Moving all query methods to RealmQuery has two advantages: 1) It helps us to reduce our overall method count and 2) It makes our API more consistent as now all RealmQuery methods are only found on RealmQuery./Added RealmCollection API's/Multi-arguments distinct(...) for Realm, DynamicRealm, RealmQuery, and RealmResults/RealmQuery.distinctAsync(), RealmResults.distinctAsync() support./"
,,Realm,"#1594 RealmChangeListener should provide the changes object/realm/collection as well (#2705)

RealmChangeListener should provide the changes object/realm/collection as well/Interface as supplement to extending RealmObject (#2599)

* Backup

* Add RealmModel

* Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests

* fixing examples

* WIP

* fix Annotation processor

* clean up

* fix javadoc

* add transformer tests & fix DynamicRealm tests

* fixing tests

* adding tests

* adding tests

* fixing tests for the realm-annotations-processor

* add tests to cover more UC for RealmList

* renamed POJO to RealmModel

* reduce method count by using proxyState only in RealmObjectProxy

* fix examples, no need to exclude RealmObject using Gson https://realm.io/docs/java/latest/#gson

* making RealmList final, remove unused method & fixing APT test/fix the String from DynamicRealmObject.toString().

Now DynamicRealmObject.toString() shows null value as ""null"".
""class_"" prefix is removed from type name in that String.
Align the format to the String from typed RealmObject./More checkings when modify RealmList

Fix #2381

* New method LinkView.getTargetTable().
* Proper checking to add DynamicRealmObject to RealmList.
* Disallow modifying with RealmObject belongs to another Realm instance./Correct checking in DynamicRealmObject.setList

* Fix #2368
* Check equality of Realm in DynamicRealmObject setters to avoid risks
  mentioned 2) in #2382/follow lint warnings/"
,,Realm,"Give async related vars better names

asyncQueryExecutor and pendingQuery seem to be very confusing since they
are used for async transaction as well./Move all query methods to RealmQuery (#2620)

Currently we have the following helper query methods on Realm/DynamicRealm: allObjects() and distinct()

However we have to be mindful about the number of methods in our API and from the projects we have seen so far, these methods does not seem to have been used much.

Moving all query methods to RealmQuery has two advantages: 1) It helps us to reduce our overall method count and 2) It makes our API more consistent as now all RealmQuery methods are only found on RealmQuery./Deprecated 3 field sort in Realm and RealmQuery (#2619)/Make Realm.createObject(Class,PrimaryKey) public (#2622)

This method is a way to solve the problem that people might have data with a primary key that is is the default value for that type, e.g a long primary key with the value 0.

Once this object is saved in Realm any calls to the normal Realm.createObject() would throw an exception as it would try to assign the default value (zero) to the object before the user could set it to something else. With this method that is no longer a problem.

Internally we already use this method and it is already public in the Dynamic API. The reason for not making this public earlier was due to fear of mis-use since there is no type-safe guarantee at compile time. However since it is already public in the Dynamic API and we haven't seen any indication of that being misused we feel it should be safe to make this public. Also our Query API is also semi-threadsafe so there is precedence there as well./Only throw RealmException if absolutely necessary (#2618)

We should avoid wrapping lower level exceptions in RealmExceptions unless there is a good reason for it. We had multiple support issues where people were confused about the RealmException and didn't understand they had to dig through the stack trace for the original exception.

This change alters the behaviour of all our JSON methods so they now only convert JSONException to RealmException in order to prevent checked exceptions to reach the user. All other exceptions should be thrown directly./Interface as supplement to extending RealmObject (#2599)

* Backup

* Add RealmModel

* Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests

* fixing examples

* WIP

* fix Annotation processor

* clean up

* fix javadoc

* add transformer tests & fix DynamicRealm tests

* fixing tests

* adding tests

* adding tests

* fixing tests for the realm-annotations-processor

* add tests to cover more UC for RealmList

* renamed POJO to RealmModel

* reduce method count by using proxyState only in RealmObjectProxy

* fix examples, no need to exclude RealmObject using Gson https://realm.io/docs/java/latest/#gson

* making RealmList final, remove unused method & fixing APT test/Added RealmConfiguration.initialData() (#2602)/Remove Realm.getTable() from public API

Close #2546/Added RealmCollection API's/migrateRealm throws when db doesn't exsit

Close #2316 ./follow lint warnings/Multi-arguments distinct(...) for Realm, DynamicRealm, RealmQuery, and RealmResults/Thread local notifactions are now triggered using the Looper instead of immediately/RealmQuery.distinctAsync(), RealmResults.distinctAsync() support./"
,,Realm,"Interface as supplement to extending RealmObject (#2599)

* Backup

* Add RealmModel

* Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests

* fixing examples

* WIP

* fix Annotation processor

* clean up

* fix javadoc

* add transformer tests & fix DynamicRealm tests

* fixing tests

* adding tests

* adding tests

* fixing tests for the realm-annotations-processor

* add tests to cover more UC for RealmList

* renamed POJO to RealmModel

* reduce method count by using proxyState only in RealmObjectProxy

* fix examples, no need to exclude RealmObject using Gson https://realm.io/docs/java/latest/#gson

* making RealmList final, remove unused method & fixing APT test/Added RealmCollection API's/"
,,Realm,RealmResults and RealmObjects are no longer accidentally GC'ed while their Observable is still alive./Realm Observables now holds a Realm instance until unsubscribed/fixed typo/
,,Realm,"Give async related vars better names

asyncQueryExecutor and pendingQuery seem to be very confusing since they
are used for async transaction as well./RealmCollection iterators are now stable (#2124)

This commit changes the semantics of how live RealmResults really are. Before this commit
RealmResults where live _all the time_, which meant that code like the below didn't work as expected (only half the elements would be deleted):

```
for (int i = 0; i < results.size(); i++) {
   results.get(i).deleteFromRealm();
}

```

The rather unintuitive work-around was counting backwards:

```
for (int i = results.size() -1; i >=0; i--) {
   results.get(i).deleteFromRealm();
}
```

This commit changes the behaviour of RealmResults, so they no longer are automatically up to date. They are instead refreshed using Looper events just like changes from other threads.

RealmList iterators are not live in the same way that RealmResults are but act more like standard ArrayLists, so they don't suffer from the same liveness issues.

The high-level arguments for doing this change was:

Pros:
* All iterators now work as you would normally expect (ease of use).
* The liveness of Realm objects are now all controlled through the Looper (simpler internal code / easier to document Realm behaviour).

Cons:
* There is a small chance that people can accidentally reference a deleted RealmObject in a RealmResult. (Can be checked through use of `RealmObject.isValid()`)/Interface as supplement to extending RealmObject (#2599)

* Backup

* Add RealmModel

* Modifying the annotation processor & byte code modification to use RealmModel, add UnitTests

* fixing examples

* WIP

* fix Annotation processor

* clean up

* fix javadoc

* add transformer tests & fix DynamicRealm tests

* fixing tests

* adding tests

* adding tests

* fixing tests for the realm-annotations-processor

* add tests to cover more UC for RealmList

* renamed POJO to RealmModel

* reduce method count by using proxyState only in RealmObjectProxy

* fix examples, no need to exclude RealmObject using Gson https://realm.io/docs/java/latest/#gson

* making RealmList final, remove unused method & fixing APT test/Fixed race condition causing BadVersionException/Thread local notifactions are now triggered using the Looper instead of immediately/"
API Management,API Management,Realm,"Expanding time resolution into millisecs (#2679)

Expanding time resolution into milliseconds but using core's new column type Timestamp. An upgrade to core version 0.100.0 is required, and some minor updates are coming from some changes in the core API./RealmCollection iterators are now stable (#2124)

This commit changes the semantics of how live RealmResults really are. Before this commit
RealmResults where live _all the time_, which meant that code like the below didn't work as expected (only half the elements would be deleted):

```
for (int i = 0; i < results.size(); i++) {
   results.get(i).deleteFromRealm();
}

```

The rather unintuitive work-around was counting backwards:

```
for (int i = results.size() -1; i >=0; i--) {
   results.get(i).deleteFromRealm();
}
```

This commit changes the behaviour of RealmResults, so they no longer are automatically up to date. They are instead refreshed using Looper events just like changes from other threads.

RealmList iterators are not live in the same way that RealmResults are but act more like standard ArrayLists, so they don't suffer from the same liveness issues.

The high-level arguments for doing this change was:

Pros:
* All iterators now work as you would normally expect (ease of use).
* The liveness of Realm objects are now all controlled through the Looper (simpler internal code / easier to document Realm behaviour).

Cons:
* There is a small chance that people can accidentally reference a deleted RealmObject in a RealmResult. (Can be checked through use of `RealmObject.isValid()`)/Multi-arguments distinct(...) for Realm, DynamicRealm, RealmQuery, and RealmResults/"
API Management,API Management,Realm,"Expanding time resolution into millisecs (#2679)

Expanding time resolution into milliseconds but using core's new column type Timestamp. An upgrade to core version 0.100.0 is required, and some minor updates are coming from some changes in the core API./"
API Management,API Management,Realm,"Expanding time resolution into millisecs (#2679)

Expanding time resolution into milliseconds but using core's new column type Timestamp. An upgrade to core version 0.100.0 is required, and some minor updates are coming from some changes in the core API./@PrimaryKey in String, Byte, Short, Integer, and Long types can be null. (#2634)

@PrimaryKey is allowed to be null for String and Boxed primitive types.
Features part of this commit are

1. A @PrimaryKey annotated field in java.lang.String/Byte/Short/Integer/Long type can be null.
2. The object with nil primary key can be updated.
3. Migration checks if existing nullable type @PrimaryKey is set to be nullable in existing file. If not, throws RealmMigrationNeeded.
4. Duplicated null as PrimaryKey throws RealmPrimaryKeyConstraintException.
5. When a new empty row is added, the default value checked for duplicated is still the same empty string ("""") at Table.addEmptyRow()
6. Since there isn't equivalent term for Optional in Java 7 yet, nullable PK types are configured as following in this PR.

                       | String PK | Number PK | Primitive PK |
------------------------------------------------------------------------
Table Column | Nullable   | Nullable	       | Not Nullable |/"
,Thread Management,Realm,Fixed race condition causing BadVersionException/Upgrading to Realm Core 0.97.0/fixes #2115 IllegalStateException Caller thread behind the worker thread/
,Thread Management,Realm,"Implementation of SharedGroup::wait_for_change() method. (#2386)

Goal
1. Add BaseRealm.waitForChange() which blocks until a change is available, then advance the Realm and continue without triggering RealmChangeListeners.
2. Add BaseRealm.stopWaitForChange() that makes any current waitForChange() return false immediately.
3. Deprecated BaseRealm.refresh().

Behaviors
1. waitForChange() will throw IllegalStateException within a transaction.
2. BaseRealm cannot wait in a thread with Looper. IllegalStateException will be thrown.
3. Once stopWaitForChange is called, all future calls to waitForChange will immediately return false.

For detailed discussions, please refer https://github.com/realm/realm-wiki/wiki/Java-Multi-threaded-wait_for_change/Upgrading to Realm Core 0.97.0/"
Feature migration,Feature Migration,Realm,Removed all deprecated methods and fixed examples./
,,Realm,"Examples should use transaction blocks (#2800)

* Moving examples over to execute transaction
* Updating comments to use executeTransaction
* Use partial mocks for Realm#executeTransaction(...) and create additional tests to illustrate mock verification of Realm#executeTransaction(...).
Had to do this because of a problem with Powermock: https://github.com/jayway/powermock/issues/649/"
,,Realm,"Removed all deprecated methods and fixed examples./Examples should use transaction blocks (#2800)

* Moving examples over to execute transaction
* Updating comments to use executeTransaction
* Use partial mocks for Realm#executeTransaction(...) and create additional tests to illustrate mock verification of Realm#executeTransaction(...).
Had to do this because of a problem with Powermock: https://github.com/jayway/powermock/issues/649/"
,,Realm,"Merge commit 'ecbacf' into merge-ecbacf-to-master/Use qualified name for model classes in generated code in order to avoid name conflict. (#3083)

* use qualified name for model classes in generated code in order to avoid name conflict.

* add model classes that test issue 3077.

* update changelog for #3077/bulk inserts (#2999)

Added `insert(RealmModel obj)`, `insertOrUpdate(RealmModel obj)`, `insert(Collection<RealmModel> collection)` and `insertOrUpdate(Collection<RealmModel> collection)` to perform batch inserts (#1684)./Objects not in Realm are now called unmananged everywhere. (#2828)/"
,,Realm,"Use qualified name for model classes in generated code in order to avoid name conflict. (#3083)

* use qualified name for model classes in generated code in order to avoid name conflict.

* add model classes that test issue 3077.

* update changelog for #3077/bulk inserts (#2999)

Added `insert(RealmModel obj)`, `insertOrUpdate(RealmModel obj)`, `insert(Collection<RealmModel> collection)` and `insertOrUpdate(Collection<RealmModel> collection)` to perform batch inserts (#1684)./"
Feature migration,Feature Migration,Realm,"Improved detection when using interfaces in Realm classes (#2821)

This commits adds check to the annotation processor so we detects if people try to use interfaces that extends RealmModel as a Generic to RealmList or a single object reference. Until we support polymorphism, you can only support reference concrete types./"
,,Realm,"Use qualified name for model classes in generated code in order to avoid name conflict. (#3083)

* use qualified name for model classes in generated code in order to avoid name conflict.

* add model classes that test issue 3077.

* update changelog for #3077/"
,,Realm,"insert(): Correctly detect multiple objects with the same primary key  (#3239)/Add quotes around terms in exceptions message (#3221)

Close #3217/Merge commit 'ecbacf' into merge-ecbacf-to-master/Use qualified name for model classes in generated code in order to avoid name conflict. (#3083)

* use qualified name for model classes in generated code in order to avoid name conflict.

* add model classes that test issue 3077.

* update changelog for #3077/bulk inserts (#2999)

Added `insert(RealmModel obj)`, `insertOrUpdate(RealmModel obj)`, `insert(Collection<RealmModel> collection)` and `insertOrUpdate(Collection<RealmModel> collection)` to perform batch inserts (#1684)./Objects not in Realm are now called unmananged everywhere. (#2828)/"
,,Realm,"Merge commit 'ecbacf' into merge-ecbacf-to-master/Use qualified name for model classes in generated code in order to avoid name conflict. (#3083)

* use qualified name for model classes in generated code in order to avoid name conflict.

* add model classes that test issue 3077.

* update changelog for #3077/bulk inserts (#2999)

Added `insert(RealmModel obj)`, `insertOrUpdate(RealmModel obj)`, `insert(Collection<RealmModel> collection)` and `insertOrUpdate(Collection<RealmModel> collection)` to perform batch inserts (#1684)./"
,,Realm,"Annotation processor no longer consume annotations (#3216)

This commit changes the Realm annotation processor so it no longer consumes the annotations. The only effect this has is that it is now possible to write multiple annotation processors for the same annotation.

It is not possible to specify the order of annotation processors from two different libraries, so if one of the processors consumed the annotation it would be random if both ran or only one of them./"
,,Realm,More GCed ref for flaky test (#3135)/
,,Realm,Asset file as an initial dataset (#2692)/
,Thread Management,Realm,"More GCed ref for flaky test (#3135)/Objects not in Realm are now called unmananged everywhere. (#2828)/add a null check to addChangeListener and removeChangeListener (#2805)

* add a null check to addChangeListener and removeChangeListener in Realm and DynamicRealm (fixes #2772)

* reflect review comments/Fixed unit tests./thread safe HandlerController#emptyAsyncRealmObject & realmObjects  (#2761)

* making HandlerController#emptyAsyncRealmObject & HandlerController#realmObjects thread safe/"
,Thread Management,Realm,"More GCed ref for flaky test (#3135)/Forward all throwables but not only excpetions (#3126)

For example the UnsatisfiedLinkError should cause the process terminated.
This is the reason of all the time out issues we found for CI./Fix async transaction problem when async query already exist. (#2780)

I have rewritten how async transactions works. Before they did this:

1) commitOnBackgroundThread() -> post REALM_CHANGED -> post onSuccess runnable

The problem with that approach was that the REALM_CHANGED would be swallowed if async queries existed which meant that the async transaction would call onSuccess on an old version of the Realm (making it look like it didn't work).

Instead we now do this:

2) commitOnBackgroundThread -> post runnable that calls HandlerController.handleAsyncTransactionCompleted(runnable)

The special runnable is treated as a combined REALM_CHANGED + Callback, which makes it possible for us to queue up callbacks until we are finally able to trigger all of them.

Unfortunately the Handler has poor support for this so it means that we no longer can detect if such a message is in the queue. This means that it introduces a slight chance of such a event plus a real REALM_CHANGED event to be in the message queue at the same time. I considered that acceptable (since it can already happen today), and since preventing this will introduce more complexity to something that is already entirely to complex./Fixed unit tests./thread safe HandlerController#emptyAsyncRealmObject & realmObjects  (#2761)

* making HandlerController#emptyAsyncRealmObject & HandlerController#realmObjects thread safe/"
,,Realm,Fixed unit tests./
,,Realm,"Remove unused mixed type (#3041)

* Remove unused mixed type/"
,,Realm,More GCed ref for flaky test (#3135)/Fixed unit tests./
,,Realm,"RealmObjectSchema.setClassName() transfers primary key for new class name (#3149)

RealmObjectSchema.setClassName() transfers a primary key for a new class name that the renamed class can maintain the old primary key./Unit tests for PrimaryKey field type change (#3076)

Unit tests for checking proper migration of PrimaryKey field type change from String to Integer/int./ PrimaryKey index rearrangement in migration revisited  (#2920)

This PR fixes two issues; 
1) When a column with a smaller index than that of a primary key field gets removed, the primary key field index decrements, which causes a cached primary key index to point a wrong field.
2) When a primary key field is renamed, the primary key meta table is not updated accordingly./addPrimaryKey() on RealmObjectSchema misses Index addition. (#2832)

RealmObjectSchema.addPrimaryKey() is supposed to add a search index to its primary key field but it is currently missing. This is to fix the addition and removal of a search index related to primary key methods in RealmObjectSchema./"
,,Realm,More GCed ref for flaky test (#3135)/GCed ref causes flaky test (#3131)/
,Thread Management,Realm,"Disable change listeners on IntentService threads (#3232)/Race condition between Realm change notifications and UI events (#2990)

This commit fixes a bug surfaced by our Realm Android Adapter example. Before this commit we posted all REALM_CHANGED events to the looper queue as normal. This meant that potential touch input events could get in front which in turn could cause a relayout of a ListView which would then crash because it detected a difference between it's cached adapter size and the actual one.

This commit now forces all local commits to be placed at the front of the queue eliminating that risk./"
,,Realm,"Unit test cleanup and parameterization for PrimaryKey (#2815)

The issue #2658 is addressed with adding more coverage and cleanup for PrimaryKey related unit tests.

1. With an exception, all the null primarykey tests in RealmTests are now parameterized.
2. Parameters in RealmJsonNullPrimaryKeyTests are cleaned up.
3. Realm.createObject(class, primaryKeyValue) is tested with null in positive and negative manner.
4. NullPrimaryKey interface is expanded to setters as well./"
Feature migration,Feature migration,Realm,"More GCed ref for flaky test (#3135)/Optimistic opening of a Realm file (#3013)

We have anecdotal evidence that multiple processes might be open for the same app. We have not been able to verify this, but the result matches the ""IncompatibleLockFile"" errors reported in #2459. This happens if the two processes have two versions of Realm (e.g. during app upgrades).

This PR is based on our assumption that such an overlap is not intentional and only happens because one process is started before the previous was completely shut down.

So this PR introduces an optimistic opening scheme where we retry for 3 second before crashing as before./Builder constructor takes custom directory path and context (#3005)

Enabled ReLinker when loading a Realm from a custom path by adding a RealmConfiguration.Builder(Context, File) constructor./add a null check to addChangeListener and removeChangeListener (#2805)

* add a null check to addChangeListener and removeChangeListener in Realm and DynamicRealm (fixes #2772)

* reflect review comments/"
,,Realm,"Add RealmQuery.in() (#3133)

* Add RealmQuery.equalToAny() (#841)

* PR feedback: equalToAny -> in

* PR feedback: apply 'beginGroup()' and 'endGroup()' on 'in' operator.

@zaki50: We should use beginGroup() and endGroup() since equalTo(""a"")
and in(""b"", ""c"", ""d"") does not equal to equalTo(""a"") and equalTo(""b"") or
equalTo(""c"") or equalTo(""d"").

* PR feedback: throws an IllegalArgumentException if values is an empty array.

* Add a test, in_date()

* Remove unnecessary tests.

* Fix type fields in in_byte()

* Add 3 tests, in_byte(), in_short() and in_int()

* Update CHANGELOG.md for RealmQuery.in()

* PR feedback: array version -> vargs version.

* PR feedback: ++i -> i++

* PR feedback: fix javadocs.

* PR feedback: remove unnecessary parenthesises.

* PR feedback: revert the variable arguments version.

* PR feedback: fix javadocs and improve error messages.

* PR feedback: Extract the constant string literal from the hard-coded error message.

* PR feedback: Adopt boxed type API.

* PR feedback: add null tests.

* Fix row_isValid

* Fix openPreNullWithRequired

* PR feedback: update javadocs.

* PR feedback: use NoPrimaryKeyNullTypes for testing instead of AllTypes.

* PR feedback: Revert AllTypes.

* Revert automatic formatting.

* Revert auto-formatting again.

* PR feedback: remove unnecessay empty lines.

* PR feedback: generalize tests.

* PR feedback: update javadocs./More GCed ref for flaky test (#3135)/Fixed unit tests./"
,Thread Management,Realm,"Disable change listeners on IntentService threads (#3232)/More GCed ref for flaky test (#3135)/Race condition between Realm change notifications and UI events (#2990)

This commit fixes a bug surfaced by our Realm Android Adapter example. Before this commit we posted all REALM_CHANGED events to the looper queue as normal. This meant that potential touch input events could get in front which in turn could cause a relayout of a ListView which would then crash because it detected a difference between it's cached adapter size and the actual one.

This commit now forces all local commits to be placed at the front of the queue eliminating that risk./Fix listeners can exposing unsynchronized RealmResults (Async queries) (#2951)

When async queries are updated, they call listeners in a specific order. Before this commit this had a very subtle bug, so accessing synchronous RealmResults from inside a async RealmResult listener might hit a detached row accessor.

Reason being is that the Realm is advanced, then all listeners called in order, but RealmResults are not synchronised until just before listener is called.

This commit fixes this so all RealmResults are now synced before the listener is called./RealmResults is not synced in global listener (#2926)

Close #2408

RealmResults are synced when calling its listener, since we need to
check the table version before calling the listener. So sync it just
after advance read won't be an option - in that way, the result's
listener won't be triggered.

So we notify the global listeners as the last thing to do, at that
point, result will be synced already.

Also a test case is added to ensure the calling sequence of synced
listeners./Fixed unit tests./"
,Thread Management,Realm,"Disable change listeners on IntentService threads (#3232)/Forward all throwables but not only excpetions (#3126)

For example the UnsatisfiedLinkError should cause the process terminated.
This is the reason of all the time out issues we found for CI./Fix async transaction problem when async query already exist. (#2780)

I have rewritten how async transactions works. Before they did this:

1) commitOnBackgroundThread() -> post REALM_CHANGED -> post onSuccess runnable

The problem with that approach was that the REALM_CHANGED would be swallowed if async queries existed which meant that the async transaction would call onSuccess on an old version of the Realm (making it look like it didn't work).

Instead we now do this:

2) commitOnBackgroundThread -> post runnable that calls HandlerController.handleAsyncTransactionCompleted(runnable)

The special runnable is treated as a combined REALM_CHANGED + Callback, which makes it possible for us to queue up callbacks until we are finally able to trigger all of them.

Unfortunately the Handler has poor support for this so it means that we no longer can detect if such a message is in the queue. This means that it introduces a slight chance of such a event plus a real REALM_CHANGED event to be in the message queue at the same time. I considered that acceptable (since it can already happen today), and since preventing this will introduce more complexity to something that is already entirely to complex./Race condition between Realm change notifications and UI events (#2990)

This commit fixes a bug surfaced by our Realm Android Adapter example. Before this commit we posted all REALM_CHANGED events to the looper queue as normal. This meant that potential touch input events could get in front which in turn could cause a relayout of a ListView which would then crash because it detected a difference between it's cached adapter size and the actual one.

This commit now forces all local commits to be placed at the front of the queue eliminating that risk./"
,,Realm,Objects not in Realm are now called unmananged everywhere. (#2828)/
,Thread Management,Realm,"Disable change listeners on IntentService threads (#3232)/add a null check to addChangeListener and removeChangeListener (#2805)

* add a null check to addChangeListener and removeChangeListener in Realm and DynamicRealm (fixes #2772)

* reflect review comments/"
,,Realm,"Avoid Table.nativeToString/nativeRowToString (#2884)

Table::row_to_string Table::to_string are for debugging purpose only, it
calls string::substr which might truncate an UTF-8 string at any char.
Thus a crash could happend in JNI when converting to UTF-16.

This will only happen when user attach a debugger and the debugger is
trying to call Table.toString.

Close #2429/"
,,Realm,"bulk inserts (#2999)

Added `insert(RealmModel obj)`, `insertOrUpdate(RealmModel obj)`, `insert(Collection<RealmModel> collection)` and `insertOrUpdate(Collection<RealmModel> collection)` to perform batch inserts (#1684)./ PrimaryKey index rearrangement in migration revisited  (#2920)

This PR fixes two issues; 
1) When a column with a smaller index than that of a primary key field gets removed, the primary key field index decrements, which causes a cached primary key index to point a wrong field.
2) When a primary key field is renamed, the primary key meta table is not updated accordingly./"
,,Realm,"Forward all throwables but not only excpetions (#3126)

For example the UnsatisfiedLinkError should cause the process terminated.
This is the reason of all the time out issues we found for CI./"
,Thread Management,Realm,"Fix async transaction problem when async query already exist. (#2780)

I have rewritten how async transactions works. Before they did this:

1) commitOnBackgroundThread() -> post REALM_CHANGED -> post onSuccess runnable

The problem with that approach was that the REALM_CHANGED would be swallowed if async queries existed which meant that the async transaction would call onSuccess on an old version of the Realm (making it look like it didn't work).

Instead we now do this:

2) commitOnBackgroundThread -> post runnable that calls HandlerController.handleAsyncTransactionCompleted(runnable)

The special runnable is treated as a combined REALM_CHANGED + Callback, which makes it possible for us to queue up callbacks until we are finally able to trigger all of them.

Unfortunately the Handler has poor support for this so it means that we no longer can detect if such a message is in the queue. This means that it introduces a slight chance of such a event plus a real REALM_CHANGED event to be in the message queue at the same time. I considered that acceptable (since it can already happen today), and since preventing this will introduce more complexity to something that is already entirely to complex./"
,,Realm,"bulk inserts (#2999)

Added `insert(RealmModel obj)`, `insertOrUpdate(RealmModel obj)`, `insert(Collection<RealmModel> collection)` and `insertOrUpdate(Collection<RealmModel> collection)` to perform batch inserts (#1684)./"
Feature migration,Feature migration,Realm,"Optimistic opening of a Realm file (#3013)

We have anecdotal evidence that multiple processes might be open for the same app. We have not been able to verify this, but the result matches the ""IncompatibleLockFile"" errors reported in #2459. This happens if the two processes have two versions of Realm (e.g. during app upgrades).

This PR is based on our assumption that such an overlap is not intentional and only happens because one process is started before the previous was completely shut down.

So this PR introduces an optimistic opening scheme where we retry for 3 second before crashing as before./"
,,Realm,"RealmObjectSchema.setClassName() transfers primary key for new class name (#3149)

RealmObjectSchema.setClassName() transfers a primary key for a new class name that the renamed class can maintain the old primary key./Redundant addIndex(fieldName) removed (#2917)

In RealmObjectSchema.addModifiers() there is a redundant addIndex(fieldName) called before addPrimaryKey(fieldName). This should have been removed at #2832./addPrimaryKey() on RealmObjectSchema misses Index addition. (#2832)

RealmObjectSchema.addPrimaryKey() is supposed to add a search index to its primary key field but it is currently missing. This is to fix the addition and removal of a search index related to primary key methods in RealmObjectSchema./"
,,Realm,"Add RealmQuery.in() (#3133)

* Add RealmQuery.equalToAny() (#841)

* PR feedback: equalToAny -> in

* PR feedback: apply 'beginGroup()' and 'endGroup()' on 'in' operator.

@zaki50: We should use beginGroup() and endGroup() since equalTo(""a"")
and in(""b"", ""c"", ""d"") does not equal to equalTo(""a"") and equalTo(""b"") or
equalTo(""c"") or equalTo(""d"").

* PR feedback: throws an IllegalArgumentException if values is an empty array.

* Add a test, in_date()

* Remove unnecessary tests.

* Fix type fields in in_byte()

* Add 3 tests, in_byte(), in_short() and in_int()

* Update CHANGELOG.md for RealmQuery.in()

* PR feedback: array version -> vargs version.

* PR feedback: ++i -> i++

* PR feedback: fix javadocs.

* PR feedback: remove unnecessary parenthesises.

* PR feedback: revert the variable arguments version.

* PR feedback: fix javadocs and improve error messages.

* PR feedback: Extract the constant string literal from the hard-coded error message.

* PR feedback: Adopt boxed type API.

* PR feedback: add null tests.

* Fix row_isValid

* Fix openPreNullWithRequired

* PR feedback: update javadocs.

* PR feedback: use NoPrimaryKeyNullTypes for testing instead of AllTypes.

* PR feedback: Revert AllTypes.

* Revert automatic formatting.

* Revert auto-formatting again.

* PR feedback: remove unnecessay empty lines.

* PR feedback: generalize tests.

* PR feedback: update javadocs./Forward all throwables but not only excpetions (#3126)

For example the UnsatisfiedLinkError should cause the process terminated.
This is the reason of all the time out issues we found for CI./"
,Thread Management,Realm,Disable change listeners on IntentService threads (#3232)/
,Thread Management,Realm,"Disable change listeners on IntentService threads (#3232)/bulk inserts (#2999)

Added `insert(RealmModel obj)`, `insertOrUpdate(RealmModel obj)`, `insert(Collection<RealmModel> collection)` and `insertOrUpdate(Collection<RealmModel> collection)` to perform batch inserts (#1684)./Fix async transaction problem when async query already exist. (#2780)

I have rewritten how async transactions works. Before they did this:

1) commitOnBackgroundThread() -> post REALM_CHANGED -> post onSuccess runnable

The problem with that approach was that the REALM_CHANGED would be swallowed if async queries existed which meant that the async transaction would call onSuccess on an old version of the Realm (making it look like it didn't work).

Instead we now do this:

2) commitOnBackgroundThread -> post runnable that calls HandlerController.handleAsyncTransactionCompleted(runnable)

The special runnable is treated as a combined REALM_CHANGED + Callback, which makes it possible for us to queue up callbacks until we are finally able to trigger all of them.

Unfortunately the Handler has poor support for this so it means that we no longer can detect if such a message is in the queue. This means that it introduces a slight chance of such a event plus a real REALM_CHANGED event to be in the message queue at the same time. I considered that acceptable (since it can already happen today), and since preventing this will introduce more complexity to something that is already entirely to complex./Objects not in Realm are now called unmananged everywhere. (#2828)/"
,,Realm,"Forward all throwables but not only excpetions (#3126)

For example the UnsatisfiedLinkError should cause the process terminated.
This is the reason of all the time out issues we found for CI./Race condition between Realm change notifications and UI events (#2990)

This commit fixes a bug surfaced by our Realm Android Adapter example. Before this commit we posted all REALM_CHANGED events to the looper queue as normal. This meant that potential touch input events could get in front which in turn could cause a relayout of a ListView which would then crash because it detected a difference between it's cached adapter size and the actual one.

This commit now forces all local commits to be placed at the front of the queue eliminating that risk./Fix listeners can exposing unsynchronized RealmResults (Async queries) (#2951)

When async queries are updated, they call listeners in a specific order. Before this commit this had a very subtle bug, so accessing synchronous RealmResults from inside a async RealmResult listener might hit a detached row accessor.

Reason being is that the Realm is advanced, then all listeners called in order, but RealmResults are not synchronised until just before listener is called.

This commit fixes this so all RealmResults are now synced before the listener is called./"
,,Realm,"JNI clean up (#3010)

* Enable -Wmissing-declarations and -Werror to ensure all global
  functions are defined with a proper declaration. JNI function not
  found problem can only be seen at run time, thus we need to do so.
* Add static keyword for local functions.
* Fix wrong JNI function declaration.
* Remove useless functions.
* TableView distinct should return void.
* Rename JNI cpp files./"
,,Realm,"bulk inserts (#2999)

Added `insert(RealmModel obj)`, `insertOrUpdate(RealmModel obj)`, `insert(Collection<RealmModel> collection)` and `insertOrUpdate(Collection<RealmModel> collection)` to perform batch inserts (#1684)./JNI clean up (#3010)

* Enable -Wmissing-declarations and -Werror to ensure all global
  functions are defined with a proper declaration. JNI function not
  found problem can only be seen at run time, thus we need to do so.
* Add static keyword for local functions.
* Fix wrong JNI function declaration.
* Remove useless functions.
* TableView distinct should return void.
* Rename JNI cpp files./"
,,Realm,"Wrong JNI function declaration caused test timeout (#2995)

The worst thing is async query won't throw those exceptions in the
background.

* Rephrase changelog/"
,,Realm,Fix JNI declaration for UncheckedRow (#3104)/
,,Realm,"Let Mixpanel track the version of sync being used (#161)

* Add support for sync to Mixpanel

Also moves the version and SHA256 of sync in the dependencies.list file
This consolidates with the other repos in the org

* Be aware of whether sync is enabled or not

* Align naming style/"
,Thread Management,Realm,"Allow to specify default value of the field in model's constructor (#3397)

* Allow to call its accessors, and replace its field accesses with accessor calls in model's constructor.

fixes #777
fixes #2536

* use field instead of checking transaction

* fix a bug that acceptDefaultValue is not set correctly

* reject default values when the getter of a model creates other model object

* add simple test for default value

* supports default value of model field

* supports default value of RealmList fields

* add tests for assignment in constructor and setter in constructor

* update javadoc comments of createObject

* always ignores the default value of primary key if the object is managed

* update javadoc

* add a test for default values handling in copyToRealm(). the last assertion of RealmTests.copyToRealm_defaultValuesAreIgnored() is failing now.

* refactor tests

* use isPrimaryKey()

* fix a bug that unexpected realm object is created by default value of RealmModel/RealmList fields

* remove extra ';' from generated code

* add more tests for default value

* fix tests

* fix a bug that creates unexpected objects

* rename internal methods

* update changelog

* update CHANGELOG

* review comments

* update CHANGELOG

* added a description of how proxy object should be created in the Javadoc comment of RealmProcessor/Supporting both additive and manual schema modes (#91)

* Adding very thin wrappers for Object Store's ObjectSchema and Property.
* Adding method for building object schema is proxy classes
* Adding rudimentary support for Object Store schemas. Using ObjectStore::update_schema() to update schema for the additive mode.
* Disallowing destructive schema changes in additive mode./Invalidate schema cache when the schema version of Realm is changed by other process (#3409).

invalidate schema cache when the schema version of Realm is changed by other process.

Now schema cache referred by Realm instance is not shared and global schema cache is introduced instead./Integrate Object Store [PART2] - SharedRealm (#3031)

This simplified our code base a lot. Basically all APIs we need from
SharedGroup/Group are wrapped in the SharedRealm. So we can just remove
those classes.

But we do need a few APIs from SharedGroup which is not supplied by
SharedRealm because of async queries. Currently we expose those in a
friend class of ShareRealm, see realm/realm-object-store#141

We are still managing Realm caches in Java although there are mechanism
in OS to do the same thing. The major reason is we have method like
deleteRealm needs information from cache to check if all Realm instances
are closed. The ShareRealm is actually a std::shared_ptr. We hold
the pointer to the std::shared_ptr in Java.
Another fundamental change is that we used to have separated
SharedGroups for DynamicRealm and typed Realm in the same thread. But
now they are using different SharedRealm but actually the different
SharedRealms are point to the same SharedGroup.

And some other code cleanup./"
,Thread Management,Realm,"add default value instruction support (#3462)

* add default value support to Table class

* Table#isNull() and TableView#isNull()

* use default value feature

* added a test to check if nullified link can be overwritten by default value

* removed duplicate thread check when constructing proxy objects (and small bugfix in setter of the list).

* added thread check

* reflect review comments

* reflect comment in JNI code/Allow to specify default value of the field in model's constructor (#3397)

* Allow to call its accessors, and replace its field accesses with accessor calls in model's constructor.

fixes #777
fixes #2536

* use field instead of checking transaction

* fix a bug that acceptDefaultValue is not set correctly

* reject default values when the getter of a model creates other model object

* add simple test for default value

* supports default value of model field

* supports default value of RealmList fields

* add tests for assignment in constructor and setter in constructor

* update javadoc comments of createObject

* always ignores the default value of primary key if the object is managed

* update javadoc

* add a test for default values handling in copyToRealm(). the last assertion of RealmTests.copyToRealm_defaultValuesAreIgnored() is failing now.

* refactor tests

* use isPrimaryKey()

* fix a bug that unexpected realm object is created by default value of RealmModel/RealmList fields

* remove extra ';' from generated code

* add more tests for default value

* fix tests

* fix a bug that creates unexpected objects

* rename internal methods

* update changelog

* update CHANGELOG

* review comments

* update CHANGELOG

* added a description of how proxy object should be created in the Javadoc comment of RealmProcessor/Supporting both additive and manual schema modes (#91)

* Adding very thin wrappers for Object Store's ObjectSchema and Property.
* Adding method for building object schema is proxy classes
* Adding rudimentary support for Object Store schemas. Using ObjectStore::update_schema() to update schema for the additive mode.
* Disallowing destructive schema changes in additive mode./Invalidate schema cache when the schema version of Realm is changed by other process (#3409).

invalidate schema cache when the schema version of Realm is changed by other process.

Now schema cache referred by Realm instance is not shared and global schema cache is introduced instead./Integrate Object Store [PART2] - SharedRealm (#3031)

This simplified our code base a lot. Basically all APIs we need from
SharedGroup/Group are wrapped in the SharedRealm. So we can just remove
those classes.

But we do need a few APIs from SharedGroup which is not supplied by
SharedRealm because of async queries. Currently we expose those in a
friend class of ShareRealm, see realm/realm-object-store#141

We are still managing Realm caches in Java although there are mechanism
in OS to do the same thing. The major reason is we have method like
deleteRealm needs information from cache to check if all Realm instances
are closed. The ShareRealm is actually a std::shared_ptr. We hold
the pointer to the std::shared_ptr in Java.
Another fundamental change is that we used to have separated
SharedGroups for DynamicRealm and typed Realm in the same thread. But
now they are using different SharedRealm but actually the different
SharedRealms are point to the same SharedGroup.

And some other code cleanup./Add isManaged() to RealmObject/RealmCollection (#3341)

* isValid() returns true for unmanaged object and collection.
* Add isManaged() to RealmObject and RealmCollection/"
,Thread Management,Realm,"Supporting both additive and manual schema modes (#91)

* Adding very thin wrappers for Object Store's ObjectSchema and Property.
* Adding method for building object schema is proxy classes
* Adding rudimentary support for Object Store schemas. Using ObjectStore::update_schema() to update schema for the additive mode.
* Disallowing destructive schema changes in additive mode./Invalidate schema cache when the schema version of Realm is changed by other process (#3409).

invalidate schema cache when the schema version of Realm is changed by other process.

Now schema cache referred by Realm instance is not shared and global schema cache is introduced instead./Integrate Object Store [PART2] - SharedRealm (#3031)

This simplified our code base a lot. Basically all APIs we need from
SharedGroup/Group are wrapped in the SharedRealm. So we can just remove
those classes.

But we do need a few APIs from SharedGroup which is not supplied by
SharedRealm because of async queries. Currently we expose those in a
friend class of ShareRealm, see realm/realm-object-store#141

We are still managing Realm caches in Java although there are mechanism
in OS to do the same thing. The major reason is we have method like
deleteRealm needs information from cache to check if all Realm instances
are closed. The ShareRealm is actually a std::shared_ptr. We hold
the pointer to the std::shared_ptr in Java.
Another fundamental change is that we used to have separated
SharedGroups for DynamicRealm and typed Realm in the same thread. But
now they are using different SharedRealm but actually the different
SharedRealms are point to the same SharedGroup.

And some other code cleanup./"
,,Realm,DefaultRealmModule not created for empty Kotlin projects (#3749)/
,,Realm,"Create object without setting PK is not allowed (#3379)

* Create object without setting PK is not allowed

* createObject must be called with PK value when creating a object with
  PK defined.
* Creating objects from JSON must have have corresponding PK defined in
  the JSON object.

Known issue:
The default values for creating object from JSONStream will be differnt
from those created by createObject. Default values from default
constructor VS default values from core. This has to be addressed by #777/"
Feature migration,Feature migration,Realm,"add default value instruction support (#3462)

* add default value support to Table class

* Table#isNull() and TableView#isNull()

* use default value feature

* added a test to check if nullified link can be overwritten by default value

* removed duplicate thread check when constructing proxy objects (and small bugfix in setter of the list).

* added thread check

* reflect review comments

* reflect comment in JNI code/Allow to specify default value of the field in model's constructor (#3397)

* Allow to call its accessors, and replace its field accesses with accessor calls in model's constructor.

fixes #777
fixes #2536

* use field instead of checking transaction

* fix a bug that acceptDefaultValue is not set correctly

* reject default values when the getter of a model creates other model object

* add simple test for default value

* supports default value of model field

* supports default value of RealmList fields

* add tests for assignment in constructor and setter in constructor

* update javadoc comments of createObject

* always ignores the default value of primary key if the object is managed

* update javadoc

* add a test for default values handling in copyToRealm(). the last assertion of RealmTests.copyToRealm_defaultValuesAreIgnored() is failing now.

* refactor tests

* use isPrimaryKey()

* fix a bug that unexpected realm object is created by default value of RealmModel/RealmList fields

* remove extra ';' from generated code

* add more tests for default value

* fix tests

* fix a bug that creates unexpected objects

* rename internal methods

* update changelog

* update CHANGELOG

* review comments

* update CHANGELOG

* added a description of how proxy object should be created in the Javadoc comment of RealmProcessor/Supporting both additive and manual schema modes (#91)

* Adding very thin wrappers for Object Store's ObjectSchema and Property.
* Adding method for building object schema is proxy classes
* Adding rudimentary support for Object Store schemas. Using ObjectStore::update_schema() to update schema for the additive mode.
* Disallowing destructive schema changes in additive mode./"
,,Realm,"Disallow changing PK after object created (#3418)

Thrown an exception if changing the pk after the object creation./Upgrade to beta-33 / 2.0.0-rc4 (#90)/Invalidate schema cache when the schema version of Realm is changed by other process (#3409).

invalidate schema cache when the schema version of Realm is changed by other process.

Now schema cache referred by Realm instance is not shared and global schema cache is introduced instead./Integrate Object Store [PART2] - SharedRealm (#3031)

This simplified our code base a lot. Basically all APIs we need from
SharedGroup/Group are wrapped in the SharedRealm. So we can just remove
those classes.

But we do need a few APIs from SharedGroup which is not supplied by
SharedRealm because of async queries. Currently we expose those in a
friend class of ShareRealm, see realm/realm-object-store#141

We are still managing Realm caches in Java although there are mechanism
in OS to do the same thing. The major reason is we have method like
deleteRealm needs information from cache to check if all Realm instances
are closed. The ShareRealm is actually a std::shared_ptr. We hold
the pointer to the std::shared_ptr in Java.
Another fundamental change is that we used to have separated
SharedGroups for DynamicRealm and typed Realm in the same thread. But
now they are using different SharedRealm but actually the different
SharedRealms are point to the same SharedGroup.

And some other code cleanup./"
,,Realm,"Add isManaged() to RealmObject/RealmCollection (#3341)

* isValid() returns true for unmanaged object and collection.
* Add isManaged() to RealmObject and RealmCollection/Nh/fixing 3105 (#3306)

* Fixing issue with Cyclic dependency insert or the existing copyToRealm, adding support for managed RealmObject/"
,,Realm,"Supporting both additive and manual schema modes (#91)

* Adding very thin wrappers for Object Store's ObjectSchema and Property.
* Adding method for building object schema is proxy classes
* Adding rudimentary support for Object Store schemas. Using ObjectStore::update_schema() to update schema for the additive mode.
* Disallowing destructive schema changes in additive mode./"
,,Realm,"Remove the dependency of Realm.Handler from tests (#3317)

Instead, those tests should use method from RunInLooperThread.
Fix this in advance for the coming OS notification integration./"
,,Realm,"allow to put Realm database file on external storage. (#3591)

* set the path of the directory of named pipes to fix #3140

* update CHANGELOG

* add a test for issue3140

* update test

* follow the chenges in object-store

* skip an external storage test on the device where SELinux is not enforced

* rename test

* rename variable

* update object-store

* make SharedRealm#temporaryDirectory volatile

* address findbugs error/RealmLog (#3368)

Moved RealmLog to the public API. Routes all log events through it, also from native code./"
,,Realm,"Add cause to RealmMigrationNeededException (#3482)/Remove deprecated constructor + add directory() (#3357)

This commit simplifies the RealmConfiguration constructors and also ensures that we always have an Android context. 

It does so by now only having the`RealmConfiguration.Builder(context)` constructor. Custom file locations are now supported through the `directory()` builder method. This also made it possible to simply `assetFile(Context, location)` to only `assetFile(location)`.

Having the Context means that we are now able to access system services and other framework classes without exposing any Android functionality in any potential interface (which will be needed to support Realm on the JVM)./Add RealmFileException

to replace RealmIOException and IncompatibleLockFileException. Also it
is mapped to the same name exception in ObjectStore to give user a
detailed kind of file exception./"
,,Realm,fix flaky test (#3626)/
,,Realm,distinctAsync now respects other query parameters. (#3539)/
,,Realm,"DeleteLocalRef when the ref is created in loop (#3366)

Add wrapper class for JNI local reference to delete the local ref after
using it.

This is reported by user on helpscout:
https://secure.helpscout.net/conversation/244053233/6163/?folderId=366141

And some useful explanation can be found:
http://stackoverflow.com/questions/24289724/jni-deletelocalref-clarification

Normally the local ref doesn't have to be deleted since they will be
cleaned up when program returns to Java from native code. Using it in a
loop is obvious a corner case: the size of local ref table is relatively
small (512 on Android). To avoid it, the local ref should be deleted
when using it in a loop./Integrate Object Store [PART2] - SharedRealm (#3031)

This simplified our code base a lot. Basically all APIs we need from
SharedGroup/Group are wrapped in the SharedRealm. So we can just remove
those classes.

But we do need a few APIs from SharedGroup which is not supplied by
SharedRealm because of async queries. Currently we expose those in a
friend class of ShareRealm, see realm/realm-object-store#141

We are still managing Realm caches in Java although there are mechanism
in OS to do the same thing. The major reason is we have method like
deleteRealm needs information from cache to check if all Realm instances
are closed. The ShareRealm is actually a std::shared_ptr. We hold
the pointer to the std::shared_ptr in Java.
Another fundamental change is that we used to have separated
SharedGroups for DynamicRealm and typed Realm in the same thread. But
now they are using different SharedRealm but actually the different
SharedRealms are point to the same SharedGroup.

And some other code cleanup./Wait all async tasks done before next test (#3319)

This is highly related with #1900.
Below things are still guaranteed by this change after commit async
transaction:
* When callback function called (onSuccess/onError), the background
  Realm is closed.
* When any change listeners called, the background Realm is closed.

What is true now but it is not guaranteed in the future:
* Background Realm might not be closed before REALM_CHANGED sent (not
  received).

Due to this, to avoid the flaky tests, we have to ensure all async tasks
quit peacefully before start the next test. This is implemented by wait
and check in the TestRealmConfigurationFactory.

NOTE: Any test, if the async tasks cannot be finished peacefully in a
certain time, it has to be considered as a problem and fixed.

This would be needed by OS notifications since Java won't have a precise
control of sending REALM_CHANGED anymore./"
,,Realm,"Nh/fixing 3105 (#3306)

* Fixing issue with Cyclic dependency insert or the existing copyToRealm, adding support for managed RealmObject/"
,,Realm,"add default value instruction support (#3462)

* add default value support to Table class

* Table#isNull() and TableView#isNull()

* use default value feature

* added a test to check if nullified link can be overwritten by default value

* removed duplicate thread check when constructing proxy objects (and small bugfix in setter of the list).

* added thread check

* reflect review comments

* reflect comment in JNI code/"
,,Realm,"Integrate Object Store [PART2] - SharedRealm (#3031)

This simplified our code base a lot. Basically all APIs we need from
SharedGroup/Group are wrapped in the SharedRealm. So we can just remove
those classes.

But we do need a few APIs from SharedGroup which is not supplied by
SharedRealm because of async queries. Currently we expose those in a
friend class of ShareRealm, see realm/realm-object-store#141

We are still managing Realm caches in Java although there are mechanism
in OS to do the same thing. The major reason is we have method like
deleteRealm needs information from cache to check if all Realm instances
are closed. The ShareRealm is actually a std::shared_ptr. We hold
the pointer to the std::shared_ptr in Java.
Another fundamental change is that we used to have separated
SharedGroups for DynamicRealm and typed Realm in the same thread. But
now they are using different SharedRealm but actually the different
SharedRealms are point to the same SharedGroup.

And some other code cleanup./"
,,Realm,Add methods to RealmQuery to support byte arrays (#3285)/
,,Realm,"Use createObject(class, primaryKey) in test (#3377)

Since Realm.createObject(class) will be deprecated on master for Classes
with primary key defined, fix the test cases which use it first to avoid
more conflicts when merging./"
,,Realm,Typed getters on DynamicRealmObject now throws a proper exception instead of creating a seg fault when called with a field name of the wrong type. (#3312)/
,,Realm,Fixed leaking unit tests./
,,Realm,Fix unstable test (#3495)/
,,Realm,"getFieldIndex returns null for non-existing field (#3295)

Close #3292/"
,,Realm,"Introduce global init (#3457)

Realm now uses a global init function instead of Context on the RealmConfiguration.Builder/Remove deprecated constructor + add directory() (#3357)

This commit simplifies the RealmConfiguration constructors and also ensures that we always have an Android context. 

It does so by now only having the`RealmConfiguration.Builder(context)` constructor. Custom file locations are now supported through the `directory()` builder method. This also made it possible to simply `assetFile(Context, location)` to only `assetFile(location)`.

Having the Context means that we are now able to access system services and other framework classes without exposing any Android functionality in any potential interface (which will be needed to support Realm on the JVM)./Wait all async tasks done before next test (#3319)

This is highly related with #1900.
Below things are still guaranteed by this change after commit async
transaction:
* When callback function called (onSuccess/onError), the background
  Realm is closed.
* When any change listeners called, the background Realm is closed.

What is true now but it is not guaranteed in the future:
* Background Realm might not be closed before REALM_CHANGED sent (not
  received).

Due to this, to avoid the flaky tests, we have to ensure all async tasks
quit peacefully before start the next test. This is implemented by wait
and check in the TestRealmConfigurationFactory.

NOTE: Any test, if the async tasks cannot be finished peacefully in a
certain time, it has to be considered as a problem and fixed.

This would be needed by OS notifications since Java won't have a precise
control of sending REALM_CHANGED anymore./"
,,Realm,"Add RealmFileException

to replace RealmIOException and IncompatibleLockFileException. Also it
is mapped to the same name exception in ObjectStore to give user a
detailed kind of file exception./Integrate Object Store [PART2] - SharedRealm (#3031)

This simplified our code base a lot. Basically all APIs we need from
SharedGroup/Group are wrapped in the SharedRealm. So we can just remove
those classes.

But we do need a few APIs from SharedGroup which is not supplied by
SharedRealm because of async queries. Currently we expose those in a
friend class of ShareRealm, see realm/realm-object-store#141

We are still managing Realm caches in Java although there are mechanism
in OS to do the same thing. The major reason is we have method like
deleteRealm needs information from cache to check if all Realm instances
are closed. The ShareRealm is actually a std::shared_ptr. We hold
the pointer to the std::shared_ptr in Java.
Another fundamental change is that we used to have separated
SharedGroups for DynamicRealm and typed Realm in the same thread. But
now they are using different SharedRealm but actually the different
SharedRealms are point to the same SharedGroup.

And some other code cleanup./"
,,Realm,Logout and Userstore (#104)/
,,Realm,"Allow to specify default value of the field in model's constructor (#3397)

* Allow to call its accessors, and replace its field accesses with accessor calls in model's constructor.

fixes #777
fixes #2536

* use field instead of checking transaction

* fix a bug that acceptDefaultValue is not set correctly

* reject default values when the getter of a model creates other model object

* add simple test for default value

* supports default value of model field

* supports default value of RealmList fields

* add tests for assignment in constructor and setter in constructor

* update javadoc comments of createObject

* always ignores the default value of primary key if the object is managed

* update javadoc

* add a test for default values handling in copyToRealm(). the last assertion of RealmTests.copyToRealm_defaultValuesAreIgnored() is failing now.

* refactor tests

* use isPrimaryKey()

* fix a bug that unexpected realm object is created by default value of RealmModel/RealmList fields

* remove extra ';' from generated code

* add more tests for default value

* fix tests

* fix a bug that creates unexpected objects

* rename internal methods

* update changelog

* update CHANGELOG

* review comments

* update CHANGELOG

* added a description of how proxy object should be created in the Javadoc comment of RealmProcessor/Invalidate schema cache when the schema version of Realm is changed by other process (#3409).

invalidate schema cache when the schema version of Realm is changed by other process.

Now schema cache referred by Realm instance is not shared and global schema cache is introduced instead./Add RealmFileException

to replace RealmIOException and IncompatibleLockFileException. Also it
is mapped to the same name exception in ObjectStore to give user a
detailed kind of file exception./Integrate Object Store [PART2] - SharedRealm (#3031)

This simplified our code base a lot. Basically all APIs we need from
SharedGroup/Group are wrapped in the SharedRealm. So we can just remove
those classes.

But we do need a few APIs from SharedGroup which is not supplied by
SharedRealm because of async queries. Currently we expose those in a
friend class of ShareRealm, see realm/realm-object-store#141

We are still managing Realm caches in Java although there are mechanism
in OS to do the same thing. The major reason is we have method like
deleteRealm needs information from cache to check if all Realm instances
are closed. The ShareRealm is actually a std::shared_ptr. We hold
the pointer to the std::shared_ptr in Java.
Another fundamental change is that we used to have separated
SharedGroups for DynamicRealm and typed Realm in the same thread. But
now they are using different SharedRealm but actually the different
SharedRealms are point to the same SharedGroup.

And some other code cleanup./"
,,Realm,"Allow to specify default value of the field in model's constructor (#3397)

* Allow to call its accessors, and replace its field accesses with accessor calls in model's constructor.

fixes #777
fixes #2536

* use field instead of checking transaction

* fix a bug that acceptDefaultValue is not set correctly

* reject default values when the getter of a model creates other model object

* add simple test for default value

* supports default value of model field

* supports default value of RealmList fields

* add tests for assignment in constructor and setter in constructor

* update javadoc comments of createObject

* always ignores the default value of primary key if the object is managed

* update javadoc

* add a test for default values handling in copyToRealm(). the last assertion of RealmTests.copyToRealm_defaultValuesAreIgnored() is failing now.

* refactor tests

* use isPrimaryKey()

* fix a bug that unexpected realm object is created by default value of RealmModel/RealmList fields

* remove extra ';' from generated code

* add more tests for default value

* fix tests

* fix a bug that creates unexpected objects

* rename internal methods

* update changelog

* update CHANGELOG

* review comments

* update CHANGELOG

* added a description of how proxy object should be created in the Javadoc comment of RealmProcessor/"
,,Realm,"Introduce global init (#3457)

Realm now uses a global init function instead of Context on the RealmConfiguration.Builder/"
,,Realm,"Disallow changing PK after object created (#3418)

Thrown an exception if changing the pk after the object creation./Add methods to RealmQuery to support byte arrays (#3285)/"
,,Realm,"Disallow changing PK after object created (#3418)

Thrown an exception if changing the pk after the object creation./"
,,Realm,"Add isManaged() to RealmObject/RealmCollection (#3341)

* isValid() returns true for unmanaged object and collection.
* Add isManaged() to RealmObject and RealmCollection/"
,,Realm,"Add RealmFileException

to replace RealmIOException and IncompatibleLockFileException. Also it
is mapped to the same name exception in ObjectStore to give user a
detailed kind of file exception./"
Feature migration,Feature migration,Realm,"Upgrade to beta-33 / 2.0.0-rc4 (#90)/RealmLog (#3368)

Moved RealmLog to the public API. Routes all log events through it, also from native code./Fix notification tests for following changes (#3301)

* Fix notification tests for following changes

To support multiprocess & OS notifications, a dedicated thread will
monitor the Realm changes and send notifications to other thread. This
means our tests cannot assume notifications will be sent for every
transaction commit.

Also, the Realm.handler will moved to a lower layer, it's better to not
rely on it in the test as well.

To avoid a massive PR, tests changes are made separately first./"
API Management,API Management,Realm,Public Sync API (#73)/
,,Realm,"Use CheckedRow when creating a DynamicRealm Object. (#3551)

And this removes a check of UncheckedRow in some constructors of DynamicRealmObject since CheckedRow is never passed to it./Add cause to RealmMigrationNeededException (#3482)/Make BaseRealm package protected again (#143)

and move SyncObjectServerFacade to internal/objectserver./Merge branch 'master' into cm/merge-globalinit-from-master

Allow to specify default value of the field in model's constructor (#3397)

* Allow to call its accessors, and replace its field accesses with accessor calls in model's constructor.

fixes #777
fixes #2536

* use field instead of checking transaction

* fix a bug that acceptDefaultValue is not set correctly

* reject default values when the getter of a model creates other model object

* add simple test for default value

* supports default value of model field

* supports default value of RealmList fields

* add tests for assignment in constructor and setter in constructor

* update javadoc comments of createObject

* always ignores the default value of primary key if the object is managed

* update javadoc

* add a test for default values handling in copyToRealm(). the last assertion of RealmTests.copyToRealm_defaultValuesAreIgnored() is failing now.

* refactor tests

* use isPrimaryKey()

* fix a bug that unexpected realm object is created by default value of RealmModel/RealmList fields

* remove extra ';' from generated code

* add more tests for default value

* fix tests

* fix a bug that creates unexpected objects

* rename internal methods

* update changelog

* update CHANGELOG

* review comments

* update CHANGELOG

* added a description of how proxy object should be created in the Javadoc comment of RealmProcessor/Supporting both additive and manual schema modes (#91)

* Adding very thin wrappers for Object Store's ObjectSchema and Property.
* Adding method for building object schema is proxy classes
* Adding rudimentary support for Object Store schemas. Using ObjectStore::update_schema() to update schema for the additive mode.
* Disallowing destructive schema changes in additive mode./Invalidate schema cache when the schema version of Realm is changed by other process (#3409).

invalidate schema cache when the schema version of Realm is changed by other process.

Now schema cache referred by Realm instance is not shared and global schema cache is introduced instead./Integrate Object Store [PART4] - OS notifications (#3370)

* Use OS's notification mechanism to notify threads.
* Create RealmNotificer interface for decouple Android related handler
  logic.
* Create AndroidNotifier for the handler notifications.

The major change of this PR is about the timing. The notifications are
not sent immediately after transaction committed. Instead, there is a
dedicated thread monitoring changes and notify others when changes
happen.

The known problem is for every RealmConfiguration, a monitor thread will
be created which is not ideal for app which is using multiple
RealmConfiguration.

There are different implementations for the monitoring thread in OS. For
Android, we can choose from generic which is based on the core's
wait_for_change() and android which is used by dotnet based on the
named pipe.
To align with dotnet, we are using the named pipe for now which also
enables notifications between realm-java and realm-dotnet./Integrate Object Store [PART2] - SharedRealm (#3031)

This simplified our code base a lot. Basically all APIs we need from
SharedGroup/Group are wrapped in the SharedRealm. So we can just remove
those classes.

But we do need a few APIs from SharedGroup which is not supplied by
SharedRealm because of async queries. Currently we expose those in a
friend class of ShareRealm, see realm/realm-object-store#141

We are still managing Realm caches in Java although there are mechanism
in OS to do the same thing. The major reason is we have method like
deleteRealm needs information from cache to check if all Realm instances
are closed. The ShareRealm is actually a std::shared_ptr. We hold
the pointer to the std::shared_ptr in Java.
Another fundamental change is that we used to have separated
SharedGroups for DynamicRealm and typed Realm in the same thread. But
now they are using different SharedRealm but actually the different
SharedRealms are point to the same SharedGroup.

And some other code cleanup./Wait all async tasks done before next test (#3319)

This is highly related with #1900.
Below things are still guaranteed by this change after commit async
transaction:
* When callback function called (onSuccess/onError), the background
  Realm is closed.
* When any change listeners called, the background Realm is closed.

What is true now but it is not guaranteed in the future:
* Background Realm might not be closed before REALM_CHANGED sent (not
  received).

Due to this, to avoid the flaky tests, we have to ensure all async tasks
quit peacefully before start the next test. This is implemented by wait
and check in the TestRealmConfigurationFactory.

NOTE: Any test, if the async tasks cannot be finished peacefully in a
certain time, it has to be considered as a problem and fixed.

This would be needed by OS notifications since Java won't have a precise
control of sending REALM_CHANGED anymore./"
,,Realm,Updated changelog; combined duplicate implementations for both .first and .last/Add ability for first and last methods to return a default/
,,Realm,"SyncConfiguration Builder now only contains allowed options (#87)/Remove deprecated constructor + add directory() (#3357)

This commit simplifies the RealmConfiguration constructors and also ensures that we always have an Android context. 

It does so by now only having the`RealmConfiguration.Builder(context)` constructor. Custom file locations are now supported through the `directory()` builder method. This also made it possible to simply `assetFile(Context, location)` to only `assetFile(location)`.

Having the Context means that we are now able to access system services and other framework classes without exposing any Android functionality in any potential interface (which will be needed to support Realm on the JVM)./Integrate Object Store [PART2] - SharedRealm (#3031)

This simplified our code base a lot. Basically all APIs we need from
SharedGroup/Group are wrapped in the SharedRealm. So we can just remove
those classes.

But we do need a few APIs from SharedGroup which is not supplied by
SharedRealm because of async queries. Currently we expose those in a
friend class of ShareRealm, see realm/realm-object-store#141

We are still managing Realm caches in Java although there are mechanism
in OS to do the same thing. The major reason is we have method like
deleteRealm needs information from cache to check if all Realm instances
are closed. The ShareRealm is actually a std::shared_ptr. We hold
the pointer to the std::shared_ptr in Java.
Another fundamental change is that we used to have separated
SharedGroups for DynamicRealm and typed Realm in the same thread. But
now they are using different SharedRealm but actually the different
SharedRealms are point to the same SharedGroup.

And some other code cleanup./"
,,Realm,"Fix unit tests./fix merge mistakes/Supporting both additive and manual schema modes (#91)

* Adding very thin wrappers for Object Store's ObjectSchema and Property.
* Adding method for building object schema is proxy classes
* Adding rudimentary support for Object Store schemas. Using ObjectStore::update_schema() to update schema for the additive mode.
* Disallowing destructive schema changes in additive mode./Integrate Object Store [PART2] - SharedRealm (#3031)

This simplified our code base a lot. Basically all APIs we need from
SharedGroup/Group are wrapped in the SharedRealm. So we can just remove
those classes.

But we do need a few APIs from SharedGroup which is not supplied by
SharedRealm because of async queries. Currently we expose those in a
friend class of ShareRealm, see realm/realm-object-store#141

We are still managing Realm caches in Java although there are mechanism
in OS to do the same thing. The major reason is we have method like
deleteRealm needs information from cache to check if all Realm instances
are closed. The ShareRealm is actually a std::shared_ptr. We hold
the pointer to the std::shared_ptr in Java.
Another fundamental change is that we used to have separated
SharedGroups for DynamicRealm and typed Realm in the same thread. But
now they are using different SharedRealm but actually the different
SharedRealms are point to the same SharedGroup.

And some other code cleanup./improve `Table` and schema cache. (#3315)

* improve `Table` and schema cache./"
,,Realm,"Invalidate schema cache when the schema version of Realm is changed by other process (#3409).

invalidate schema cache when the schema version of Realm is changed by other process.

Now schema cache referred by Realm instance is not shared and global schema cache is introduced instead./"
,,Realm,Fix native crash in DynamicRealmObject#setList() (#3550)/
Feature migration,Feature migration,Realm,Upgrade to beta-33 / 2.0.0-rc4 (#90)/
,,Realm,"Use set_string_unique to set primary key (#3488)

* Migrate PK table when get 1st Realm instance
* migratePrimaryKeyTableIfNeeded will be called when the first time
  Realm instance gets opened.
* Get miss-deleted tests case for pk table back.
* Update Object Store/Logout and Userstore (#104)/Invalidate schema cache when the schema version of Realm is changed by other process (#3409).

invalidate schema cache when the schema version of Realm is changed by other process.

Now schema cache referred by Realm instance is not shared and global schema cache is introduced instead./RealmLog (#3368)

Moved RealmLog to the public API. Routes all log events through it, also from native code./"
,,Realm,"Use CheckedRow when creating a DynamicRealm Object. (#3551)

And this removes a check of UncheckedRow in some constructors of DynamicRealmObject since CheckedRow is never passed to it./Sync facade to make spliting lib possible (#116)

* Sync facade to make spliting lib possible

* Add class SyncObjectServerFacade which will only exist in the sync
  lib.
* Check if SyncObjectServerFacade exists and create an singleton
  instance.
* Empty implementations for base ObjectServerFacade.
* Add RealmConfiguration.isSyncConfiguration to make the checks faster.
* Other cleanups.

Close #112/Supporting both additive and manual schema modes (#91)

* Adding very thin wrappers for Object Store's ObjectSchema and Property.
* Adding method for building object schema is proxy classes
* Adding rudimentary support for Object Store schemas. Using ObjectStore::update_schema() to update schema for the additive mode.
* Disallowing destructive schema changes in additive mode./Integrate Object Store [PART2] - SharedRealm (#3031)

This simplified our code base a lot. Basically all APIs we need from
SharedGroup/Group are wrapped in the SharedRealm. So we can just remove
those classes.

But we do need a few APIs from SharedGroup which is not supplied by
SharedRealm because of async queries. Currently we expose those in a
friend class of ShareRealm, see realm/realm-object-store#141

We are still managing Realm caches in Java although there are mechanism
in OS to do the same thing. The major reason is we have method like
deleteRealm needs information from cache to check if all Realm instances
are closed. The ShareRealm is actually a std::shared_ptr. We hold
the pointer to the std::shared_ptr in Java.
Another fundamental change is that we used to have separated
SharedGroups for DynamicRealm and typed Realm in the same thread. But
now they are using different SharedRealm but actually the different
SharedRealms are point to the same SharedGroup.

And some other code cleanup./getFieldIndex returns null for non-existing field (#3295)

Close #3292/"
,,Realm,"Update core to 2.0.0-rc4 (#3384)

* And with some code cleanup.
* Throw an runtime exception when input java bytes array cannot be read.
* Update Object Store to solve the breaking change caused failure.
   See https://github.com/realm/realm-object-store//158
* Use '-O2' instead '-Os' since it seems a gcc bug hangs encryption releated
   tests with '-Os' enabled in JNI build./Integrate Object Store [PART2] - SharedRealm (#3031)

This simplified our code base a lot. Basically all APIs we need from
SharedGroup/Group are wrapped in the SharedRealm. So we can just remove
those classes.

But we do need a few APIs from SharedGroup which is not supplied by
SharedRealm because of async queries. Currently we expose those in a
friend class of ShareRealm, see realm/realm-object-store#141

We are still managing Realm caches in Java although there are mechanism
in OS to do the same thing. The major reason is we have method like
deleteRealm needs information from cache to check if all Realm instances
are closed. The ShareRealm is actually a std::shared_ptr. We hold
the pointer to the std::shared_ptr in Java.
Another fundamental change is that we used to have separated
SharedGroups for DynamicRealm and typed Realm in the same thread. But
now they are using different SharedRealm but actually the different
SharedRealms are point to the same SharedGroup.

And some other code cleanup./Add methods to RealmQuery to support byte arrays (#3285)/"
API Management,API Management,Realm,"Disallow changing PK after object created (#3418)

Thrown an exception if changing the pk after the object creation./Integrate Object Store [PART2] - SharedRealm (#3031)

This simplified our code base a lot. Basically all APIs we need from
SharedGroup/Group are wrapped in the SharedRealm. So we can just remove
those classes.

But we do need a few APIs from SharedGroup which is not supplied by
SharedRealm because of async queries. Currently we expose those in a
friend class of ShareRealm, see realm/realm-object-store#141

We are still managing Realm caches in Java although there are mechanism
in OS to do the same thing. The major reason is we have method like
deleteRealm needs information from cache to check if all Realm instances
are closed. The ShareRealm is actually a std::shared_ptr. We hold
the pointer to the std::shared_ptr in Java.
Another fundamental change is that we used to have separated
SharedGroups for DynamicRealm and typed Realm in the same thread. But
now they are using different SharedRealm but actually the different
SharedRealms are point to the same SharedGroup.

And some other code cleanup./"
Feature migration,Feature migration,Realm,"allow to put Realm database file on external storage. (#3591)

* set the path of the directory of named pipes to fix #3140

* update CHANGELOG

* add a test for issue3140

* update test

* follow the chenges in object-store

* skip an external storage test on the device where SELinux is not enforced

* rename test

* rename variable

* update object-store

* make SharedRealm#temporaryDirectory volatile

* address findbugs error/Fixed bug in path when getting access tokens for Realms (#157)/Add cause to RealmMigrationNeededException (#3482)/Introduce global init (#3457)

Realm now uses a global init function instead of Context on the RealmConfiguration.Builder/fix merge mistakes/Merge branch 'master' into master-sync
Supporting both additive and manual schema modes (#91)

* Adding very thin wrappers for Object Store's ObjectSchema and Property.
* Adding method for building object schema is proxy classes
* Adding rudimentary support for Object Store schemas. Using ObjectStore::update_schema() to update schema for the additive mode.
* Disallowing destructive schema changes in additive mode./Upgrade to beta-33 / 2.0.0-rc4 (#90)/Invalidate schema cache when the schema version of Realm is changed by other process (#3409).

invalidate schema cache when the schema version of Realm is changed by other process.

Now schema cache referred by Realm instance is not shared and global schema cache is introduced instead./Public Sync API (#73)/Integrate Object Store [PART2] - SharedRealm (#3031)

This simplified our code base a lot. Basically all APIs we need from
SharedGroup/Group are wrapped in the SharedRealm. So we can just remove
those classes.

But we do need a few APIs from SharedGroup which is not supplied by
SharedRealm because of async queries. Currently we expose those in a
friend class of ShareRealm, see realm/realm-object-store#141

We are still managing Realm caches in Java although there are mechanism
in OS to do the same thing. The major reason is we have method like
deleteRealm needs information from cache to check if all Realm instances
are closed. The ShareRealm is actually a std::shared_ptr. We hold
the pointer to the std::shared_ptr in Java.
Another fundamental change is that we used to have separated
SharedGroups for DynamicRealm and typed Realm in the same thread. But
now they are using different SharedRealm but actually the different
SharedRealms are point to the same SharedGroup.

And some other code cleanup./Wait all async tasks done before next test (#3319)

This is highly related with #1900.
Below things are still guaranteed by this change after commit async
transaction:
* When callback function called (onSuccess/onError), the background
  Realm is closed.
* When any change listeners called, the background Realm is closed.

What is true now but it is not guaranteed in the future:
* Background Realm might not be closed before REALM_CHANGED sent (not
  received).

Due to this, to avoid the flaky tests, we have to ensure all async tasks
quit peacefully before start the next test. This is implemented by wait
and check in the TestRealmConfigurationFactory.

NOTE: Any test, if the async tasks cannot be finished peacefully in a
certain time, it has to be considered as a problem and fixed.

This would be needed by OS notifications since Java won't have a precise
control of sending REALM_CHANGED anymore./"
,,Realm,Updated changelog; combined duplicate implementations for both .first and .last/
,,Realm,Static native method needs jclass (#3276)/
,,Realm,"Nh/fix 3966 (#3979)

Realm migration is triggered, when the primary key definition is altered (#3966)/Fixed a bug that caused unexpected MigrationNeededException in very rare case. (#3768)

In sync mode, `validateTable()` must get all fields in the tabla./"
Data conversion,Data conversion,PlJava,"Testcase covering bug #1317   ,  Rewrite of the type mapping system   ,  "
Data conversion,Data conversion,PlJava,"Don't type parameters the same way as returns

What the TypeMapper was doing was correct for a function return value,
matching the return value to the narrowest mapped type that the actual
return value could be assigned to.

It was not correct for a function parameter, which needs to be matched
to the widest mapped type that could be assigned to the parameter.

Jens, if you ever read this, forgive me. At least I caught it quickly.   ,  Snippet implicitly requires its implementor-name.

That way, a snippet that will test some condition and enable that
implementor-name at install time can be declared to provide it, and
be emitted earlier. However, not to insist on supplying a snippet
that explicitly provides every implementor name ... some could just
be selected by the user with SET LOCAL pljava.implementors TO ...
before installing. So the implicit requirement will be counted in
indegree initially, to delay such a snippet as far as practical, but
only until a cycle-breaker releases it when no other progress can
be made.   ,  SQL generator now reports require cycles usefully.

Should have done that all along.   ,  Now Snippet can carry implementor-name.

Nothing sets it yet, but it now defaults to PostgreSQL for everything, in
recognition of the fact that hardly any of the PG SQL code that gets
generated here will be in the exact ISO-prescribed syntax of the exact
five allowed, unadorned SQL statements.

Yes, that makes DDR files bigger. The zip format will compress them well.   ,  Changes are required because of move of GETSTRUCT() and timeout
handling framework changes done in PG 9.3. Along with that I fixes
minor issue in DDRProcessor.java that is causing ""illegal start of
expression"" error. Maven did not worked for me to build it and hang
endlessly while building c source code (pljava-so). As a workaround I
temporarily fixed makefiles to test PG9.3 related fix that seems
worked and generated pljava.jar and pljava.so files and their basic
sanity seems working fine.   ,  Add a class annotation to make a base/scalar UDT.

- Add the annotation, have DDRProcessor recognize it.
- Check the annotated class for the required properties and members.
- The snippets map formerly allowed only one type of annotation on
  a given element. That happened to work, but not now when a class
  could have both a UDT and a SQLAction annotation, for example.
  Adapt the map to key by element and snippet class, so snippets
  of different classes can be hung on an element and selectively
  retrieved.
- An old comment in populateAnnotationImpl suggested it would reduce
  boilerplate setter code if, failing to find a setter method, the
  field itself could be reflectively looked up and stored. That's done
  now, so setter methods are needed only when something more special
  has to be done.
- Function declarations will be synthesized automagically for the four
  mandatory (in, out, recv, send) methods, but to allow their properties
  to be individually adjusted, they can still accept Function
  annotations. That's done by hanging a new subclass of FunctionImpl on
  those elements, that generates the right special form of declaration,
  and has setters that refuse changes to certain properties where that
  wouldn't make sense.
- Treat the default implementor-tag (PostgreSQL if not changed with new
  ddr.implementor command line property) specially. Now that everything
  is getting wrapped with an implementor tag by default, the implied
  requires=""implementor-tag"" was holding everything back until released
  by the cycle-breaker, sometimes in puzzling order. The implementor tag
  that happens to be the default one should /not/ be treated as a
  pending requirement.
- Move ComplexScalar to annotation subpackage, in preparation for
  revamping it as an annotation example. So git won't lose track of it,
  this is a breaking change, to be fixed in the next commit with edits
  corresponding to the move.   ,  Conform to BaseUDT rename, add MappedUDT.

Supporting MappedUDT is much simpler. It emits a
CREATE TYPE foo AS ( ... structure ... )
if a structure is provided, or not, if it isn't, followed by a
SELECT sqlj.add_type_mapping( ... ), and that's it.

Much javadoc also added.

This commit ends with pure renames of the Point and ComplexTuple
examples in preparation to make them annotation examples ... breaking
compilation until they are fixed up in the next commit, but git can
see where they went.   ,  Workaround the problem reported in #39.

The annotation processor in javac runs in multiple 'rounds', as long as
generated source files keep appearing, and one final round after they
don't. This code used to save some mapping of Class objects to the
javax.lang.model objects (TypeElement/TypeMirror) until the final round,
which worked in Java 6, but as of 7 one gets back different model objects
in different rounds, for the same types, and they don't match, so type
mapping breaks.

This workaround moves all those lookups to before or during round 1, when
a consistent set of model objects can be looked up. So, it will work as
long as all the source files that might contain pljava annotations will be
found in round 1. That should always be the case unless someone is using a
very fancy build with _other_ annotation processors generating new source
files with pljava annotations that have to be processed in additional
rounds. For now, that won't work, because their types won't seem to match
what was computed in round 1. So don't do that.

http://bugs.java.com/bugdatabase/view_bug.do?bug_id=8038455 might refer
to this problem, and promises a fix in Java 9, for what it's worth.   ,  Update annotation-driven SQL generation to Java 6.

Remove the dependency on org.objectweb.asm in favor of the
annotation processing facilities now built in to javac as of
Java 6. Add annotations to override the auto-determined type
for function parameters and/or supply default values, and to
supply arbitrary SQL commands to be included in the generated
deployment descriptor.   ,  "
,,PlJava,"Starting javadoc tweaks, some lexical definitions.   ,  "
,,PlJava,"(Unbreaking) complete the ComplexScalar rename.

Make the ComplexScalar example an annotation example, and remove the
corresponding code from the hand-maintained examples.ddr.   ,  Change annotation keywords discussed on -dev.

@Function(type=  ->  effects=
@Function(complexType= -> type=
@Function(trust=RESTRICTED|UNRESTRICTED -> SANDBOXED|UNSANDBOXED

@Trigger(when= -> called=
@Trigger(when='a boolean condition' new tracks PG9.0+

Also added called=INSTEAD_OF and columns={} to track PG.
That is, the SQL generator can recognize them and generate
the DDR; nothing is changed in PL/Java runtime yet.   ,  "
,,PlJava,"Conform to Point/ComplexTuple renames.

Completes the work on MappedUDT, with Point and ComplexTuple now
annotation examples, and the corresponding code removed from the
old hand-maintained examples.ddr.

In a couple last this-really-has-to-be-it review passes...

- some msg() calls had the Element parameter in the wrong place (and,
  being variadic and expecting extra parameters, didn't warn about it).
- whether to include length/alignment/storage in the output when 'like'
  is used should explicitly depend on whether they were set explicitly
  (is that explicit enough?); otherwise, in some cases it wouldn't be
  possible to override the values copied by like.
- removed a goofy constant in the VarlenaUDTTest left over from very
  early sanity checking.
- Shortened unnecessary full-qualification on some enums actually in
  scope.   ,  ComplexTuple.logAndReturn needs IMMUTABLE.

That's how it was declared in the hand-maintained examples.ddr, and
letting it default to VOLATILE makes it produce TupleDesc reference
leak warnings for some reason, so I've added IMMUTABLE to the annotation.

At the same time, onNullInput=RETURNS_NULL is clearly needed, just by
glancing at the function, which does no null checking. That's also true
of Point, so that's added both places. This demonstrates an advantage of
the SQL generation ... the place where you put the annotation is exactly
where you're looking when you're looking at the code.   ,  Change annotation keywords discussed on -dev.

@Function(type=  ->  effects=
@Function(complexType= -> type=
@Function(trust=RESTRICTED|UNRESTRICTED -> SANDBOXED|UNSANDBOXED

@Trigger(when= -> called=
@Trigger(when='a boolean condition' new tracks PG9.0+

Also added called=INSTEAD_OF and columns={} to track PG.
That is, the SQL generator can recognize them and generate
the DDR; nothing is changed in PL/Java runtime yet.   ,  "
,,PlJava,"Conditional DDR as proposed on pljava-dev.

Delayed by a nice pljava trivia brain teaser ... IF a function's
SQL declaration says it's immutable (or even stable, I guess), AND
you call it from the deployment descriptor install commands of the
jar that contains it, AND that's the first reference you've made to
its containing class ... ClassNotFoundException. Go ahead, explain THAT
one to Aunt Tillie.... (Hint: SPI change visibility rules.)   ,  "
,,PlJava,"Add testcase with enum parameter and return types.

Issue #4 causes these to fail.   ,  Change annotation keywords discussed on -dev.

@Function(type=  ->  effects=
@Function(complexType= -> type=
@Function(trust=RESTRICTED|UNRESTRICTED -> SANDBOXED|UNSANDBOXED

@Trigger(when= -> called=
@Trigger(when='a boolean condition' new tracks PG9.0+

Also added called=INSTEAD_OF and columns={} to track PG.
That is, the SQL generator can recognize them and generate
the DDR; nothing is changed in PL/Java runtime yet.   ,  "
,,PlJava,"Just noticed this example left the schema name off.   ,  Get annotated examples working in pljava-examples

For now, nothing done but to dump the annotated examples in with
the other ones (and change the name of one method that duplicated
one of the non-annotated ones), and further annotate the trigger
example to create the tables it refers to. (Also, one example
referred to an example.properties file, so one has been created.)

For now, the result is a jar with two deployment descriptors (which
is supposed to be ok per SQL/JRT 2003, so is now accepted): the
original one and the generated one.

Notes on things I know aren't perfect yet:

1. Maven doesn't put things in the jar in a deterministic order.
The generated ddr depends on the javatest schema and _properties
type created in the old ddr, but sometimes ""mvn clean install"" will
put the two ddrs in the jar in the other order. This is just a
maven issue; pljava correctly follows the spec and uses the order of
the ddrs found in the jar. It wouldn't be a problem if the two ddrs
had no interdependencies, or if the maven build could be tweaked
to force a certain order in the jar, or if the old ddr were done away
with entirely by annotating all the old examples so the generator
generates everything.

2. The generated ddr only reflects annotations seen in the compile run.
If a non-clean build run only runs javac on a few changed files, the
generated ddr can be incomplete. So for now it's best to run javac on
everything (or do a mvn clean before building). There may be a way to
improve the annotation processor to save a record of all the files that
would need to be reexamined in a partial rebuild. What I don't know yet
is how (or whether) that can be communicated back to javac to get those
files examined in a later round.

3. Naturally until issue #8 is fixed, none of the no-arg example
functions can actually be used.

4. The old examples.ddr contains a set search_path making javatest the
current schema, which causes deployRemove to fail later. The trouble is
that if deployRemove alters the search path on the current schema, it will
try to restore the old value after undeploying, and that fails if the
undeploy made that schema go away. Possibly deployRemove could be made to
just fail silently if that happens, but that's an issue for another day.   ,  "
,,PlJava,"Run unicode roundtrip test only on UTF8 server.

PostgreSQL's different definition of chr() for every encoding
other than UTF8 leads to spurious test failure for other
encodings.

Pushing directly - trivial change.   ,  Run UnicodeRoundTripTest only on 9.0+.

The query used in the test has ORDER BY inside aggregates
which would be a syntax error pre-9.0. Good use for deployment
descriptor conditionals. Also fix one silly error in the test.

Presuming that this also doesn't need a  .   ,  Test case for issue 21 Unicode handling.

This test case can be improved if rebased over   56
conditionals in DDR. The query contains ORDER BY in aggregates,
so needs to be conditional on PG 9.0+. I have also duplicated
the declaration change for the logmessage function, so that will
merge away if rebased over PR 56.   ,  "
,,PlJava,"fix 'SETOF SETOF VARCHAR' in autogenerated SQL

The annotation '@Function( complexType = ""SETOF VARCHAR"" )
was expanded to ""SETOF SETOF VARCHAR"" by the DDRWriter   ,  reactivate compilation of module pljava-examples   ,  "
,,PlJava,"Get that issue 52 test into the tree.

It is much slimmer when able to use a UDT annotation. :)   ,  "
DataBase Management,DataBase Management,PlJava,"Cluster name as property and in jps title.

Make the cluster name easily available if it is set (to a
nonempty string, in PostgreSQL versions where it is available),
as a java system property and in the title visible to jps
and jvisualvm.

In passing, lose the default vmoption to lock out jvisualvm local
connections. Noah Misch persuades me that PostgreSQL itself doesn't
make special efforts to prevent things that already require access
as the postgres user on the server host, and it would be unpostgresy
to do so here. Instead, simply document how to disable attachment,
if the admin so desires.   ,  Fix to allow building pljava with Microsoft Visual C

Code changes to allow compilation and linking with Microsoft
Visual C. Maven build process conditionalized to to detect Visual C
and adjust options appropriately. See msvc-build-notes.txt for
full details. Property names updated for clarity   ,  Add a GUC option for libjvm location.

The linker used to embed a dependency for libjvm into the pljava shared
object, which required using one of several system-specific ways to get
the system's library loader (not PostgreSQL's) to be able to find libjvm,
or loading pljava would simply fail.

By omitting the linked-in dependency on libjvm, pljava can now successfully
load and then use PostgreSQL's own dlopen wrapper to find libjvm using the
pljava.libjvm_location option, or give a helpful error report.

The history of PGDLLEXPORT through the years has been
somewhat bewildering, and it begins to seem tidier to cleanly define
a PLJAVADLLEXPORT that is used only here and doesn't change its meaning
across PG releases.   ,  PL/Java as a PostgreSQL 9.1+ extension.

Because PL/Java installation is already touched off by a simple
library LOAD (even on pre-9.1 databases without the extension
framework), extension support is largely a matter of having
extension scripts that touch off a LOAD. But the library does need
to detect that case and change some behaviors slightly. When LOAD
is used directly and something fails because the settings aren't
right, Backend reports a WARNING rather than an ERROR, to make it
as easy as possible to keep explore settings and eventually get them
right, without forcing a rollback. However, under CREATE EXTENSION,
it must be reported as an error, or the extension machinery will
think all is well.

Also, playing with the settings after CREATE EXTENSION failed can
result in getting PL/Java successfully installed but not as an
extension--easily remedied with CREATE EXTENSION FROM UNPACKAGED,
so a NOTICE suggesting that is reported in that case.

The method of finding the module pathname from the LOAD argument
doesn't work under CREATE EXTENSION (ActivePortal still refers to
the CREATE EXTENSION command itself, not the LOAD command in the
script), so as a workaround the script creates a temporary table
with the module pathname in it. This turns out to be useful anyway,
because it allows distinguishing the case of PL/Java itself being
installed by CREATE EXTENSION from the case where PL/Java is already
installed and its library is getting loaded in passing during
CREATE EXTENSION for something that depends on it. (Actual support
for PL/Java-managed extensions will have to come later, but the
case needs to be recognized even to say ""you can't do that yet."")

Reorganized the archives created by pljava-packaging so files are
at directory prefixes like pljava/pkglibdir, pljava/sharedir, etc.
That makes it easy to know where things should go in a manual
installation, but also opens the possibility of a self-extracting
archive that will use pg_config to look up the corresponding paths
at extraction time.   ,  MSVC profile using profile more, properties less.

This will have to be tested by someone with MSVC access. Its behavior
should be close to what I _think_ it should do. Possible nits: it puts
the MSVC extra include paths late in the list; if that's a problem
because they need to be early, I found syntax for that too, only a
little bit uglier. Also, an MSVC link will mention libjvm twice, once
in the right place (I think), but also still in the wrong one; I _think_
that should be harmless. Nothing another barrage of XML can't fix, if
it's a problem.   ,  Proclaim a default for pljava.classpath.

There may as well be an understood place to put the jar file so
the variable need not always be set.   ,  Workaround Windows creating_extension visibility.

Should now detect (in most cases?) when an extension is
being created, even in versions where creating_extension
isn't visible in Windows. Test depends on seeing the command
in ActivePortal; I am not sure what contexts could be contrived
where that wouldn't work right, but ordinary foreseeable cases
seem to work.

Got rid of pljavaInExtension: the idea that two cases have to be
distinguished (loading PL/Java itself as an extension, or using it
in the creation of some other extension) was sound, but the second
case isn't something that can be checked once at load time; it needs
a backend function that sqlj.install_jar can invoke whenever needed.   ,  Begin improving message helpfulness.

Some GUC hooks allow easily resuming the process if a setting
needed to be changed (as LOAD is a no-op after the first time,
so that isn't a way to try again).   ,  Begin new build and installation instructions.

Also document the existing examples, and add a new one, a fully worked
hello, world to demonstrate building, install_jar, and set_classpath.

Also add pljava.enable to simplify <9.2 installs, as explained in the
docs.

Alter the errhint for saving the settings in the <9.2 case.
Saving the settings with ALTER DATABASE ... SET won't work there, they
have to go in postgresql.conf.

Also masks the exact postgresql.conf path if the user isn't superuser.
I'm fairly sure there's no code path to produce this message if the user
isn't superuser, but a simple check is more readable than a proof.   ,  Conditional DDR as proposed on pljava-dev.

Delayed by a nice pljava trivia brain teaser ... IF a function's
SQL declaration says it's immutable (or even stable, I guess), AND
you call it from the deployment descriptor install commands of the
jar that contains it, AND that's the first reference you've made to
its containing class ... ClassNotFoundException. Go ahead, explain THAT
one to Aunt Tillie.... (Hint: SPI change visibility rules.)   ,  Add InstallHelper to keep hairy code from Backend.

Executive decision: attempts to ""finish"" the installation (creating
schema, language handlers, etc.) will only be made in the LOAD case.
In any other case where initialization is ed, assume things are
already set up, or explicitly being set up.   ,  Factor init sequence into a restartable routine.

The initsequencer routine can pick up where it left off, whether
started by _PG_init, by the internal call handler, or by a GUC hook
if the admin is trying a different config setting.

Continue moving bits out of the initializeJavaVM, etc., routines
into additional stages in the sequencer. Eventually: have a stage where
the JVM is running and a Java method can be called to check for and do
the SQL function declarations, SQLJ schema population, etc.

Ability to re-create the VM to try a new classpath is what I had hoped
for, but in many years of the JNI docs the restriction to one JVM per
process hasn't been relaxed yet, and it seems to mean you can't start
a second even after shutting down the first. So in many cases the admin
will have to exit and start a new session if installation fails at that
point. That's still not too bad.   ,  Shortcircuit assign hooks during abort.

Make sure assign hooks won't do things that could throw errors
during transaction abort.   ,  "
,,PlJava,"Make DEBUG1 quieter.

Move a bunch of DEBUG1s to DEBUG2, leaving DEBUG1 for the
initial load message that identifies PL/Java and JVM versions.
(This is NOTICE if PL/Java is explicitly LOADed, so it's seen
by default, but in other cases you can now see it by enabling
DEBUG1, and not mixed in with a lot of other stuff.)   ,  "
,,PlJava,"Thanks to Ken Olson reporting MSVC2010 complaints.   ,  InstallHelper groundwork() ensures good schema.

The language handler functions are created with CREATE OR REPLACE
so they will be sure to refer to the newly-specified native library,
without requiring a cascading drop of everything depending on them.

Most of what belongs in the schema can be autogenerated into a
pljava.ddr by annotations in Commands.java. No Deployer or install.sql
to keep in sync. (For some reason the byte[] parameters had to be
explicitly SQLType'd bytea, even though the DDRProcessor type mapper
is supposed to know that. A puzzle for another day.)   ,  Fail CREATE EXTENSION on nonempty schema.

Otherwise, someone might say CREATE EXTENSION meaning
CREATE EXTENSION FROM unpackaged, and have it appear to
succeed but without packaging any of the objects.   ,  Add InstallHelper to keep hairy code from Backend.

Executive decision: attempts to ""finish"" the installation (creating
schema, language handlers, etc.) will only be made in the LOAD case.
In any other case where initialization is ed, assume things are
already set up, or explicitly being set up.   ,  PL/Java as a PostgreSQL 9.1+ extension.

Because PL/Java installation is already touched off by a simple
library LOAD (even on pre-9.1 databases without the extension
framework), extension support is largely a matter of having
extension scripts that touch off a LOAD. But the library does need
to detect that case and change some behaviors slightly. When LOAD
is used directly and something fails because the settings aren't
right, Backend reports a WARNING rather than an ERROR, to make it
as easy as possible to keep explore settings and eventually get them
right, without forcing a rollback. However, under CREATE EXTENSION,
it must be reported as an error, or the extension machinery will
think all is well.

Also, playing with the settings after CREATE EXTENSION failed can
result in getting PL/Java successfully installed but not as an
extension--easily remedied with CREATE EXTENSION FROM UNPACKAGED,
so a NOTICE suggesting that is reported in that case.

The method of finding the module pathname from the LOAD argument
doesn't work under CREATE EXTENSION (ActivePortal still refers to
the CREATE EXTENSION command itself, not the LOAD command in the
script), so as a workaround the script creates a temporary table
with the module pathname in it. This turns out to be useful anyway,
because it allows distinguishing the case of PL/Java itself being
installed by CREATE EXTENSION from the case where PL/Java is already
installed and its library is getting loaded in passing during
CREATE EXTENSION for something that depends on it. (Actual support
for PL/Java-managed extensions will have to come later, but the
case needs to be recognized even to say ""you can't do that yet."")

Reorganized the archives created by pljava-packaging so files are
at directory prefixes like pljava/pkglibdir, pljava/sharedir, etc.
That makes it easy to know where things should go in a manual
installation, but also opens the possibility of a self-extracting
archive that will use pg_config to look up the corresponding paths
at extraction time.   ,  "
Threads Management,Threads Management,PlJava,"Fix to allow building pljava with Microsoft Visual C

Code changes to allow compilation and linking with Microsoft
Visual C. Maven build process conditionalized to to detect Visual C
and adjust options appropriately. See msvc-build-notes.txt for
full details. Property names updated for clarity   ,  Eliminate threadlock ops in string conversion.

The Java methods related to charset encoding/decoding may be called
repeatedly, and they don't require the threadlock to be released and
reacquired. I don't measure much difference in timing (I don't really
have a good ""average text"" corpus to test on; the test case for this
bug is worst case because it uses all of the biggest characters.)

Even without a compelling timing difference, the Java charset encoders/
decoders aren't thread safe, so I feel just that much better holding
on to the lock.   ,  "
,,PlJava,"Mavenized the C shared object library ""pljava.so"".

Moved C sources from src/C to their own Maven sub-module ""pljava-so"".
Adjusted .gitignore and the top level pom.xml accordingly.   ,  Fix latent bug in unused method.

Looks as if String_createJavaString would return null
for a zero-length text argument ... if anything ever
calls it, which may not be the case AFAICT.   ,  Last method to update: createJavaString(text*).

Caution: untested ... nothing in the current code base seems
to call this.   ,  Make String.c use Java's charset en/decoders.

In this commit, all the conversions in String.c except
String_createJavaString(text*) which can have a bug fixed first.   ,  "
,,PlJava,"Fix to allow building pljava with Microsoft Visual C

Code changes to allow compilation and linking with Microsoft
Visual C. Maven build process conditionalized to to detect Visual C
and adjust options appropriately. See msvc-build-notes.txt for
full details. Property names updated for clarity   ,  "
API Management,"API Management, database Management",PlJava,"Use PG 8.3 find_coercion_pathway API.

Ok, this compiles and doesn't break any -examples tests (and will give
errors for unhandled return cases, or a warning when disregarding
domain constraints). The reason it went unnoticed so long is that
Type_getCoerce{In,Out} are really very seldom called. They are almost
dead code--fairly contrived function declarations are needed even to
force them to be called for test purposes.

That seems to be mostly because Type_getCoerce{In,Out} won't be
called if Type_canReplaceType returns true, which it very often does,
because (a) Type_fromOid silently replaces domains with their base
types, and (b) Function builds the method signature by mapping the
SQL return type to a Java type and looking for a matching method, failing
if none is found, rather than looking for a method by its name and
parameter signature only, then validating its return type. So, essentially
by construction, coercion of the return type can't ever turn out to be
necessary.

It could, however, if the AS string gives the Java return type explicitly,
and it requires coercion to the (base type of the) SQL type. SQL/JRT has
no syntax to specify the Java return type, but as an extension it seems
PL/Java does: ""Function mapping"" in the wiki gives an example where it
precedes class.method, separated by a space. Only the example doesn't
work. That's because the getAS code accepts the syntax only when the
return type is purely alphanumeric (no dots, so non-package-qualified).
Not a recent change, has been that way since 2006. An = is also accepted,
though, even after a package-qualified return type. The example works if
retried with an =.

SQL/JRT, it seems to me, specifies the other approach to method
resolution, that is, find the method by name and parameter signature,
then work out what to do with its return type, but that would be a change
to current PL/Java behavior.

The unimplemented warning for RELABELTYPE to a domain type essentially
can't be triggered, because of the way any domain is replaced by its base
type before consulting canReplaceType. That's harmless for IN parameters.
For the return type, it's a type-safety hole: a PL/Java function can
return a value that isn't valid for its declared result domain. Fixing
that should be possible, but beyond the scope of this issue.

The not-implemented errors for COERCEVIAIO and ARRAYCOERCE can be
triggered, just by constructing exactly the sort of function declarations
where you would expect those things to happen. They would still be
arguably contrived cases, where the AS string specifies a method with
different types than would naturally correspond to the SQL ones.
Nevertheless, the ""Function mappping"" wiki page says ""PL/Java will use
a standard PostgreSQL explicit cast when the SQL type of the parameter
or return value does not correspond..."" and that's not completely true
at the moment, as long as some features available in standard PostgreSQL
explicit casts, like array or I/O coercion, aren't yet implemented.
Again, beyond the scope of issue 65. At least those cases now give clear
errors, instead of crashes or Krueger numbers as I just confirmed in a
build without this change, so this does in fact fix a latent bug.

Note that the single-row result set writers used for functions that
return composite or set-of-composite results have their own coercion
logic completely unrelated to this, and implemented in SPIConnection.java.
There are more moving parts here than I had hoped....   ,  Type_getCoerceOut correct statement order.

Ken Olson pointed this out as a case of 'dead code', but in fact it was
live use of an unassigned variable just before the function call that
was going to assign it. Had to be an editing mixup, has been that way
a very long time, and has probably not caused more problems only because
it's statistically rare for a random stack location to be exactly equal
to InvalidOid.

Scratches the surface of #65. A full solution for that issue will have to
come later, but this much was an obvious Heisenbug with an obvious fix.   ,  "
DataBase Management,DataBase Management,PlJava,"Add InstallHelper to keep hairy code from Backend.

Executive decision: attempts to ""finish"" the installation (creating
schema, language handlers, etc.) will only be made in the LOAD case.
In any other case where initialization is ed, assume things are
already set up, or explicitly being set up.   ,  PL/Java as a PostgreSQL 9.1+ extension.

Because PL/Java installation is already touched off by a simple
library LOAD (even on pre-9.1 databases without the extension
framework), extension support is largely a matter of having
extension scripts that touch off a LOAD. But the library does need
to detect that case and change some behaviors slightly. When LOAD
is used directly and something fails because the settings aren't
right, Backend reports a WARNING rather than an ERROR, to make it
as easy as possible to keep explore settings and eventually get them
right, without forcing a rollback. However, under CREATE EXTENSION,
it must be reported as an error, or the extension machinery will
think all is well.

Also, playing with the settings after CREATE EXTENSION failed can
result in getting PL/Java successfully installed but not as an
extension--easily remedied with CREATE EXTENSION FROM UNPACKAGED,
so a NOTICE suggesting that is reported in that case.

The method of finding the module pathname from the LOAD argument
doesn't work under CREATE EXTENSION (ActivePortal still refers to
the CREATE EXTENSION command itself, not the LOAD command in the
script), so as a workaround the script creates a temporary table
with the module pathname in it. This turns out to be useful anyway,
because it allows distinguishing the case of PL/Java itself being
installed by CREATE EXTENSION from the case where PL/Java is already
installed and its library is getting loaded in passing during
CREATE EXTENSION for something that depends on it. (Actual support
for PL/Java-managed extensions will have to come later, but the
case needs to be recognized even to say ""you can't do that yet."")

Reorganized the archives created by pljava-packaging so files are
at directory prefixes like pljava/pkglibdir, pljava/sharedir, etc.
That makes it easy to know where things should go in a manual
installation, but also opens the possibility of a self-extracting
archive that will use pg_config to look up the corresponding paths
at extraction time.   ,  "
Database Management,Database Management,PlJava,"Track PostgreSQL's changes to stack base access.

set_stack_base() and restore_stack_base() were added in preference to
direct access to stack_base_ptr (for 9.2, backpatched into 9.1.4,
9.0.8, 8.4.12, and 8.3.19). They require miscadmin.h to be included
in those source files that use the STACK_BASE_PUSH / STACK_BASE_POP
macros (there is so much in miscadmin.h, I did not want to add it in
pljava.h and have it processed everywhere).

Also added example ThreadTest.java just to exercise the new logic.
However, if enabled to run from the deployment descriptor, it does
deadlock; apparently PG -> J -> (otherthread PG) works, but the deeper
PG -> J -> PG -> J -> (otherthread PG) does not, and I *suspect* it's
because of the JNI warning against mispairing bytecode monitorenters with
JNI monitorexits. But that is what the code has done for years now, so
if nothing has been reported, there must not be high demand for that use
pattern.   ,  Make DEBUG1 quieter.

Move a bunch of DEBUG1s to DEBUG2, leaving DEBUG1 for the
initial load message that identifies PL/Java and JVM versions.
(This is NOTICE if PL/Java is explicitly LOADed, so it's seen
by default, but in other cases you can now see it by enabling
DEBUG1, and not mixed in with a lot of other stuff.)   ,  Rototill C source for PG_VERSION_NUM.

Having committed to PG 8.2 as a minimum version, simplify all
PGSQL_{MAJOR,MINOR,PATCH}_VER compiler conditionals to use
PG_VERSION_NUM (which appeared in 8.2), and completely eliminate
the branches for PG < 8.2.   ,  "
Feature migration,Feature migration,PlJava,"InstallHelper groundwork() ensures good schema.

The language handler functions are created with CREATE OR REPLACE
so they will be sure to refer to the newly-specified native library,
without requiring a cascading drop of everything depending on them.

Most of what belongs in the schema can be autogenerated into a
pljava.ddr by annotations in Commands.java. No Deployer or install.sql
to keep in sync. (For some reason the byte[] parameters had to be
explicitly SQLType'd bytea, even though the DDRProcessor type mapper
is supposed to know that. A puzzle for another day.)   ,  Fail CREATE EXTENSION on nonempty schema.

Otherwise, someone might say CREATE EXTENSION meaning
CREATE EXTENSION FROM unpackaged, and have it appear to
succeed but without packaging any of the objects.   ,  Part of issue #12, schema migration.

Add a naming system for schema variants that have existed
during PL/Java's VCS history. A quick check looks at the existing
tables, if any, to infer what schema is there.

This allows a second or subsequent LOAD to succeed without trying
to recreate tables, if a quick check suggests the existing tables
are as the code expects. If an earlier PL/Java schema is recognized,
and a recipe for migrating from it is available, the schema is
migrated. (Recommendation: do PL/Java upgrade in a transaction, so
schema changes can be rolled back if something is unsatisfactory.)

The two supplied recipes handle schemas as far back as 2006-02-26,
anyway.   ,  "
,,PlJava,"Updated to Java7 and clang for Mac OS X Mavericks   ,  "
,,PlJava,"Conditional DDR as proposed on pljava-dev.

Delayed by a nice pljava trivia brain teaser ... IF a function's
SQL declaration says it's immutable (or even stable, I guess), AND
you call it from the deployment descriptor install commands of the
jar that contains it, AND that's the first reference you've made to
its containing class ... ClassNotFoundException. Go ahead, explain THAT
one to Aunt Tillie.... (Hint: SPI change visibility rules.)   ,  InstallHelper groundwork() ensures good schema.

The language handler functions are created with CREATE OR REPLACE
so they will be sure to refer to the newly-specified native library,
without requiring a cascading drop of everything depending on them.

Most of what belongs in the schema can be autogenerated into a
pljava.ddr by annotations in Commands.java. No Deployer or install.sql
to keep in sync. (For some reason the byte[] parameters had to be
explicitly SQLType'd bytea, even though the DDRProcessor type mapper
is supposed to know that. A puzzle for another day.)   ,  Fix obsoleted query in sqlj.replace_jar.

Ken Olson caught this a year ago in his   #29, but
it was my oversight; when rearranging the sqlj relations back
in #10 to accommodate more than one deployment descriptor, I
updated the sql in install_jar and remove_jar (and deployInstall
and deployRemove), but I just totally overlooked replace_jar. Oops.   ,  SQL/JRT 2003: Jar may have > 1 deployment descr

Moving the annotation examples into pljava-examples creates a jar
with two deployment descriptors, the old one for the non-annotated
examples and the generated one. That's actually ok per SQL/JRT 2003,
but small changes to Commands.java and a new sqlj.jar_descriptors
table are needed to remove the former arbitary restriction to one
descriptor only.

Eventually the non-annotated examples could all be annotated,
leaving the generated descriptor as the only one. But as the
limit to one descriptor was at variance with the standard anyway,
it seemed worth removing.   ,  Spelling conformity

The jar_repository and jar_entry table names aren't plural, so
jar_descriptor should not be either.   ,  Run deployment descriptors in correct order.

Previously determined the order of multiple deployment descriptors in
a single jar according to the order of those entries as stored in the jar
(used in that order for install, and that order reversed for remove).

But that wasn't correct. I got my hands on 2003 and 2006 drafts of the
SQL/JRT spec and they both clearly say it is the order _as the entries
are listed in the manifest_ that matters (again, in that order for install,
and the reverse for remove).

This should be a welcome improvement, because I had noted back in
commit 0edc9e5f that maven doesn't always put things in a jar in
the same order, and that was causing the pljava-examples jar to be
broken about half the time (for autodeployment anyway). But the manifest
is a static file listing the ddrs in the right order, so as long as
maven doesn't reorder it while putting it in the jar, that behavior
should now be stable.   ,  "
Restructuring the code,Restructuring the code,PlJava,"Refactor SQLDeploymentDescriptor to check tags late.

No functional change (modulo log messages) should be seen yet.   ,  "
,,PlJava,"Generate SQL for trigger transition tables.

To accommodate PotgreSQL 10 transition tables, allow the Trigger
annotation to specify tableOld and/or tableNew, check that they
are allowed (trigger must be AFTER and include events capable of
populating the table), and generate the corresponding REFERENCING
OLD TABLE AS ... NEW TABLE AS ... in the trigger declaration.   ,  "
,,PlJava,"More regexps for common lexicals, a la jdk7.   ,  "
,,PlJava,"Add example for trigger transition tables.

Expand the Triggers example so the trigger actually does something,
and add a new trigger to test transition table functionality in
PostgreSQL 10 or later.   ,  "
,,PlJava,"Fulfill old todo for setting a libjvm default.

By passing -Dpljava.libjvmdefault=... on the mvn command line,
the downstream maintainer of a packaging system for a platform
where the standard Java install location is known can build
a package where the default for pljava.libjvm_location is
usually right.   ,  "
,,PlJava,"Register a trigger's transition tables.

The magic that makes tableOld/tableNew appear to exist under
the chosen names, for purposes of queries in SPI, happens when
SPI_register_trigger_data is called, passing the TriggerData
struct that was passed to the handler function, and that struct
has the chosen names and the tuplestores.

PL/Java doesn't do SPI_connect unless and until the called Java
function wants to use the jdbc:default:connection, at which point
the connection is created in Invocation.c and remembered for the
current level. So, that's a fine place to call SPI_register_trigger_data.
The TriggerData struct simply needs to be stashed in the current
Invocation at the time of function entry, so it can be passed to the
SPI function when the time comes.   ,  "
API Management,API Management,PlJava,"Handle widening of portalPos and demise of posOverflow.

No need for version conditionals around isPosOverflow: it isn't in
an API class, and nothing in this code base uses it, so out it goes.

This eliminates the other compile-time error that was blocking
compilation for PG 9.6, but it's not the last thing to have had
its width changed.   ,  "
,,PlJava,"Keep making DEBUG1 quieter.

These sites were missed in commit 1eb3bd8, trying to get the
PL/Java-loaded-versions announcement to be the only thing at DEBUG1.   ,  "
,,PlJava,"Handle the PG 9.6 widening of SPI_processed.

Includes a start on some Java 8 JDBC additions: getLargeUpdateCount()
and executeLargeBatch().   ,  "
,,PlJava,"Make create-extension-didn't message clearer.

The only error message resulting from a CREATE EXTENSION attempt
that failed (because the library had been loaded before in the
session, therefore LOAD was a no-op) is one about a table that
already exists.

But the name of the table appears in the error message, so the
table may as well be named
""see doc: do CREATE EXTENSION PLJAVA in new session""
which may serve to get the point across.   ,  "
,,PlJava,"Annotations doesn't support CREATE CONSTRAINT TRIGGER and clause FROM
schema.table
#138   ,  "
Data conversion,"Data conversion, database management",PlJava,"Include return types in most DDR method signatures

The 'AS' syntax for PL/Java function declarations already accepts
explicit return types as well as the parameter types, but the DDR
generator has not been emitting them. That means the magic doesn't
happen when, say, you have a Java method returning SQLXML but give
it type=""text"" in the @Function annotation. For PL/Java to notice
the automatic coercion needed there, it has to see that the Java
return type is not the one that naturally corresponds to the SQL
type ... so the Java return type needs to be included in the
signature.

Avoid, however, including the return type in the case of trigger
functions, or those that return sets or composite types. Those
already get special treatment in the function parsing code, and
the explicit return types would get in the way.   ,  Annotations doesn't support CREATE CONSTRAINT TRIGGER and clause FROM
schema.table
#138   ,  Recognize JDBC 4.2/JSR 310 types in DDR generator.   ,  Emit REMOVE section of DDR in right order (#163).

The implicitly-added provides/requires relationships between implementor
tags and the snippets that test the environment to activate or suppress
them need to be treated specially. Unlike other dependency
relationships, which have their sense reversed between the INSTALL and
REMOVE action groups, these implicit ones need to keep the same sense.
It always works better to test a condition before needing to use the
result.

The old augmentRequires method, which created the implied dependencies
by outright adding them to the snippet's other requires, is now gone;
the implied relationship is simply handled within the ordering code.
Two DAGs are created, one for install and one for remove, and an order
is found independently for each.

It's also necessary to use the deployStrings of tag-testing snippets
at undeploy time, at least in the case they don't have explicit
undeployStrings supplied, which is the usual case, because the
conditions to be tested are the same deploying and undeploying.   ,  Types.getArrayType() does the trick.   ,  Recognize type SQLXML in the annotation processor.   ,  Don't repeat yourself with UDT mappings.

It was never satisfying to define a UDT in Java through annotations
and still have the DDR generator give ""no mapping to an SQL type"" errors
for functions accepting or returning that type. It was easy enough (if
tedious) to work around by adding explicit type= or @SQLType annotations,
but the DDR generator had all the necessary information to figure that out
without such manual help. Now it does, eliminating the tedium
illustrated in jcflack/pljava-udt-type-extension@bcb5734.

To accommodate mappings added from the source being compiled, the
protoMappings collection is now of TypeMirror instances rather than
of Class instances.

Still future work: also add implied ordering dependencies for uses of
the new type (such as functions that have it as parameter or return
types and have to follow the type declaration, as illustrated in
jcflack/pljava-udt-type-extension@225ef5c, or that are named in it as,
for example, typmod in/out functions, and have to precede it).   ,  "
,,PlJava,"Test for when cursor/portal gets closed.

The _pljavaPortalCleanup callback actually doesn't get called until
end of transaction. But SPI_finish gets called at routine exit, which
is enough to make later use of the portal fail, if SPI_connect isn't
called first.   ,  "
,,PlJava,"Complete the mapped composite example.   ,  Confirm SQLXML can be read from a mapped composite.

It can; nothing more needed to be done (readSQLXML still
throws SQLFeatureNotSupported, but readObject returns SQLXML).

On the SQLOutput side, there is no general-purpose writeObject
as there is for statements and result sets. Checking that will
have to wait until writeSQLXML no longer throws NotSupported.   ,  Let PL/Java build where PG lacks XML type.

In Java, SQLXML will still be available, and can even be used as
function parameter and return types, as long as these are mapped
in the function declaration to the PostgreSQL type text, as the
type xml will not be present).   ,  "
,,PlJava,"Let annotations give row-type parameters defaults.

The @SQLType annotation can now supply an array of strings as
defaultValue for a parameter that has a row type (either the RECORD
type, whose structure is unknown, or a named row type).

In the case of RECORD, the only default that will be accepted is
{} (the array of no strings). While a bit limiting, that is still
just the ticket when a RECORD-typed parameter is being used to
supply a function with an optional, arbitrary sequence of named, typed
parameters.

In the case of a named row type, the default should be an array of
as many strings as the components of the type, and the strings should
be castable to the corresponding component types. The DDR generator
has no way to check that at compile time, but PostgreSQL will report
the error at jar install time if there is a mismatch.   ,  "
,,PlJava,"Do casts in ResultSetHandle by column definition list

A function declared to return RECORD or SETOF RECORD is required
by SQL to be followed by a column definition list in any query using it.
If the function returns SETOF RECORD using ResultSetHandle, there is a
possibility that one or more corresponding columns in the definition list
and the ResultSet offered by ResultSetHandle differ in type. Let the type
be cast automatically, just as it would be when using ResultSetProvider
and storing each value into the output ResultSet.

Also convert the SetOfRecordTest example to an annotation-style example
and add an SQLAction that tests it.

Addresses issue #146.   ,  "
,,PlJava,"Add a constraint-trigger example.

A more interesting test would be to actually try to insert 44, and
verify that the exception happens. (Oh, for the chance to use pgTAP...).   ,  "
,,PlJava,"Drop GUC_LIST_QUOTE flag on pljava.implementors.

It becomes disallowed for extensions to define GUC_LIST_QUOTE
variables as of PG 11 (846b5a5).   ,  Back-compat: fix Backend.c for PG < 8.4.

Among nits to fix, it pleasantly turns out that GUCs had boot values
even pre-8.4. They just had to be assigned in a different way. This
both tidies up the code in Backend and improves the experience of
setting up PL/Java in the oldest PG versions it supports.   ,  "
,,PlJava,"Introduce DualState class.

Yes, it is *another* pattern for objects that have Java and native state.
For now, the existing ones are being left alone. The existing patterns
do not seem to include anything for objects that are spec'd to live for
the whole transaction (as JDBC specs SQLXML to do). This pattern hooks
into PostgreSQL ResourceOwner mechanisms (and there is a TopTransaction
ResourceOwner) in order to be usable for that. If nothing blows up, it
may be possible to migrate other existing patterns to it later.   ,  "
,,PlJava,"Add PostgreSQL versions to greeting on load.

Some troubleshooting exercise somewhere is bound to be simplified
by reporting, at load time, the PostgreSQL version that was built
against and the version currently running.   ,  "
,,PlJava,"Use SPI_result_code_string more.

While thinking of SPI and its error codes, report those
as strings rather than numeric codes in a couple more places.   ,  "
Memory Management,Memory Management,PlJava,"Add VarlenaWrapper.Output.   ,  Allow returning/storing a 'readable' SQLXML.

Relax the distinction between 'readable' and 'writable' SQLXML
instances just enough to allow a readable one passed in (or, later,
retrieved from a ResultSet) to be directly returned (or, later,
passed as a parameter or into a ResultSet), as long as it is still
readable, i.e., has not had any of the reading methods called by Java.

It can be used that way exactly once, as the native code retakes
ownership of it at that point.   ,  Add VarlenaWrapper.

A VarlenaWrapper.Input presents a readable view of a detoasted
datum residing in native memory. It is constructed from native code,
specifying the memory context to (always copy or) detoast into, and
a ResourceOwner bounding the life of the native copy (which must be
released no later than the specified memory context is reset).

The available() method returns the actual detoasted size.

It would also be easy to support mark/reset, but nothing urgently
needs them at the moment.   ,  "
,,PlJava,"Mention JDBC 4.0 to 4.2 types in Oid.c.

These are still not given any mapping other than to InvalidOid,
but at least they are mentioned (conditionally, for the JDBC 4.2
ones) in the switch statement now.   ,  "
,,PlJava,"Add SQLXMLImpl.Readable.

This implements the reading (get...) operations, so it is the kind
of SQLXML instance that would be passed by PostgreSQL to a Java function
as a parameter, or retrieved from the ResultSet of a query.   ,  Allow returning/storing a 'readable' SQLXML.

Relax the distinction between 'readable' and 'writable' SQLXML
instances just enough to allow a readable one passed in (or, later,
retrieved from a ResultSet) to be directly returned (or, later,
passed as a parameter or into a ResultSet), as long as it is still
readable, i.e., has not had any of the reading methods called by Java.

It can be used that way exactly once, as the native code retakes
ownership of it at that point.   ,  Let PL/Java build where PG lacks XML type.

In Java, SQLXML will still be available, and can even be used as
function parameter and return types, as long as these are mapped
in the function declaration to the PostgreSQL type text, as the
type xml will not be present).   ,  "
,,PlJava,"Likewise for java.time.LocalDateTime.   ,  "
Database Management,Database Management,PlJava,"Introduce DualState class.

Yes, it is *another* pattern for objects that have Java and native state.
For now, the existing ones are being left alone. The existing patterns
do not seem to include anything for objects that are spec'd to live for
the whole transaction (as JDBC specs SQLXML to do). This pattern hooks
into PostgreSQL ResourceOwner mechanisms (and there is a TopTransaction
ResourceOwner) in order to be usable for that. If nothing blows up, it
may be possible to migrate other existing patterns to it later.   ,  Javadoc 8 nits, and other tidying.

Flesh out the existing toString methods to produce more useful
information for debugging.   ,  Add a SinglePfree subclass of DualState.

This should cover many cases where the native component of the state
is a single pointer that only wants to be pfree'd when no longer needed.   ,  Make toString more useful for DualState.   ,  Lazy detoast, working in PG 11beta3, 10, 9.6.

Introduce lazy detoasting, where if the value underlying a readable
SQLXML instance is found not to be fully detoasted at the time of
construction, it's allowed to stay that way until the contents are
actually needed, as discussed in

https://www.postgresql.org/message-id/1c64290b-b729-eeab-219e-1577a12e9b5a%40anastigmatix.net

(and above and below in the same message thread), implementing the
""just use the oldest one"" snapshot selection strategy from

https://www.postgresql.org/message-id/8ca78589-734b-f904-1cc5-007eeb5d4737%40anastigmatix.net

The implementation passes the following test inspired by Andrew Gierth in
https://www.postgresql.org/message-id/877eovbjc3.fsf%40news-spur.riddles.org.uk

CREATE TABLE t(x xml);
BEGIN READ WRITE, ISOLATION LEVEL READ COMMITTED;
/*
 * In other session: INSERT INTO t(x)
 * SELECT table_to_xml('pg_operator', true, false, '');
 */
SELECT javatest.echoxmlparameter(x, 0, 5) FROM t; -- 0 => stash x
/*
 * In other session: DELETE FROM t;
 * VACUUM t;
 */
SELECT javatest.echoxmlparameter(null, 5, 5); -- null => unstash
COMMIT;

And, indeed, the same test is made to fail by commenting out the
snapshot registrations/unregistrations in VarlenaWrapper.c, so
they are performing the expected valuable service. Without them:
ERROR:  missing chunk number 0 for toast value 16716 in pg_toast_16708   ,  Add DualState.SingleMemContextDelete.

A subclass of DualState that encapsulates a reference to a single
PostgreSQL memory context, and will delete it if the Java reference
is released or garbage collected.   ,  "
,,PlJava,"Use getSQLTypeName, and add a test.

Provide one of the short-term solutions suggested in issue #149.

As the current PreparedStatement implementation does not get the
inferred parameter types from PostgreSQL (which became possible
with SPI only as recently as PostgreSQL 9.0), its setObject method
must make a best effort to map in the other direction, finding the
PostgreSQL type that corresponds to the Java parameter value. In
one case, this is easily made much more reliable: when the Java
parameter value is an SQLData instance (a UDT), and therefore has
a getSQLTypeName method, unused until now.

Add a test method (in the ComplexTuple UDT example) to confirm that
the type is properly mapped when passed as a parameter to a
PreparedStatement.   ,  "
Database Management,Database Management,PlJava,"Add VarlenaWrapper.Verifier.

A Verifier can be set on a VarlenaWrapper.Output to read along as bytes
are written to the varlena, and throw an exception if they do not make a
well-formed representation of the expected type. This is not necessary
if the PL/Java code for a given type retains complete control over what
is written to the varlena, but is, if an API requires exposing the
VarlenaWrapper.Output more or less directly to client code that could
write arbitrary content on it.

The code that implements a specific data type over varlena can create a
subclass of Verifier.Base, whose verify(InputStream) method simply reads
the whole stream and returns successfully if nothing is wrong, otherwise
throws an exception. VarlenaWrapper.Output will take care of submitting
that verify method to a thread pool, feeding it buffers of written data
as they are filled, and collecting its success or exception status when
the stream is closed (or canceling the task when appropriate).   ,  Add VarlenaWrapper.Output.   ,  Allow returning/storing a 'readable' SQLXML.

Relax the distinction between 'readable' and 'writable' SQLXML
instances just enough to allow a readable one passed in (or, later,
retrieved from a ResultSet) to be directly returned (or, later,
passed as a parameter or into a ResultSet), as long as it is still
readable, i.e., has not had any of the reading methods called by Java.

It can be used that way exactly once, as the native code retakes
ownership of it at that point.   ,  Javadoc 8 nits, and other tidying.

Flesh out the existing toString methods to produce more useful
information for debugging.   ,  Re{factor|package} M.S.I.Stream and VarlenaWrapper

There is nothing very JDBC-specific about MarkableSequenceInputStream,
so move it from o.p.p.jdbc to o.p.p.internal. VarlenaWrapper.Input is
mostly made up of what could be a more general ByteBufferInputStream,
so factor that out into its own class.   ,  Add VarlenaWrapper.

A VarlenaWrapper.Input presents a readable view of a detoasted
datum residing in native memory. It is constructed from native code,
specifying the memory context to (always copy or) detoast into, and
a ResourceOwner bounding the life of the native copy (which must be
released no later than the specified memory context is reset).

The available() method returns the actual detoasted size.

It would also be easy to support mark/reset, but nothing urgently
needs them at the moment.   ,  "
,,PlJava,"Javadoc, Overrides in ObjectResultSet.   ,  Implement JDBC 4.1 ResultSet.getObject.

This is the version that takes a Class to determine the class of result
to return. To preserve back-compatibility for now, the underlying
implementation method is not constrained to return an object of the
ed class, and accepts null for that argument, and all the
pre-existing methods call it that way, to get the previous behavior.

Per a comment in Type.h, it's always been the idea that TypeObtainer
methods return singletons (or return from a small number of allocated
instances, if there need to be instances with a few different typeIds).
That doesn't seem to have been followed always, but it grows in
importance with this change; certainly any new TypeClass that is added
and expected to be used in a JDBC 4.1/4.2ish way should be careful not
to allocate a new Type instance every time it gets mentioned.   ,  "
API Management,API Management,PlJava,"Annotate Connection methods and add more comments.

Add @Override to the methods in SPIConnection that are specified
by the Connection API (to make it easier to spot the ones that
aren't). For those added in JDBC 4.1, the annotation is commented out,
as PL/Java 1.5 still strives to be buildable with Java 6. Once the
back-compatibility horizon is Java 7 or later, those @Overrides can
be uncommented.

It would also be fair to say this has added annotations (or
commented-out annotations) through JDBC 4.2, as it didn't add
any new Connection methods.

The PL/Java-specific and internal methods are now easier to pick out
(they're the ones without @Override annotations), and have some more
extensive comments about what they're doing there. Also moved one
method to be nearer the stuff it pertains to. No code changes (except
to add the specified generic signature on getTypeMap/setTypeMap).
Indentation adjusted in a couple contiguous areas.   ,  "
,,PlJava,"Put TypeBridge in play for SPIPreparedStatement.

In passing, complete the implementation of the 3-argument form of
setNull (where the third argument is a PostgreSQL type name), which is
the only method on a PreparedStatement by which a parameter type can be
set to a specific PostgreSQL type (as opposed to a generic JDBC type
with some default mapping to a PostgreSQL type).

However, the implementation is still limited, in that any subsequent
non-null assignment will reset the PostgreSQL type to the default
mapping, if done through one of the pre-JDBC 4.2 setter methods, or
setObject with one of the pre-JDBC 4.2 accepted object classes. That is
to avoid a material behavior change in a minor release.

By contrast, if setObject is called with an object of one of the
newly-accepted classes (which had no prior behavior to try to match),
any already-assigned PostgreSQL type (as by setNull) will be respected,
and the setObject treated as a coercion to it (which can entail an
exception if no such coercion is available).

That new behavior ought to be the way everything behaves in some future
major release, which should use the PG 9.0 improved SPI to determine
the parameters' PostgreSQL types from what PG's type inference came up
with, and treat all the setter methods as coercions to the determined
types.   ,  Use getSQLTypeName, and add a test.

Provide one of the short-term solutions suggested in issue #149.

As the current PreparedStatement implementation does not get the
inferred parameter types from PostgreSQL (which became possible
with SPI only as recently as PostgreSQL 9.0), its setObject method
must make a best effort to map in the other direction, finding the
PostgreSQL type that corresponds to the Java parameter value. In
one case, this is easily made much more reliable: when the Java
parameter value is an SQLData instance (a UDT), and therefore has
a getSQLTypeName method, unused until now.

Add a test method (in the ComplexTuple UDT example) to confirm that
the type is properly mapped when passed as a parameter to a
PreparedStatement.   ,  "
API Management,API Management,PlJava,"Implement JDBC 4.2 SQLInput.getObject.   ,  Merge branch 'REL1_5_STABLE' into trackjdbc/REL1_5_STABLE/SQLXML

That makes it possible for SQLXML to be implemented on top of the
recently merged JDBC 4.1/4.2 methods that allow a Class<?> parameter
to Object getter methods (and, in the other direction, can accept
objects of multiple classes and appropriately convert them to the
underlying PostgreSQL type).

This solves a problem with including SQLXML in a minor PL/Java
release: it should not change what existing application code
expects the getter methods to return for an XML type, which has
historically been String. It will still be String by default,
but an application can pass SQLXML.class to the explicit getter
methods and take advantage of this API. Function parameters and
returns can be given either type String or type SQLXML with the
expected results.

In technical terms, this means when pljava_SQLXMLImpl_initialize
calls Type_registerType2, it now passes InvalidOid instead of
XMLOID. It therefore does not become the primary mapping for XMLOID,
but does associate itself with the java.sql.SQLXML interface.
There is also a TypeBridge registered for it, which happens in
Type.c: registration of TypeBridges should happen all in that one
place, because (for now, anyway) the maintainer is responsible
for registering them in an order that does not allow a superclass/
superinterface to be found before a narrower contained type.

MANUAL ADAPTATIONS ARE CONTAINED IN THIS MERGE COMMIT as described
above.   ,  "
,,PlJava,"Take off the training wheels.   ,  "
Data conversion,"Data conversion, database management",PlJava,"Close SAXResult upon endDocument().

To comply with the API spec, SQLXML has to throw an exception if
passed to the database to be stored before being closed. However,
a SAXResult exposes no way for the caller to explicitly close it,
so wrap it in an adapter that will do so after the endDocument()
is processed.   ,  Add SQLXMLImpl.Readable.

This implements the reading (get...) operations, so it is the kind
of SQLXML instance that would be passed by PostgreSQL to a Java function
as a parameter, or retrieved from the ResultSet of a query.   ,  Test imperfectly for DOCUMENT/CONTENT in Readable.

The current stored form of XML in PostgreSQL does not save any
indication of whether the XML is in DOCUMENT or CONTENT form (which,
to determine correctly, requires fully reparsing it). Java's bundled
XML parsers do not accept the CONTENT form, requiring a synthetic
document root element to be wrapped around the stream content first,
then filtered out of the parse result. This, of course, is not necessary
if the input is in DOCUMENT form, but is harmless unless the input is in
DOCUMENT form with a DTD, in which case such wrapping must be avoided.

An imperfect test is added here that uses an input stream supporting
mark/reset, and s a few initial parse events before resetting the
stream for the application to parse. Those initial events only need to
be ed until a DTD is parsed (meaning the input is definitely
DOCUMENT), or a disallowed-in-prolog production is parsed (meaning it
definitely isn't), or an element-start is parsed (meaning it could be
either, barring a full reparse to settle the question). The last two
cases can both be handled by the wrapping technique, so don't need to be
distinguished further.

The JDBC spec is mute on how SQLXML should handle DOCUMENT vs.
CONTENT. For the forms of Source that expose parse results,
wrapping and filtering is effective, but it would seem a POLA
violation to include synthetic wrapper elements via
getBinaryStream, getCharacterStream, or getString (or getSource
with StreamSource.class). So, for those cases, no wrapping.
The application code will need to know what it's doing if the
XML value might be of CONTENT form.

Because the JRE-supplied SequenceInputStream doesn't support mark/reset,
adds a (possibly general-purpose) MarkableSequenceInputStream that does,
as long as its constituent input streams do.   ,  Allow returning/storing a 'readable' SQLXML.

Relax the distinction between 'readable' and 'writable' SQLXML
instances just enough to allow a readable one passed in (or, later,
retrieved from a ResultSet) to be directly returned (or, later,
passed as a parameter or into a ResultSet), as long as it is still
readable, i.e., has not had any of the reading methods called by Java.

It can be used that way exactly once, as the native code retakes
ownership of it at that point.   ,  Add a Verifier for SQLXMLImpl.Writable.

Because the stored form in PostgreSQL of the XML data type is just its
character serialization, of all the methods offered by java.sql.SQLXML
for reading/writing the content, the ones based on strings, character
streams, and byte streams are the quickest ones to get working in a lazy
sort of way.

But the lazy way isn't appropriate for production, at least for the
writing methods, because type safety precludes letting client code write
arbitrary bytes into a varlena that PostgreSQL believes to be of XML
type. So the quick, lazy methods ultimately become the ones that need
the complexity of a Verifier making sure what is written is XML.

The other writing methods, where the serialization is done via Java XML
APIs, are left with the NoOp verifier, trusting the APIs to produce XML.   ,  Honor API spec for namespaces with getSource().

""Sources for XML parsers will have namespace processing on by default.""   ,  Add SQLXMLImpl.Writable.   ,  Ensure declaration, if any, is accurate.

The current PostgreSQL stored form for the XML type is a serialized
character string in the server encoding, still preceded by any
XMLDecl or TextDecl that preceded it, even if wrong (about the
encoding, anyway). PG's xml_out and xml_send deal with that by
replacing the decl, if any, with a newly-constructed one, and have
their own problems, but that's the basic technique reproduced here.   ,  "
API Management,API Management,PlJava,"Derive a cache token from the entryId.

This is dead simple, and workable because (in the current, 1.5.0
sqlj schema), the entryId is a SERIAL column, and all entries are
deleted/reinserted by replace_jar. Of course this means that every
cached class from the jar is invalidated even if few have changed,
but optimizing that can be future work.   ,  Share PL/Java and application classes in OpenJ9.

The OpenJ9 JVM has a facility for dynamically saving AOT-compiled
classes in a shared cache, which is even available for classes of
PL/Java application code, provided PL/Java's class loader does a
bit of coordination with J9's class sharing API.

The token generated for the J9 SharedClassTokenHelper is, for now,
fixed, which will be bad news if a jar is updated. It should be
computed in a way that ensures it is different after an update.   ,  "
,,PlJava,"Use script task to quote libjvmdefault as C string

This is needed because the current practice of passing the default JVM
path as a command-line macro definition to the C compiler and relying
on C preprocessor stringification to make a string literal of it only
works when the path includes no word-like components that collide with
macros in existence during the build. (Issue #176)

It seemed tidier to use scriptdef, but that plain doesn't work
from a maven-antrun-plugin configuration; the plugin writes out a
script file containing every part of the scriptdef except the script.


So, for now, this is done with a script tag, no scriptdef.   ,  "
,,PlJava,"DualState-ify ExecutionPlan.   ,  I've had it with these typos.

Also, there was an orphaned comment in pljava.h describing the field
that was no longer there, but in Invocation.h. The comment for it in
Invocation.h is adequate, so the one in pljava.h is removed.   ,  "
Memory Management,Memory Management,PlJava,"DualState-ify Tuple.

Still has a public getNativePointer() because Relation, TriggerData,
SQLOutputToTuple, and SingleRowWriter refer to it.

In passing, it appears that TupleTable is implemented by constructing
a Java array populated with individual Java Tuple instances,
one for every tuple in the table.

There's got to be a lazier way....   ,  DualState-ify ErrorData.   ,  DualState-ify TupleDesc.

Still has a public getNativePointer() because both SingleRowReader
and Tuple refer to it.   ,  Prepare to migrate to DualState from other idioms.

Older code where a Java object is associated with some native state has
been using some combination of Object.finalize() (deprecated for removal
in newer Java environments), JavaWrapper, Invocation_createLocalWrapper,
with a lot of details scattered in a lot of places, and not a lot of
explanation.

The DualState mechanism introduced with VarlenaWrapper should be able
to subsume all of those cases, eliminate the use of Object.finalize,
and reduce (maybe eliminate?) the use of JNI from threads other than
the main PG thread.

Prepare the way by declaring that the ""resource owner"" associated with
a DualState does not have to be only a PG ResourceOwner. It can also be
the address of some other sort of allocated struct that has a scope-like
behavior. In PG 9.5 and later, a MemoryContext with a
MemoryContextCallback could serve, and PL/Java's currentInvocation can
be used to scope a state to the currently-executing PL/Java function.
Unless the memory allocator's broken, such alternative allocated
addresses won't be mistakable for actual ResourceOwners, so no special
magic should be needed for DualState objects scoped in any of those ways
to coexist.

In passing, change the queue implementation for s_liveInstances in
DualState. LinkedBlockingDeque was more heavyweight than needed; it
was chosen in haste after starting with ConcurrentLinkedDeque and then
finding it wasn't in Java 6, but ConcurrentLinkedQueue is, and has all
the capabilities needed. Its iterator().remove() method really runs in
constant amortized time, as it just leaves the unlinking to be done on
a subsequent traversal, which happens regularly in our case.

If it can be proven that no manipulation of s_liveInstances will happen
except on one thread interacting with PG, it could be overkill to be
using a concurrent queue at all.

Get rid of HeapTupleHeader.java, which has been dead since a4f6c9e.   ,  An optimization for non-transient native scopes.

DualState works when NULL is passed as a resource owner, in which
case the native state had better be in some long-lived area like
JavaMemoryContext, because no call to nativeRelease with a real pointer
will ever match it, so javaStateUnreachable or javaStateReleased will
be the only ways it can be freed. (It follows that any DualState flavor
that will be used that way had better have a freeing action as part
of its javaStateUnreachable / javaStateReleased.)

It seems preferable to keep things in more tightly scoped contexts and
give them real owners, but this pattern was used in enough of the
former-JavaWrapper cases just migrated that it's worth supporting
(at least until further analysis can migrate some of those to use
appropriate scopes).

So, a worthwhile optimization is to track objects that lack resource
owners in a simpler data structure, that does not need to support any
retrieval by resource owner. They still have to be kept live somehow
(otherwise Java forgets about them and will not enqueue them when their
referents are unreachable), but a simple IdentityHashMap suffices, and
they can be removed in O(1) when found unreachable or released by Java.

For objects with resource owners, the tracking structure is now a hash
map from owner to doubly-linked list of the associated instances. The
list is directly implemented with prev and next references in the
instances themselves, rather than using a Collections class, so that
when an instance is found unreachable or released from Java, it also can
be removed from the list in O(1) with no searching. Exit of the native
resource owner scope is, of course, handled by looking up the owner in
the map and processing every instance remaining on the list.

The structures used now are plain vanilla java.util implementations with
no claimed thread safety properties (and the same is true of the
directly-implemented doubly-linked list code). The Javadoc now explains
at length the usage requirements that must be observed for the design to
be safe.   ,  Expose some DualState statistics as an MBean.   ,  "
Threads Management,Threads Management,PlJava,"Simplify ""thread ID"" tracking.

The mechanism for fudging the stack base to mollify PostgreSQL's
stack depth check when the Java thread executing in PG changes
(only possible when pljava.java_thread_pg_entry is 'allow') has
long involved computing the System.identityHashCode() of the
current Thread object, and passing that to the native code to
be compared to that of the last known thread running in PG code.

That was unnecessarily roundabout, given that every native method
call passes a JNIEnv pointer to the native code, which the JNI spec
""design overview"" clarifies to be valid only per thread, while
guaranteeing to pass the same pointer in calls from the same thread,
so it has the properties needed of an ID.   ,  DualState-ify ExecutionPlan.   ,  "
,,PlJava,"Added a thorough test of DatabaseMetaData features.   ,  Fixed bug occuring sporadically on re-entry.   ,  *** empty log message ***   ,  Testcase covering bug #1317   ,  *** empty log message ***   ,  *** empty log message ***   ,  Added ResultSetTest   ,  Added SecurityManagers for trusted and untrusted versions   ,  *** empty log message ***   ,  Added language javaU with an untrusted call handler   ,  Rewrite of the type mapping system   ,  Added class resource handling and example of use.   ,  Added the ability to return a ResultSet
Added object pooling capabilities   ,  *** empty log message ***   ,  *** empty log message ***   ,  Fixed bug causing primitive arrays to fail.   ,  *** empty log message ***   ,  "
,,PlJava,Improve Data Access
,,PlJava,Exception Management Throw Exception Null Pointer
,,PlJava,Fix of bug 915 and 916/7.4 compatibility/try/catch changes/
,,PlJava,Fixed some gcjh compiler problems/
,,PlJava,GCJ bug #20193/
,,PlJava,"Fixed bug causing primitive arrays to fail./Improved caching of class./More forgiving getXXX methods on ResultSet with respect to types.
Made use of Oid more strict. The Oid class is used instead of int.
Moved TypeMap functionality into the Oid class./"
,,PlJava,"From now getWarning throws an exception on a closed statement, according to JDBC specification/"
,,PlJava,Coersion for getXXX now applies to updateXXX as well/
,,PlJava,Added test of transaction recovery using an anonymous savepoint/
,,PlJava,"Added the ability to return a ResultSet
Added object pooling capabilities/"
,,PlJava,Added missing FlushErrorState after CopyError/Added BlobValue coersion to the bytea type./Fix for bug #1228/
Data conversion,Data conversion,PlJava,Added BlobValue coersion to the bytea type./
,,PlJava,"Backport of things missing in PostgreSQL 7.4/Backport of things missing in PostgreSQL 7.4/Fixed bug# 1218/Removed some obsolete #ifdef GCJ/*** empty log message ***/Added the ability to return a ResultSet
Added object pooling capabilities/"
,,PlJava,Runtime detection of integer-datetimes/
,,PlJava,Fixed bug #1215/
,,PlJava,Rewrite of fence mechanism/
Memory Management,Memory Management,PlJava,Removed JavaHandle and complex MemoryContext stuff that was no longer needed./Rewrite of fence mechanism/
Memory Management,Memory Management,PlJava,"Rewrite of fence mechanism/Dropped support for versions prior to 8.0.
Fixed bug causing stack check failur when backend was called from thread other than main
Fixed stability issue related to GC and MemoryContexts/"
,,PlJava,Fixed bug causing crash after fence rewrite when returning tuples./Changes needed in order to compile with 8.0.x/Rewrite of fence mechanism/
Restructuring the code,Restructuring the code,PlJava,"Reinstated Exception traces when log level >= DEBUG1
Merged Invocation and CallContext
Cleanup and refactoring/Rewrite of fence mechanism/"
,,PlJava,Fixed signed/unsigned warnings/Fixed signed/unsigned warnings/Rewrite of fence mechanism/
Threads Management,Threads Management,PlJava,"Dropped support for versions prior to 8.0.
Fixed bug causing stack check failur when backend was called from thread other than main
Fixed stability issue related to GC and MemoryContexts/Changes needed in order to build with 8.1beta2/"
,,PlJava,Changes needed in order to compile with 8.0.x/Rewrite of fence mechanism/
,Restructuring the code,PlJava,Refactoring and cleaning up./Rewrite of fence mechanism/
,,PlJava,Testcase covering bug #1317/
,,PlJava,Fixed bug #1506. Jar owner in the sqlj.jar_repository table changed type from oid to name./
Memory Management,Memory Management,PlJava,Fixed some memory leaks/
,,PlJava,Fixed bug causing incorrect representation of Time when DST is in effect./
Data conversion,Data conversion,PlJava,Rewrite of the type mapping system/
Restructuring the code,Restructuring the code,PlJava,Type system refactoring/Added primitive array types/
Restructuring the code,Restructuring the code,PlJava,Type system refactoring/
,,PlJava,Fixed some issues with Meta-data/
,,PlJava,"Batch execution of PreparedStatements failed because addBatch was
only saving off the parameter values while it ignored the parameter
types.  Save both the types and values.

Reported by Lucas Madar
Bug #1010183/"
,,PlJava,"fix 'SETOF SETOF VARCHAR' in autogenerated SQL

The annotation '@Function( complexType = ""SETOF VARCHAR"" )
was expanded to ""SETOF SETOF VARCHAR"" by the DDRWriter/reactivate compilation of module pljava-examples/"
Restructuring the code,Restructuring the code,PlJava,"Refactor SQLDeploymentDescriptor to check tags late.

No functional change (modulo log messages) should be seen yet./"
,,PlJava,Just noticed this example left the schema name off./
,,PlJava,"Handle PostgreSQL 8.1+ roles correctly.

At least, cover the cases that obviously mess up in execution of
deployment descriptors.

This represents an executive decision that PG < 8.1 is no longer
supported./"
,,PlJava,"Run unicode roundtrip test only on UTF8 server.

PostgreSQL's different definition of chr() for every encoding
other than UTF8 leads to spurious test failure for other
encodings.

Pushing directly - trivial change./"
,,PlJava,"More regexps for common lexicals, a la jdk7./"
Data conversion,Data conversion,PlJava,Limit add/drop of type mappings./
,,PlJava,"Rework SQLInputFromChunk using direct bytebuffers.

This will allow accommodating different byte orders, using the
provisions built into ByteBuffer./"
,,PlJava,"Rework SQLOutputToChunk using direct bytebuffers.

This will allow accommodating different byte orders using the provisions
built into ByteBuffer./"
,,PlJava,"Keep making DEBUG1 quieter.

These sites were missed in commit 1eb3bd8, trying to get the
PL/Java-loaded-versions announcement to be the only thing at DEBUG1./"
,,PlJava,"Annotations doesn't support CREATE CONSTRAINT TRIGGER and clause FROM
schema.table
#138/"
,,PlJava,"Add a constraint-trigger example.

A more interesting test would be to actually try to insert 44, and
verify that the exception happens. (Oh, for the chance to use pgTAP...)./"
,,PlJava,"Use SPI_result_code_string more.

While thinking of SPI and its error codes, report those
as strings rather than numeric codes in a couple more places./"
Compiler Management,Compiler Management,Jpype,"Fixed tests not compiling   ,  "
Exception Management,Exception Management,Jpype,"Make jpype.JException catch exceptions from subclasses.   ,  "
Restructuring the code,Restructuring the code,Jpype,"avoid copy construction in getters   ,  "
Restructuring the code,Restructuring the code,Jpype,"[platform-adapter] minor changes   ,  "
Compiler Management,Compiler Management,Jpype,"[setup] use a fallback jni.h, if user did not succeed in providing a valid JAVA_HOME containing a JDK.   ,  "
Compiler Management,Compiler Management,Jpype,"fix some compiler warnings -wwrite-strings   ,  "
Memory Management,Memory Management,Jpype,"some global references were cleaned with local reference cleaners and vice versa (reverted from commit c2f1ecb51149228f1fc195c2a7855c24eff97cb0)   ,  "
Exception Management,Exception Management,Jpype,"add exception enhancement from tcalmant

https://github.com/tcalmant/jpype-py3/wiki/py3_cpp   ,  "
Compiler Management,Compiler Management,Jpype,"Improve proxy tests and make them python 3 compatible.   ,  "
Memory Management,Memory Management,Jpype,"improve overload resolution and support default methods
 * resolve overloads according to ""maximally specific overload"" rule
 * cache types and overload order
 * support java 1.8 default methods by loading all public methods (not only declared)   ,  "
Memory Management,Memory Management,Jpype,"improve overload resolution and support default methods
 * resolve overloads according to ""maximally specific overload"" rule
 * cache types and overload order
 * support java 1.8 default methods by loading all public methods (not only declared)   ,  Support conversion of Class objects to java

Passing objects into java goes via JPClass::convertToJava, which
transforms source objects into java handles. However, conversion _from_
java class objects isn't currently implemented, and thus any class
argument gets erroneously transformed to null.   ,  "
Exception Management,Exception Management,Jpype,"fix calls from python to methods taking integer/long primitives for larger integers
   * overload resolution with JLong wrapper did not work (was checking for _int instead of _long)
   * on 64bit linux systems LONG_MAX is 2^63-1 and therefore returning JPyInt::asLong as a jint cuts of higher bits for values >= 2**31 or < -2**31
   * similar problem as above for all py3 on all systems as we have no long type anymore but only int
   * if a python exception was set we need to throw a PythonException not a JPypeException otherwise the message is overwritten
   * ref to member of temporary fixed in jpype_javaarray
   * tp_init has to either return 0 or -1   ,  "
Compiler Management,Compiler Management,Jpype,"Test that passing Class objs from python works   ,  "
Memory Management,Memory Management,Jpype,"Work on fixing leak.   ,  "
Memory Management,Memory Management,Jpype,"Support for varargs.   ,  Work on fixing leaks.   ,  "
Memory Management,Memory Management,Jpype,"Work on cleanup getClass.   ,  Work on fixing leaks.   ,  Work on fixing leak.   ,  Work on local reference leaks.   ,  "
Memory Management,Memory Management,Jpype,"Work on local reference leaks.   ,  "
Exception Management,Exception Management,Jpype,"Mark destructor that throws as noexcept(false) for >=C++11   ,  "
Compiler Management,Compiler Management,Jpype,"Support for varargs.   ,  "
Compiler Management,Compiler Management,Jpype,"Reapply cygwin patch.   ,  "
,,Jpype,"Apparently there is still one more.   ,  "
Memory Management,Memory Management,Jpype,"All tests pass on devel system.   ,  Support for varargs.   ,  Work on fixing leaks.   ,  One bug removed, one bug to go.   ,  Apparently there is still one more.   ,  Work on support for native java.lang.Class<>   ,  Work on local reference leaks.   ,  "
Memory Management,Memory Management,Jpype,"Cleanup, trying to isolate last bug.   ,  Work for leak fix.   ,  Work on fixing leak.   ,  Apparently there is still one more.   ,  Work on support for native java.lang.Class<>   ,  Work on cleanup getClass.   ,  Test of java.lang.Class<>   ,  Work on fixing leaks.   ,  Reapply boxed patch   ,  Work on local reference leaks.   ,  "
Memory Management,Memory Management,Jpype,"Work on fixing leak.   ,  Work on local reference leaks.   ,  "
Exception Management,Exception Management,Jpype,"Cleanup, trying to isolate last bug.   ,  Work on cleanup getClass.   ,  Work on float bug.   ,  Mark destructor that throws as noexcept(false) for >=C++11   ,  "
Compiler Management,Compiler Management,Jpype,"Support for varargs.   ,  Closeable support for java.io.Closeable.   ,  "
Memory Management,Memory Management,Jpype,"Cleanup, trying to isolate last bug.   ,  Work on fixing leak.   ,  Work on cleanup getClass.   ,  Support for varargs.   ,  Work on fixing leaks.   ,  Work on local reference leaks.   ,  "
Memory Management,Memory Management,Jpype,"Attempt to fix proxy thread issue.   ,  Work for leak fix.   ,  Work on fixing leak.   ,  Work on fixing leaks.   ,  Work on local reference leaks.   ,  "
Memory Management,Memory Management,Jpype,"Work on fixing leaks.   ,  "
Memory Management,Memory Management,Jpype,"Remove extra local ref.   ,  "
Restructuring the code,"Exception Management, Restructuring the code",Jpype,"Reorganization (#334)

* Completion of reorg.
* Squash CI porting work.
* Fix exception
* Applied autopep8   ,  "
,,Jpype,"Hardening patch for detached threads and shutdown.   ,  "
Restructuring the code,"Exception Management, Restructuring the c",Jpype,"Reorganization (#334)

* Completion of reorg.
* Squash CI porting work.
* Fix exception
* Applied autopep8   ,  Hardening patch for detached threads and shutdown.   ,  "
Compiler Management,Compiler Management,Jpype,"Working on startJVM   ,  "
Restructuring the code,"Restructuring the code, exception management",Jpype,"Remove extra local ref.   ,  Reorganization (#334)

* Completion of reorg.
* Squash CI porting work.
* Fix exception
* Applied autopep8   ,  Hardening patch for detached threads and shutdown.   ,  "
Restructuring the code,"Restructuring the code, exception management",Jpype,"Remove extra local ref.   ,  Reorganization (#334)

* Completion of reorg.
* Squash CI porting work.
* Fix exception
* Applied autopep8   ,  Hardening patch for detached threads and shutdown.   ,  Work on casting bugs.   ,  "
Restructuring the code,Exception Management,Jpype,"Reorganization (#334)

* Completion of reorg.
* Squash CI porting work.
* Fix exception
* Applied autopep8   ,  Hardening patch for detached threads and shutdown.   ,  Print method sigs when overloaded match could not be found. (#310)

Added some text to the raised exception an overloaded method could not be matched.  The exception string contains all possible method signatures that match the method name.   ,  "
Restructuring the code,"Exception Management, Restructuring the code",Jpype,"Fix virtual method bug. Fixes #445, Fixes #441   ,  Reorganization (#334)

* Completion of reorg.
* Squash CI porting work.
* Fix exception
* Applied autopep8   ,  Hardening patch for detached threads and shutdown.   ,  "
Restructuring the code,"Exception Management, Restructuring the code",Jpype,"Eol (#326)

* Reenable archival thunk code and rework setup.

* Fixed EOL to LF.   ,  Reorganization (#334)

* Completion of reorg.
* Squash CI porting work.
* Fix exception
* Applied autopep8   ,  Hardening patch for detached threads and shutdown.   ,  "
,,Jpype,"Eol (#326)

* Reenable archival thunk code and rework setup.

* Fixed EOL to LF.   ,  Hardening patch for detached threads and shutdown.   ,  "
Compiler Management,Compiler Management,Jpype,"Rework reference handler.   ,  Added compiler to allow for dynamic classes.   ,  "
Compiler Management,Compiler Management,Jpype,"Added compiler to allow for dynamic classes.   ,  "
Memory Management,Memory Management,Jpype,"Rework reference handler.   ,  "
Restructuring the code,"Exception Management, Restructuring the code",Jpype,"Reorganization (#334)

* Completion of reorg.
* Squash CI porting work.
* Fix exception
* Applied autopep8   ,  Work on conversion.   ,  Tests for duck typing of primitives. (#369)

Add support for primitives using duck typing.   ,  "
Restructuring the code,"Exception Management, Restructuring the code",Jpype,"Reorganization (#334)

* Completion of reorg.
* Squash CI porting work.
* Fix exception
* Applied autopep8   ,  Work to verify conversions.   ,  "
Restructuring the code,"Exception Management, Restructuring the code",Jpype,"Reorganization (#334)

* Completion of reorg.
* Squash CI porting work.
* Fix exception
* Applied autopep8   ,  Backporting bug fixes.   ,  "
Restructuring the code,"Exception Management, Restructuring the code",Jpype,"Reorganization (#334)

* Completion of reorg.
* Squash CI porting work.
* Fix exception
* Applied autopep8   ,  Enable option to support legacy conversion of strings.
Fixes #468   ,  "
,,Jpype,Message_induce
Compiler Management,Compiler Management,Jpype,Fixed tests not compiling/
Exception Management,Exception Management,Jpype,Make jpype.JException catch exceptions from subclasses./
,,Jpype,[JPypeJavaArray] use fast 2Seq impl for primitive array types/
Memory Management,Memory Management,Jpype,"[JPTypeManager] fixed memory leak for type objects. Made static type maps static members of TypeManager. Added destructor to TypeManager. Changed JPType virtual destructor visability from protected to public (hidden virtual dtors do not make sense) - and this was necessary, because we now delete children of JPType./"
Restructuring the code,Restructuring the code,Jpype,[JPTypeName] avoid string copying. Added assignment operator. Added const-correctness/
Restructuring the code,Restructuring the code,Jpype,Update jp_array.cpp/[JPArray] added method getSequenceFromRange/
Data conversion,Data conversion,Jpype,[PrimitiveTypes] fixed length of created sequence in getArrayRangeToSequence()/[Primitive Types] impled getArrayRangeToSequence/
Restructuring the code,Restructuring the code,Jpype,"[JPTypeManager] added method to delete primitive types at module unload.
    The destructor was not called, since no instance has been bound./"
Memory Management,Memory Management,Jpype,added bounds check/
Memory Management,Memory Management,Jpype,"FIX #62.
Method setArrayRange in jp_primitivetypes leaked memory in Python code as an
temporary PythonObject has not been refcounted properly.

Reworked single array element setters/getters to avoid a copy of the whole array
in worst case (use jni Get/SetArrayRange functions)./added error checking via python internal casting routine and exception handling/* impled getRange with direct pointer to python sequence, uses direct cast instead of convertToJava methods.
* removed unused jpcleaners/"
Memory Management,Memory Management,Jpype,[primitive arrays] set a range in an array via an underlying buffer if it exists./
Compiler Management,Compiler Management,Jpype,"Pass char* instead of void* to destructor for capsule compatibility

compiles for capsules/Python3 module init and use PyVarObject_HEAD_INIT/some global references were cleaned with local reference cleaners and vice versa (reverted from commit c2f1ecb51149228f1fc195c2a7855c24eff97cb0)/"
Compiler Management,Compiler Management,Jpype,"Pass char* instead of void* to destructor for capsule compatibility

compiles for capsules/use PyCapsule_Type instead of PyCObject_Type/"
Exception Management,Exception Management,Jpype,"add exception enhancement from tcalmant

https://github.com/tcalmant/jpype-py3/wiki/py3_cpp/encode strings/Pass char* instead of void* to destructor for capsule compatibility

compiles for capsules/Use PyCapsule_CheckExact instead of PyCObject_Check/Use Capsule functions instead of CObject ones

Python2.6 will have the Capsule functions converted to CObject calls by capsulethunk.h/Python3 uses PyBytes instead of PyString/"
Exception Management,Exception Management,Jpype,"add exception enhancement from tcalmant

https://github.com/tcalmant/jpype-py3/wiki/py3_cpp/Pass char* instead of void* to destructor for capsule compatibility

compiles for capsules/Need capsule thunk for 2.6 and 3.0/"
Compiler Management,Compiler Management,Jpype,"extract data from capsule when deleting/Pass char* instead of void* to destructor for capsule compatibility

compiles for capsules/"
Compiler Management,Compiler Management,Jpype,"Pass char* instead of void* to destructor for capsule compatibility

compiles for capsules/"
Compiler Management,Compiler Management,Jpype,Python3 module init and use PyVarObject_HEAD_INIT/
Compiler Management,Compiler Management,Jpype,Python3 module init and use PyVarObject_HEAD_INIT/use PyCapsule_Type instead of PyCObject_Type/
Data conversion,Data conversion,Jpype,use PyCapsule_Type instead of PyCObject_Type/
Compiler Management,Compiler Management,Jpype,"Pass char* instead of void* to destructor for capsule compatibility

compiles for capsules/Python3 module init and use PyVarObject_HEAD_INIT/"
Memory Management,Memory Management,Jpype,some global references were cleaned with local reference cleaners and vice versa (reverted from commit c2f1ecb51149228f1fc195c2a7855c24eff97cb0)/
Exception Management,Exception Management,Jpype,"add exception enhancement from tcalmant

https://github.com/tcalmant/jpype-py3/wiki/py3_cpp/"
Compiler Management,Compiler Management,Jpype,"Python3.5 removed PyObject_REPR

http://python.readthedocs.org/en/latest/whatsnew/3.5.html/"
Exception Management,Exception Management,Jpype,"improve overload resolution and support default methods
 * resolve overloads according to ""maximally specific overload"" rule
 * cache types and overload order
 * support java 1.8 default methods by loading all public methods (not only declared)/fix calls from python to methods taking integer/long primitives for larger integers
   * overload resolution with JLong wrapper did not work (was checking for _int instead of _long)
   * on 64bit linux systems LONG_MAX is 2^63-1 and therefore returning JPyInt::asLong as a jint cuts of higher bits for values >= 2**31 or < -2**31
   * similar problem as above for all py3 on all systems as we have no long type anymore but only int
   * if a python exception was set we need to throw a PythonException not a JPypeException otherwise the message is overwritten
   * ref to member of temporary fixed in jpype_javaarray
   * tp_init has to either return 0 or -1/"
Memory Management,Memory Management,Jpype,"improve overload resolution and support default methods
 * resolve overloads according to ""maximally specific overload"" rule
 * cache types and overload order
 * support java 1.8 default methods by loading all public methods (not only declared)/"
Exception Management,Exception Management,Jpype,"fix calls from python to methods taking integer/long primitives for larger integers
   * overload resolution with JLong wrapper did not work (was checking for _int instead of _long)
   * on 64bit linux systems LONG_MAX is 2^63-1 and therefore returning JPyInt::asLong as a jint cuts of higher bits for values >= 2**31 or < -2**31
   * similar problem as above for all py3 on all systems as we have no long type anymore but only int
   * if a python exception was set we need to throw a PythonException not a JPypeException otherwise the message is overwritten
   * ref to member of temporary fixed in jpype_javaarray
   * tp_init has to either return 0 or -1/"
Data Conversion,Data Conversion,Jpype,"Support conversion of Class objects to java

Passing objects into java goes via JPClass::convertToJava, which
transforms source objects into java handles. However, conversion _from_
java class objects isn't currently implemented, and thus any class
argument gets erroneously transformed to null./"
Compiler Management,Compiler Management,Jpype,Test that passing Class objs from python works/
Exception Management,Exception Management,Jpype,Mark destructor that throws as noexcept(false) for >=C++11/
Memory Management,Memory Management,Jpype,Work on local reference leaks./Work on fixing leaks./Work on fixing leak./Work on support for native java.lang.Class<>/Verifies entry points will not segfault./
,,Jpype,Verifies entry points will not segfault./
Compiler Management,Compiler Management,Jpype,Work on support for native java.lang.Class<>/Verifies entry points will not segfault./
Compiler Management,Compiler Management,Jpype,Work on support for native java.lang.Class<>/
Memory Management,Memory Management,Jpype,Apparently there is still one more./Work for leak fix./Work on fixing leak./Work on support for native java.lang.Class<>/
Memory Management,Memory Management,Jpype,Work on local reference leaks./Work on fixing leak./
Memory Management,Memory Management,Jpype,Work on fixing leak./Work on support for native java.lang.Class<>/Add startup hardening./Verifies entry points will not segfault./
Memory Management,Memory Management,Jpype,"Cleanup, trying to isolate last bug./Work on local reference leaks./Work on cleanup getClass./"
Compiler Management,Compiler Management,Jpype,Closeable support for java.io.Closeable./Support for varargs./
Memory Management,Memory Management,Jpype,Work on local reference leaks./Work on fixing leaks./Work on fixing leak./
,,Jpype,Apparently there is still one more./
Memory Management,Memory Management,Jpype,Work on local reference leaks./
Compiler Management,Compiler Management,Jpype,Support for varargs./
Memory Management,Memory Management,Jpype,Work on fixing leak./
Memory Management,Memory Management,Jpype,Work on fixing leaks./Support for varargs./
Memory Management,Memory Management,Jpype,Work on fixing leaks./
Memory Management,Memory Management,Jpype,Work on fixing leaks./Work on fixing leak./
Compiler Management,Compiler Management,Jpype,Reapply cygwin patch./
Memory Management,Memory Management,Jpype,Work on local reference leaks./Work on cleanup getClass./Work on fixing leaks./Work on fixing leak./
Memory Management,Memory Management,Jpype,"All tests pass on devel system./Apparently there is still one more./One bug removed, one bug to go./Work on local reference leaks./Work on fixing leaks./Work on support for native java.lang.Class<>/Support for varargs./"
Memory Management,Memory Management,Jpype,Work on local reference leaks./Work for leak fix./Work on fixing leaks./Work on fixing leak./
,,Jpype,Reapply boxed patch/
Memory Management,Memory Management,Jpype,Work on cleanup getClass./Tests pass./Work on fixing leaks./Reapply boxed patch/
Compiler management,Compiler management,Jpype,"Cleanup, trying to isolate last bug./Work on cleanup getClass./"
Memory Management,Memory Management,Jpype,"Apparently there is still one more./Cleanup, trying to isolate last bug./Work on local reference leaks./Work on cleanup getClass./Work for leak fix./Work on fixing leaks./Work on fixing leak./Test of java.lang.Class<>/Work on support for native java.lang.Class<>/Reapply boxed patch/"
Memory Management,Memory Management,Jpype,"Cleanup, trying to isolate last bug./Work on local reference leaks./Work on cleanup getClass./Work on fixing leaks./Work on fixing leak./Support for varargs./"
,,Jpype,Hardening patch for detached threads and shutdown./
Memory Management,Memory Management,Jpype,Hardening patch for detached threads and shutdown./Remove extra local ref./
Exception Management,Exception Management,Jpype,"Hardening patch for detached threads and shutdown./Print method sigs when overloaded match could not be found. (#310)

Added some text to the raised exception an overloaded method could not be matched.  The exception string contains all possible method signatures that match the method name./"
Memory Management,Memory Management,Jpype,Remove extra local ref./
,,Jpype,"Eol (#326)

* Reenable archival thunk code and rework setup.

* Fixed EOL to LF./Hardening patch for detached threads and shutdown./Attempt to fix proxy thread issue./"
Compiler management,Compiler management,Jpype,Hardening patch for detached threads and shutdown./Work on float bug./
Restructuring the code,"exception Management, Restructuring the code",Jpype,"Reorganization (#334)

* Completion of reorg.
* Squash CI porting work.
* Fix exception
* Applied autopep8/"
Restructuring the code,Restructuring the code,Jpype,Utf8 support./
Restructuring the code,Restructuring the code,Jpype,"Tests for duck typing of primitives. (#369)

Add support for primitives using duck typing./"
Compiler management,Compiler management,Jpype,Work on field type bug./
Data Conversion,Data Conversion,Jpype,Work to verify conversions./
Compiler management,Compiler management,Jpype,Fix edge cases./Fix for lost global reference./Work on casting bugs./
Compiler management,Compiler management,Jpype,Backporting bug fixes./
Data Conversion,"Data Conversion, compiler Management",Jpype,"Enable option to support legacy conversion of strings.
Fixes #468/"
,,Jpype, changes from tcalmant/
Compiler management,Compiler management,Jpype,Backported fixes from 0.8/Backporting bug fixes./
Data Conversion,Data Conversion,Jpype,Work on conversion./
Compiler Management,Compiler Management,Jpype,Pass a python object into Java and return unharmed./
Compiler Management,Compiler Management,Jpype,Working on startJVM/
Compiler management,Compiler management,Jpype,Work on casting bugs./
Compiler management,Compiler management,Jpype,"Fix virtual method bug. Fixes #445, Fixes #441/"
,,javacpp," * Added new `Pointer.limit` property, mainly useful to get the `size` of an output parameter, as returned by an adapter specified with the `@Adapter` annotation
 * Renamed the `capacity` field of an adapter to `size` as it now maps to both `Pointer.limit` and `Pointer.capacity` (the latter only for new allocations)/ * Added `Pointer.put(Pointer)` method, the counterpart of `Buffer.put(Buffer)`, to call the native `memcpy()` function on two `Pointer` objects/Initial commit/"
,,javacpp," * Before loading the JNI library, the `Loader` now also tries to extract and load libraries listed in the `@Platform(link={...}, preload={...})` annotation values, and to support library names with version numbers, each value has to follow the format ""libname@version"" (or ""libname@@version"" to have `Builder` use it for the compiler as well), where ""version"" is the version number found in the filename as required by the native dynamic linker, usually a short sequence of digits and dots, but it can be anything (e.g.: ""mylib@.4.2"" would map to ""libmylib.so.4.2"", ""libmylib.4.2.dylib"", and ""mylib.4.2.dll"" under Linux, Mac OS X, and Windows respectively)
 * Stopped using `java.net.URL` as hash key in `Loader` (very bad idea)/ * Refactored `Builder` to remove the need of the `Builder.Main` class
 * Added CUDA properties for Windows and Mac OS X as well/Added an `environmentVariables` configuration option to `BuildMojo`, along with some slight refactoring of `Builder.Main`/Added `BuildMojo` class and made other adjustments for Maven (second attempt)/Fixed Maven build and Mac OS X `-framework` option (issue #10) and other minor things/Initial commit/"
,,javacpp,Initial commit/
Feature migration,Feature migration,javacpp," * Before loading the JNI library, the `Loader` now also tries to extract and load libraries listed in the `@Platform(link={...}, preload={...})` annotation values, and to support library names with version numbers, each value has to follow the format ""libname@version"" (or ""libname@@version"" to have `Builder` use it for the compiler as well), where ""version"" is the version number found in the filename as required by the native dynamic linker, usually a short sequence of digits and dots, but it can be anything (e.g.: ""mylib@.4.2"" would map to ""libmylib.so.4.2"", ""libmylib.4.2.dylib"", and ""mylib.4.2.dll"" under Linux, Mac OS X, and Windows respectively)
 * Stopped using `java.net.URL` as hash key in `Loader` (very bad idea)/ * New `Loader.loadLibrary()` method similar to `System.loadLibrary()`, but before searching the library path, it tries to extract and load the librar
 * `Generator` now accepts `@Const` on `FunctionPointer` class declarations
 * Added new `@Adapter.cast()` value to cast explicitly the output of a C++ adapter object
 * Upgraded references of the Android NDK to version r8
 * Included new command line option ""-Xcompiler"" to pass options such as ""-Wl,-static"" directly to the compiler
 * Made other various minor changes and enhancements/Fixed Maven build and Mac OS X `-framework` option (issue #10) and other minor things/Initial commit/"
Feature migration,Feature migration,javacpp," * New `Loader.loadLibrary()` method similar to `System.loadLibrary()`, but before searching the library path, it tries to extract and load the librar
 * `Generator` now accepts `@Const` on `FunctionPointer` class declarations
 * Added new `@Adapter.cast()` value to cast explicitly the output of a C++ adapter object
 * Upgraded references of the Android NDK to version r8
 * Included new command line option ""-Xcompiler"" to pass options such as ""-Wl,-static"" directly to the compiler
 * Made other various minor changes and enhancements/Initial commit/"
Feature migration,Feature migration,javacpp," * Added new `Pointer.limit` property, mainly useful to get the `size` of an output parameter, as returned by an adapter specified with the `@Adapter` annotation
 * Renamed the `capacity` field of an adapter to `size` as it now maps to both `Pointer.limit` and `Pointer.capacity` (the latter only for new allocations)/ * Added `Pointer.put(Pointer)` method, the counterpart of `Buffer.put(Buffer)`, to call the native `memcpy()` function on two `Pointer` objects/ * New `@NoException` annotation to reduce the size of generated code and optimize runtime performance of functions that are guaranteed not to throw exceptions, or for cases when we do not mind that the JVM may crash and burn
 * Trying to generate code for non-static native methods inside a class not extending `Pointer` now generates proper warning (issue #19)
 * Fixed regression where the `@Adapter` notation generates incorrect code for types other than `Pointer` (issue #20)/ * Fixed `@Adapter` generating incorrect code on primitive types/ * Trying to generate code for non-static native methods inside a class not extending `Pointer` now generates proper warning (issue #19)
 * Fixed regression where the `@Adapter` notation generates incorrect code for types other than `Pointer` (issue #20)/ * Started using version numbers, friendly to tools like Maven, and placing packages in a sort of [Maven repository http://maven2.javacpp.googlecode.com/git/] (issue #10)
 * All files now get extracted into a temporary subdirectory, and with the appropriate platform-dependent linker options, or with libraries patched up after the fact with tools such as `install_name_tool` of Mac OS X, most native dynamic linkers can load dependent libraries from there
 * Changed the default value of the `@Index` annotation from 0 to 1, and fixed the `Generator` when it is used with member getters and setters
 * Renamed `mingw-*.properties` to `windows-*-mingw.properties` for consistency
 * Made the `Generator` allocate native heap memory for callback arguments passed `@ByVal` (in addition to `FunctionPointer`), rendering their behavior consistent with return `@ByVal` in the case of function calls (issue #16)
 * `Generator` now uses `std::runtime_error(std::string&)` instead of assuming that some nonstandard `std::exception(std::string&)` constructor exists (issue #17)
 * Fixed `Generator` producing incorrect code when applying invalid annotations such as `@ByVal` on a method that returns something else than a `Pointer` object (issue #18)/ * New `Loader.loadLibrary()` method similar to `System.loadLibrary()`, but before searching the library path, it tries to extract and load the librar
 * `Generator` now accepts `@Const` on `FunctionPointer` class declarations
 * Added new `@Adapter.cast()` value to cast explicitly the output of a C++ adapter object
 * Upgraded references of the Android NDK to version r8
 * Included new command line option ""-Xcompiler"" to pass options such as ""-Wl,-static"" directly to the compiler
 * Made other various minor changes and enhancements/ * Fixed syntax error in `VectorAdapter`, which GCC and Visual C++ would still happily compile
 * Added new `source.suffix` property to have the names of generated source files end with something else than `.cpp` and support frameworks like CUDA that require filenames with a `.cu` extension to compile properly, and also changed the `-cpp` command line option to `-nocompile`/Initial commit/"
,,javacpp," * `Generator` would ignore `Pointer.position()` in the case of `@ByPtrPtr` and `@ByPtrRef` parameters
 * Replaced hack to create a `Pointer` from a `Buffer` object with something more standard/ * Fixed `Pointer.equals(null)` throwing `NullPointerException` (issue #22)
 * `@NoOffset` would erroneously prevent `sizeof()` operations from getting generated/"
,,javacpp, * Added functionality to access `FunctionPointer` callbacks by their name from C/C++: We can annotate them with `@Name` and build with the new `-header` option to get their declarations in a header file/
,,javacpp," * `Generator` would ignore `Pointer.position()` in the case of `@ByPtrPtr` and `@ByPtrRef` parameters
 * Replaced hack to create a `Pointer` from a `Buffer` object with something more standard/"
,,javacpp," * Added functionality to access `FunctionPointer` callbacks by their name from C/C++: We can annotate them with `@Name` and build with the new `-header` option to get their declarations in a header file/ * Added support for C++ ""functors"" based on the `operator()`, which gets used when annotating a `FunctionPointer` method parameter with `@ByRef`
 * For convenience in Scala, added `apply()` as an acceptable method name within a `FunctionPointer`, in addition to `call()`/ * Fixed `@Cast` not working along parameters with an `@Adapter`/ * `Generator` would ignore `Pointer.position()` in the case of `@ByPtrPtr` and `@ByPtrRef` parameters
 * Replaced hack to create a `Pointer` from a `Buffer` object with something more standard/ * Fixed `Loader.sizeof(Pointer.class)` to return the `sizeof(void*)`
 * In addition to methods and parameters, we may now apply `@Adapter` to annotation types as well, allowing us to shorten expressions like `@Adapter(""VectorAdapter<int>"") int[]` to `@StdVector int[]`, to support `std::vector<int>`, and similarly for `@StdString` and `std::string`/ * Fixed callback parameter casting of primitive and `String` types
 * An empty `@Namespace` can now be used to let `Generator` know of entities that are not part of any scope, such as macros and operators
 * Turned `FunctionPointer` into an `abstract class` with `protected` constructors, but if users still try to use it as function parameters, `Generator` now logs a warning indicating that a subclass should be used (issue #23)/ * Removed the `out` value of the `@Adapter` annotation: All adapters are now ""out"" by default, unless `@Const` also appears on the same element/ * Fixed `Pointer.equals(null)` throwing `NullPointerException` (issue #22)
 * `@NoOffset` would erroneously prevent `sizeof()` operations from getting generated/ * Fixed problems when trying to map `java.lang.String` to other native types than `char*`, such as `unsigned char*`/ * JavaCPP now uses the `new (std::nothrow)` operator for allocation, which guarantees that allocation of primitive native arrays won't throw exceptions, making it possible to build C++ exception free JNI libraries/"
,,javacpp," * Exported `Loader.isLoadLibraries()`, which always returns true, except when the `Builder` loads the classes
 * Made it possible to specify a nested class (with a '$' character in the name) on the command line
 * When `Pointer.limit == 0`, the methods `put()`, `zero()`, and `asBuffer()` now assume a size of 1/ * Added `Pointer.withDeallocator(Pointer)` method to attach easily a custom `Deallocator` created out of a `static void deallocate(Pointer)` method in the subclass, including native ones such as `@Namespace @Name(""delete"") static void deallocate(Pointer)`/ * Fixed memory corruption when returning by value an `std::vector<>` using an adapter
 * Added `Pointer.zero()` method that calls `memset(0)` on the range
 * For easier memory management, more than one `Pointer` now allowed to share the `deallocator` when ""casting"" them/"
,,javacpp," * Added the ability to change the name of the class of function objects created when defining a `FunctionPointer` with the `@Name` annotation
 * `Builder` would go on a compile spree when all classes specified on the command line could not be loaded/ * Exported `Loader.isLoadLibraries()`, which always returns true, except when the `Builder` loads the classes
 * Made it possible to specify a nested class (with a '$' character in the name) on the command line
 * When `Pointer.limit == 0`, the methods `put()`, `zero()`, and `asBuffer()` now assume a size of 1/Modified `Builder` to prevent requiring the `jvm` library for cases where linking has a high probability of failure/ * Removed confusing `cast` value of `@Adapter` instead relying on new `String[]` value of `@Cast` to order multiple casts
 * The `Builder` was not linking with the `jvm` library by default/"
Data conversion,Data conversion,javacpp," * Removed confusing `cast` value of `@Adapter` instead relying on new `String[]` value of `@Cast` to order multiple casts
 * The `Builder` was not linking with the `jvm` library by default/"
,,javacpp,"Fixed compile error of `FunctionPointer` deallocator with `@Name`/ * Added the ability to change the name of the class of function objects created when defining a `FunctionPointer` with the `@Name` annotation
 * `Builder` would go on a compile spree when all classes specified on the command line could not be loaded/ * Fixed compiler error on 32-bit Mac OS X/Fixed a few small things/ * A name starting with ""::"", for example `@Name(""::std::string"")` or `@Namespace(""::std"")`, now drops the remaining enclosing scope/Modified `Builder` to prevent requiring the `jvm` library for cases where linking has a high probability of failure/ * Removed confusing `cast` value of `@Adapter` instead relying on new `String[]` value of `@Cast` to order multiple casts
 * The `Builder` was not linking with the `jvm` library by default/ * Renamed various variables in `Generator` to make the generated code more readable
 * Fixed memory corruption when using `@ByRef` on a function that returns by value an `std::string` (issue #26)/ * Fixed memory corruption when returning by value an `std::vector<>` using an adapter
 * Added `Pointer.zero()` method that calls `memset(0)` on the range
 * For easier memory management, more than one `Pointer` now allowed to share the `deallocator` when ""casting"" them/ * Fixed `@Cast` not working when attempting to `return` the argument/Fixed a few things related to named `FunctionPointer` callbacks/"
,,javacpp," * Fixed callbacks not working on Android anymore (issue #30)
 * Added some Javadoc to most of the code/"
,,javacpp," * Fixed callbacks not working on Android anymore (issue #30)
 * Added some Javadoc to most of the code/Removed the need to provide a `@Platform(library=""..."")` name for enclosing but nested classes/ * Provided new `@Platform(library=""..."")` annotation value to let users specify the name of the native library used by both `Builder` and `Loader`, where different classes with the same name get built together, which also works on nested classes (issue #29)/"
,,javacpp," * To help diagnose `UnsatisfiedLinkError` thrown by `Loader.load()`, they have been augmented with a potential cause originating from the ""preloading"" of libraries, whose premature deletion has also been fixed/ * Provided new `@Platform(library=""..."")` annotation value to let users specify the name of the native library used by both `Builder` and `Loader`, where different classes with the same name get built together, which also works on nested classes (issue #29)/"
Feature migration,Feature migration,javacpp," * Let arrays of primitive values be valid return arguments, mostly useful when used along with the `@StdVector` annotation, or some other custom adapter/Released version 0.4
* Fixed potential problem with methods of `FunctionPointer` annotated with `@Cast(""const..."")`
* Upgraded references of the Android NDK to version r8d/ * Fixed callbacks not working on Android anymore (issue #30)
 * Added some Javadoc to most of the code/"
Feature migration,Feature migration,javacpp," * Upgraded references of the Android NDK to version r9
 * Added new `Builder` option ""-copylibs"" that copies into the build directory any dependent shared libraries listed in the `@Platform(link={...}, preload={...})` annotation
 * `Loader.getPlatformName()` can now be overridden by setting the `com.googlecode.javacpp.platform.name` system property
 * Refactored the loading code for `@Properties()` into a neat `Loader.ClassProperties` class, among a few other small changes in `Loader`, `Builder`, `Generator`, and the properties/"
Feature migration,Feature migration,javacpp," * Upgraded references of the Android NDK to version r9
 * Added new `Builder` option ""-copylibs"" that copies into the build directory any dependent shared libraries listed in the `@Platform(link={...}, preload={...})` annotation
 * `Loader.getPlatformName()` can now be overridden by setting the `com.googlecode.javacpp.platform.name` system property
 * Refactored the loading code for `@Properties()` into a neat `Loader.ClassProperties` class, among a few other small changes in `Loader`, `Builder`, `Generator`, and the properties/ * Included often used directories such as `/usr/local/include/` and `/usr/local/lib/` to `compiler.includepath` and `compiler.linkpath` default properties
 * New `@Properties(inherit={Class})` value lets users specify properties in common on a similarly annotated shared config class of sorts/"
Feature migration,Feature migration,javacpp," * Upgraded references of the Android NDK to version r9
 * Added new `Builder` option ""-copylibs"" that copies into the build directory any dependent shared libraries listed in the `@Platform(link={...}, preload={...})` annotation
 * `Loader.getPlatformName()` can now be overridden by setting the `com.googlecode.javacpp.platform.name` system property
 * Refactored the loading code for `@Properties()` into a neat `Loader.ClassProperties` class, among a few other small changes in `Loader`, `Builder`, `Generator`, and the properties/ * Included often used directories such as `/usr/local/include/` and `/usr/local/lib/` to `compiler.includepath` and `compiler.linkpath` default properties
 * New `@Properties(inherit={Class})` value lets users specify properties in common on a similarly annotated shared config class of sorts/ * Fixed callbacks when used with custom class loaders such as with Web containers/ * Fixed using `@StdString` (or other `@Adapter` with `@Cast` annotations) on callbacks (issue #34), incidentally allowing them to return a `String`/ * Incorporated missing explicit cast on return values when using the `@Cast` annotation/"
,,javacpp,"Released version 0.6
 * Added new very preliminary `Parser` to produce Java interface files almost automatically from C/C++ header files; please refer to the new JavaCPP Presets subproject for details/"
Exception Management,Exception Management,javacpp," * Fixed invalid code generated for `FunctionPointer` parameters annotated with `@Const @ByRef`/ * When catching a C++ exception, the first class declared after `throws` now gets thrown (issue #36) instead of `RuntimeException`, which is still used by default
 * Fixed Java resource leak after catching a C++ exception/ * Let `Buffer` or arrays of primitive values be valid callback arguments/"
Feature migration,Feature migration,javacpp,"Released version 0.7
 * Tweaked a few things to support RoboVM and target iOS, but `JNI_OnLoad()` does not appear to get called...
 * Upgraded references of the Android NDK to version r9c
 * Improved the C++ support of the `Parser` for templates and overloaded operators/ * Made `Loader.load()` work, within reason, even when all annotations and resources have been removed, for example, by ProGuard
 * Fixed compile error when using a `FunctionPointer` as parameter from outside its top-level enclosing class
 * The `Parser` now filters tokens appropriately with preprocessor directives
 * Improved the C++ support of the `Parser` for macros, templates, etc/"
,,javacpp," * Made `Loader.load()` work, within reason, even when all annotations and resources have been removed, for example, by ProGuard
 * Fixed compile error when using a `FunctionPointer` as parameter from outside its top-level enclosing class
 * The `Parser` now filters tokens appropriately with preprocessor directives
 * Improved the C++ support of the `Parser` for macros, templates, etc/"
Feature migration,Feature migration,javacpp," * Continued to clean up the `Parser` and improve the support of comments, templates, overloaded operators, and namespaces, for the most part
 * Unified the function pointer type of native deallocators to `void (*)(void*)`
 * Removed dependency on (efficient) `AllocObject()` and `CallNonvirtualVoidMethodA()` JNI functions, which are not supported by Avian/ * Cleaned up and optimized `Generator` a bit, and fixed a crash that could occur when `FindClass()` returns NULL/Released version 0.7
 * Tweaked a few things to support RoboVM and target iOS, but `JNI_OnLoad()` does not appear to get called...
 * Upgraded references of the Android NDK to version r9c
 * Improved the C++ support of the `Parser` for templates and overloaded operators/ * Made `Loader.load()` work, within reason, even when all annotations and resources have been removed, for example, by ProGuard
 * Fixed compile error when using a `FunctionPointer` as parameter from outside its top-level enclosing class
 * The `Parser` now filters tokens appropriately with preprocessor directives
 * Improved the C++ support of the `Parser` for macros, templates, etc/"
,,javacpp," * Continued to clean up the `Parser` and improve the support of comments, templates, overloaded operators, namespaces, and standard containers, for the most part/"
,,javacpp,Cleaned up the code a bit/ * Unified the property names with the `@Properties` and `@Platform` annotations into a consistent naming scheme/
,,javacpp," * Introduced a simple `Logger` class and unified the logging output calls around it
 * Continued to clean up the `Parser` and improve the support of, for the most part, default parameter arguments, multiple inheritance, custom names of wrapped declarators, and helper classes written in Java/"
,,javacpp,"Add support for virtual functions to let C++ call back methods implemented in Java

 * Add `Info.virtualize` to have the `Parser` generate `@Virtual abstract` for pure virtual functions in the given classes
 * Add `@Virtual` annotation and update `Generator` to support callback by overriding such annotated `native` or `abstract` methods/ * Add hack for `typedef void*` definitions and parameters with a double indirection to them/ * Fix `Generator.checkPlatform()` not checking super classes/ * Fix issues with pointers to function pointers/ * Fix issues in the `Parser` with the `long double`, `ptrdiff_t`, `intptr_t`, `uintptr_t`, `off_t` types, optional parentheses, const data types in templates, declarator names equal to a type name, friend functions, inline constructors, `typedef void` declarations within namespaces/ * Add `includePath`, `linkPath`, and `preloadPath` parameters to `BuildMojo` to let Maven users append paths to the properties easily
 * In consequence, remove too arbitrary ""local"" paths from the default platform properties/ * Fix issues in the `Parser` with access specifiers and casting of const values by reference/ * Allow users to instruct the `Parser` to skip the expansion of specific macro invocations
 * Let `Parser` concatenate tokens when expanding macros containing the `##` operator/"
,,javacpp," * Fix issues in the `Parser` with the `long double`, `ptrdiff_t`, `intptr_t`, `uintptr_t`, `off_t` types, optional parentheses, const data types in templates, declarator names equal to a type name, friend functions, inline constructors, `typedef void` declarations within namespaces/"
,,javacpp," * Allow users to instruct the `Parser` to skip the expansion of specific macro invocations
 * Let `Parser` concatenate tokens when expanding macros containing the `##` operator/"
,,javacpp," * Add `includePath`, `linkPath`, and `preloadPath` parameters to `BuildMojo` to let Maven users append paths to the properties easily
 * In consequence, remove too arbitrary ""local"" paths from the default platform properties/"
,,javacpp,"Add support for virtual functions to let C++ call back methods implemented in Java

 * Add `Info.virtualize` to have the `Parser` generate `@Virtual abstract` for pure virtual functions in the given classes
 * Add `@Virtual` annotation and update `Generator` to support callback by overriding such annotated `native` or `abstract` methods/ * Fix `Generator.checkPlatform()` not checking super classes/ * Fix issues with pointers to function pointers/"
Exception Management,"Exception Management, data conversion",javacpp," * Fix some `Parser` exceptions on valid declarations with macro expansions or overloaded cast operators/ * Fix some `Parser` exceptions on valid declarations with template arguments, and make `Info.javaName` usable in the case of `enum`/ * Use `Long.decode()` inside the `Tokenizer` to test more precisely when integer values are larger than 32 bits
 * Have the `Parser` produce `@Name(""operator="") ... put(... )` methods for standard C++ containers, avoiding mistaken calls to `Pointer.put(Pointer)` ([issue javacv:34](https://github.com/bytedeco/javacv/issues/34))
 * Let the `Parser` apply `Info.skip` in the case of macros as well
 * Remove warning log messages when using the `@Raw` annotation/"
Exception Management,"Exception Management,data conversion",javacpp, * Fix some `Parser` exceptions on valid declarations with macro expansions or overloaded cast operators/
,,javacpp," * Use `Long.decode()` inside the `Tokenizer` to test more precisely when integer values are larger than 32 bits
 * Have the `Parser` produce `@Name(""operator="") ... put(... )` methods for standard C++ containers, avoiding mistaken calls to `Pointer.put(Pointer)` ([issue javacv:34](https://github.com/bytedeco/javacv/issues/34))
 * Let the `Parser` apply `Info.skip` in the case of macros as well
 * Remove warning log messages when using the `@Raw` annotation/"
,,javacpp," * Let `@Virtual @MemberGetter` annotated methods return member function pointers of functions defined with `@Virtual`, useful for frameworks like Cocos2d-x/"
,,javacpp,"Update version in the `pom.xml` file to 0.11-SNAPSHOT
 * Provide `UByteIndexer` and `UShortIndexer`, treating array and buffer data as unsigned integers, for ease of use
 * Clean up Windows `java.io.tmpdir` even when program messes with `java.class.path` (issue #12)/"
,,javacpp," * Work around a few additional corner cases with the `Parser`/ * Fix a few more small issues with the `Parser` and the `Generator`/ * Add `SharedPtrAdapter` and corresponding `@SharedPtr` annotation to support `shared_ptr` containers
 * Fix a few small issues with the `Parser` and the `Generator`/ * Adjust `TokenIndexer` and `Parser` to handle `#if`, `#ifdef`, `#ifndef`, `#elif`, `#else`, and `#endif` preprocessor directives more appropriately, even when placed in the middle of declarations/"
,,javacpp," * Add `SharedPtrAdapter` and corresponding `@SharedPtr` annotation to support `shared_ptr` containers
 * Fix a few small issues with the `Parser` and the `Generator`/"
,,javacpp," * Adjust `TokenIndexer` and `Parser` to handle `#if`, `#ifdef`, `#ifndef`, `#elif`, `#else`, and `#endif` preprocessor directives more appropriately, even when placed in the middle of declarations/"
,,javacpp," * Append `@Documented` to annotation types to have them picked up by Javadoc
 * Fix `friend` functions not getting skipped by the `Parser`
 * Add `Info` for `__int8`, `__int16`, `__int32`, and `__int64` to `InfoMap.defaults`/"
,,javacpp," * Fix `Parser` errors caused by constructors in nested `typedef struct` constructs, and skip over pointer names too (issue bytedeco/javacpp-presets#62)/ * Add `Parser` support for the `interface` keyword of the Microsoft C/C++ Compiler/ * Fix `Parser` errors on unnamed `namespace` blocks, preprocessor directives with comments, and empty macros/ * Introduce a `nullValue` to `@ByRef` and `@ByVal` annotations to let us specify what to do when passed `null`/ * Fix and enhance in various ways the support of `Parser` and `Generator` for function pointers, virtual functions, and abstract classes
 * Improve `Parser` check for `const` references and pointers required to output appropriate `@Const` annotation
 * Add `Info.purify` to force the `Parser` in producing abstract classes/ * Fix `Tokenizer` failing on some character and string literals
 * Fix `Parser` errors caused by constructors in `typedef struct` constructs
 * Generalize `Info.virtualize` to let non-pure virtual functions get annotated with `@Virtual native`
 * Make `VectorAdapter` work even with elements that have no default constructor
 * Add `Parser` support for `std::pair` as a sort of zero-dimensional container type/ * Clarify with documentation comments various constructors produced by the `Parser`/"
,,javacpp," * Fix and enhance in various ways the support of `Parser` and `Generator` for function pointers, virtual functions, and abstract classes
 * Improve `Parser` check for `const` references and pointers required to output appropriate `@Const` annotation
 * Add `Info.purify` to force the `Parser` in producing abstract classes/ * Fix `Tokenizer` failing on some character and string literals
 * Fix `Parser` errors caused by constructors in `typedef struct` constructs
 * Generalize `Info.virtualize` to let non-pure virtual functions get annotated with `@Virtual native`
 * Make `VectorAdapter` work even with elements that have no default constructor
 * Add `Parser` support for `std::pair` as a sort of zero-dimensional container type/"
,,javacpp," * Offer the Apache License, Version 2.0, as a new choice of license, in addition to the GPLv2 with Classpath exception/"
,,javacpp,"* Fix `Generator` performance issue on classes with a lot of methods (issue bytedeco/javacpp-presets#36)/ * Fix and enhance in various ways the support of `Parser` and `Generator` for function pointers, virtual functions, and abstract classes
 * Improve `Parser` check for `const` references and pointers required to output appropriate `@Const` annotation
 * Add `Info.purify` to force the `Parser` in producing abstract classes/"
,,javacpp," * Log when `Pointer.deallocator` gets registered, garbage collected, or deallocated manually, if `Logger.isDebugEnabled()` (redirectable to SLF4J)
 * Make `Pointer implements AutoCloseable` to let us try-with-resources, thus bumping requirements to Java SE 7 and Android 4.0/Update version in the `pom.xml` file to 1.1-SNAPSHOT

 * Introduce the concept of ""owner address"" to integrate `Pointer` transparently with `std::shared_ptr`, etc (Thanks to Cyprien Noel for the idea!)/ * Add new ""org.bytedeco.javacpp.nopointergc"" system property to prevent `Pointer` from registering deallocators with the garbage collector/"
,,javacpp," * Log when `Pointer.deallocator` gets registered, garbage collected, or deallocated manually, if `Logger.isDebugEnabled()` (redirectable to SLF4J)
 * Make `Pointer implements AutoCloseable` to let us try-with-resources, thus bumping requirements to Java SE 7 and Android 4.0/ * Fix `Parser` support for the `interface` keyword of the Microsoft C/C++ Compiler/ * Let `Parser` pick up names from `Info.pointerTypes` in the case of function pointers as well/ * Fix `Parser` bug involving simple types and skipped identifiers/ * Add `@Properties(names=...)` value to specify a list of default platform names that can be inherited by other classes/ * Fix a couple of `Parser` issues on complex template types (issue #37)/ * Add `Parser` support for the `std::bitset` ""container""/ * Properly parse overloaded `new` and `delete` operators, `friend` declarations, and default constructors with an explicit `void` parameter (issue #31)
 * Fix a couple of potential `NullPointerException` in `Parser` (issue #30)/ * Have the `Parser` wrap the `iterator` of some standard C++ containers when useful/ * Forbid `Parser` from producing `abstract` classes, preventing C++ factory methods and such from working properly (issue #25)/"
,,javacpp," * Fix a couple of `Parser` issues on complex template types (issue #37)/ * Properly parse overloaded `new` and `delete` operators, `friend` declarations, and default constructors with an explicit `void` parameter (issue #31)
 * Fix a couple of potential `NullPointerException` in `Parser` (issue #30)/"
,,javacpp," * Log when `Pointer.deallocator` gets registered, garbage collected, or deallocated manually, if `Logger.isDebugEnabled()` (redirectable to SLF4J)
 * Make `Pointer implements AutoCloseable` to let us try-with-resources, thus bumping requirements to Java SE 7 and Android 4.0/"
,,javacpp," * Add `Parser` support for the `std::bitset` ""container""/"
,,javacpp," * Provide `@Virtual(true)` to specify pure virtual functions and prevent `Generator` from making undefined calls
 * Update properties for Android to detect undefined symbols at compile time/Update version in the `pom.xml` file to 1.1-SNAPSHOT

 * Introduce the concept of ""owner address"" to integrate `Pointer` transparently with `std::shared_ptr`, etc (Thanks to Cyprien Noel for the idea!)/ * Insure `Generator` casts properly to `jweak` when calling `DeleteWeakGlobalRef()` (issue #23)/"
,,javacpp," * Add new ""org.bytedeco.javacpp.cachedir"" system property to specify where to extract and leave native libraries to share across multiple JVM instances/"
,,javacpp," * Add `Info.flatten` to duplicate class declarations into their subclasses, useful when a subclass pointer cannot be used for the base class as well/ * Enhance basic support for containers of the style `std::vector<std::pair< ... > >` with user-friendly array-based setter methods
 * Fix `Generator` not passing function objects even when annotating `FunctionPointer` parameters with `@ByVal` or `@ByRef`
 * Map `bool*` to `boolean[]` tentatively in `Parser` since `sizeof(bool) == sizeof(jboolean)` on most platforms/ * Allow `Parser` to generate `@Cast()` annotations and overloaded `put()` methods in basic containers too
 * Move list of basic containers and types to `Info.cppTypes` of the ""basic/containers"" and ""basic/types"" `InfoMap` entries, letting users change them at build time
 * Fix some `Parser` issues with `typedef` and forward declarations inside `class` definitions/ * Insure `Parser` maps 64-bit values in C++ `enum` to `long` variables (issue #48)/ * Fix `Generator` trying to cast improperly objects on return from  `@Virtual` functions
 * Make `Parser` take `constexpr` into account/ * Make `Parser` take namespace aliases into account, and fix a couple of preprocessing issues with `TokenIndexer`/ * Make `Parser` ignore namespace aliases/ * Fix primitive arrays and NIO buffers not getting updated on return when used as arguments with adapters (issue bytedeco/javacpp-presets#109)
 * Remove confusing and now unnecessary empty constructors/Update version in the `pom.xml` file to 1.2-SNAPSHOT

 * Enhance a bit the conversion from Doxygen-style documentation comments to Javadoc-style
 * Remove class check in allocators, which prevented peer classes from being extended in Java, instead relying on `super((Pointer)null)` in child peer classes/Remove unnecessary use of `LinkedList` and switch to `ArrayList` for overall efficiency/"
,,javacpp," * Add missing calls to `close()` for `InputStream` and `OutputStream` in `Loader` (issue #53)
 * Remove `Piper` class no longer needed with Java SE 7/Remove unnecessary use of `LinkedList` and switch to `ArrayList` for overall efficiency/"
,,javacpp," * Allow `Parser` to generate `@Cast()` annotations and overloaded `put()` methods in basic containers too
 * Move list of basic containers and types to `Info.cppTypes` of the ""basic/containers"" and ""basic/types"" `InfoMap` entries, letting users change them at build time
 * Fix some `Parser` issues with `typedef` and forward declarations inside `class` definitions/ * Make `Parser` take namespace aliases into account, and fix a couple of preprocessing issues with `TokenIndexer`/"
Data conversion,Data conversion,javacpp," * Allow `Parser` to generate `@Cast()` annotations and overloaded `put()` methods in basic containers too
 * Move list of basic containers and types to `Info.cppTypes` of the ""basic/containers"" and ""basic/types"" `InfoMap` entries, letting users change them at build time
 * Fix some `Parser` issues with `typedef` and forward declarations inside `class` definitions/"
,,javacpp," * Improve the performance of `BytePointer.getString()` by using `strlen()`/ * Prevent `Generator` from initializing classes when preloading them, which can cause problems (issue bytedeco/javacpp-presets#126)/ * Allow `Parser` to generate `@Cast()` annotations and overloaded `put()` methods in basic containers too
 * Move list of basic containers and types to `Info.cppTypes` of the ""basic/containers"" and ""basic/types"" `InfoMap` entries, letting users change them at build time
 * Fix some `Parser` issues with `typedef` and forward declarations inside `class` definitions/Update version in the `pom.xml` file to 1.2-SNAPSHOT

 * Enhance a bit the conversion from Doxygen-style documentation comments to Javadoc-style
 * Remove class check in allocators, which prevented peer classes from being extended in Java, instead relying on `super((Pointer)null)` in child peer classes/Remove unnecessary use of `LinkedList` and switch to `ArrayList` for overall efficiency/"
,,javacpp," * Prevent `Generator` from initializing classes when preloading them, which can cause problems (issue bytedeco/javacpp-presets#126)/ * Add logging to `Loader.loadLibrary()` to help diagnose loading problems (issue #41)/"
,,javacpp," * Prevent `Loader` from extracting libraries more than once, which can cause problems (issue bytedeco/javacpp-presets#126)
 * Make `Indexer implements AutoCloseable` to let us try-with-resources/"
,,javacpp," * Add ""org.bytedeco.javacpp.maxbytes"" system property, forcing a call to `System.gc()` when this amount of memory tracked with deallocators is reached/ * Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,javacpp," * Add ""org.bytedeco.javacpp.maxbytes"" system property, forcing a call to `System.gc()` when this amount of memory tracked with deallocators is reached/long pointers/squid:S00108 - Nested blocks of code should not be left empty/"
,,javacpp," * Make `Parser` take into account Java keywords not reserved in C++, casting issues with `int64_t`, and `const` value types in basic containers/ * Lengthen the `position`, `limit`, and `capacity` fields of `Pointer` using `long`/Mark functions from c includes with noexception

This allows the Generator to avoid wrapping the function invocations with
try/catch when no exception is possible/ * Add support for `enum` without enumerator list (issue #78)/ * Add support for C++11 typed `enum` (issue #78)/ * Add missing space for `const` types when normalizing template arguments in `Parser` (issue bytedeco/javacpp-presets#165)/squid:S2095 - Resources should be closed/squid:S1854 - Dead stores should be removed/ * Let `Parser` use adapters in the case `FunctionPointer` as well (issue bytedeco/javacpp-presets#145)/ * Split type names at `::` delimiters before mapping them against templates in `Parser`/ * Make `Parser` take `Info.skip` into account for `enum` declarations as well/Fix up functionality for `Info.flatten`/"
,,javacpp," * Set the internal DT_SONAME field in libraries created for Android (issue bytedeco/javacpp-presets#188)/Add flag to keep the generated cpp files/Use binary output folder as the compiler's working directory

This should prevent MSVC from dumping it's object files
in the project's root directory./"
,,javacpp," * Let users define `NATIVE_ALLOCATOR` and `NATIVE_DEALLOCATOR` macros to overload global `new` and `delete` operators/ * Map `jint` to `int` and `jlong` to `long long` on Windows as well as all platforms with GCC (or Clang)/ * Fix corner cases when checking for the platform in `Generator` and `Parser`/ * Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/ * Lengthen the `position`, `limit`, and `capacity` fields of `Pointer` using `long`/ * Allow `Builder` to generate native libraries with empty `@Platform` annotation even without user defined `native` methods/ * Adjust a few things in `Generator` preventing `@Virtual` from working properly in some cases (issue bytedeco/javacpp-presets#143)/"
,,javacpp, * Allow `Builder` to generate native libraries with empty `@Platform` annotation even without user defined `native` methods/
,,javacpp,"squid:S00108 - Nested blocks of code should not be left empty/ * Fix swallowed `InterruptedException` (issue bytedeco/javacv#315)/squid:S00117 - Local variable and method parameter names should comply with a naming convention
squid:S1197 - Array designators ""[]"" should be on the type, not the variable
squid:S00115 - Constant names should comply with a naming convention/"
,,javacpp," * Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now/"
,,javacpp,"Fix potentially failing tests/ * Add ""org.bytedeco.javacpp.maxretries"" system property, the number times to call `System.gc()` before giving up (defaults to 10)/"
,,javacpp," * Fix `IndexerTest` potentially failing with `OutOfMemoryError` (issue bytedeco/javacpp-presets#234)
 * Preload libraries to work around some cases when they refuse to load once renamed (issue deeplearning4j/libnd4j#235)
 * Fix compilation error on some `linux-ppc64le` platforms (issue deeplearning4j/libnd4j#232)/"
Memory Management,Memory Management,javacpp," * Add ""org.bytedeco.javacpp.maxretries"" system property, the number times to call `System.gc()` before giving up (defaults to 10)/ * Deallocate native memory in a dedicated thread to reduce lock contention (issue #103)/Wait a bit longer for `System.gc()` to reclaim memory/Add `Pointer.maxBytes()` and `totalBytes()` to monitor amount of memory tracked/"
,,javacpp," * Fix potential `ParserException` on comments found after annotations before function declarations/Release version 1.2/ * Use `Info.cppTypes` for all `Parser` type substitutions, in addition to macros and templates (issue bytedeco/javacpp-presets#192)/"
,,javacpp, * Add parameters to `Loader.load()` offering more flexibility over the platform properties and library paths/
,,javacpp,Add `Pointer.maxBytes()` and `totalBytes()` to monitor amount of memory tracked/ * Treat all `String` with `Charset.defaultCharset()` (or define `MODIFIED_UTF8_STRING` for old behavior) (issue #70)/
,,javacpp, * Throw `OutOfMemoryError` on `allocateArray()` for `Pointer` of primitive types with `size > 0 && address == 0`/
,,javacpp," * Prevent Android system libraries from getting copied or extracted/ * Fix `IndexerTest` potentially failing with `OutOfMemoryError` (issue bytedeco/javacpp-presets#234)
 * Preload libraries to work around some cases when they refuse to load once renamed (issue deeplearning4j/libnd4j#235)
 * Fix compilation error on some `linux-ppc64le` platforms (issue deeplearning4j/libnd4j#232)/* Fix `Loader` crashing on Android (issue bytedeco/javacv#412)/ * Add the ability the specify, after a `#` character, the output filename of libraries extracted by `Loader.load()`/ * Add parameters to `Loader.load()` offering more flexibility over the platform properties and library paths/ * Fix `Loader.load()` error when called right after `Builder.build()` within the same process/"
,,javacpp," * Add ""org.bytedeco.javacpp.maxphysicalbytes"" system property to force calls to `System.gc()` based on `Pointer.physicalBytes()`
 * Allow strings ending with ""t"", ""g"", ""m"", etc to specify the number of bytes in system properties (issue #125)/"
,,javacpp, * Add `HalfIndexer` to access `short` arrays as half-precision floating point numbers/
,,javacpp," * Accelerate call to `Pointer.physicalBytes()` on Linux (issue #133)/ * Add ""org.bytedeco.javacpp.maxphysicalbytes"" system property to force calls to `System.gc()` based on `Pointer.physicalBytes()`
 * Allow strings ending with ""t"", ""g"", ""m"", etc to specify the number of bytes in system properties (issue #125)/Simplify synchronization of memory allocation to avoid `OutOfMemoryError` when low on memory/ * Synchronize memory allocation in `Pointer` when low on memory to avoid `OutOfMemoryError`/"
Data conversion,Data conversion,javacpp," * Have `Parser` annotate the `allocate()` functions and not the actual constructors (issue bytedeco/javacpp-presets#297)/ * Fix `Parser` incorrectly skipping over some template function declarations/ * Insure `Parser` properly ignores the `auto`, `mutable`, `register`, `thread_local`, and `volatile` C++ keywords for storage classes
 * Fix `Generator` and `Parser` for types like `std::unordered_map<std::string,std::pair<int,int> >` (issue bytedeco/javacpp-presets#266)/Fix `Parser` incorrectly escaping quotes for multiline `nullValue` of `@ByRef` or `@ByVal`/Fix #264 unexpected token ::

This problem arose with tensorflow 0.10, javacpp was unable to parse a
file that contained friend class with global scope identifier./escape empty strings for nullValue/ * Add support for data member pointers as pseudo-`FunctionPointer` (issue #114)/ * Create all missing directories in the path to the target file of `Parser`/"
,,javacpp," * Output compiled libraries to user specified class path by default for input classes inside JAR files, etc/"
,,javacpp," * Accelerate call to `Pointer.physicalBytes()` on Linux (issue #133)/ * Create all missing directories in the paths to the source files created by `Generator`/Add test for `Pointer.physicalBytes()`/ * Add ""org.bytedeco.javacpp.maxphysicalbytes"" system property to force calls to `System.gc()` based on `Pointer.physicalBytes()`
 * Allow strings ending with ""t"", ""g"", ""m"", etc to specify the number of bytes in system properties (issue #125)/ * Add `UniquePtrAdapter` and corresponding `@UniquePtr` annotation to support `unique_ptr` containers (issue bytedeco/javacpp-presets#266)/ * Add support for data member pointers as pseudo-`FunctionPointer` (issue #114)/"
,,javacpp,Throw more information when `OutOfMemoryError` occurs on `allocateArray()` for `Pointer` of primitive types/
,,javacpp, * Make `Loader` cache libraries (in `~/.javacpp/cache/` by default) instead of using temporary files/
,,javacpp,"Update version in the `pom.xml` file to 1.3-SNAPSHOT

 * Print memory sizes in a human-readable format with `Pointer.formatBytes()`
 * Map standard `malloc()`, `calloc()`, `realloc()`, and `free()` functions (issue #136)/"
Memory Management,Memory Management,javacpp,"Update version in the `pom.xml` file to 1.3.2-SNAPSHOT

 * Make `Pointer.asBuffer()` thread-safe (issue #155)/Release version 1.3/Update version in the `pom.xml` file to 1.3-SNAPSHOT

 * Print memory sizes in a human-readable format with `Pointer.formatBytes()`
 * Map standard `malloc()`, `calloc()`, `realloc()`, and `free()` functions (issue #136)/"
,,javacpp," * Add support for `decltype()` declarations (issue #135)/ * Fix `Parser` not properly mapping the type of `long` anonymous enums/ * Take into account `const` on function parameters when looking up in `InfoMap`, and fix some incorrectly translated macros into variables
 * Add to `InfoMap.defaults` more names that are reserved in Java, but not in C++
 * Add via `@ByPtrRef` support for function pointers passed by reference, as well as support for `Info.javaText` with `typedef`/"
,,javacpp," * Fix broken `outputDirectory` property and corresponding `-d` command line option (issue #153)
 * Add `Loader.extractResources()` and `cacheResources()` methods to extract or cache all resources with given name/"
,,javacpp," * Fix broken `outputDirectory` property and corresponding `-d` command line option (issue #153)
 * Add `Loader.extractResources()` and `cacheResources()` methods to extract or cache all resources with given name/Update version in the `pom.xml` file to 1.3.1-SNAPSHOT

 * Fix potential issues with `Parser` repeating the `@ByPtrPtr` or `@ByPtrRef` annotations on parameters
 * To support Scala singleton objects better, consider as `static` methods from objects that are not `Pointer`
 * Allow `Loader.extractResource()` and `cacheResource()` to extract or cache all files from a directory in a JAR file
 * Create version-less symbolic links to libraries in cache on those platforms where it is useful to link easily
 * Use `java.io.tmpdir` as fallback in `Loader.getCacheDir()`, and throw a clear exception on failure/Release version 1.2.7

 * Fix `Loader` errors that could occur due to recent changes/ * Improve `Loader` handling of duplicate libraries found in different JAR files using symbolic links (useful for MKL, etc)/ * Prevent `Loader` from overwriting previously extracted and renamed libraries (issue deeplearning4j/nd4j#1460)/"
,,javacpp, * Provide `BytePointer` with value getters and setters for primitive types other than `byte` to facilitate unaligned memory accesses/
,,javacpp, * Let `Pointer` log debug messages when forced to call `System.gc()`/
,,javacpp," * Fix `Parser` not considering empty `class`, `struct`, or `union` declarations as opaque forward declarations/ * Fix `Parser` handling of `std::map` and of documentation comments containing the ""*/"" sequence/"
,,javacpp, * Provide `BytePointer` with value getters and setters for primitive types other than `byte` to facilitate unaligned memory accesses/ * Add a `BuildMojo.buildCommand` parameter that lets users execute arbitrary system commands easily with `ProcessBuilder`/
,,javacpp," * Provide `BytePointer` with value getters and setters for primitive types other than `byte` to facilitate unaligned memory accesses/Fix `totalProcessors()`, `totalCores()`, `totalChips()` methods on Android/ * Add portable and efficient `totalChips()` methods/ * Add portable and efficient `totalPhysicalBytes()`, `availablePhysicalBytes()`, `totalProcessors()`, and `totalCores()` methods/"
,,javacpp," * Prevent `Loader` from loading system libraries, which causes problems on Android 7.x (issue bytedeco/javacv#617)/ * Avoid `Loader` issues with spaces, etc in paths to library files (issue deeplearning4j/nd4j#1564)/"
,,javacpp,  * Make public the `Pointer.formatBytes()` and `Pointer.parseBytes()` static methods/ * Fix potential formatting issues with `OutOfMemoryError` thrown from `Pointer`/
,,javacpp," * Use `Integer.decode()` instead of `parseInt()` on integer literals to support hexadecimal and octal numbers/ * Add `Builder.encoding` option to let users specify I/O character set name (issue bytedeco/javacpp-presets#195)/ * Prevent `Parser` from overwriting target classes when nothing was parsed/Fix bugs in Parser

 * Fix `Parser` incorrectly recognizing values as pointers when `const` is placed after type (issue #173)
 * Add `Parser` support for C++11 `using` declarations that act as `typedef` (issue #169)
 * Let `Parser` accept variables initialized with parentheses (issue #179)
 * Fix `Parser` confusion between attributes and namespace-less templates (issue #181)/ * Prevent `Parser` from outputting setters for `const` member pointers/ * Add support for arrays of function pointers/"
,,javacpp,"Use `BUILD_PATH_SEPARATOR` instead of `PATH_SEPARATOR`, which conflicts with MSYS2/ * Make the arbitrary resources available to process executed with `Builder.buildCommand` via the `BUILD_PATH` environment variable/ * Let users bundle arbitrary resources, have them extracted in cache, and used as `include` or `link` paths ( #43)/"
,,javacpp," * Call `malloc_trim(0)` after `System.gc()` on Linux to make sure memory gets released (issue bytedeco/javacpp-presets#423)/ * Add `Builder.encoding` option to let users specify I/O character set name (issue bytedeco/javacpp-presets#195)/ * Fix potential compile errors with Android caused by superfluous `typedef` from `Generator` (issue #186)/Fix bugs in Parser

 * Fix `Parser` incorrectly recognizing values as pointers when `const` is placed after type (issue #173)
 * Add `Parser` support for C++11 `using` declarations that act as `typedef` (issue #169)
 * Let `Parser` accept variables initialized with parentheses (issue #179)
 * Fix `Parser` confusion between attributes and namespace-less templates (issue #181)/"
,,javacpp," * Let users bundle arbitrary resources, have them extracted in cache, and used as `include` or `link` paths ( #43)/"
,,javacpp," * Prevent race condition that could occur in `Loader.cacheResource()` ( #188)/ * Let users bundle arbitrary resources, have them extracted in cache, and used as `include` or `link` paths ( #43)/"
,,javacpp," * Add `Loader.addressof()` to access native symbols, usable via optional `ValueGetter/ValueSetter` in `FunctionPointer`/"
Data conversion,Data conversion,javacpp," * Fix `Parser` incorrectly resolving type definitions with classes of the same name in parent namespaces
 * Fix `Generator` compile errors for `const` template types of `@Adapter` classes using the `@Cast` annotation/"
,,javacpp, * Try to use symbolic links in `Loader.load()` for output filenames specified with the `#` character (useful for libraries like MKL)/
,,javacpp,Refine support for `platform.extension` by moving it to platform properties/
Data conversion,Data conversion,javacpp," * Allow `Parser` to map and cast function pointers to `Pointer`/ * Fix `Parser` not producing `@Cast` annotations for types with `Info.cast()` on `operator()`, as well as failing on `using operator` statements/ * Produce `pop_back()` and `push_back()` for relevant basic containers in `Parser` (issue bytedeco/javacv#659)/ * Output single value setters for containers in `Parser` to avoid surprises (issue #217)/ * Add `Parser` support for C++11 `using` declarations inheriting constructors (issue bytedeco/javacpp-presets#491)
 * Fix compiler error when defining `std::set` or `std::unordered_set` with `Parser`/ * Make `Parser` take `Info.skip()` into account for enumerators as well/ * Move `sizeof()` and `offsetof()` data to global variables to prevent `StackOverflowError` in `JNI_OnLoad()` (issue bytedeco/javacpp-presets#331)
 * Propagate within `Parser` type information from macros to other macros referencing them

Also fix a few more issues with `Parser`/ * Add support for `JNI_OnLoad_libname()` naming scheme for iOS via new `platform.library.static=true` property
 * Improve the clarity of error messages on `Parser` failures
 * Fix `Parser` issues with multiple `typedef` declarations in a single statement
 * Require `Info.annotations(""@Name"")` to pick up alternate names from attributes/ * Add `@Platform(exclude=...)` annotation value to remove header files from inherited `@Platform(include=...`

Also fix a few more issues with `Parser`/ * Fix a few issues with `Parser`, including missing `PointerPointer` member setters (issue bytedeco/javacpp-presets#478)/ * Fix potential race conditions and various issues with `Loader` that could prevent libraries like MKL from working properly/"
,,javacpp, * Output to log all commands executed for `Builder.buildCommand` via `ProcessBuilder`/Refine support for `platform.extension` by moving it to platform properties/ * Have `Builder` generate base JNI functions into `jnijavacpp.cpp` for better iOS support (issue #213)/
,,javacpp,"Refine support for `platform.extension` by moving it to platform properties/ * Have `Builder` generate base JNI functions into `jnijavacpp.cpp` for better iOS support (issue #213)/ * Move `sizeof()` and `offsetof()` data to global variables to prevent `StackOverflowError` in `JNI_OnLoad()` (issue bytedeco/javacpp-presets#331)
 * Propagate within `Parser` type information from macros to other macros referencing them

Also fix a few more issues with `Parser`/ * Add support for `JNI_OnLoad_libname()` naming scheme for iOS via new `platform.library.static=true` property
 * Improve the clarity of error messages on `Parser` failures
 * Fix `Parser` issues with multiple `typedef` declarations in a single statement
 * Require `Info.annotations(""@Name"")` to pick up alternate names from attributes/ * Add `@Platform(exclude=...)` annotation value to remove header files from inherited `@Platform(include=...`

Also fix a few more issues with `Parser`/"
,,javacpp, * Create symbolic links to libraries preloaded by `Loader` as needed on Mac for renamed libraries/ * Fix potential race conditions and various issues with `Loader` that could prevent libraries like MKL from working properly/
,,javacpp," * Fix `Parser` failing on constructors of class templates/ * Allow using `new Info().enumerate()` to map all C++ `enum` to Java `enum` types by default
 * Fix `Parser` issues surrounding enum classes, anonymous namespaces, and pure virtual classes/ * Fix `Parser` failing on `enum` declarations where the first line is a macro (issue #230)/ * Add `Info.enumerate` to let `Parser` map C++ enum classes to Java enum types (issue #108)/Fix `Parser` failing on `enum` declarations with attributes/Fix various small issues with `Parser`/ * Make it possible to define read-only containers with `Parser` by prepending `const ` (issue #223)/ * Access elements of basic containers defined in `Parser` with `at()` instead of `operator[]` (issue #223)/"
,,javacpp," * Enhance `Loader.createLibraryLink()` by allowing to create symbolic links in other directories/ * Add ""org.bytedeco.javacpp.pathsfirst"" system property to let users search ""java.library.path"", etc before the class path/ * Work around in `Builder` the inability to pass empty arguments on Windows/"
,,javacpp," * Add `Parser` support for `_Bool`, `_Complex`, `_Imaginary`, `complex`, `imaginary` types from C99/"
,,javacpp, * Fix `SharedPtrAdapter` and `UniquePtrAdapter` failing to take ownership of temporary objects/ * Let `Generator` pick up `@NoException` annotations from super classes as well/
,,javacpp, * Catch more exceptions that can occur in `Loader` when caching resources ( #226)/
,,javacpp,Add `PointerScope.getInnerScope()` that can be called from any context/ * Add `PointerScope` to manage more easily the resources of a group of `Pointer` objects/
,,javacpp," * Have `Parser` wrap the `insert()` and `erase()` methods of basic containers to allow modifying lists and sets/ * Fix `Parser` from outputting accessors not available with `std::forward_list` or `std::list`/ * Make `Parser` take into account implicit constructors even when inheriting some with `using` declarations
 * Pick up `Parser` translation of enum and macro expressions from `Info.javaNames`
 * Let `Parser` define `Info.pointerTypes` also for partially specialized templates with default arguments/"
Data conversion,Data conversion,javacpp, * Enhance `InfoMap` and `StringAdapter` with default mappings and casts for `std::wstring`/
,,javacpp," * Synchronize `Loader.loadLibrary()` to fix potential race condition ( #246)/ * Fall back on Android-friendly `System.loadLibrary()` in `Loader.load()` instead of ""java.library.path"" (issue bytedeco/javacv#970)/"
,,javacpp,Add `PointerTest.testPointerPointer()` to make sure it works correctly (issue #251)/
,,javacpp, * Add `BooleanPointer` and `BooleanIndexer` to access arrays of boolean values with `sizeof(jboolean) == 1`/
,,javacpp," * Add `Loader.getLoadedLibraries()` method for debugging purposes and fix flaky `BuilderTest` (issue #245)
 * Call `PointerScope.attach()` as part of `Pointer.deallocator()`, instead of `init()`, to support custom deallocators as well
 * Prevent `Parser` from appending annotations to setter methods of variables to satisfy the `Generator`/"
,,javacpp," * Make sure `Parser` always uses the short version of identifiers for Java class declarations
 * Prevent `Parser` from inheriting constructors with `using` when not accessible or of incomplete template instances/ * Add `@Properties(global=...)` value to allow `Parser` to target Java packages ( #252)/ * Fix `Parser` failing when a value of an `std::pair` basic container is also an `std::pair` (issue bytedeco/javacpp-presets#614)/"
,,javacpp," * Allow `Builder` to execute `javac` and `java` for convenience, and remove ""."" from class path (issue #192)/ * Make `Builder` accept multiple options for `platform.link.prefix` and `platform.link.suffix` ( #250)/"
,,javacpp, * Fix `Generator` output for `@Const` parameters of function pointers/
,,javacpp, * Add `Bfloat16Indexer` to access `short` arrays as `bfloat16` floating point numbers/
,,javacpp," * Enhance `UniquePtrAdapter` with the ability to move pointers out with the `&&` operator
 * Let `Parser` map constructors also for abstract classes with `Info.virtualize`/ * Fix `Parser` taking the global package as the target package even when both are set/Refine `Parser` skipping over C++11 style `{ ... }` member initializer lists ( bytedeco/javacpp-presets#642)/ * Let `Parser` skip over C++11 style `{ ... }` member initializer lists ( bytedeco/javacpp-presets#642)/Use `@Properties(inherit=...)` instead of `@Platform(library=...)` for classes targeted by `@Properties(global=...)`/"
,,javacpp,"Use `@Properties(inherit=...)` instead of `@Platform(library=...)` for classes targeted by `@Properties(global=...)`/ * Use simple name from `@Properties(target=..., global=...)` class as default for `@Platform(library=...)` name/"
,,javacpp," * Allow users to override platform properties via system properties starting with ""org.bytedeco.javacpp.platform.""/ * Replace calls to `Class.getResource()` with `Loader.findResource()` to work around issues with JPMS ([ #276](https://github.com/bytedeco/javacpp//276))
 * Enhance `Loader.findResources()` with `Class.getResource()` and search among parent packages
 * Take shortest common package name among all user classes for the default output path of `Builder`/"
,,javacpp," * Support multiple instances of `FunctionPointer` subclasses, up to the value in `@Allocator(max=...)` (issue bytedeco/javacpp-presets#683)/ * Let `Parser` pick up `Info` explicitly for all constructors by considering their names as functions (issue #284)/ * Fix `Parser` overlooking `Info` for constructors inside templates (issue #284)/ * Fix `Parser` overlooking `Info` for constructors inside a namespace (issue #284)/"
,,javacpp, * Add `platform.executable` and `platform.executablepath` properties to bundle executables and extract them with `Loader.load()`/
,,javacpp," * Support multiple instances of `FunctionPointer` subclasses, up to the value in `@Allocator(max=...)` (issue bytedeco/javacpp-presets#683)/"
,,javacpp,Fix spurious library generation when overriding `@Properties` with an additional `@Platform` annotation/
,,javacpp, * Add `Loader.loadGlobal()` to load symbols globally as often required by Python libraries (issue ContinuumIO/anaconda-issues#6401)/
,,javacpp,Clarify what values of 0 or less mean for `Pointer.maxBytes` and `maxPhysicalBytes`/
,,javacpp," * Avoid `Parser` writing `allocateArray()` when single `int`, `long`, `float`, or `double` constructor already exists (issue bytedeco/javacv#1224)/"
,,javacpp," * Make sure `Generator` ignores deallocators on `const` values returned from adapters (issue #317)/ * Expose all platform properties to process executed with `Builder.buildCommand` via environment variables, with names uppercase and all `.` replaced with `_`/"
,,javacpp," * Fix `Loader.cacheResource()` with the ""jrt"" protocol as used by jlink ( #305)/"
,,javacpp, * Consider `Pointer` values `maxBytes` or `maxPhysicalBytes` suffixed with `%` as relative to `Runtime.maxMemory()` ( #365)/
,,javacpp, * Fix `Parser` incorrectly inheriting default constructors multiple times with `using`/
,,javacpp," * Provide `ByteIndexer` with value getters and setters for unsigned `byte` or `short`, `half`, `bfloat16`, and `boolean` types as well/"
,,javacpp," * Include in the defaults of `InfoMap` mappings missing for the `std::array` and `jchar` types
 * Fix various `Parser` failures with attributes on constructors, empty macros, enum classes, friend classes, inherited constructors, and keywords in parameter names
 * Add to `Parser` support for C++11 attributes found within `[[` and `]]` brackets/"
,,javacpp," * Add new `indexer` package containing a set of `Indexer` for easy and efficient multidimensional access of arrays and buffers ([issue javacv:317](http://code.google.com/p/javacv/issues/detail?id=317))   ,  "
,,javacpp," * Let `@Virtual @MemberGetter` annotated methods return member function pointers of functions defined with `@Virtual`, useful for frameworks like Cocos2d-x   ,  Add support for virtual functions to let C++ call back methods implemented in Java

 * Add `Info.virtualize` to have the `Parser` generate `@Virtual abstract` for pure virtual functions in the given classes
 * Add `@Virtual` annotation and update `Generator` to support callback by overriding such annotated `native` or `abstract` methods   ,  "
Data conversion,Data conversion,javacpp," * Fix some `Parser` exceptions on valid declarations with template arguments, and make `Info.javaName` usable in the case of `enum`   ,   * Fix some `Parser` exceptions on valid declarations with macro expansions or overloaded cast operators   ,   * Use `Long.decode()` inside the `Tokenizer` to test more precisely when integer values are larger than 32 bits
 * Have the `Parser` produce `@Name(""operator="") ... put(... )` methods for standard C++ containers, avoiding mistaken calls to `Pointer.put(Pointer)` ([issue javacv:34](https://github.com/bytedeco/javacv/issues/34))
 * Let the `Parser` apply `Info.skip` in the case of macros as well
 * Remove warning log messages when using the `@Raw` annotation   ,  Add support for virtual functions to let C++ call back methods implemented in Java

 * Add `Info.virtualize` to have the `Parser` generate `@Virtual abstract` for pure virtual functions in the given classes
 * Add `@Virtual` annotation and update `Generator` to support callback by overriding such annotated `native` or `abstract` methods   ,   * Add hack for `typedef void*` definitions and parameters with a double indirection to them   ,  "
Data conversion,Data conversion,javacpp," * Fix some `Parser` exceptions on valid declarations with macro expansions or overloaded cast operators   ,  "
,,javacpp," * Use `Long.decode()` inside the `Tokenizer` to test more precisely when integer values are larger than 32 bits
 * Have the `Parser` produce `@Name(""operator="") ... put(... )` methods for standard C++ containers, avoiding mistaken calls to `Pointer.put(Pointer)` ([issue javacv:34](https://github.com/bytedeco/javacv/issues/34))
 * Let the `Parser` apply `Info.skip` in the case of macros as well
 * Remove warning log messages when using the `@Raw` annotation   ,  "
,,javacpp," * Allow `Builder` to generate native libraries with empty `@Platform` annotation even without user defined `native` methods   ,  "
,,javacpp,"squid:S00117 - Local variable and method parameter names should comply with a naming convention
squid:S1197 - Array designators ""[]"" should be on the type, not the variable
squid:S00115 - Constant names should comply with a naming convention   ,   * Fix swallowed `InterruptedException` (issue bytedeco/javacv#315)   ,   * Add parameters to `Loader.load()` offering more flexibility over the platform properties and library paths   ,   * Fix `Loader.load()` error when called right after `Builder.build()` within the same process   ,  squid:S00108 - Nested blocks of code should not be left empty   ,   * Prevent `Generator` from initializing classes when preloading them, which can cause problems (issue bytedeco/javacpp-presets#126)   ,  "
,,javacpp,"long pointers   ,  squid:S00108 - Nested blocks of code should not be left empty   ,   * Add ""org.bytedeco.javacpp.maxbytes"" system property, forcing a call to `System.gc()` when this amount of memory tracked with deallocators is reached   ,  Wait a bit longer for `System.gc()` to reclaim memory   ,  Add `Pointer.maxBytes()` and `totalBytes()` to monitor amount of memory tracked   ,  "
,,javacpp," * Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now   ,  "
,,javacpp," * Add parameters to `Loader.load()` offering more flexibility over the platform properties and library paths   ,  "
,,javacpp," * Add missing calls to `close()` for `InputStream` and `OutputStream` in `Loader` (issue #53)
 * Remove `Piper` class no longer needed with Java SE 7   ,  Use binary output folder as the compiler's working directory

This should prevent MSVC from dumping it's object files
in the project's root directory.   ,  Add flag to keep the generated cpp files   ,   * Set the internal DT_SONAME field in libraries created for Android (issue bytedeco/javacpp-presets#188)   ,  "
,,javacpp," * Make `Parser` take namespace aliases into account, and fix a couple of preprocessing issues with `TokenIndexer`   ,  "
Data conversion,Data conversion,javacpp," * Treat all `String` with `Charset.defaultCharset()` (or define `MODIFIED_UTF8_STRING` for old behavior) (issue #70)   ,   * Fix corner cases when checking for the platform in `Generator` and `Parser`   ,   * Adjust a few things in `Generator` preventing `@Virtual` from working properly in some cases (issue bytedeco/javacpp-presets#143)   ,   * Allow `Builder` to generate native libraries with empty `@Platform` annotation even without user defined `native` methods   ,  Add `Pointer.maxBytes()` and `totalBytes()` to monitor amount of memory tracked   ,   * Lengthen the `position`, `limit`, and `capacity` fields of `Pointer` using `long`   ,   * Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now   ,   * Prevent `Generator` from initializing classes when preloading them, which can cause problems (issue bytedeco/javacpp-presets#126)   ,   * Allow `Parser` to generate `@Cast()` annotations and overloaded `put()` methods in basic containers too
 * Move list of basic containers and types to `Info.cppTypes` of the ""basic/containers"" and ""basic/types"" `InfoMap` entries, letting users change them at build time
 * Fix some `Parser` issues with `typedef` and forward declarations inside `class` definitions   ,   * Map `jint` to `int` and `jlong` to `long long` on Windows as well as all platforms with GCC (or Clang)   ,   * Let users define `NATIVE_ALLOCATOR` and `NATIVE_DEALLOCATOR` macros to overload global `new` and `delete` operators   ,   * Improve the performance of `BytePointer.getString()` by using `strlen()`   ,  Update version in the `pom.xml` file to 1.2-SNAPSHOT

 * Enhance a bit the conversion from Doxygen-style documentation comments to Javadoc-style
 * Remove class check in allocators, which prevented peer classes from being extended in Java, instead relying on `super((Pointer)null)` in child peer classes   ,  "
Data conversion,Data conversion,javacpp," * Allow `Parser` to generate `@Cast()` annotations and overloaded `put()` methods in basic containers too
 * Move list of basic containers and types to `Info.cppTypes` of the ""basic/containers"" and ""basic/types"" `InfoMap` entries, letting users change them at build time
 * Fix some `Parser` issues with `typedef` and forward declarations inside `class` definitions   ,  "
Data conversion,Data conversion,javacpp," * Add missing space for `const` types when normalizing template arguments in `Parser` (issue bytedeco/javacpp-presets#165)   ,   * Let `Parser` use adapters in the case `FunctionPointer` as well (issue bytedeco/javacpp-presets#145)   ,   * Fix primitive arrays and NIO buffers not getting updated on return when used as arguments with adapters (issue bytedeco/javacpp-presets#109)
 * Remove confusing and now unnecessary empty constructors   ,   * Split type names at `::` delimiters before mapping them against templates in `Parser`   ,   * Use `Info.cppTypes` for all `Parser` type substitutions, in addition to macros and templates (issue bytedeco/javacpp-presets#192)   ,   * Insure `Parser` maps 64-bit values in C++ `enum` to `long` variables (issue #48)   ,   * Add support for C++11 typed `enum` (issue #78)   ,  Release version 1.2   ,  Fix up functionality for `Info.flatten`   ,   * Add `Info.flatten` to duplicate class declarations into their subclasses, useful when a subclass pointer cannot be used for the base class as well   ,   * Fix `Generator` trying to cast improperly objects on return from  `@Virtual` functions
 * Make `Parser` take `constexpr` into account   ,   * Make `Parser` ignore namespace aliases   ,   * Make `Parser` take into account Java keywords not reserved in C++, casting issues with `int64_t`, and `const` value types in basic containers   ,  Mark functions from c includes with noexception

This allows the Generator to avoid wrapping the function invocations with
try/catch when no exception is possible   ,   * Lengthen the `position`, `limit`, and `capacity` fields of `Pointer` using `long`   ,   * Add support for `enum` without enumerator list (issue #78)   ,  squid:S1854 - Dead stores should be removed   ,   * Allow `Parser` to generate `@Cast()` annotations and overloaded `put()` methods in basic containers too
 * Move list of basic containers and types to `Info.cppTypes` of the ""basic/containers"" and ""basic/types"" `InfoMap` entries, letting users change them at build time
 * Fix some `Parser` issues with `typedef` and forward declarations inside `class` definitions   ,   * Make `Parser` take namespace aliases into account, and fix a couple of preprocessing issues with `TokenIndexer`   ,   * Enhance basic support for containers of the style `std::vector<std::pair< ... > >` with user-friendly array-based setter methods
 * Fix `Generator` not passing function objects even when annotating `FunctionPointer` parameters with `@ByVal` or `@ByRef`
 * Map `bool*` to `boolean[]` tentatively in `Parser` since `sizeof(bool) == sizeof(jboolean)` on most platforms   ,   * Make `Parser` take `Info.skip` into account for `enum` declarations as well   ,  Update version in the `pom.xml` file to 1.2-SNAPSHOT

 * Enhance a bit the conversion from Doxygen-style documentation comments to Javadoc-style
 * Remove class check in allocators, which prevented peer classes from being extended in Java, instead relying on `super((Pointer)null)` in child peer classes   ,  squid:S2095 - Resources should be closed   ,  "
,,javacpp," * Enhance the `indexer` package with `long` indexing, initially via the `sun.misc.Unsafe`, for now   ,   * Add ""org.bytedeco.javacpp.maxbytes"" system property, forcing a call to `System.gc()` when this amount of memory tracked with deallocators is reached   ,  "
,,javacpp," * Add ""org.bytedeco.javacpp.maxbytes"" system property, forcing a call to `System.gc()` when this amount of memory tracked with deallocators is reached   ,  "
,,javacpp," * Prevent Android system libraries from getting copied or extracted   ,   * Fix `IndexerTest` potentially failing with `OutOfMemoryError` (issue bytedeco/javacpp-presets#234)
 * Preload libraries to work around some cases when they refuse to load once renamed (issue deeplearning4j/libnd4j#235)
 * Fix compilation error on some `linux-ppc64le` platforms (issue deeplearning4j/libnd4j#232)   ,  "
,Memory Management,javacpp," * Add ""org.bytedeco.javacpp.maxretries"" system property, the number times to call `System.gc()` before giving up (defaults to 10)   ,   * Deallocate native memory in a dedicated thread to reduce lock contention (issue #103)   ,  "
Exception Management,,javacpp," * Fix potential `ParserException` on comments found after annotations before function declarations   ,  "
,,javacpp,"Fix potentially failing tests   ,   * Add ""org.bytedeco.javacpp.maxretries"" system property, the number times to call `System.gc()` before giving up (defaults to 10)   ,  "
,,javacpp,"Release version 1.2.7

 * Fix `Loader` errors that could occur due to recent changes   ,  "
,,javacpp,"Update version in the `pom.xml` file to 1.3-SNAPSHOT

 * Print memory sizes in a human-readable format with `Pointer.formatBytes()`
 * Map standard `malloc()`, `calloc()`, `realloc()`, and `free()` functions (issue #136)   ,  Release version 1.3   ,  "
,,javacpp,"Update version in the `pom.xml` file to 1.3-SNAPSHOT

 * Print memory sizes in a human-readable format with `Pointer.formatBytes()`
 * Map standard `malloc()`, `calloc()`, `realloc()`, and `free()` functions (issue #136)   ,  "
,,javacpp," * Throw `OutOfMemoryError` on `allocateArray()` for `Pointer` of primitive types with `size > 0 && address == 0`   ,  "
,,javacpp,"* Fix `Loader` crashing on Android (issue bytedeco/javacv#412)   ,   * Add the ability the specify, after a `#` character, the output filename of libraries extracted by `Loader.load()`   ,  "
,,javacpp,"Release version 1.2   ,  "
,,javacpp," * Let users bundle arbitrary resources, have them extracted in cache, and used as `include` or `link` paths ( #43)   ,  "
,,javacpp," * Prevent race condition that could occur in `Loader.cacheResource()` ( #188)   ,   * Let users bundle arbitrary resources, have them extracted in cache, and used as `include` or `link` paths ( #43)   ,  "
,,javacpp," * Fix potential formatting issues with `OutOfMemoryError` thrown from `Pointer`   ,  "
,,javacpp," * Make the arbitrary resources available to process executed with `Builder.buildCommand` via the `BUILD_PATH` environment variable   ,   * Let users bundle arbitrary resources, have them extracted in cache, and used as `include` or `link` paths ( #43)   ,   * Provide `BytePointer` with value getters and setters for primitive types other than `byte` to facilitate unaligned memory accesses   ,  Use `BUILD_PATH_SEPARATOR` instead of `PATH_SEPARATOR`, which conflicts with MSYS2   ,   * Add a `BuildMojo.buildCommand` parameter that lets users execute arbitrary system commands easily with `ProcessBuilder`   ,  "
,,javacpp," * Fix potential compile errors with Android caused by superfluous `typedef` from `Generator` (issue #186)   ,   * Add `Builder.encoding` option to let users specify I/O character set name (issue bytedeco/javacpp-presets#195)   ,  Fix bugs in Parser

 * Fix `Parser` incorrectly recognizing values as pointers when `const` is placed after type (issue #173)
 * Add `Parser` support for C++11 `using` declarations that act as `typedef` (issue #169)
 * Let `Parser` accept variables initialized with parentheses (issue #179)
 * Fix `Parser` confusion between attributes and namespace-less templates (issue #181)   ,   * Call `malloc_trim(0)` after `System.gc()` on Linux to make sure memory gets released (issue bytedeco/javacpp-presets#423)   ,   * Provide `BytePointer` with value getters and setters for primitive types other than `byte` to facilitate unaligned memory accesses   ,  "
,,javacpp," * Prevent `Parser` from overwriting target classes when nothing was parsed   ,  Fix bugs in Parser

 * Fix `Parser` incorrectly recognizing values as pointers when `const` is placed after type (issue #173)
 * Add `Parser` support for C++11 `using` declarations that act as `typedef` (issue #169)
 * Let `Parser` accept variables initialized with parentheses (issue #179)
 * Fix `Parser` confusion between attributes and namespace-less templates (issue #181)   ,   * Add support for arrays of function pointers   ,   * Prevent `Parser` from outputting setters for `const` member pointers   ,   * Fix `Parser` not considering empty `class`, `struct`, or `union` declarations as opaque forward declarations   ,   * Use `Integer.decode()` instead of `parseInt()` on integer literals to support hexadecimal and octal numbers   ,   * Add `Builder.encoding` option to let users specify I/O character set name (issue bytedeco/javacpp-presets#195)   ,  "
,,javacpp," * Fix broken `outputDirectory` property and corresponding `-d` command line option (issue #153)
 * Add `Loader.extractResources()` and `cacheResources()` methods to extract or cache all resources with given name   ,  Update version in the `pom.xml` file to 1.3.1-SNAPSHOT

 * Fix potential issues with `Parser` repeating the `@ByPtrPtr` or `@ByPtrRef` annotations on parameters
 * To support Scala singleton objects better, consider as `static` methods from objects that are not `Pointer`
 * Allow `Loader.extractResource()` and `cacheResource()` to extract or cache all files from a directory in a JAR file
 * Create version-less symbolic links to libraries in cache on those platforms where it is useful to link easily
 * Use `java.io.tmpdir` as fallback in `Loader.getCacheDir()`, and throw a clear exception on failure   ,  "
,,javacpp,"Release version 1.3   ,  "
,,javacpp," * Fix broken `outputDirectory` property and corresponding `-d` command line option (issue #153)
 * Add `Loader.extractResources()` and `cacheResources()` methods to extract or cache all resources with given name   ,  "
,,javacpp," * Synchronize `Loader.loadLibrary()` to fix potential race condition ( #246)   ,  "
,,javacpp," * Make `Builder` accept multiple options for `platform.link.prefix` and `platform.link.suffix` ( #250)   ,  "
,,javacpp," * Fix `Parser` failing when a value of an `std::pair` basic container is also an `std::pair` (issue bytedeco/javacpp-presets#614)   ,   * Have `Parser` wrap the `insert()` and `erase()` methods of basic containers to allow modifying lists and sets   ,  "
,,javacpp," * Add `Loader.getLoadedLibraries()` method for debugging purposes and fix flaky `BuilderTest` (issue #245)
 * Call `PointerScope.attach()` as part of `Pointer.deallocator()`, instead of `init()`, to support custom deallocators as well
 * Prevent `Parser` from appending annotations to setter methods of variables to satisfy the `Generator`   ,  "
,,javacpp," * Add `platform.executable` and `platform.executablepath` properties to bundle executables and extract them with `Loader.load()`   ,  "
,,javacpp," * Enhance `UniquePtrAdapter` with the ability to move pointers out with the `&&` operator
 * Let `Parser` map constructors also for abstract classes with `Info.virtualize`   ,   * Fix `Parser` overlooking `Info` for constructors inside templates (issue #284)   ,   * Fix `Parser` taking the global package as the target package even when both are set   ,   * Fix `Parser` overlooking `Info` for constructors inside a namespace (issue #284)   ,   * Let `Parser` pick up `Info` explicitly for all constructors by considering their names as functions (issue #284)   ,  "
,,javacpp," * Catch more exceptions that can occur in `Loader` when caching resources ( #226)   ,  "
,,javacpp," * Work around in `Builder` the inability to pass empty arguments on Windows   ,   * Enhance `Loader.createLibraryLink()` by allowing to create symbolic links in other directories   ,   * Add ""org.bytedeco.javacpp.pathsfirst"" system property to let users search ""java.library.path"", etc before the class path   ,  "
Exception Management,Exception Management,javacpp," * Let `Generator` pick up `@NoException` annotations from super classes as well   ,   * Fix `SharedPtrAdapter` and `UniquePtrAdapter` failing to take ownership of temporary objects   ,  "
Data conversion,Data conversion,javacpp," * Fix `Parser` failing on `enum` declarations where the first line is a macro (issue #230)   ,  Fix various small issues with `Parser`   ,   * Allow `Parser` to map and cast function pointers to `Pointer`   ,   * Access elements of basic containers defined in `Parser` with `at()` instead of `operator[]` (issue #223)   ,   * Add `Info.enumerate` to let `Parser` map C++ enum classes to Java enum types (issue #108)   ,   * Make it possible to define read-only containers with `Parser` by prepending `const ` (issue #223)   ,  "
,,javacpp," * Add `Info.enumerate` to let `Parser` map C++ enum classes to Java enum types (issue #108)   ,  "
,,javacpp," * Add `Loader.loadGlobal()` to load symbols globally as often required by Python libraries (issue ContinuumIO/anaconda-issues#6401)   ,  "
,,javacpp," * Support multiple instances of `FunctionPointer` subclasses, up to the value in `@Allocator(max=...)` (issue bytedeco/javacpp-presets#683)   ,   * Expose all platform properties to process executed with `Builder.buildCommand` via environment variables, with names uppercase and all `.` replaced with `_`   ,  "
,,javacpp," * Avoid `Parser` writing `allocateArray()` when single `int`, `long`, `float`, or `double` constructor already exists (issue bytedeco/javacv#1224)   ,   * Support multiple instances of `FunctionPointer` subclasses, up to the value in `@Allocator(max=...)` (issue bytedeco/javacpp-presets#683)   ,  "
,,javacpp,"Add support for virtual functions to let C++ call back methods implemented in Java

 * Add `Info.virtualize` to have the `Parser` generate `@Virtual abstract` for pure virtual functions in the given classes
 * Add `@Virtual` annotation and update `Generator` to support callback by overriding such annotated `native` or `abstract` methods   ,  "
,,javasmt,"Adds an ability to specify a floating point rounding mode.

Either using an option, or explicitly.
Closes #63.   ,  "
,,javasmt,"Add more numbers to float-conversion
test, add new unit-test for float-conversion"
Data Conversion,Data conversion,javasmt,"Restructure handling of MathSAT model failures.

Without interpolation, no failures are expected,
thus always throw unchecked exceptions.
With interpolation, we catch all failures and mark them as expected
MathSAT failure by throwing SolverException because there are known
problems in MathSAT interpolation that lead to wrong answers ""SAT"".   ,  "
Exception Management,Exception Management,javasmt,"Simplify handling of MathSAT failures:
currently we throw an exception in a utility method and catch it
immediately afterwards (in multiple places) just to convert it into a
different exception.

Also make matching more strict (equality instead of contains),
and add two more failure messages for interpolation with arrays.   ,  "
Exception Management,"API Management, exception Management",javasmt,"Restructure handling of MathSAT model failures.

Without interpolation, no failures are expected,
thus always throw unchecked exceptions.
With interpolation, we catch all failures and mark them as expected
MathSAT failure by throwing SolverException because there are known
problems in MathSAT interpolation that lead to wrong answers ""SAT"".   ,  Simplify handling of MathSAT failures:
currently we throw an exception in a utility method and catch it
immediately afterwards (in multiple places) just to convert it into a
different exception.

Also make matching more strict (equality instead of contains),
and add two more failure messages for interpolation with arrays.   ,  Import order.

Checkstyle being this picky is quite annoying,
I am thinking about disabling this check altogether.   ,  Restructure JavaSMT package.

Taking opportunity before 1.0.0.

The root package is now ""java_smt"".
The solver bindings are in the package ""solvers"".
User-facing API is in the API package, apart from the
SolverConteFactory, which stands out as the JavaSMT main entry point.   ,  "
,,javasmt,"assert that ""modulo"" is always positive for modularCongruence.   ,  "
,,javasmt,"Move UfElimination to new package utils and add SolverUtils.

See comments on 1d2bae6.   ,  "
,,javasmt,"Update Princess.

This includes a bugfix for interpolation and the ability to specify a
random seed.   ,  Replace CustomLibraryPathClassLoader with the one from SoSy-Lab Common.   ,  Organize imports according to current Google Java Style Guide.

The new version of google-java-format will also do import reordering.
See #91 for detail.   ,  "
API Management,API Management,javasmt,"fix inheritence for backwards compatibility.   ,  API change: move method for allsat upwards.

Now also Interpolation- and OptimizationProver support AllSat.

The change is mostly backwards compatible,
because it is just an extension of the API.   ,  make ProverEnv and BasicProverEnv backwards compatible by using default methods.   ,  "
,,javasmt,"add stricter check against malformed tree-structure for tree-interpolation.

Until now, only SMTInterpol checked the tree before computing interpolants.
Now all itp-solvers use the same check.   ,  "
,,javasmt,"fix inheritence for backwards compatibility.   ,  make ProverEnv and BasicProverEnv backwards compatible by using default methods.   ,  "
,,javasmt,"Organize imports according to current Google Java Style Guide.

The new version of google-java-format will also do import reordering.
See #91 for detail.   ,  "
API Management,API Management,javasmt,"Fix a problem from last commit.

The API extractVariablesAndUFs is actually bad, because the values for
each UF in the returned map are arbitrary.   ,  Remove a source of non-determinism: instead of returning a HashMap, return an ImmutableMap   ,  Organize imports according to current Google Java Style Guide.

The new version of google-java-format will also do import reordering.
See #91 for detail.   ,  "
API Management,API Management,javasmt,"API change: move method for allsat upwards.

Now also Interpolation- and OptimizationProver support AllSat.

The change is mostly backwards compatible,
because it is just an extension of the API.   ,  "
,,javasmt,"add some documentation   ,  "
Exception Management,Exception Management,javasmt,"improve example: do not throw exception when some solver is not available, but at least execute the other solvers.

While SMTInterpol should always be available,
Z3 and MathSat are only available on Linux systems by default.   ,  "
,,javasmt,"add assertions to check presence of optional value.   ,  fix checkstyle warning.   ,  add example for optimization of weights of formulas.   ,  "
,,javasmt,"add assertions to check presence of optional value.   ,  add example for optimization, which shows the difference between ints and rationals.   ,  fix checkstyle warning.   ,  "
API Management,API Management,javasmt,"API change: move methods for unsat-core upwards.

Now also Interpolation- and OptimizationProver support unsat-core generation,
if the corresponding option is enabled.

This change is mostly backwards compatible,
because it is just an extension of the API.   ,  "
,,javasmt,"Update to error-prone 2.2.0

The biggest change is that it now forbids String.split.   ,  "
,,javasmt,"Update to Google Java-Format 1.4 and apply its new comment formatting   ,  "
,,javasmt,"Simplify options handling for Princess.   ,  Change how Princess is instantiated:
- Enable assertions to use them automatically when the JAR with
  assertions is on the class path.
- Expose feature for dumping queries as Scala code.   ,  "
Data Conversion,Data Conversion,javasmt,"Add more numbers to float-conversion test, add new unit-test for float-conversion"
,,javasmt,"Princess: simplify constructor   ,  "
,,javasmt,"fix visit of princess: in case of multiplication we had no chance of getting the factor.   ,  "
Feature migration,Feature migration,javasmt,"PrincessItpProver: replace recursive post-order-algorithm with GuavaTreeTraversal.   ,  Update dependencies

Remove API for old Truth versions because the necessary classes are no
longer present, but we need to upgrade Truth to a current version
because these are not backwards-compatible.   ,  "
,,javasmt,"Update Princess.

This includes a bugfix for interpolation and the ability to specify a
random seed.   ,  Simplify options handling for Princess.   ,  Princess: inlining method 'getVersion'   ,  Organize imports according to current Google Java Style Guide.

The new version of google-java-format will also do import reordering.
See #91 for detail.   ,  "
,,javasmt,"better output after assertion.   ,  "
,,javasmt,"Rename field to avoid hiding (error-prone will reject this in the future)   ,  Z3Model: improve array-evaluation for constant arrays.   ,  "
,,javasmt,"Update some libraries, mostly Truth.

Also migrate to Truth8's OptionalSubject.   ,  "
,,javasmt,"Expose methods for converting floats to IEEE bitvectors and vice versa.   ,  add method 'round' into FP-manager.

- add implementation for Z3 and Mathsat.
- add tests.   ,  "
,,javasmt,"ModelTest: more complex formulas with arrays.   ,  "
,,javasmt,"do not throw a generic exception if not needed.   ,  format-source   ,  "
Exception Management,Exception Management,javasmt,"do not throw a generic exception if not needed.   ,  adding test for Z3-based quantifier elimination.

The Junit-test will break until a bugfix is applied.   ,  "
,,javasmt,"Fixing warnings.   ,  "
,,javasmt,"Update SMTInterpol.

This version explicitly forbids variable names that are illegal
(such as containing ""|"").
Thus we adapt the tests for variable names accordingly.
Fixes #82.   ,  Update to Truth 0.34   ,  "
,,javasmt,"fix for last commit.

Forgotten negation in check and test.   ,  move constants representing invalid names from Formula into FormulaManager.

And provide a (currently unused) method in the API,
such that users can validate their names before using them.   ,  add modulo to forbidden symbols.   ,  "
,,javasmt,"InterpolatingProverEnvironment: allow Collections instead of Sets, and add methods for syntactic sugar.   ,  "
,,javasmt,"stricter check for invalid variable names.

SMTInterpol does not allow some symbol names that are
not SMT-LIB2-compliant (e.g., ""select"", ""and"", ""true"").
It also does not allow symbolnames containing ""|"" or plain ""\"".
Other solvers would even allow such symbol names.
To have a uniform API with exchangeable solvers,
we completely disallow such symbol names.

This change might break use cases in existing software,
where any possible String was used as symbol name.
To avoid problems, developers might want to escape
such special symbol names with a given prefix or escape-symbol.   ,  do no longer allow mathematical operators as symbol names.

We do no longer accept special names as identifiers for variables or UFs,
because they easily misguide the user.   ,  move constants representing invalid names from Formula into FormulaManager.

And provide a (currently unused) method in the API,
such that users can validate their names before using them.   ,  "
,,javasmt,"replace for-loop with while to avoid SpotBugs warning.

SpotBugs does not like modificatio  of control variable 'i'.   ,  add methods for escaping and unescaping keywords in Strings.

see #26 for discussion.   ,  next try to get 'isValidName' into our API.

This solution also uses CharMatcher as suggested by Philipp.
Visibility of constants is reduced, except for testing.
The two methods for checking valid names are moved together.   ,  "
,,javasmt,"introduce a new ProverOption for generating all satisfying assignments.

The option GENERATE_MODELS seems to cause overhead in some solvers,
i.e., Mathsat5 in CPAchecker (TerminationAnalysis).
As only Z3 uses model-generation for allSat-generation,
a separate option is better here.

This commit removes the need of GENERATE_MODELS for AllSat computation
and adds a new option GENERATE_ALL_SAT for this case.   ,  "
,,javasmt,"SMTInterpol fails early on unsupported names.

Improving check for UF-names.   ,  "
,,javasmt,"add example for solving allsat-queries.   ,  "
,,javasmt,"remove suppression of warning.

Our examples should be free of warnings and also free of ignored warnings.   ,  "
,,javasmt,"MathSat5: add checkc for literals for assumptions.

Otherwise we get wrong results.
Other solvers support more than literals, for example whole formulas.   ,  "
,,javasmt,"Move AbstractUFManager.declareUninterpretedFunctionImpl to FormulaCreator

This makes it easier to call the method from other classes in the future.
The AbstractUFManager sub-classes are now empty and could be deleted,
but I kept them for now if a reason to use them appears again.   ,  "
,,javasmt,"add new method to model API that allows the evaluation of formulas to formulas.

For further information see #137.   ,  MathsatModel: simplification, use existing method.   ,  "
,,javasmt,"Princess bit-vector support (work in progress) (#121)

* switched to native nonlinear integer solver

* show proper version; use more recent Princess

* replaced PrincessTermType with proper class Sort

* properly handle equations

* generalised mult handling

* initial support for bitvectors (some testcases still failing)

* more bit-vector support

* comment

* incorrect use of smod instead of srem

* coding style

* format-source

* more guidelines

* formatting

* assertion

* formatting

* updated version   ,  "
,,javasmt,"Unify model-evaluation via `AbstractModel.evalImpl` and `AbstractModel.evaluateImpl`.

We always use the method `evaluateImpl`,
which returns a simplified formula (or null).
Then we extract the simple-typed Java-object from the result.

fixes #143 .   ,  Princess: apply changes in Model-API of solver Princess.

Several internal model-value-wrapper-classes are replaced by nicer IExpressions.   ,  Model-API extension: ValueAssignment gets new fields for formulas.

We include the value-formula and an assignment-formula in the ValueAssignment.   ,  fix model-value for UF- and array-indizes.

JUnit-tests expect BigInteger instead of IIntLit/IExpression in the ValueAssignment.

... and formatting.   ,  Princess: improve model evaluation for bitvectors.   ,  remove curly brackets.

Formatter and CheckStyle have different opinions on simple brackets.
Lets avoid problems by removing brackets.   ,  Princess: possible bugfix to avoid invalid array-assignments.   ,  "
,,javasmt,"fix newline in log-output.   ,  simplify interpolation code for SMTInterpol.

SMTInterpol internally uses tree-interpolants for interpolation anyway.
No need to implement sequential interpolation on top of it again.   ,  SMTInterpol: improve logger for InterpolatingProver.   ,  "
,,javasmt,"SMTInterpolModel: move code for converting terms to primitive type into creator.

Lets keep the same structure as MathsatModel.   ,  "
,,javasmt,"Migrate most annotations like @Nullable away from javax.annotation (jsr305.jar)

This package creates problems with the module system of Java 9,
and Guava and other libraries have also started migrating away from it.
We now use it only for @ParametersAreNonnullByDefault, like Guava.   ,  SMTInterpolModel: move code for converting terms to primitive type into creator.

Lets keep the same structure as MathsatModel.   ,  apply updated commits from branch `smtinterpol_model_api`.

This is a compressed commit of several changes from that branch.
Some updates are already included, they would cause merge-conflicts otherwise.   ,  "
,,javasmt,"fix checkstyle warning.   ,  add support for more types of linear multiplication/division for SMTInterpol.

Previously, we only checked for direct numeric values when building multiplication terms.
Now, we also allow composed terms, e.g., ""1+2+3"". SMTInterpol can handle this.   ,  "
,,javasmt,"Z3: visibility.   ,  "
,,javasmt,"Migrate most annotations like @Nullable away from javax.annotation (jsr305.jar)

This package creates problems with the module system of Java 9,
and Guava and other libraries have also started migrating away from it.
We now use it only for @ParametersAreNonnullByDefault, like Guava.   ,  add new method to model API that allows the evaluation of formulas to formulas.

For further information see #137.   ,  make Model#close idempotent, i.e. allow to close it several times.   ,  "
,,javasmt,"adding some tests for BV-theory.   ,  "
,,javasmt,"improved JUnit test for variable names.

We also check whether we can get model values for escaped symbols.   ,  Revert ""update google libraries to latest versions.""

This reverts commit cf140dfd39faa6a4249886205cc5882e2f125844.

We sadly have to revert some tests to satisfy all dependencies.
GoogleTruth does not report a dependency on GoogleCommons,
but uses it internally causing problems with JUnit-tests.   ,  update google libraries to latest versions.

Updating tests to new Truth-API.   ,  "
,,javasmt,"FloatingPoint test: more tests on floats.

Mathsat seems to fail for some numbers.   ,  Add more numbers to float-conversion test   ,  add new unit-test for float-conversion.   ,  formatting.   ,  "
,,javasmt,"more tests on sequential interpolation.

remove unnecessary precondition for Z3 interpolation.   ,  simplify unit-test.

Less nested lists.
Less ignored warnings.   ,  more tests   ,  improve behaviour for sequential interpolation with zero or one partition of formulas.

ZERO: IllegalArgumentException.
ONE: empty list of interpolants.
N>1: (N-1) interpolants   ,  "
,,javasmt,"add tests for new model-evaluation.   ,  "
,,javasmt,"make Model#close idempotent, i.e. allow to close it several times.   ,  new test for setting ProverOption for generating models.   ,  "
,,javasmt,"Revert ""update google libraries to latest versions.""

This reverts commit cf140dfd39faa6a4249886205cc5882e2f125844.

We sadly have to revert some tests to satisfy all dependencies.
GoogleTruth does not report a dependency on GoogleCommons,
but uses it internally causing problems with JUnit-tests.   ,  update google libraries to latest versions.

Updating tests to new Truth-API.   ,  "
,,javasmt,"fix UtilityClass for wrapping a solver with assumption-solving-support.

Adding JUnit-test.

MathSat5 seems to fail the test.
It does not use the changed class.
This might be a bug in MathSat5.   ,  MathSat5: add checkc for literals for assumptions.

Otherwise we get wrong results.
Other solvers support more than literals, for example whole formulas.   ,  "
,,javasmt,"annotate unused return value to satisfy SpotBugs and Junit :-)   ,  make Prover#close idempotent, i.e. allow to close it several times.   ,  SMTInterpol should always throw an exception when creating a second environment and first is not empty.

We cannot avoid all use cases without tracking all instances.
We try to at least avoid creating multiple environments when we see a non-empty stack.

We might get strange errors like ""(pop 1) not possible due to empty stack""
when using this solver with multiple stacks. Be aware of them!

Adding junit test.   ,  "
,,javasmt,"update Princess to latest version 2.12 from 2018-07-30.

and add test for fixed bug.   ,  "
,,javasmt,"stricter check for invalid variable names.

SMTInterpol does not allow some symbol names that are
not SMT-LIB2-compliant (e.g., ""select"", ""and"", ""true"").
It also does not allow symbolnames containing ""|"" or plain ""\"".
Other solvers would even allow such symbol names.
To have a uniform API with exchangeable solvers,
we completely disallow such symbol names.

This change might break use cases in existing software,
where any possible String was used as symbol name.
To avoid problems, developers might want to escape
such special symbol names with a given prefix or escape-symbol.   ,  improve test for invalid names.   ,  fix for last commit.

Forgotten negation in check and test.   ,  improve tests for symbol names.

Current status:
- all solvers allow Strings as names and use them ""AS IS"".
- the toString-methods of some solvers are broken,
i.e., they quote if needed, but do not double quote already quoted Strings.
- some solvers do not allow special names,
i.e., SMTInterpol does not like ""|"" or ""\\"" due to internal logging.   ,  move constants representing invalid names from Formula into FormulaManager.

And provide a (currently unused) method in the API,
such that users can validate their names before using them.   ,  add JUnit-test for equal behavior of 'checkVariableName' and 'isValidName'.   ,  "
,,javasmt,"fix more functions for SMTInterpol.   ,  "
,,javasmt,"adding example for solving a Sudoku puzzle with an SMT solver.

The example is roughly based on the implementation of Daniel B.,
but might be easier to understand in some cases.
It also ignores several problematic parts, such as
exceptions (simply thrown, we cannot handle them here anyway)
or invalid input (invalid chars are simply ignored).   ,  "
,,javasmt,"API: add new method to convert formulas from BV to INT and vice versa.   ,  "
API Usage,API Usage,Conscrypt,"Use public API to get session ID. (#348)   , "
,,Conscrypt," Implement unadorned ChaCha20. (#367)

ChaCha20 is a stream cipher in its own right, and it was pointed
out that it was weird that ing ""ChaCha20"" alone from Conscrypt
returned an implementation of ChaCha20+Poly1305.  This implements
plain ChaCha20 and makes it the default implementation, so to access
ChaCha20+Poly1305 the caller must ask for ChaCha20/Poly1305/NoPadding
explicitly.

We haven't made a release with ChaCha20 in it yet, so it should be fine
to change the meaning of ing ""ChaCha20"".   , "
,,Conscrypt," Implement ChaCha20 support. (#356)

Adds Cipher, KeyGenerator, and AlgorithmParameters for ChaCha20.

Main changes are in OpenSSLCipher$EVP_AEAD.  Refactored so the GCM-
specific portions are all in the GCM subclass and the generic AEAD
portions (such as not allowing reuse of key/IV combinations) are in
the superclass.

Also updates so that calling Cipher.init() on an instance of
AES/GCM/NoPadding with a GCM AlgorithmParameters will pick up the
values from the AlgorithmParameters (previously, it would only work
with GCMParameterSpec or something convertable to IvParameterSpec).   ,  "
,,Conscrypt,"Implement ChaCha20 support. (#356)

Adds Cipher, KeyGenerator, and AlgorithmParameters for ChaCha20.

Main changes are in OpenSSLCipher$EVP_AEAD.  Refactored so the GCM-
specific portions are all in the GCM subclass and the generic AEAD
portions (such as not allowing reuse of key/IV combinations) are in
the superclass.

Also updates so that calling Cipher.init() on an instance of
AES/GCM/NoPadding with a GCM AlgorithmParameters will pick up the
values from the AlgorithmParameters (previously, it would only work
with GCMParameterSpec or something convertable to IvParameterSpec).   ,"
,,Conscrypt,"  Cleaning up various warnings. (#365)   , "
,,Conscrypt," Allow modes (ECB and NONE) and padding (NOPADDING) for ARC4. (#361)

This allows callers to  ARC4/NONE/NOPADDING as an equivalent
name to ing just ARC4.  Both NONE and ECB are supported as
modes for RSA and for other providers' implementations of stream
ciphers like ARC4 that don't use modes, so we accept both here as
well.   ,  "
,,Conscrypt,"Implement unadorned ChaCha20. (#367)

ChaCha20 is a stream cipher in its own right, and it was pointed
out that it was weird that ing ""ChaCha20"" alone from Conscrypt
returned an implementation of ChaCha20+Poly1305.  This implements
plain ChaCha20 and makes it the default implementation, so to access
ChaCha20+Poly1305 the caller must ask for ChaCha20/Poly1305/NoPadding
explicitly.

We haven't made a release with ChaCha20 in it yet, so it should be fine
to change the meaning of ing ""ChaCha20"".   ,  "
,,Conscrypt,"Implement ChaCha20 support. (#356)

Adds Cipher, KeyGenerator, and AlgorithmParameters for ChaCha20.

Main changes are in OpenSSLCipher$EVP_AEAD.  Refactored so the GCM-
specific portions are all in the GCM subclass and the generic AEAD
portions (such as not allowing reuse of key/IV combinations) are in
the superclass.

Also updates so that calling Cipher.init() on an instance of
AES/GCM/NoPadding with a GCM AlgorithmParameters will pick up the
values from the AlgorithmParameters (previously, it would only work
with GCMParameterSpec or something convertable to IvParameterSpec).   ,  "
,,Conscrypt,"Specify all TLS versions 1.0-1.2. (#360)

If you specify a set of TLS versions with a gap in them, all versions
above the gap are ignored, because the TLS protocol actually only
includes a single number (the highest protocol version that's allowable).
So the current version was specifying TLS 1.0 only.  This fixes it to use
whatever version is the highest available.   ,  "
Network Management,Network Management,Conscrypt,"Mark unused parameters in native_crypto.cc (#412)

Our Android build rules generate errors for unused parameters.  We
can't enable the warnings in the external build rules because
BoringSSL has many unused parameters and we build the two together in
the external build.   ,"
,,Conscrypt,"  Pass NativeSsl references to NativeCrypto (#408)

* Pass NativeSsl references to NativeCrypto

The existing implementation of passing raw addresses to NativeCrypto
can cause issues where the native code may still be executing when the
finalizer runs and frees the underlying native resources.  A call to
NativeSsl.read(), for instance, is not enough to keep the NativeSsl or
its owning socket alive, so if it's waiting for input the finalizer
can run.  Switching to passing the Java object to native code keeps
the Java object alive for GC purposes, preventing its finalizer from
running.

As part of this, also move the freeing of NativeSsl instances into a
finalizer on NativeSsl instead of on the sockets.  The sockets can
still become garbage even if the NativeSsl is kept alive, so we only
want to free it when the NativeSsl itself is garbage.

We will also want to do this for other native objects, but SSL*
instances are by far the most-used native objects and the most likely
to be used in a long-running I/O operation, so starting here gives us
a lot of benefit.

* Reliably close objects in tests.

* Pass both pointer and Java reference.

This allows us to access the SSL* pointer without having to indirect
through the Java object's fields, but still prevents the NativeSsl
from being GCed while the method is being run.

* Explain unsafe finalization fix in NativeCrypto Javadoc.   ,"
,,Conscrypt,"  More finalization safety. (#410)

This updates OpenSSLX509Certificate and OpenSSLX509CRL in the same way
as NativeSsl was done previously.   , "
,,Conscrypt," Fix error detection in RSA_generate_key_ex. (#398)

RSA_generate_key_ex returns 1 on success and 0 on failure, so we could
never detect failures that happened.  Also update an allocation
failure to throw OutOfMemoryError instead of RuntimeException.   ,  "
API Usage,API Usage,Conscrypt,"Move session value API into ProvidedSessionDecorator. (#389)

* Move session value API into ProvidedSessionDecorator.

The application-level value API on SSLSession objects makes them
mutable, which means that using singleton objects (like we do with
SSLNullSession) or swapping out implementations (like we do with
ActiveSession/SessionSnapshot) doesn't work properly if you actually
want to use this API.  By moving it into ProvidedSessionDecorator, it
can be used without any of these problems.

* Rename ProvidedSessionDecorator to ExternalSession.   ,  "
Network Management,Network Management,Conscrypt,"Pass NativeSsl references to NativeCrypto (#408)

* Pass NativeSsl references to NativeCrypto

The existing implementation of passing raw addresses to NativeCrypto
can cause issues where the native code may still be executing when the
finalizer runs and frees the underlying native resources.  A call to
NativeSsl.read(), for instance, is not enough to keep the NativeSsl or
its owning socket alive, so if it's waiting for input the finalizer
can run.  Switching to passing the Java object to native code keeps
the Java object alive for GC purposes, preventing its finalizer from
running.

As part of this, also move the freeing of NativeSsl instances into a
finalizer on NativeSsl instead of on the sockets.  The sockets can
still become garbage even if the NativeSsl is kept alive, so we only
want to free it when the NativeSsl itself is garbage.

We will also want to do this for other native objects, but SSL*
instances are by far the most-used native objects and the most likely
to be used in a long-running I/O operation, so starting here gives us
a lot of benefit.

* Reliably close objects in tests.

* Pass both pointer and Java reference.

This allows us to access the SSL* pointer without having to indirect
through the Java object's fields, but still prevents the NativeSsl
from being GCed while the method is being run.

* Explain unsafe finalization fix in NativeCrypto Javadoc.   ,  "
,,Conscrypt,"More finalization safety. (#410)

This updates OpenSSLX509Certificate and OpenSSLX509CRL in the same way
as NativeSsl was done previously.   ,  "
,,Conscrypt,"Update testing infrastructure. (#393)

This is part one of migrating the source of truth for tests from
Conscrypt to AOSP.  This sets up the basic layout we want to use:
tests under org.conscrypt.** that test the installed provider (or all
installed providers, for future crypto-related tests), and a separate
test suite class used by the Gradle build that installs the provider.
That way these tests can easily be used in any environment that can
properly install the provider to be tested, including AOSP.

This also updates the test classes with a few changes that had been
made to AOSP, mostly additional test cases.   ,  "
,,Conscrypt,"Update more testing infrastructure. (#395)

* Update testing build rules.
* Move TestKeyStore to org.conscrypt.
* Fix up a couple tests so they work in AOSP.   ,  "
Network Management,Network Management,Conscrypt,"Exposing SSL_max_seal_overhead (#135)

Also adding a method to calculate the maximum buffer size required for a wrap operation.   , "
,,Conscrypt," Configure OCSP and SCTs on the SSL, not SSL_CTX.

As Conscrypt is currently set up, one SSL_CTX (owned, ultimately, by the
SSLContext) may correspond to multiple SSLParameters which, in the Java
API, are configured on the SSLSocket or SSLEngine directly. Thus we
should use the SSL versions of the APIs which now exist. This avoids
mutating an SSL_CTX which may be shared by multiple SSLs with different
configurations.

Change-Id: I19485c316087004c6050d85520b0169f2ca0d493   ,  "
Network Management,Network Management,Conscrypt,"Configure OCSP and SCTs on the SSL, not SSL_CTX.

As Conscrypt is currently set up, one SSL_CTX (owned, ultimately, by the
SSLContext) may correspond to multiple SSLParameters which, in the Java
API, are configured on the SSLSocket or SSLEngine directly. Thus we
should use the SSL versions of the APIs which now exist. This avoids
mutating an SSL_CTX which may be shared by multiple SSLs with different
configurations.

Change-Id: I19485c316087004c6050d85520b0169f2ca0d493   ,  "
,,Conscrypt,"Refactoring session management (#172)

This change breaks session management into two distinct types:

- SslSessionWrapper: These are created as BoringSSL calls back the new session handler, allowing the application to cache sessions. Clients will also offer these to BoringSSL for reuse if a compatible session was found. BoringSSL is free to use it or not, but the Conscrypt code no longer makes assumptions here. Instead, it always uses the ActiveSession.

- ActiveSession: This is a session that wraps the SSL instance (rather than the SSL_SESSION wherever possible). That way no concern has to be paid to what BoringSSL is doing with sessions under the covers.

Fixes #98   ,  "
Network Management,Network Management,Conscrypt,"Use buffer allocator in engine (#236)

Also modifying the engine so that when running with no allocator
(i.e. default), that it lazy-creates a direct buffer for JNI operations.
This is a trade-off of performance for memory. It requires the
allocation of a single direct buffer (~16k) that it uses to copy
from/to heap buffers provided by the application.

Fixes #226

Benchmarks show a huge improvement. The engine socket now blows away
old file-based socket. Also the engine, itself, is performing on par with Netty.

Client Socket:


Server socket:


Engine:
```

```   ,  Refactoring benchmarks. (#233)

Adding a set of base benchmarks that are framework agnostic. This
will allow us to extend them to support caliper or JMH. We don't
want to mix/match caliper and JMH in the same code because Android
can now support caliper, but not JMH.   ,  "
,,Conscrypt,"Expanding benchmarks. (#239)

Including benchmarks for:

- handshake
- Netty's OPENSSL_REFCNT provider

Partial fix for: #156

Handshake benchmarks show conscrypt may have some work to do, but
it's not clear yet how much of the cost is creation of the engines.

```
Benchmark                                                  (a_cipher)  (b_buffer)          (c_engine)   Mode  Cnt     Score     Error  Units

```   ,  "
,,Conscrypt,"Refactoring benchmarks. (#233)

Adding a set of base benchmarks that are framework agnostic. This
will allow us to extend them to support caliper or JMH. We don't
want to mix/match caliper and JMH in the same code because Android
can now support caliper, but not JMH.   ,  "
,,Conscrypt,"Lint C++ code and fix errors (#270)

Using cpplint, if available on the platform. (e.g. pip install cpplint)   ,  "
,,Conscrypt,"Fix native build on osx (#274)   ,  Lint C++ code and fix errors (#270)

Using cpplint, if available on the platform. (e.g. pip install cpplint)   ,  "
,,Conscrypt,"Refactoring session management (#172)

This change breaks session management into two distinct types:

- SslSessionWrapper: These are created as BoringSSL calls back the new session handler, allowing the application to cache sessions. Clients will also offer these to BoringSSL for reuse if a compatible session was found. BoringSSL is free to use it or not, but the Conscrypt code no longer makes assumptions here. Instead, it always uses the ActiveSession.

- ActiveSession: This is a session that wraps the SSL instance (rather than the SSL_SESSION wherever possible). That way no concern has to be paid to what BoringSSL is doing with sessions under the covers.

Fixes #98   ,  Avoid reprocessing the local certificates. (#250)

We were needlessly querying the SSL for the local certificates
during the handshake. Avoiding this shows a minor bump in
handshake performance

See #247   ,  "
API Usage,API Usage,Conscrypt,"Introducing buffer allocation API. (#235)

This has been borrowed from some experimental work in protobuf and
was heavily influenced by the Netty buffer API.

This will be needed to improve the performance of the Conscrypt
engine to more closely align with Netty when using heap-based buffers.   ,  "
,,Conscrypt,"Add availability checks (#216)

Fixes #211   ,  "
Network Management,Network Management,Conscrypt,"Rename onHandshakeCompleted.

This makes it a bit clearer when it should be called.   , "
,,Conscrypt," Refactoring session management (#172)

This change breaks session management into two distinct types:

- SslSessionWrapper: These are created as BoringSSL calls back the new session handler, allowing the application to cache sessions. Clients will also offer these to BoringSSL for reuse if a compatible session was found. BoringSSL is free to use it or not, but the Conscrypt code no longer makes assumptions here. Instead, it always uses the ActiveSession.

- ActiveSession: This is a session that wraps the SSL instance (rather than the SSL_SESSION wherever possible). That way no concern has to be paid to what BoringSSL is doing with sessions under the covers.

Fixes #98   , "
,,Conscrypt," Always use state in ConscryptEngine.is{In,Out}boundClosed() (#242)   , "
,,Conscrypt," Fix inconsistency with engine socket (#238)

The SSLSocketTest (an android integ test) fails with the
engine socket due to a minor inconsistency in the behavior
of the handshakeSession method.   , "
,,Conscrypt," Use buffer allocator in engine (#236)

Also modifying the engine so that when running with no allocator
(i.e. default), that it lazy-creates a direct buffer for JNI operations.
This is a trade-off of performance for memory. It requires the
allocation of a single direct buffer (~16k) that it uses to copy
from/to heap buffers provided by the application.

Fixes #226

Benchmarks show a huge improvement. The engine socket now blows away
old file-based socket. Also the engine, itself, is performing on par with Netty.

Client Socket:
```
Benchmark                                (channelType)  (messageSize)      (socketType)           Score           Error  Units


Server socket:
```
Benchmark                                (channelType)  (messageSize)      (socketType)           Score           Error  Units

```

Engine:
```
Benchmark                                                      (cipher)  (messageSize)            (sslProvider)           Score           Error  Units

```   ,  "
,,Conscrypt,"Rename onHandshakeCompleted.

This makes it a bit clearer when it should be called.   ,  "
,,Conscrypt,"Refactoring session management (#172)

This change breaks session management into two distinct types:

- SslSessionWrapper: These are created as BoringSSL calls back the new session handler, allowing the application to cache sessions. Clients will also offer these to BoringSSL for reuse if a compatible session was found. BoringSSL is free to use it or not, but the Conscrypt code no longer makes assumptions here. Instead, it always uses the ActiveSession.

- ActiveSession: This is a session that wraps the SSL instance (rather than the SSL_SESSION wherever possible). That way no concern has to be paid to what BoringSSL is doing with sessions under the covers.

Fixes #98   ,  Encode certs in verify callback (#248)

This code was copied from Netty/netty-tcnative and seems to
significantly increases performance of the verify callback. Before
we call back to Java, we first encode all of the certs and then
decode them in Java into X509Certificate instances. Previous code
was calling into JNI for each method in the certificate.

This helps in addressing #247.   ,"
,,Conscrypt,"  Avoid reprocessing the local certificates. (#250)

We were needlessly querying the SSL for the local certificates
during the handshake. Avoiding this shows a minor bump in
handshake performance

See #247   ,  "
Network Management,Network Management,Conscrypt,"Remove Java <-> OpenSSL name mapping. (#227)

As of [0], BoringSSL supports the standard cipher suite names. The Java
names are the same, with the exception of
TLS_RSA_WITH_3DES_EDE_CBC_SHA/SSL_RSA_WITH_3DES_EDE_CBC_SHA for
historical reasons. Add code to map between that exception but otherwise
rely on the native support.
  ,"
,,Conscrypt,"  Refactoring session management (#172)

This change breaks session management into two distinct types:

- SslSessionWrapper: These are created as BoringSSL calls back the new session handler, allowing the application to cache sessions. Clients will also offer these to BoringSSL for reuse if a compatible session was found. BoringSSL is free to use it or not, but the Conscrypt code no longer makes assumptions here. Instead, it always uses the ActiveSession.

- ActiveSession: This is a session that wraps the SSL instance (rather than the SSL_SESSION wherever possible). That way no concern has to be paid to what BoringSSL is doing with sessions under the covers.

Fixes #98   , "
,,Conscrypt," Add availability checks (#216)

Fixes #211   , "
,,Conscrypt," Some parsing and serializing fixes. (#219)

This fixes a memory leak in NativeCrypto_i2d_PKCS7. It never frees
derBytes. Also removing a dependency on the legacy ASN.1 stack.   ,"
,,Conscrypt,"  Switch to SSL_get0_peer_certificates.

This works towards issue #258. So the exception can be routed out
properly, this moves the SSL_get0_peer_certificates call to after
doHandshake completes in ConscryptFileDescriptorSocket.

Although, due to False Start (the ""cut-through"" logic in that class),
the handshake may not be fully complete at the time, BoringSSL's API is
such that the certificates and other properties will be available once
SSL_do_handshake first completes.   , "
,,Conscrypt," Cleaning up JNI exceptions (#252)

There were a bunch of exceptions that are being thrown from JNI methods that aren't currently declared.

Also removed a few unused JNI methods and duplicate constants, preferring those from NativeConstants.   ,"
,,Conscrypt,"  Pass encoded local certs to BoringSSL (#253)

Current code was encoding and then decoding the certs before finally
passing them to the native code. We were also separately setting
and then verifying the private key.  All of this can be replaced
with a single JNI call SSL_set_chain_and_key, which accepts the
encoded certs (we don't have to decode them again).

See #247

This shows a perf bump for the handshake (from ~750 to 800 ops/sec).   , "
,,Conscrypt," Implement AlgorithmParameters.GCM in Conscrypt. (#217)

* Implement AlgorithmParameters.GCM in Conscrypt.

In order to handle the ASN.1 encoding, exposes a subset of the ASN.1
encoding API from BoringSSL in NativeCrypto.

* Rename {write,read}_integer to {write,read}_uint64.

Add a UniquePtr to ensure exceptions don't cause a memory leak.   ,  "
Network Management,Network Management,Conscrypt,"Implement AlgorithmParameters.OAEP in Conscrypt. (#240)

* Implement AlgorithmParameters.OAEP in Conscrypt.

* Use JNI_TRUE and JNI_FALSE as return values, and clarify some exception messages.   ,  "
,,Conscrypt,"Fix a bunch of warnings. (#254)

Removes unused variables, fixes cases of non-final ALL_CAPS_VARS, removes
wildcard import.   ,  "
,,Conscrypt,"Some parsing and serializing fixes. (#219)

This fixes a memory leak in NativeCrypto_i2d_PKCS7. It never frees
derBytes. Also removing a dependency on the legacy ASN.1 stack.   ,  "
,,Conscrypt,"Refactoring session management (#172)

This change breaks session management into two distinct types:

- SslSessionWrapper: These are created as BoringSSL calls back the new session handler, allowing the application to cache sessions. Clients will also offer these to BoringSSL for reuse if a compatible session was found. BoringSSL is free to use it or not, but the Conscrypt code no longer makes assumptions here. Instead, it always uses the ActiveSession.

- ActiveSession: This is a session that wraps the SSL instance (rather than the SSL_SESSION wherever possible). That way no concern has to be paid to what BoringSSL is doing with sessions under the covers.

Fixes #98   , "
,,Conscrypt," Fix a bunch of warnings. (#254)

Removes unused variables, fixes cases of non-final ALL_CAPS_VARS, removes
wildcard import.   ,  "
,,Conscrypt," Encode certs in verify callback (#248)

This code was copied from Netty/netty-tcnative and seems to
significantly increases performance of the verify callback. Before
we call back to Java, we first encode all of the certs and then
decode them in Java into X509Certificate instances. Previous code
was calling into JNI for each method in the certificate.

This helps in addressing #247.   ,  "
Network Management,Network Management,Conscrypt,"Adding bechmark for ALPN (#245)

Fixes #156

The performance seems to be little different than that of a non-ALPN
handshake.

 ,  "
,,Conscrypt," Expanding benchmarks. (#239)

Including benchmarks for:

- handshake
- Netty's OPENSSL_REFCNT provider

Partial fix for: #156

Handshake benchmarks show conscrypt may have some work to do, but
it's not clear yet how much of the cost is creation of the engines.
  ,  Use buffer allocator in engine (#236)

Also modifying the engine so that when running with no allocator
(i.e. default), that it lazy-creates a direct buffer for JNI operations.
This is a trade-off of performance for memory. It requires the
allocation of a single direct buffer (~16k) that it uses to copy
from/to heap buffers provided by the application.

Fixes #226

Benchmarks show a huge improvement. The engine socket now blows away
old file-based socket. Also the engine, itself, is performing on par with Netty.

Client Socket:

Server socket

Engine:
```
```   ,  "
Network Management,Network Management,Conscrypt,"Adding test for wrapping SSL socket. (#243)   ,  "
Network Management,Network Management,Conscrypt,"Pass encoded local certs to BoringSSL (#253)

Current code was encoding and then decoding the certs before finally
passing them to the native code. We were also separately setting
and then verifying the private key.  All of this can be replaced
with a single JNI call SSL_set_chain_and_key, which accepts the
encoded certs (we don't have to decode them again).

See #247

This shows a perf bump for the handshake (from ~750 to 800 ops/sec).   , "
,,Conscrypt," Switch to SSL_get0_peer_certificates.

This works towards issue #258. So the exception can be routed out
properly, this moves the SSL_get0_peer_certificates call to after
doHandshake completes in ConscryptFileDescriptorSocket.

Although, due to False Start (the ""cut-through"" logic in that class),
the handshake may not be fully complete at the time, BoringSSL's API is
such that the certificates and other properties will be available once
SSL_do_handshake first completes.   ,  Remove Java <-> OpenSSL name mapping. (#227)

As of [0], BoringSSL supports the standard cipher suite names. The Java
names are the same, with the exception of
TLS_RSA_WITH_3DES_EDE_CBC_SHA/SSL_RSA_WITH_3DES_EDE_CBC_SHA for
historical reasons. Add code to map between that exception but otherwise
rely on the native support.
   , "
,,Conscrypt," Cleaning up JNI exceptions (#252)

There were a bunch of exceptions that are being thrown from JNI methods that aren't currently declared.

Also removed a few unused JNI methods and duplicate constants, preferring those from NativeConstants.   ,  "
,,Conscrypt,"Refactoring benchmarks. (#233)

Adding a set of base benchmarks that are framework agnostic. This
will allow us to extend them to support caliper or JMH. We don't
want to mix/match caliper and JMH in the same code because Android
can now support caliper, but not JMH.   , "
,,Conscrypt," Expanding benchmarks. (#239)

Including benchmarks for:

- handshake
- Netty's OPENSSL_REFCNT provider

Partial fix for: #156

Handshake benchmarks show conscrypt may have some work to do, but
it's not clear yet how much of the cost is creation of the engines.

```
Benchmark                                                  (a_cipher)  (b_buffer)          (c_engine)   Mode  Cnt     Score     Error  Units

```   ,  "
,,Conscrypt,"Add logging macros that work on all platforms. (#462)

This adds CONSCRYPT_LOG_X macros that redirect to either ALOG on
Android or fprintf(stderr) on non-Android.  In the future, we could
use these to allow users to register a logging callback and send the
logs to a destination of their choice (via java.util.Logger or log4j
or what have you), but for now we'll keep it simple.

Fixes #460.   ,  "
Network Management,Network Management,Conscrypt,"Add support for token binding and EKM. (#445)

Token binding allows higher-level protocols to bind their higher-level
authentication tokens to their TLS sessions, making it more difficult
for attackers to present those higher-level tokens in a future
session.  At the TLS level, all that needs to occur is a parameter
negotiation, the rest of the token binding protocol is left up to the
caller.

Parameter options are specified as integers, and I decided not to
supply constants for the currently-defined values.  This is a niche
enough use case that any user of it should be able to decide what
values they want to support (and will want to share constants with
whatever higher-level protocol they're using with token binding).
BoringSSL also doesn't supply constants for these values, so we're in
good company there.

Keying material exporting (aka EKM, for exported keying material) is
specified in RFC 5705, and is necessary for implementing token binding
as well as other protocols.   , "
,,Conscrypt," Add logging macros that work on all platforms. (#462)

This adds CONSCRYPT_LOG_X macros that redirect to either ALOG on
Android or fprintf(stderr) on non-Android.  In the future, we could
use these to allow users to register a logging callback and send the
logs to a destination of their choice (via java.util.Logger or log4j
or what have you), but for now we'll keep it simple.

Fixes #460.   , "
,,Conscrypt," Throw SocketException on ERR_SSL_SYSCALL. (#430)

SSLSocketTest#test_SSLSocket_interrupt_readWrapperAndCloseUnderlying
is failing periodically on our internal continuous builds, and it
appears to be happening due to a race condition.

The test is testing what happens when an SSLSocket that's wrapping an
underlying Socket is blocked on a read and then underlying socket is
closed by another thread.  There appears to be a race condition
between the OS waking up the reading thread and the write of -1 to
java.io.FileDescriptor's private field.  If the reading thread wakes
up and proceeds past the check of the file descriptor's validity
before the field write is visible, then it will attempt to call
SSL_read() and get ERR_SSL_SYSCALL, and it responds by returning -1,
whereas the test expects SocketException to be thrown (which it does
if the file descriptor is invalid).

This changes the code to always throw SocketException when
ERR_SSL_SYSCALL is reported with a return value of 0, which the
BoringSSL docs say happens ""if the transport returned EOF"", which
should mean the file descriptor is closed.   ,  "
,,Conscrypt,"Use printf when tracing on non-Android platforms. (#437)

If the ALOG macro isn't defined, we define it in macros.h to just do
nothing, which means that enabling tracing on OpenJDK or other
platforms doesn't actually result in any tracing output.  On
non-Android platforms, we can make the tracing macros use printf()
instead of ALOG.   ,  "
Network Management,Network Management,Conscrypt,"Disallow invalid SNI hostnames in setHostname(). (#470)

The code that sets the SNI value on the connection checks for

impl.getUseSni() && AddressUtils.isValidSniHostname(hostname)

which is good for hostnames that were supplied as the hostname to
connect to, since they may or may not be valid for SNI, but means that
if you set an invalid hostname with setHostname() it will just
silently be omitted from the connection and no SNI extension will be
included in the handshake.  Better to reject the hostname immediately.

Also disallow hostnames with trailing dots, which aren't legal SNI
hostnames per RFC 6066.

Also disallow null bytes.   , "
,,Conscrypt," Allow localhost as an SNI hostname. (#475)

As an exception to the normal rule that you can't have an SNI hostname
without dots, localhost is a fine hostname for SNI.   ,  "
,,Conscrypt,"Disallow invalid SNI hostnames in setHostname(). (#470)

The code that sets the SNI value on the connection checks for

impl.getUseSni() && AddressUtils.isValidSniHostname(hostname)

which is good for hostnames that were supplied as the hostname to
connect to, since they may or may not be valid for SNI, but means that
if you set an invalid hostname with setHostname() it will just
silently be omitted from the connection and no SNI extension will be
included in the handshake.  Better to reject the hostname immediately.

Also disallow hostnames with trailing dots, which aren't legal SNI
hostnames per RFC 6066.

Also disallow null bytes.   ,  "
,,Conscrypt,"Calculate output size ourselves for AES/GCM and ChaCha20. (#438)

Since both AES/GCM and ChaCha20 don't have any variance in the length
of their AEAD tags, we can just calculate the output length directly
rather than making a JNI call.  This gives better bounds for the
necessary buffer size; in particular, we previously weren't factoring
in the removal of the tag during decryption, so we always demanded an
unnecessarily-long buffer.   ,  Update short buffer handling. (#440)

This fixes a number of problems in Conscrypt's ciphers when they are
given an output buffer that is too small for the output.

We didn't specifically handle CIPHER_R_BUFFER_TOO_SMALL errors from
BoringSSL, so we threw RuntimeException on encountering them.  We
should be throwing ShortBufferException.

Raw ChaCha20 didn't check for a short buffer at all, so it just passed
the arrays down to native code, which could cause crashes or other
weird behavior.

EVP_AEAD ciphers used update() to only record data that needed to be
encrypted and then did the actual encrypting in doFinal().  This
combined with our implementation that implements doFinal() as a
combination of updateInternal() + doFinalInternal() led to a situation
where if the buffer passed to doFinal() was too small, the data would
get added to the buffer to be encrypted in updateInternal() and then
doFinalInternal() call would fail, which would mean a future call to
doFinal() with the same data would end up encrypting that data twice.
This call pattern is used by some internal CipherSpi methods from
OpenJDK, so you could see it in practice even if the caller did
nothing wrong.

Also add tests around short buffer handling.   ,  "
,,Conscrypt,"Add tests for classes in java.security. (#434)

These tests are ed from AOSP with minor modifications.  They test
the behavior of the CertificateFactory, MessageDigest, and Signature
classes, as well as a bunch of varieties of
AlgorithmParameterGenerator, AlgorithmParameters, KeyFactory, and
KeyPairGenerator classes for all installed providers (which includes
Conscrypt when run under the ConscryptSuite).

As usual, I've adapted them to JUnit4, removed the tests for the
classes themselves and the provider framework, and made them work
under OpenJDK as well as Android.

The quality of these tests is again variable, but that can be improved
once they're in and running.   ,  "
Network Management,Network Management,Conscrypt,"Support TLS 1.3 (#524)

Enables support for negotiating TLS 1.3.  TLS 1.3 is not enabled
unless SSLContext.TLSv1.3 is ed or setEnabledProtocols() is
called with a set of values that includes TLSv1.3.

Detailed changes:

Adds protocol constants for TLS 1.3, and provides SSLContext.TLSv1.3,
which has TLS 1.3 enabled by default.

Adjusts cipher suite code for TLS 1.3 suites.  When enabled, all TLS
1.3 cipher suites are always returned from supportedCipherSuites() and
enabledCipherSuites().  Attempts to customize TLS 1.3 cipher suites in
setEnabledCipherSuites() are ignored.

Splits {SSLEngine,SSLSocket}Test into version-dependent and
version-independent tests.  The latter remain in
{SSLEngine,SSLSocket}Test and the former move into new files
{SSLEngine,SSLSocket}VersionCompatibilityTest, which are parameterized
to test all combinations of client and server on TLS 1.2 and TLS 1.3.

Remove a pile of RI-specific TLS-related expectation declarations from
StandardNames.  We don't actually verify the behavior of the RI at any
point, so it was just making the code more confusing.

Fixes #479.   ,  "
,,Conscrypt,"Drop support for Java 6 (#606)   ,  "
,,Conscrypt,"Switch preferred SSLContext to TLSv13   ,  "
,,Conscrypt,"AI 144418: am: CL 144381 am: CL 144356 Synchronized code that touches native SSL sessions.
  Original author: crazybob
  Merged from: //branches/cupcake/...
  Original author: android-build

Automated import of CL 144418/auto import from //depot/cupcake/@135843/"
,,Conscrypt,"auto import from //depot/cupcake/@135843/
,,Conscrypt,auto import //branches/donutburger/...@140359/"
,,Conscrypt,"AI 144381: am: CL 144356 Synchronized code that touches native SSL sessions.
  Original author: crazybob
  Merged from: //branches/cupcake/...

Automated import of CL 144381/auto import from //depot/cupcake/@135843/"
Network Management,Network Management,Conscrypt,"Update x-net to Harmony r802921.

Notable changes
 - synchronization added where it was needed!
 - try/finally added to reliably tear down in DefaultSSLContext
 - ContextImpl deleted, it wasn't necessary
 - methods reordered to make statics first in the class
 - PrivilegedActions parameterized with <Void>
 - DigitalSignature now throws AssertionErrors in impossible states
   and throws AlertExceptions on invalid keys (rather than dumping
   a stacktrace)
 - ValueKeys added to SSLSessionImpl instead of TwoKeyMaps
 - SSLSessionImpl.clone() simplified to do a traditional clone

Squashed commit of the following:

commit 2d9e43d542ab7086af271bf52e847c582decbab1
Merge: 8b79eb4 a8dc377
Author: Jesse Wilson <jessewilson@google.com>
Date:   Tue Aug 25 15:25:21 2009 -0700

    Merge branch 'x-net_802921' into x-net_dalvik


    Small changes missed in the original submission of 22482./"
,,Conscrypt,"Each time we start an SSL session, we have to find the trust anchor. This used to be an O(N) operation. If the trust anchor we're looking for was close to N, finding it could take a couple seconds. This change makes the operation O(1)./"
,,Conscrypt,"Fix OpenSSLSessionImpl.getCreationTime and getLastAccessedTime.

This addresses one part of this abandoned change from ursg:
  https://android-git.corp.google.com/g/4743

I've also tidied up the native method names to use the harmony ""-Impl""
convention, removed useless methods that just forward to a native method,
and removed dead code. I've canonicalized some of the duplication too,
but I want to go through the rest of out OpenSSL code before I really start
trying to remove the duplication.

When this is submitted, I'll fix the other (unrelated) bug the abandoned
change addressed./"
Network Management,Network Management,Conscrypt,Remove duplication in OpenSSLSocket/OpenSSLServerSocket./
Network Management,Network Management,Conscrypt,"Rewrite JSSE code to use one openssl SSL per SSLSocket an one SSL_CTX per SSLSessionContext

Summary:

  b/1758225: Revisit OpenSSL locking
    Removed the locking original put in to address b/1678800 which
    had been causing problems for the HeapWorker thread which was
    timing out waiting for the lock in the finalizers while other
    threads were connecting.

  b/1678800: Reliability tool: Crash in libcrypto @ https://opac.ntu.ac.uk
    Properly fixed the native crash by avoid sharing SSL_SESSION objects
    between SSL_CTX objects

Testing:
- adb shell run-core-tests --verbose tests.xnet.AllTests
- adb shell run-core-tests --verbose javax.net.ssl.AllTests
- Test app that reloads https://opac.ntu.ac.uk

Details:
    Each AbstractSessionContext now has an associated SSL_CTX,
    referenced through the sslCtxNativePointer. SSL_CTX on the native
    side defines the scope of SSL_SESSION caching, and this brings the
    Java SSLSessionContext caching into alignment with the native
    code. OpenSSLSessionImpl now uses AbstractSessionContext instead
    of SSLSessionContext for access to the underlying SSL_CTX.


    Added AbstractSessionContext.putSession so OpenSSLSocketImpl/OpenSSLSessionImpl can
    directly assign to the current AbstractSessionContext (whether it
    be a ClientSessionContext or a ServerSessionContext) without
    casting.


  Cleaning up use of SSL_CTX and SSL instances in SSLSocket/SSLServerSocket implementation

    The major change is that openssl SSL instances are allocated for
    the life of the matching Java object, replacing the SSL_CTX and
    the SSL objects that had previously been allocated only starting
    at handshake time. We should never have been sharing SSL_SESSION
    instances between SSL_CTX instances, which was the source of the
    native crashes dating back to cupcake which the
    OpenSSLSocket.class locking had been preventing.

    - NativeCrypto now has better defined and independant wrappers on
      openssl functionality. A followon checkin should move the
      remaining openssl JNI code here with the intent of being able to
      write and end-to-end test of the openssl code using NativeCrypto
      without the JSSE implementation classes. The following gives a
      list of the new native functions with a mapping to the old
      implementation code. The new code has a more functional style
      where SSL_CTX and SSL instances are passed and returned as
      arguments, not extracted from Java instances

      SSL_CTX_new                       OpenSSLSocketImpl.nativeinit, OpenSSLServerSocketImpl.nativeinit, SSLParameters.nativeinitsslctx
      SSL_CTX_get_ciphers_list          OpenSSLSocketImpl.nativeGetEnabledCipherSuites
      SSL_CTX_free                      OpenSSLSocketImpl.nativefree, OpenSSLServerSocketImpl.nativefree

      SSL_new                           OpenSSLSocketImpl.nativeinit, OpenSSLSocketImpl.init, OpenSSLServerSocketImpl.nativeinit, OpenSSLServerSocketImpl.init
      SSL_get_options                   OpenSSLSocketImpl.nativesetenabledprotocols
      SSL_set_options                   OpenSSLSocketImpl.nativesetenabledprotocols
      SSL_get_ciphers                   OpenSSLSocketImpl.nativeGetEnabledCipherSuites
      SSL_set_cipher_list               OpenSSLSocketImpl.nativeSetEnabledCipherSuites
      SSL_free                          OpenSSLSocketImpl.nativefree, OpenSSLServerSocketImpl.nativefree

    - While the focus in NativeCrypto is on native code, it also
      contains some helpers/wrappers especially for code that doesn't
      depend on specific SSL_CTX, SSL instances or that needs to do
      massaging of data formats between Java and OpenSSL. Some of
      these had previously been duplicated in the client and server
      versions of the code. For example:

        getSupportedCipherSuites	OpenSSLSocketImpl.nativegetsupportedciphersuites, OpenSSLServerSocketImpl.nativegetsupportedciphersuites
        getSupportedProtocols		OpenSSLSocketImpl.getSupportedProtocols, OpenSSLServerSocketImpl.getSupportedProtocols
        getEnabledProtocols             OpenSSLSocketImpl.getEnabledProtocols,OpenSSLServerSocketImpl.getEnabledProtocols
        setEnabledProtocols             OpenSSLSocketImpl.setEnabledProtocols
        setEnabledCipherSuites		OpenSSLSocketImpl.setEnabledCipherSuites

    - Moved JNI initialization from OpenSSLSocketImpl to NativeCrypto
      which is the future home of all the openssl related native code.

        clinit                          OpenSSLSocketImpl.nativeinitstatic

    - NativeCrypto.CertificateChainVerifier is a new interface to
      decouple callbacks from openssl from a specific dependence on a
      OpenSSLSocketImpl.verify_callback method. Changed to return
      boolean instead of int.

    - Renamed OpenSSLSocketImpl.ssl to OpenSSLSocketImpl.sslNativePointer for consistency

    - Changed OpenSSLSocketImpl nativeconnect, nativegetsslsession,
      nativecipherauthenticationmethod, nativeaccept, nativeread,
      nativewrite, nativeinterrupt, nativeclose, nativefree to take
      arguments instead of inspect object state in preparation for
      moving to NativeCrypto

    - other notable NativeCrypto changes included
      * adding SSL_SESSION_get_peer_cert_chain,
        SSL_SESSION_get_version, and SSL_get_version (and
        get_ssl_version) which are ""missing methods"" in openssl
      * ssl_msg_callback_LOG callback and get_content_type for handshake debugging
      * removing jfieldID's for our classes now that we pass in values in arguments
      * changed aliveAndKicking to be volative since we poll on it to communicate between threads
      * changed from C style declarations at beginning of block to C++ at first use on methods with major changes
      * stop freeing SSL instances on error, only SSL_clear it
      * improved session reuse logging when reproducing b/1678800
      * change verify_callback to return verifyCertificateChain result


    When we accept a server socket, we pass the existing SSL state
    instance from the server socket to the newly accepted socket via
    the constructor where it is copied with SSL_dup, instead of
    through both the constructor and later the accept method.

    Cleaned up nativesetclientauth from using SSL_CTX to SSL, passing
    ssl as argument in preparation for future movement to
    NativeCrypto.


    Removed ssl_op_no cache for rarely used enabled protocol methods
    so that code could more easily be shared in NativeCrypto between
    client and server.

    Changed public getId, getCreationTime, getPeerCertificates,
    getCipherSuite, getProtocol from being instance methods that
    looked at the OpenSSLSessionImpl object state to be static mthods
    that take the native pointers as arguments in preparation for
    moving to NativeCrypto. Rename session -> sslSessionNativePointer
    for consistency.  Inlined initializeNative, which wasn't really
    the native code.


    Removed lock on OpenSSLSocketImpl.class lock from around
    OpenSSLSocketImpl's use of nativeconnect, nativegetsslsession, and
    nativecipherauthenticationmethod as well as OpenSSLSessionImpl's
    use of freeImpl, fixing b/1758225: Revisit OpenSSL locking


Unrelated changes

    Removed unused ssl_ctx, nativeinitsslctx, getSSLCTX


    Fix bug in both putSession implementations where we cached
    sessions with zero length id. Also change indexById to pass in id
    in client implementation.


    Make sure we clone SSLParameters passed to the SSLSocketFactory
    and SSLServerSocketFactory so that muting the client instance does
    not change the server instance and vice versa. Explicitly set
    setUseClientMode(false) on the server SSLParameters. These changes
    are to bring things more into alignment with the original harmony
    classes which properly support client/server role switching during
    handshaking.


    Spelling SSLInputStream to SSLOutputStream in comment

	

    Changed shutdownInput and shutdownOutput to call to the underlying socket

	
    Set sslNativePointer to 0 when freeing underlying SSL object

Change-Id: I99975ae22599c7df0d249fa013ae7ea7c9c08051/"
Network Management,Network Management,Conscrypt,"Rewrite JSSE code to use one openssl SSL per SSLSocket an one SSL_CTX per SSLSessionContext

Summary:

  b/1758225: Revisit OpenSSL locking
    Removed the locking original put in to address b/1678800 which
    had been causing problems for the HeapWorker thread which was
    timing out waiting for the lock in the finalizers while other
    threads were connecting.

  b/1678800: Reliability tool: Crash in libcrypto @ https://opac.ntu.ac.uk
    Properly fixed the native crash by avoid sharing SSL_SESSION objects
    between SSL_CTX objects

Testing:
- adb shell run-core-tests --verbose tests.xnet.AllTests
- adb shell run-core-tests --verbose javax.net.ssl.AllTests
- Test app that reloads https://opac.ntu.ac.uk

Details:
    Each AbstractSessionContext now has an associated SSL_CTX,
    referenced through the sslCtxNativePointer. SSL_CTX on the native
    side defines the scope of SSL_SESSION caching, and this brings the
    Java SSLSessionContext caching into alignment with the native
    code. OpenSSLSessionImpl now uses AbstractSessionContext instead
    of SSLSessionContext for access to the underlying SSL_CTX.


    Added AbstractSessionContext.putSession so OpenSSLSocketImpl/OpenSSLSessionImpl can
    directly assign to the current AbstractSessionContext (whether it
    be a ClientSessionContext or a ServerSessionContext) without
    casting.

  Cleaning up use of SSL_CTX and SSL instances in SSLSocket/SSLServerSocket implementation

    The major change is that openssl SSL instances are allocated for
    the life of the matching Java object, replacing the SSL_CTX and
    the SSL objects that had previously been allocated only starting
    at handshake time. We should never have been sharing SSL_SESSION
    instances between SSL_CTX instances, which was the source of the
    native crashes dating back to cupcake which the
    OpenSSLSocket.class locking had been preventing.

    - NativeCrypto now has better defined and independant wrappers on
      openssl functionality. A followon checkin should move the
      remaining openssl JNI code here with the intent of being able to
      write and end-to-end test of the openssl code using NativeCrypto
      without the JSSE implementation classes. The following gives a
      list of the new native functions with a mapping to the old
      implementation code. The new code has a more functional style
      where SSL_CTX and SSL instances are passed and returned as
      arguments, not extracted from Java instances

      SSL_CTX_new                       OpenSSLSocketImpl.nativeinit, OpenSSLServerSocketImpl.nativeinit, SSLParameters.nativeinitsslctx
      SSL_CTX_get_ciphers_list          OpenSSLSocketImpl.nativeGetEnabledCipherSuites
      SSL_CTX_free                      OpenSSLSocketImpl.nativefree, OpenSSLServerSocketImpl.nativefree

      SSL_new                           OpenSSLSocketImpl.nativeinit, OpenSSLSocketImpl.init, OpenSSLServerSocketImpl.nativeinit, OpenSSLServerSocketImpl.init
      SSL_get_options                   OpenSSLSocketImpl.nativesetenabledprotocols
      SSL_set_options                   OpenSSLSocketImpl.nativesetenabledprotocols
      SSL_get_ciphers                   OpenSSLSocketImpl.nativeGetEnabledCipherSuites
      SSL_set_cipher_list               OpenSSLSocketImpl.nativeSetEnabledCipherSuites
      SSL_free                          OpenSSLSocketImpl.nativefree, OpenSSLServerSocketImpl.nativefree

    - While the focus in NativeCrypto is on native code, it also
      contains some helpers/wrappers especially for code that doesn't
      depend on specific SSL_CTX, SSL instances or that needs to do
      massaging of data formats between Java and OpenSSL. Some of
      these had previously been duplicated in the client and server
      versions of the code. For example:

        getSupportedCipherSuites	OpenSSLSocketImpl.nativegetsupportedciphersuites, OpenSSLServerSocketImpl.nativegetsupportedciphersuites
        getSupportedProtocols		OpenSSLSocketImpl.getSupportedProtocols, OpenSSLServerSocketImpl.getSupportedProtocols
        getEnabledProtocols             OpenSSLSocketImpl.getEnabledProtocols,OpenSSLServerSocketImpl.getEnabledProtocols
        setEnabledProtocols             OpenSSLSocketImpl.setEnabledProtocols
        setEnabledCipherSuites		OpenSSLSocketImpl.setEnabledCipherSuites

    - Moved JNI initialization from OpenSSLSocketImpl to NativeCrypto
      which is the future home of all the openssl related native code.

        clinit                          OpenSSLSocketImpl.nativeinitstatic

    - NativeCrypto.CertificateChainVerifier is a new interface to
      decouple callbacks from openssl from a specific dependence on a
      OpenSSLSocketImpl.verify_callback method. Changed to return
      boolean instead of int.

    - Renamed OpenSSLSocketImpl.ssl to OpenSSLSocketImpl.sslNativePointer for consistency

    - Changed OpenSSLSocketImpl nativeconnect, nativegetsslsession,
      nativecipherauthenticationmethod, nativeaccept, nativeread,
      nativewrite, nativeinterrupt, nativeclose, nativefree to take
      arguments instead of inspect object state in preparation for
      moving to NativeCrypto

    - other notable NativeCrypto changes included
      * adding SSL_SESSION_get_peer_cert_chain,
        SSL_SESSION_get_version, and SSL_get_version (and
        get_ssl_version) which are ""missing methods"" in openssl
      * ssl_msg_callback_LOG callback and get_content_type for handshake debugging
      * removing jfieldID's for our classes now that we pass in values in arguments
      * changed aliveAndKicking to be volative since we poll on it to communicate between threads
      * changed from C style declarations at beginning of block to C++ at first use on methods with major changes
      * stop freeing SSL instances on error, only SSL_clear it
      * improved session reuse logging when reproducing b/1678800
      * change verify_callback to return verifyCertificateChain result


    When we accept a server socket, we pass the existing SSL state
    instance from the server socket to the newly accepted socket via
    the constructor where it is copied with SSL_dup, instead of
    through both the constructor and later the accept method.

    Cleaned up nativesetclientauth from using SSL_CTX to SSL, passing
    ssl as argument in preparation for future movement to
    NativeCrypto.


    Removed ssl_op_no cache for rarely used enabled protocol methods
    so that code could more easily be shared in NativeCrypto between
    client and server.

    Changed public getId, getCreationTime, getPeerCertificates,
    getCipherSuite, getProtocol from being instance methods that
    looked at the OpenSSLSessionImpl object state to be static mthods
    that take the native pointers as arguments in preparation for
    moving to NativeCrypto. Rename session -> sslSessionNativePointer
    for consistency.  Inlined initializeNative, which wasn't really
    the native code.


    Removed lock on OpenSSLSocketImpl.class lock from around
    OpenSSLSocketImpl's use of nativeconnect, nativegetsslsession, and
    nativecipherauthenticationmethod as well as OpenSSLSessionImpl's
    use of freeImpl, fixing b/1758225: Revisit OpenSSL locking



    Make sure we clone SSLParameters passed to the SSLSocketFactory
    and SSLServerSocketFactory so that muting the client instance does
    not change the server instance and vice versa. Explicitly set
    setUseClientMode(false) on the server SSLParameters. These changes
    are to bring things more into alignment with the original harmony
    classes which properly support client/server role switching during
    handshaking.

Change-Id: I99975ae22599c7df0d249fa013ae7ea7c9c08051/Add a setHandshakeTimeout() to OpenSSLSocketImpl, which sets
a read timeout that only applies to the SSL handshake step.

Bug: 2362543/"
Network Management,Network Management,Conscrypt,"Fix OpenSSLSessionImpl.getValueNames regression

In e32b21f14d52bac429a9c54fe031f9e92c911d64, the code was converted to
use Objects.equals. However, because of a typo, an autoboxed Boolean
was passed instead of an AccessControlContext. I reviewed the rest of
the original change to make sure there were no other instances of this
regression.

Also cleaned up the SSLSessionTest (fixing two broken tests
test_getLocalPrincipal and test_getPeerPrincipal) and fixed a
whitespace issue in AccessControlContext.

Change-Id: Icaee8a0c2f5f527bea7a80037fe3f99c509d9f42/"
,,Conscrypt,"Fix HttpsURLConnectionTest failures

Focusing on HttpsURLConnectionTest.test_doOutput found a number of
unrelated issues, all of which are addressed by this change:
- {HttpURLConnection,HttpsURLConnection}.connect not ignored on subsequent calls
- OpenSSLSessionImpl.{getPeerCertificates,getPeerCertificateChain} did not include client certificate
- OpenSSLSocketImpl.getSession did not skip handshake when SSLSession was already available
- Fix 3 test issues in HttpsURLConnectionTest
- Fix 2 test issues in NativeCryptoTest

Details:

    HttpsURLConnectionTest tests (such as test_doOutput) that
    tried to call URLConnection.connect() at the end of the test
    were raising exception. The RI URLConnection.connect
    documentation says calls on connected URLConnections should be ignored.

      Use ""connected"" instead of ""connection != null"" as reason to ignore ""connect""


    Converted one caller of getPeerCertificateChain to
    getPeerCertificates which is the new fast path.  Track
    OpenSSLSessionImpl change to take ""java"" vs ""javax"" certificates.

    Move SSL_SESSION_get_peer_cert_chain to be SSL_get_peer_cert_chain
    (similar to SSL_get_certificate). The problem was that
    SSL_SESSION_get_peer_cert_chain used SSL_get_peer_cert_chain which
    in the server case did not include the client cert itself, which
    required a call to SSL_get_peer_certificate, which needed the
    SSL instance pointer.


    Improved NativeCrypto_SSL_set_verify tracing

    As a side effect of the move to
    NativeCrypto.SSL_get_peer_certificate, it no longer made sense to
    lazily create the peer certificate chain since the SSLSession
    should not depend on a particular SSL instance. The peer chain is
    now passed in as part of the constructor and the peerCertifcates
    in the OpenSSLSession can be final (also made localCertificates
    final). Since peerCertifcates is the newew (java not javax) API
    and more commonly used, it is what is created from the native
    code, and peerCertificateChain is not derived from peerCertifcates
    instead of vice versa.

    Factored out code to used to create local certificate chain to
    from array of DER byte arrays into createCertChain so it can be
    reused to create peer certificate chain.

    Fix OpenSSLSocketImpl.getSession to check for existing sslSession
    to and skip handshake, which was causing an exception if the
    connection had already been closed.


    Fix test issues: Removed PrintStream wrapper of System.out which
    was causing vogar to lose output. Added null check in closeSocket,
    which can happen in timeout case. Removed use of
    InputStream.available which in OpenSSLSocket case returned 0,
    causing test to fail incorrectly.


    Updating to track change to SSL_get_peer_cert_chain. Also fixed
    some other unrelated test failures caused by IOException on
    shutdown and false start (aka SSL_MODE_HANDSHAKE_CUTTHROUGH)
    causing clientCallback.handshakeCompleted to be false.

Bug: b/2981767
Change-Id: Id083beb6496558296c2f74f51ab0970e158b23a9/b/2914872: fix concurrent initialization problem with peer certificate chain fields

Change-Id: Ib76dd826c8f3616d4a3aed608aef432a1b99f3d6/"
Network Management,Network Management,Conscrypt,"TrustManager improvements

Overhaul of TrustManagerImpl
- PKIXParameters can now be final in TrustManagerImpl because we
  always immediately create an IndexedPKIXParameters instead of only
  doing it in SSLParametersImpl.createDefaultTrustManager.
- Use new KeyStore constructor for IndexedPKIXParameters to remove
  duplicate logic for creating set of TrustAnchors from a KeyStore.
- Improved checkTrusted/cleanupCertChain to remove special cases for
  directly trusting the end cert or pruning only self signed certs. To
  support b/2530852, we need to stop prune the chain as soon as we
  find any trust anchor (using newly improved
  TrustManagerImpl.isTrustAnchor), which could be at the beginning,
  middle, or end. That means cleanupCertChain can return an empty
  chain if everything was trusted directly. (and we don't need to do
  extra checks on exception cases to see if the problem was just that
  the trust anchor was in the chain)
- isDirectlyTrusted -> isTrustAnchor here as well, using new
  IndexedPKIXParameters.isTrustAnchor APIs
- Fix incorrect assumption in getAcceptedIssuers that all TrustAnchor
  instances have non-null results for getTrustedCert.


Overhaul of IndexedPKIXParameters
- Single map from subject X500Principal to TrustAnchors
  instead of two different X500Principal keyed maps to check
- Removed map based on encoded cert. For b/2530852, we want to treat
  certs as equal if they have the same name and public key, not
  byte-for-byte equality, which can be done with the remaining map.
  Revamped isDirectlyTrusted into isTrustAnchor(cert) to perform this
  new name/key based comparison.
- Added helper isTrustAnchor(cert, anchors) to reuse code in
  non-IndexedPKIXParameters case in TrustManagerImpl.
- Added constructor from KeyStore
- Moved anchor indexing code to index() from old constructor



TestKeyStore.getPrivateKey allowed some existing test simplification.


Added missing ""fail()"" before catching expected exceptions.


Expanded KeyManagerFactoryTest to excercise ManagerFactoryParameters b/1628001


Added KeyStoreBuilderParametersTest because I thought I saw a bug in
KeyStoreBuilderParameters, but this convinced me otherwise.


New TrustManagerFactory test modeled on expanded KeyManagerFactoryTest.
test_TrustManagerFactory_intermediate specifically is targeting the
new functionality of b/2530852 to handling trust anchors within the
chain.


Some initial on tests for Elliptic Curve (b/3058375) after the RI
started reporting it was supported. Removed old @KnownFailure
tags. Skipped a test on the RI that it can't handle. Improved some
assert messages.

Change-Id: I8347bc625480a1c37a1ed9976193ddfedeb00bbc/"
,,Conscrypt,"Rename internal SSLParameters to SSLParametersImpl to avoid collision with new javax.net.ssl.SSLParameters

Bug: 2672817
Change-Id: Iadf21b848eaf8850fce22721b9ba3739ab2e9fca/"
,,Conscrypt,"Scrub missing calls to super.finalize()

Bug: 3024226
Change-Id: I6642cb9d4929ba72244529efe4ebdfa595ae4fa7/Restore OpenSSLMessageDigestJDK.digest reset behavior

SSLEngine tests started failing due to the recent incorrect change to
OpenSSLMessageDigestJDK.digest() that removed the reset of
MessageDigest state on call to digest(). The problem was not that the
digest was resetting, but that it was resetting to use a SHA-0
algorithm. See recent change c38b8476e7e4bd4b091d9f0e8fe8b2b972e7bc81.

Change-Id: I40ef4e18a1b546eac5a487cb8a808d4897b301b0/OpenSSLMessageDigestJDK.reset should not change from SHA-1 to SHA-0

For SHA-1, the OpenSSLMessageDigestJDK constructor was called with the
algorithm name ""SHA-1"", which it passed to the superclass constructor
for use as the algorithm field. However, MessageDigest.getInstance
would then override this value with the its own algorithm argument. In the
case of getInstance(""SHA""), this mean the constructor would set the
value to ""SHA-1"" (from the OpenSSLMessageDigestJDK.SHA1 subclass
constructor) which would then be overridden by getInstance to
""SHA"". Because the OpenSSLMessageDigestJDK would then initialize using
""SHA-1"", the MessageDigest worked in the common case. However, when it
was MessageDigest.reset(), it called getAlgorithm() which returned
""SHA"", which was then passed to OpenSSL as ""sha"" which interpretted
this as ""SHA-0"".

The fix is to change to pass both a standard name (e.g ""SHA-1"") as
well as openssl name expliclty (e.g. ""sha1""), removing the somewhat
hacky code that tried to algorithmically transform from the standard
names to the openssl ones.

The same fix needs to be made to OpenSSLDigest. We also are removing
SHA-0 from openssl since it is unneeded and would have cause an clear
error if it had been absent.

Change-Id: Iaa8f5b93a572fb043fa4f2618070ebb5054f82b1/Implement OpenSSLMessageDigestJDK.clone and fix OpenSSLMessageDigestJDK.digest

DigestInputStream2Test.test_onZ was failing because OpenSSLMessageDigestJDK did not implement Clonable
- Implementing Clonable required a new NativeCrypto.EVP_MD_CTX_copy method
- While adding NativeCrypto.EVP_MD_CTX_copy, noticed other methods
  were not properly named in NativeCrypto.EVP_MD_CTX_* convention.
- Converted rest of NativeCrypto.cpp to JNI_TRACE logging while debugging

DigestOutputStreamTest.test_onZ was failing because OpenSSLMessageDigestJDK.digest did an engineReset
- Removing the engineReset revealed that digest() could not be called
  repeatedly on an OpenSSLMessageDigestJDK. Problem was that
  EVP_DigestFinal can only be called once per digest.
- Changed engineDigest implementation to use new EVP_MD_CTX_copy to
  create a temp EVP_MD_CTX which can be used to retreive the digest
  and then discarded.

Bug: 2997405
Change-Id: Ie97c22be245911300d2e729e451a9c4afdb27937/"
,,Conscrypt,"Implement OpenSSLMessageDigestJDK.clone and fix OpenSSLMessageDigestJDK.digest

DigestInputStream2Test.test_onZ was failing because OpenSSLMessageDigestJDK did not implement Clonable
- Implementing Clonable required a new NativeCrypto.EVP_MD_CTX_copy method
- While adding NativeCrypto.EVP_MD_CTX_copy, noticed other methods
  were not properly named in NativeCrypto.EVP_MD_CTX_* convention.
- Converted rest of NativeCrypto.cpp to JNI_TRACE logging while debugging

DigestOutputStreamTest.test_onZ was failing because OpenSSLMessageDigestJDK.digest did an engineReset
- Removing the engineReset revealed that digest() could not be called
  repeatedly on an OpenSSLMessageDigestJDK. Problem was that
  EVP_DigestFinal can only be called once per digest.
- Changed engineDigest implementation to use new EVP_MD_CTX_copy to
  create a temp EVP_MD_CTX which can be used to retreive the digest
  and then discarded.

Bug: 2997405
Change-Id: Ie97c22be245911300d2e729e451a9c4afdb27937/"
Network Management,Network Management,Conscrypt,"TrustManager improvements

Overhaul of TrustManagerImpl
- PKIXParameters can now be final in TrustManagerImpl because we
  always immediately create an IndexedPKIXParameters instead of only
  doing it in SSLParametersImpl.createDefaultTrustManager.
- Use new KeyStore constructor for IndexedPKIXParameters to remove
  duplicate logic for creating set of TrustAnchors from a KeyStore.
- Improved checkTrusted/cleanupCertChain to remove special cases for
  directly trusting the end cert or pruning only self signed certs. To
  support b/2530852, we need to stop prune the chain as soon as we
  find any trust anchor (using newly improved
  TrustManagerImpl.isTrustAnchor), which could be at the beginning,
  middle, or end. That means cleanupCertChain can return an empty
  chain if everything was trusted directly. (and we don't need to do
  extra checks on exception cases to see if the problem was just that
  the trust anchor was in the chain)
- isDirectlyTrusted -> isTrustAnchor here as well, using new
  IndexedPKIXParameters.isTrustAnchor APIs
- Fix incorrect assumption in getAcceptedIssuers that all TrustAnchor
  instances have non-null results for getTrustedCert.


    Removed indexing in createDefaultTrustManager since we always index now


Overhaul of IndexedPKIXParameters
- Single map from subject X500Principal to TrustAnchors
  instead of two different X500Principal keyed maps to check
- Removed map based on encoded cert. For b/2530852, we want to treat
  certs as equal if they have the same name and public key, not
  byte-for-byte equality, which can be done with the remaining map.
  Revamped isDirectlyTrusted into isTrustAnchor(cert) to perform this
  new name/key based comparison.
- Added helper isTrustAnchor(cert, anchors) to reuse code in
  non-IndexedPKIXParameters case in TrustManagerImpl.
- Added constructor from KeyStore
- Moved anchor indexing code to index() from old constructor


TestKeyStore.getPrivateKey allowed some existing test simplification.


Added missing ""fail()"" before catching expected exceptions.


Expanded KeyManagerFactoryTest to excercise ManagerFactoryParameters b/1628001


Added KeyStoreBuilderParametersTest because I thought I saw a bug in
KeyStoreBuilderParameters, but this convinced me otherwise.


New TrustManagerFactory test modeled on expanded KeyManagerFactoryTest.
test_TrustManagerFactory_intermediate specifically is targeting the
new functionality of b/2530852 to handling trust anchors within the
chain.

Some initial on tests for Elliptic Curve (b/3058375) after the RI
started reporting it was supported. Removed old @KnownFailure
tags. Skipped a test on the RI that it can't handle. Improved some
assert messages.

Removed super()

Bug: 2530852
Change-Id: I95e0c7ee6a2f66b6986b3a9da9583d1ae52f94dd/Move improved cert chain handling from CertificateChainValidator to TrustManagerImpl

Bug: 2658463

Change-Id: I014ebfee1f6e2f46b7a842b5bbf6549bf484f3c0/"
Network Management,Network Management,Conscrypt,"Stop allocating empty arrays.

Bug: 3166662
Change-Id: I151de373b2bf53786d19824336fa434c02b0b0e8/SSL* AppData should not hold onto JNI global references

Summary:

NativeCrypto.SSL_do_handshake stored JNI global references in its
AppData instance for use in upcalls from OpenSSL that invoke Java
callbacks. However, one of the references was to the
SSLHandshakeCallbacks which in the common case of OpenSSLSocketImpl is
the OpenSSLSocketImpl instance itself. This meant that if code dropped
the OpenSSLSocketImpl without closing (such as Apache HTTP Client),
the instances would never be collected, and perhaps more importantly,
file descriptors would not be closed.

The fix is to pass in the objects required during a callback in all
downcalls to SSL_* methods that could result in a callback and clear
them on return. The existing code already did this for the JNIEnv*, so
that code was expanded to handle setting the jobjects as well.

Details:

In the native code used to extract the FileDescriptor object from a
Socket on the call to NativeCrypto.SSL_do_handshake. However, since we
need this for every read and write operations, we now do this in Java
to avoid the repeated overhead. NativeCrypto.SSL_do_handshake now
takes a FileDescriptor, which it extracted from the Socket the
convenience function using NativeCrypto.getFileDescriptor(Socket)


   In addition to tracking changes to pass FileDescriptor and
   SSLHandshakeCallbacks, removed final uses of getFieldId since the
   code no longer needs to extract FileDescriptors itself


The Socket field used to be non-null in the wrapper case and null in
the non-wrapper case. To simplify things a bit, ""socket == this"" in
the non-wrapper case. The socket field is now also final and joined by
a final FileDescriptor field.


Updated NativeCryptoTest to track FileDescriptor and
SSLHandshakeCallbacks by expanding the Hooks.afterHandshake to provide
them. Also changed to add a 5 second timeout to many test cases.

Bug: 2989218

Change-Id: Iccef92b59475f3c1929e990893579493ece9d442/"
Network Management,Network Management,Conscrypt,"Avoid races between OpenSSLSocketImpl I/O and close()

The previous change:

    commit 5f2e6872311240319509aed64d9f58cd5b64719b
    Author: Brian Carlstrom <bdc@google.com>
    Date:   Mon Aug 23 14:06:51 2010 -0700

    SSLSocket.read should throw SocketException not NullPointerException

added checkOpen() to throw SocketException instead of
NullPointerException, but there was still a race between read/write on
one thread and close on another that could allow a
NullPointerException to escape. This change moves checkOpen() calls to
be protected by the existing writeLock/readLock/handshakeLock
synchronzied blocks to avoid this case.

byte buffer error checking for read/write is also moved into the to
lock region to preserve compatability as measured by the test:
    libcore.javax.net.ssl.SSLSocketTest#test_SSLSocket_close

Bug: 3153162
Change-Id: I16299f09dc91871407e88eb718073d21a816f683/CloseGuard: finalizers for closeable objects should log complaints

Introducing CloseGuard which warns when resources are implictly
cleaned up by finalizers when an explicit termination method, to use
the Effective Java ""Issue 7: Avoid finalizers"" terminology, should
have been used by the caller.

libcore classes that can use CloseGuard now do so.

Bug: 3041575
Change-Id: I4a4e3554addaf3075c823feb0a0ff0ad1c1f6196/"
,,Conscrypt,"SSL* AppData should not hold onto JNI global references

Summary:

NativeCrypto.SSL_do_handshake stored JNI global references in its
AppData instance for use in upcalls from OpenSSL that invoke Java
callbacks. However, one of the references was to the
SSLHandshakeCallbacks which in the common case of OpenSSLSocketImpl is
the OpenSSLSocketImpl instance itself. This meant that if code dropped
the OpenSSLSocketImpl without closing (such as Apache HTTP Client),
the instances would never be collected, and perhaps more importantly,
file descriptors would not be closed.

The fix is to pass in the objects required during a callback in all
downcalls to SSL_* methods that could result in a callback and clear
them on return. The existing code already did this for the JNIEnv*, so
that code was expanded to handle setting the jobjects as well.

Details:

In the native code used to extract the FileDescriptor object from a
Socket on the call to NativeCrypto.SSL_do_handshake. However, since we
need this for every read and write operations, we now do this in Java
to avoid the repeated overhead. NativeCrypto.SSL_do_handshake now
takes a FileDescriptor, which it extracted from the Socket the
convenience function using NativeCrypto.getFileDescriptor(Socket)


   In addition to tracking changes to pass FileDescriptor and
   SSLHandshakeCallbacks, removed final uses of getFieldId since the
   code no longer needs to extract FileDescriptors itself

The Socket field used to be non-null in the wrapper case and null in
the non-wrapper case. To simplify things a bit, ""socket == this"" in
the non-wrapper case. The socket field is now also final and joined by
a final FileDescriptor field.


Updated NativeCryptoTest to track FileDescriptor and
SSLHandshakeCallbacks by expanding the Hooks.afterHandshake to provide
them. Also changed to add a 5 second timeout to many test cases.

Bug: 2989218

Change-Id: Iccef92b59475f3c1929e990893579493ece9d442/"
,,Conscrypt,"OpenSSLSocketImpl should not call NativeCrypto.SSL_set_client_CA_list with an empty array

Bug: 3034616
Change-Id: Ib39ebfa737910f0ebce5ac2ad87715579bd7aa3d/"
,,Conscrypt,"SSLSocket should respect timeout of a wrapped Socket

Change to using getSoTimeout in OpenSSLSocketImpl instead of directly
using the timeout field. This means the proper timeout will be used
for instances of the OpenSSLSocketImplWrapper subclass, which is used
when an SSLSocket is wrapped around an existing connected non-SSL
Socket. The code still maintains the local timeout field, now renamed
timeoutMilliseconds, which is now accesed via
OpenSSLSocketImpl.getSoTimeout. Doing so prevents a getsockopt syscall
that otherwise would be necessary if the super.getSoTimeout() was used.

Added two unit tests for testing timeouts with SSLSockets wrapped
around Socket. One is simply for getters/setters. The second makes
sure the timeout is functioning when set on the underlying socket.

Bug: 2973305
Change-Id: Idac52853f5d777fae5060a840eefbfe85d448e4c/"
,,Conscrypt,"Fix HttpsURLConnectionTest failures

Focusing on HttpsURLConnectionTest.test_doOutput found a number of
unrelated issues, all of which are addressed by this change:
- {HttpURLConnection,HttpsURLConnection}.connect not ignored on subsequent calls
- OpenSSLSessionImpl.{getPeerCertificates,getPeerCertificateChain} did not include client certificate
- OpenSSLSocketImpl.getSession did not skip handshake when SSLSession was already available
- Fix 3 test issues in HttpsURLConnectionTest
- Fix 2 test issues in NativeCryptoTest

Details:

    HttpsURLConnectionTest tests (such as test_doOutput) that
    tried to call URLConnection.connect() at the end of the test
    were raising exception. The RI URLConnection.connect
    documentation says calls on connected URLConnections should be ignored.

      Use ""connected"" instead of ""connection != null"" as reason to ignore ""connect""

    Converted one caller of getPeerCertificateChain to
    getPeerCertificates which is the new fast path.  Track
    OpenSSLSessionImpl change to take ""java"" vs ""javax"" certificates.

    Move SSL_SESSION_get_peer_cert_chain to be SSL_get_peer_cert_chain
    (similar to SSL_get_certificate). The problem was that
    SSL_SESSION_get_peer_cert_chain used SSL_get_peer_cert_chain which
    in the server case did not include the client cert itself, which
    required a call to SSL_get_peer_certificate, which needed the
    SSL instance pointer.

    Improved NativeCrypto_SSL_set_verify tracing

	luni/src/main/native/NativeCrypto.cpp

    As a side effect of the move to
    NativeCrypto.SSL_get_peer_certificate, it no longer made sense to
    lazily create the peer certificate chain since the SSLSession
    should not depend on a particular SSL instance. The peer chain is
    now passed in as part of the constructor and the peerCertifcates
    in the OpenSSLSession can be final (also made localCertificates
    final). Since peerCertifcates is the newew (java not javax) API
    and more commonly used, it is what is created from the native
    code, and peerCertificateChain is not derived from peerCertifcates
    instead of vice versa.

    Factored out code to used to create local certificate chain to
    from array of DER byte arrays into createCertChain so it can be
    reused to create peer certificate chain.

    Fix OpenSSLSocketImpl.getSession to check for existing sslSession
    to and skip handshake, which was causing an exception if the
    connection had already been closed.

    Fix test issues: Removed PrintStream wrapper of System.out which
    was causing vogar to lose output. Added null check in closeSocket,
    which can happen in timeout case. Removed use of
    InputStream.available which in OpenSSLSocket case returned 0,
    causing test to fail incorrectly.

    Updating to track change to SSL_get_peer_cert_chain. Also fixed
    some other unrelated test failures caused by IOException on
    shutdown and false start (aka SSL_MODE_HANDSHAKE_CUTTHROUGH)
    causing clientCallback.handshakeCompleted to be false.


Bug: b/2981767
Change-Id: Id083beb6496558296c2f74f51ab0970e158b23a9/"
,,Conscrypt,"Use BlockGuard for OpenSSL sockets.

StrictMode wasn't catching network usage via SSL.

Bug: 2976407
Change-Id: I31fe09861e3aca7b26724b94af88687fb6b9442b/"
,,Conscrypt,"SSLSocket.read should throw SocketException not NullPointerException

OpenSSLSocketImpl now uses checkOpen similar to Socket's
checkOpenAndCreate to ensure that SocketExceptions are thrown if
certain operations are tried after the socket is closed.

Also added *_setUseClientMode_afterHandshake tests for SSLSocket and
SSLEngine. We properly through IllegalArgument exception in this case,
but it wasn't covered by the tests previously.

Bug: 2918499
Change-Id: I393ad39bed40a33725d2c0f3f08b9d0b0d3ff85f/"
,,Conscrypt,"Stop allocating empty arrays.

Bug: 3166662
Change-Id: I151de373b2bf53786d19824336fa434c02b0b0e8/"
,,Conscrypt,"CloseGuard: finalizers for closeable objects should log complaints

Introducing CloseGuard which warns when resources are implictly
cleaned up by finalizers when an explicit termination method, to use
the Effective Java ""Issue 7: Avoid finalizers"" terminology, should
have been used by the caller.

libcore classes that can use CloseGuard now do so.

Bug: 3041575
Change-Id: I4a4e3554addaf3075c823feb0a0ff0ad1c1f6196/"
,,Conscrypt,"Use BufferedInputStream when reading cacerts.bks

Change-Id: Ibc20bdcadb5c3bc4bcebfeb96b10c42d9c05e7c8/"
,,Conscrypt,"Move libcore.base classes to libcore.util and libcore.io.

Change-Id: I2340a9dbad3561fa681a8ab47d4f406e72c913e3/"
,,Conscrypt,"Remove non-API uses of Vector.

Change-Id: I27902950af0349619f4cb826d41db8926df0d34a/"
Network Management,Network Management,Conscrypt,"Elliptic Crypto support for OpenSSLSocketImpl

Summary:
- Enable Elliptic Crypto support for OpenSSL based SSLSocket instances
- More RI compliant usage of key types, client auth types, and server auth types
- Steps toward TLS_EMPTY_RENEGOTIATION_INFO_SCSV support, currently test updates

Details:

Elliptic Curve changes

    CipherSuite updates for EC
    - Adding KEY_EXCHANGE_EC* and corresponding CipherSuites Updated
      isAnonymous, getKeyType (now renamed getServerKeyType) to handle
      new EC cases.  Added new getAuthType for use by
      checkServerTrusted callers.
    - Restructured code to handle two SUITES_BY_CODE_* arrays
    - Remove KEY_EXCHANGE_DH_* definitions which unused because the
      corresponding CipherSuites were previously disabled.
    - Changed AES CipherSuites definitions to use ""_CBC"" to match other definitions.

Tests

   Differentiate between supported list of cipher suites openssl-based
   SSLSocket and SSLEngine based, since the SSLEngine code does not support EC.
   
   Updated to handle new EC cipher suites codes. Added test for new getClientKeyType.

   Better use of ""standard names"" particularly to correctly deal with
   the subtle differences between key types, client auth types, and
   server auth types. TestKeyManager and TestTrustManager now verify
   the values they are passed are acceptable.

   Changed to timeout after 30 seconds and to log to reveal both client and server issues.

Bug: 3058375
Change-Id: I14d1d0285d591c99cc211324f3595a5be682cab1/"
,,Conscrypt,"Test updates for Elliptic Curve

    Updated with Elliptic Curve (EC) (and SunPKCS11-NSS) names for use by ProviderTest

    Enhance test_KeyStore_cacerts_bks to verify PublicKey can be
    retreived. Before this the test would pass even though an
    ECPublicKey could not be accessed. With EC support in
    external/bouncycastle, this test now passes.


    New SignatureTest to cover ECDSA, replaces the old one that
    required a subclass per tested algorithm.


    Improve ProviderTest logging while debugging SunPKCS11-NSS
    provider issues. Added some exceptions for RI missing classes.

    Changed style slightly to match KeyPairGeneratorTest, where +N is
    used to indicated when multiples of a increments of a certain
    amount are required for valid key sizes.


    Fix readability


Bug: 3058375
Change-Id: I99cd93ad66372e8512d993168550cc1d471d3248/"
Exception Management,Exception Management,Conscrypt,"Most callers of toLowerCase/toUpperCase should pass Locale.US to avoid problems in Turkey.

Some callers should be replaced with equalsIgnoreCase instead.

The one exception is StreamTokenizer, where the RI uses the default
locale, which is arguably the right thing to do. No-one cares because
that's legacy API, but I've added a test anyway.

I've left HttpCookie and GeneralName for my co-conspirators because the
appropriate resolutions aren't as obvious there...

Bug: 3325637
Change-Id: Ia37a1caaa91b11763ae43e61e445adb45c30f793/"
Network Management,Network Management,Conscrypt,"Toward EC TLS support

Summary:
- javax.net.ssl tests are now working on the RI
- KeyManager can now handle EC_EC and EC_RSA
- OpenSSLSocketImpl.startHandshake now works if KeyManager contains EC certificates

Details:

Add CipherSuite.getKeyType to provide X509KeyManager key type strings,
refactored from OpenSSLServerSocketImpl.checkEnabledCipherSuites.
getKeyType is now also used in OpenSSLSocketImpl.startHandshake to
avoid calling setCertificate for unnecessary key types.

   New CipherSuiteTest to cover new getKeyType as well as existing functionality


Add support to KeyManager implementation for key types of the form
EC_EC and EC_RSA. The first part implies the KeyPair algorithm (EC in
these new key types) with a potentially different signature algorithm
(EC vs RSA in these)

Update NativeCrypto.keyType to support EC_EC and EC_RSA in addition to
EC which was added earlier. Change from array of KEY_TYPES to named
KEY_TYPE_* constants.


Overhauled KeyManagerFactoryTest to cover EC, EC_EC, EC_RSA cases

Changed TestKeyStore.createKeyStore from always using BKS to now use
JKS on the RI between BC EC Keys and RI X509 certificates. Because JKS
requires a password, we now default ""password"" on the RI.

TestKeyStore.create now accepts key types like EC_RSA. Changed
TestKeyStore.createKeys to allow a PrivateKeyEntry to be specified for
signing to enable creation of EC_RSA test certificate. Added
getRootCertificate/rootCertificate to allow lookup of PrivateKeyEntry
for signing. Changed TestKeyStore.getPrivateKey to take explicit
signature algorithm to retrieve EC_EC vs EC_RSA entries.

Added support for EC cipher suites on the RI.  Also test with and
without new TLS_EMPTY_RENEGOTIATION_INFO_SCSV cipher suite which is
used to specify the new TLS secure renegotiation.

New TestKeyManager and additional logging in TestTrustManager. Logging
in both is disabled by default using DevNullPrintStream.


Bug: 3058375
Change-Id: Ia5e2a00a025858e10d1076b900886994b481e05a/"
Network Management,Network Management,Conscrypt,"Remember intermediate CAs in TrustMangerImpl's IndexedPKIXParameters

Bug: 3404902
Change-Id: I4a3c35fd2981933c255e5d3a620675b9575083d4/"
,,Conscrypt,"TrustManager should include PrivateKeyEntry CAs, OpenSSLSocketImpl close fix, and debugging improvements

   Revert to older behavior of creating TrustAnchors from both
   PrivateKeyEntry and TrustedCertificateEntry values from the
   KeyStore. Added tests to better ensure this slighlt different
   behavior from PKIXParameters. Also create the acceptedIssuers
   proactively since the real memory cost is the X509Certificates
   which are already found in the params.

   Don't just free native state on issue with startHandshake, close
   the SSLSocket. While the former addressed a CloseGuard issue, the
   latter make sure that checkOpen throws SocketExceptions and we don't
   leak a NullPointerException from NativeCrypto.


   Debugging improvements including minor refinements to recently
   added NativeCrypto logging, more verbose TestKeyStore.dump output,
   and a new TestTrustManager proxy class for logging X509TrustManager
   behavior.


Change-Id: I317e1ca34d8e20c77e5cb9c5a5a58cb4ae98d829/"
Network Management,Network Management,Conscrypt,"Add support for TLS_EMPTY_RENEGOTIATION_INFO_SCSV cipher suite

""TLS_EMPTY_RENEGOTIATION_INFO_SCSV"" is RFC 5746's renegotiation
indication signaling cipher suite value. It is not a real cipher
suite. It is just an indication in the default and supported cipher
suite lists indicates that the implementation supports secure
renegotiation.

In the RI, its presence means that the SCSV is sent in the cipher
suite list to indicate secure renegotiation support and its absence
means to send an empty TLS renegotiation info extension instead.

However, OpenSSL doesn't provide an API to give this level of control,
instead always sending the SCSV and always including the empty
renegotiation info if TLS is used (as opposed to SSL). So we simply
allow TLS_EMPTY_RENEGOTIATION_INFO_SCSV to be passed for compatibility
as to provide the hint that we support secure renegotiation.

Change-Id: I0850bea47568edcfb1f7df99d4e8a747f938406d/"
,,Conscrypt,"Toward EC TLS support

Summary:
- javax.net.ssl tests are now working on the RI
- KeyManager can now handle EC_EC and EC_RSA
- OpenSSLSocketImpl.startHandshake now works if KeyManager contains EC certificates

Details:

Add CipherSuite.getKeyType to provide X509KeyManager key type strings,
refactored from OpenSSLServerSocketImpl.checkEnabledCipherSuites.
getKeyType is now also used in OpenSSLSocketImpl.startHandshake to
avoid calling setCertificate for unnecessary key types.

   New CipherSuiteTest to cover new getKeyType as well as existing functionality

Add support to KeyManager implementation for key types of the form
EC_EC and EC_RSA. The first part implies the KeyPair algorithm (EC in
these new key types) with a potentially different signature algorithm
(EC vs RSA in these)

Update NativeCrypto.keyType to support EC_EC and EC_RSA in addition to
EC which was added earlier. Change from array of KEY_TYPES to named
KEY_TYPE_* constants.

Overhauled KeyManagerFactoryTest to cover EC, EC_EC, EC_RSA cases

Changed TestKeyStore.createKeyStore from always using BKS to now use
JKS on the RI between BC EC Keys and RI X509 certificates. Because JKS
requires a password, we now default ""password"" on the RI.

TestKeyStore.create now accepts key types like EC_RSA. Changed
TestKeyStore.createKeys to allow a PrivateKeyEntry to be specified for
signing to enable creation of EC_RSA test certificate. Added
getRootCertificate/rootCertificate to allow lookup of PrivateKeyEntry
for signing. Changed TestKeyStore.getPrivateKey to take explicit
signature algorithm to retrieve EC_EC vs EC_RSA entries.

Added support for EC cipher suites on the RI.  Also test with and
without new TLS_EMPTY_RENEGOTIATION_INFO_SCSV cipher suite which is
used to specify the new TLS secure renegotiation.

New TestKeyManager and additional logging in TestTrustManager. Logging
in both is disabled by default using DevNullPrintStream.

Bug: 3058375
Change-Id: Ia5e2a00a025858e10d1076b900886994b481e05a/"
Network Management,Network Management,Conscrypt,"resolved conflicts for merge of 5fc737eb to master

Change-Id: Ifc2a4fd44cef525709a3b9dc0a502b1a0690c6fd/Lots more bounds-checking/"
,,Conscrypt,"exception-throwing consistency.

Overflow-safe checks all round, plus better detail messages. This isn't
quite everything, but it's a large chunk of the work. Most notably, this
is all of io and nio.

There are numerous changes of exception priority here, and the harmony
tests noticed a subset of them in the nio code. I've modified our checked-out
copy of the tests to accept any of the throwable exceptions.

Change-Id: Id185f1228fb9a1d5fc9494e78375b5623fb0fe14/Rewrite all backwards comparisons.

Strictly, all the ones I could find. This is everything with 0 or null on the
left-hand side.

Note that this touches several incorrect bounds checks, which I haven't fixed:
I'm going to come back and finish that independent cleanup separately.

Change-Id: Ibdb054b53df9aace47c7d2a00ff19122190053e8/"
,,Conscrypt,"Toward EC TLS support

Summary:
- javax.net.ssl tests are now working on the RI
- KeyManager can now handle EC_EC and EC_RSA
- OpenSSLSocketImpl.startHandshake now works if KeyManager contains EC certificates

Details:

Add CipherSuite.getKeyType to provide X509KeyManager key type strings,
refactored from OpenSSLServerSocketImpl.checkEnabledCipherSuites.
getKeyType is now also used in OpenSSLSocketImpl.startHandshake to
avoid calling setCertificate for unnecessary key types.

   New CipherSuiteTest to cover new getKeyType as well as existing functionality

Add support to KeyManager implementation for key types of the form
EC_EC and EC_RSA. The first part implies the KeyPair algorithm (EC in
these new key types) with a potentially different signature algorithm
(EC vs RSA in these)

Update NativeCrypto.keyType to support EC_EC and EC_RSA in addition to
EC which was added earlier. Change from array of KEY_TYPES to named
KEY_TYPE_* constants.

Overhauled KeyManagerFactoryTest to cover EC, EC_EC, EC_RSA cases


Changed TestKeyStore.createKeyStore from always using BKS to now use
JKS on the RI between BC EC Keys and RI X509 certificates. Because JKS
requires a password, we now default ""password"" on the RI.

TestKeyStore.create now accepts key types like EC_RSA. Changed
TestKeyStore.createKeys to allow a PrivateKeyEntry to be specified for
signing to enable creation of EC_RSA test certificate. Added
getRootCertificate/rootCertificate to allow lookup of PrivateKeyEntry
for signing. Changed TestKeyStore.getPrivateKey to take explicit
signature algorithm to retrieve EC_EC vs EC_RSA entries.

New TestKeyManager and additional logging in TestTrustManager. Logging
in both is disabled by default using DevNullPrintStream.


Bug: 3058375
Change-Id: Ia5e2a00a025858e10d1076b900886994b481e05a/"
,,Conscrypt,"Remove useless android-changed comments.

I've changed useful ones to regular comments or TODOs, as appropriate.

I've left ones in code like java.util.concurrent where we really are
tracking an upstream source, making the change markers useful.

I've left a handful of others where I intend to actually investigate
the implied TODOs before deciding how to resolve them.

Change-Id: Iaf71059b818596351cf8ee5a3cf3c85586051fa6/"
,,Conscrypt,"Retire SecurityManager.

This change removes all the code that was calling getSecurityManager, and
removes all use of AccessController.doPrivileged. It also changes the
implementation of AccessController so it doesn't actually do anything; it's
only there for source-level compatibility.

Bug: 2585285
Change-Id: I1f0295a4f12bce0316d8073011d8593fee116f71/"
Network Management,Network Management,Conscrypt,"Make CertInstaller installed CA certs trusted by applications via default TrustManager (2 of 6)

frameworks/base

    Adding IKeyChainService APIs for CertInstaller and Settings use

libcore

    Improve exceptions to include more information

    Move guts of RootKeyStoreSpi to TrustedCertificateStore, leaving only KeyStoreSpi methods.
    Added support for adding user CAs in a separate directory for system.
    Added support for removing system CAs by placing a copy in a sytem directory

    Added test for NativeCrypto.X509_NAME_hash_old and X509_NAME_hash
    to make sure the implementing algorithms doe not change since
    TrustedCertificateStore depend on X509_NAME_hash_old (OpenSSL
    changed the algorithm from MD5 to SHA1 when moving from 0.9.8 to
    1.0.0)

    Extensive test of new TrustedCertificateStore behavior

    TestKeyStore improvements
    - Refactored TestKeyStore to provide simpler createCA method (and
      internal createCertificate)
    - Cleaned up to remove use of BouncyCastle specific X509Principal
      in the TestKeyStore API when the public X500Principal would do.
    - Cleaned up TestKeyStore support methods to not throw Exception
      to remove need for static blocks for catch clauses in tests.

    Added private PKIXParameters contructor for use by
    IndexedPKIXParameters to avoid wart of having to lookup and pass
    a TrustAnchor to satisfy the super-class sanity check.

    Change CertInstaller to call IKeyChainService.installCertificate
    for CA certs to pass them to the KeyChainServiceTest which will
    make them available to all apps through the
    TrustedCertificateStore. Change PKCS12 extraction to use AsyncTask.

    Added installCaCertsToKeyChain and hasCaCerts accessor for use by
    CertInstaller. Use hasUserCertificate() internally. Cleanup coding
    style.

packages/apps/KeyChain

    Added MANAGE_ACCOUNTS so that IKeyChainService.reset
    implementation can remove KeyChain accounts.

	AndroidManifest.xml

    Implement new IKeyChainService methods:
    - Added IKeyChainService.installCaCertificate to install certs
      provided by CertInstaller using the TrustedCertificateStore.
    - Added IKeyChainService.reset to allow Settings to remove the
      KeyChain accounts so that any app granted access to keystore
      credentials are revoked when the keystore is reset.

packages/apps/Settings

    Changed com.android.credentials.RESET credential reset action to
    also call IKeyChainService.reset to remove any installed user CAs
    and remove KeyChain accounts to have AccountManager revoke
    credential granted to private keys removed during the RESET.

    Added toast text value for failure case

	res/values/strings.xml

system/core

    Have init create world readable /data/misc/keychain to allow apps
    to access user added CA certificates installed by the CertInstaller.

	rootdir/init.rc

Change-Id: Ief57672eea38b3eece23b14c94dedb9ea4713744/"
,,Conscrypt,"Avoid loading all CA certs into Zygote memory, lazily load instead (2 of 3)

Previously the CA certs stored in the BKS KeyStore at
/system/etc/security/cacerts.bks was loaded in the Zygote. As the the
number of CAs are started to increase, this is causing more and more
memory to be used for rarely used CAs. The new AndroidCAStore KeyStore
implementation reads the CAs as needed out of individual PEM
certificate files. The files can be efficiently found because they are
named based on a hash CA's subject name, similar to OpenSSL.

Bug: 1109242

Details:

build

    Removing old cacerts.bks from GRANDFATHERED_ALL_PREBUILT and

libcore

    cacerts build changes. Move cacerts prebuilt logic to new
    CaCerts.mk from NativeCode.mk where it didn't make sense. Updated
    Android.mk's dalvik-host target to install new cacerts files.

	Android.mk
	CaCerts.mk
	NativeCode.mk

    Remove old cacerts.bks and add remove certimport.sh script used to
    generate it. Preserved the useful comments from certimport.sh in
    the new README.cacerts

    Recanonicalize cacerts files using updated vendor/google/tools/cacerts/certimport.py
    (See below discussion of certimport.py changes for details)

    Change IntegralToString.intToHexString to take width argument to
    allow for leading zero padding. Updated existing callers to
    specify 0 padding desired. Add testing of new padding
    functionality.

    Improved to throw Exceptions with proper causes

    Indentation fixes

	
    Fix X509CRLSelector.getIssuerNames to clone result and added test to cover this.

    Fixed bug where we created an X500Principal via a String
    representation instead of from its original encoded bytes. This
    led to a difficult to track down bug where CA 418595b9.0 where the
    NativeCode.X509_NAME_hash of a Harmony (but not BouncyCastle)
    X509Certificate would not hash to the expected value because the
    encoded form used an ASN.1 PrintableString instead of the
    UTF8String form found in the original certificate.

    Add a new RootKeyStoreSpi and register it as the
    AndroidCAStore. This new read-only KeyStore implementation that
    looks for certificates in $ANDROID_ROOT/etc/security/cacerts/
    directory, which is /system/etc/security/cacerts/ on devices. The
    files are stored in the directory based on the older md5 based
    OpenSSL X509_NAME_hash function (now referred to as
    X509_NAME_hash_old in OpenSSL 1.0)


    Added OpenSSL compatible X509_NAME_hash and X509_NAME_hash_old
    functions for producting an int hash value from an X500Principal.


    Changed TrustManagerImpl to be AndroidCAStore aware. If it detects
    an AndroidCAStore, it avoids generating the acceptedIssuers array
    at constructions, since doing so would force us to parse all
    certificates in the store and the value is only typically used by
    SSLServerSockets when requesting a client certifcate. Because we
    don't load all the trusted CAs into the IndexedPKIXParameters at
    startup in the case of AndroidCAStore, we now check for new CAs
    when examining the cert chain for unnecessary TrustAnchors and for
    a newly discovered issuer at the end of the chain before
    validation.

    Updated KeyStoreTest to cope with read only KeyStore. Update
    test_cacerts_bks (now renamed test_cacerts) to use the
    AndroidCAStore for validating system CA certificate
    validity. Register AndroidCAStore as an expected KeyStore type
    with StandardNames.

    Added test of X500Principal serialization while investigating Name
    encoding issue. However, the actual Name bug was found and
    verified by the new test_cacerts test.


vendor/google

    Change canonical format for checked in cacerts to have PEM
    certificate at the top, as required by Harmony's X.509
    CertificateFactory.

Change-Id: If0c9de430f13babb07f96a1177897c536f3db08d/"
,,Conscrypt,"Fix NativeCrypto FindBugs warnings.

Change-Id: I102367575b1257582bb20c659223e3f02650fda4/"
Network Management,Network Management,Conscrypt,"Make CertInstaller installed CA certs trusted by applications via default TrustManager (2 of 6)

frameworks/base

    Adding IKeyChainService APIs for CertInstaller and Settings use
	keystore/java/android/security/IKeyChainService.aidl

libcore

    Improve exceptions to include more information

    Move guts of RootKeyStoreSpi to TrustedCertificateStore, leaving only KeyStoreSpi methods.
    Added support for adding user CAs in a separate directory for system.
    Added support for removing system CAs by placing a copy in a sytem directory

    Formerly static methods on RootKeyStoreSpi are now instance methods on TrustedCertificateStore

    Added test for NativeCrypto.X509_NAME_hash_old and X509_NAME_hash
    to make sure the implementing algorithms doe not change since
    TrustedCertificateStore depend on X509_NAME_hash_old (OpenSSL
    changed the algorithm from MD5 to SHA1 when moving from 0.9.8 to
    1.0.0)

    TestKeyStore improvements
    - Refactored TestKeyStore to provide simpler createCA method (and
      internal createCertificate)
    - Cleaned up to remove use of BouncyCastle specific X509Principal
      in the TestKeyStore API when the public X500Principal would do.
    - Cleaned up TestKeyStore support methods to not throw Exception
      to remove need for static blocks for catch clauses in tests.

    Added private PKIXParameters contructor for use by
    IndexedPKIXParameters to avoid wart of having to lookup and pass
    a TrustAnchor to satisfy the super-class sanity check.

packages/apps/CertInstaller

    Change CertInstaller to call IKeyChainService.installCertificate
    for CA certs to pass them to the KeyChainServiceTest which will
    make them available to all apps through the
    TrustedCertificateStore. Change PKCS12 extraction to use AsyncTask.

    Added installCaCertsToKeyChain and hasCaCerts accessor for use by
    CertInstaller. Use hasUserCertificate() internally. Cleanup coding
    style.

packages/apps/KeyChain

    Added MANAGE_ACCOUNTS so that IKeyChainService.reset
    implementation can remove KeyChain accounts.



    Implement new IKeyChainService methods:
    - Added IKeyChainService.installCaCertificate to install certs
      provided by CertInstaller using the TrustedCertificateStore.
    - Added IKeyChainService.reset to allow Settings to remove the
      KeyChain accounts so that any app granted access to keystore
      credentials are revoked when the keystore is reset.


packages/apps/Settings

    Changed com.android.credentials.RESET credential reset action to
    also call IKeyChainService.reset to remove any installed user CAs
    and remove KeyChain accounts to have AccountManager revoke
    credential granted to private keys removed during the RESET.


    Added toast text value for failure case


system/core

    Have init create world readable /data/misc/keychain to allow apps
    to access user added CA certificates installed by the CertInstaller.

	rootdir/init.rc

Change-Id: Ief57672eea38b3eece23b14c94dedb9ea4713744/"
,,Conscrypt,"Don't cache the underlying Socket's underlying SocketImpl's underlying FileDescriptor in OpenSSLSocketImpl.

(OpenSSLSocketImpl, of course, being a Socket, not a SocketImpl.)

Bug: 4192414

Change-Id: I3c7d0fed70b1b98dc8fcc73f35b3feb0e1eeb2f9/"
,,Conscrypt,"Remove IndexedPKIXParameters

Change-Id: Idaaa1952d1b6148c51b3da5d1771105e8bde8a03/"
Network Management,Network Management,Conscrypt,"Don't trigger a reverse DNS lookup from a log statement.

Also nuke a bunch of redundant Javadoc and promote the
shutdownInput/shutdownOutput methods that always throw
to SSLSocket.

Change-Id: I077f7413bb6cba66be6204c68f7911b51a191643
http://code.google.com/p/android/issues/detail?id=13117
http://b/3478027/"
,,Conscrypt,"Add OpenSSL KeyPairGenerator and KeyFactory

Refactor the way OpenSSL keys are handled so we can generate OpenSSL
keys with the KeyPairGenerator and KeyFactory and pass them around
without keeping the context in the OpenSSLSignature where it originated.

Change-Id: Ib66bd1914e241a240cd97b1ea37e8526998107d9/Add signature generation to OpenSSLSignature

Change-Id: I1203516d95a937edb48959146bbec64b338e4f1e/"
,,Conscrypt,"Add a way to clear stored trusted certificates.

Bug: 6009802

Update the TrustManagerImpl Api to allow clearing stored certificates.
This is needed so we can remove CAs when credential storage is updated.

Change-Id: I024f7e8b12b60ea0ee35d7f94280e0e3d6db039f/"
Network Management,Network Management,Conscrypt,"Tracking openssl-1.0.1

Bug: 6168278

Change-Id: I240d2cbc91f616fd486efc5203e2221c9896d90f/"
,,Conscrypt,"OpenSSL block ciphers, part 1

This implements the NativeCrypto piece necessary to do basic block
cipher operations. More work will need to be done to enable useful
modes.

This gives us the ability to replace BouncyCastle's ECB mode that it
bases the higher level CBC, CTR, etc modes on. Socket However, calling through
JNI to OpenSSL for 16-byte blocks for AES ends up being the same speed
as the Java implementation. ssl

Further enhancements to use large blocks during the JNI call should show
marked improvements in speed.

Change-Id: I594a6d13ce5101a1ef2877b84edaa5e5b65e1e71/"
,,Conscrypt,"Support in-memory HTTPS session caching for wrapped sockets.

Previously we couldn't reuse sessions with HttpsURLConnection
because the host was incorrect (getInetAddress returns null
for wrapped sockets) and because the compression method was
different (NULL vs. ZLIB).

This improves HttpsURLConnection /response time on
localhost from ~275ms to ~145ms (without connection pooling).

Change-Id: I97bc343326658690b00589c0c804c2378b91ae61/"
Network Management,Network Management,Conscrypt,"Support in-memory HTTPS session caching for wrapped sockets. 

Previously we couldn't reuse sessions with HttpsURLConnection
because the host was incorrect (getInetAddress returns null
for wrapped sockets) and because the compression method was
different (NULL vs. ZLIB). socket

This improves HttpsURLConnection /response time on
localhost from ~275ms to ~145ms (without connection pooling).

Change-Id: I97bc343326658690b00589c0c804c2378b91ae61/"
,,Conscrypt,"Add signatures to the OpenSSLProvider

Now that OpenSSLSignature is a full-fledged Signature provider, we can
add it to our OpenSSLProvider.

Change-Id: If8539acdf895082cef38eed97a706dbbcdff6853/"
,,Conscrypt,"NativeCrypto should honor timeout less than one second

Bug: http://code.google.com/p/android/issues/detail?id=29680
Change-Id: I4507a1e9fe37b1c095f7bb4d3e3a55d6d738f7ad/Tracking openssl-1.0.1b

Change-Id: I418a5b36670c6cc72e1e6cc29add950409f97f9f/Expose NPN in OpenSSL.

This is derived from costin's change Ib18da136cb628515d6909c438cd0809452d7058a.
It moves the protocols data to the AppData's callbacks so the memory can be
released when the handshake completes.

Change-Id: Id61feaa6f28250e393f5c8093688b099e92dce9c/"
,,Conscrypt,"Expose NPN in OpenSSL.

This is derived from costin's change Ib18da136cb628515d6909c438cd0809452d7058a.
It moves the protocols data to the AppData's callbacks so the memory can be
released when the handshake completes.

Change-Id: Id61feaa6f28250e393f5c8093688b099e92dce9c/"
,,Conscrypt,"Add logging to detect cert pin failures caused by MITM proxies.

Change-Id: Ie9554aaa824506a75534d888432ed8a91e14e386/Added basic cert pinning support.

This has four main changes:

First, it adds a CertPinManager to TrustManagerImpl that checks to
ensure that the chain is properly pinned.

Second, it adds the CertPinManager and associated classes to
implement cert pinning at this level.

Third, it changes the callers of checkServerTrusted to pass in a
hostname where possible, allowing them to make use of the pinning
transparently.

Finally, it changes checkServerTrusted to return the ultimate
chain that was verified, which is useful for implementing pinning
at a higher level.

cherry-picked from 5315f29b2de4aace0077b78f0b99634fda440b85

Change-Id: I150e010da3e2aeed57bd5330ff113d3a7fbbee2a/"
Network Management,Network Management,Conscrypt,"Fix OpenSSLSocketImpl.close race

Move the NativeCrypto.SSL_interrupt call within the close
synchronization. Otherwise there can be problems if
NativeCrypto_SSL_interrupt tries to use the SSL* and another thread
has called NativeCrypto_SSL_free.

Bug: 6707288
Change-Id: Id8b0311b10124f2a08f8e0f24595a6ee46805c33/"
,,Conscrypt,"Add raw RSA signature support

With the new Keystore changes, this is the only way you can get raw RSA
signatures which a lot of native code expects to be able to do.

(cherry-picked from c531f5f402b4cedcc35a0b7f0b540dc84c545106)

Bug: 6787078
Change-Id: I1c5ddd5287be1ab71347eedc864a41c24e156cb4/"
,,Conscrypt,"NativeCrypto: add EC key pairs

Change-Id: I8240df5ff12e38dd935258def099aed4663955ea/"
,,Conscrypt,"NativeCrypto: be more tolerant during translateKey

Change-Id: I284bdb4d037d511390a6beb8518eb9bf9c50f25f/"
,,Conscrypt,"use longs instead of ints to store pointers in OpenSSL binding

This allows the code to be used on 64-bit VMs.

Change-Id: I6c0ef28c55160186c7d59e88ef6fcde4f4a41907/"
,,Conscrypt,"NativeCrypto: add OpenSSL X.509 certificate/CRLs

Initial implementation of parsing X.509 certificates and certificate
revocation lists (CRLs). This lacks support for generating CertPath
objects, but that will be added at a later time. compression dictionary

This currently isn't the default provider so anything that doesn't
explicitly  this provider will not get this implementation.

Change-Id: I07ae9f333763087cb6ce47e20e12ceeed750920d/NativeCrypto: output named curves when possible

When converting to ASN.1 format from a named curve, try to make sure we
can output those named curves whenever possible instead of all the
parameters.

Also make sure we output in uncompressed point format for compatibility
with other implementations.

Change-Id: I3f370be694ac709f02e3043a2c1152ad4838ef41/"
,,Conscrypt,"OpenSSLKey: easier creation and use of OpenSSL keys

For some future changes, it will be easier to convert OpenSSL objects to
real objects and back from native pointers. Make it easier to add new
EVP_PKEY types without adding them in if/else statements everywhere.

Change-Id: I19095bfc5f00835a266f572bc62e2e8d0a8cd544/"
API Usage,API Usage,Conscrypt,"Invert pinning API name

The pinning API used a negative name for the API which made it difficult
to reason about logically. Change them to positive names so the returned
booleans don't need to be inverted in my head.

Change-Id: Iad89d9cec33b3ef27e80a3344a5b23dec023d636/"
,,Conscrypt,"Conscrypt: use certificate references in SSL code

Instead of marshalling and unmarshalling to ASN.1 DER, just use
references to OpenSSL X509 objects everywhere applicable.

Change-Id: I1a28ae9232091ee199a9d4c7cd3c7bbd1efa1ca4/"
,,Conscrypt,"Conscrypt: remove dependence on stlport

This helps with unbundling of Conscrypt by not forcing the app to
include a static version of stlport in their program.

Change-Id: I5bd17213059b8ae4d8d86921d82b43465253a62f/"
,,Conscrypt,"Properly refcount X509 instances

We were leaking X509 references from stacks before so we could get away
with reusing references that should have been freed. Since we're properly
tracking references now, we need to up the reference of things we're
using.

(cherry picked from commit 499f7cd642cc32f89f793fe356afbebeba8bf9c1)

Bug: 10610037
Change-Id: I4a4beda9b635881c51194410a6da8274c3c1d429/"
,,Conscrypt,"Fix BIO_OutputStream::write to return the correct length.

This was leaving bad OpenSSL error states lying around for later
innocent calls to trip over.

Also clean up some of the other error reporting/handling.

Bug: 9822466
Bug: 10344304
Change-Id: I9e6d6fd9a6c5e466336217b47f45c211aff5555d/"
,,Conscrypt,"Fix libcore's NativeCode.mk so we actually compile with -Werror.

Change-Id: Ib665ea7c6f54e43851bc04f0265e65218407c70f/"
Network Management,Network Management,Conscrypt,"Self-seed OpenSSL-backed SecureRandom from /dev/urandom.

OpenSSL-backed SecureRandom instances do not currently self-seed.
These instances are backed by OpenSSL's default RAND engine (SSLeay)
which initilizes itself only once per process from /dev/urandom. As a
result, these SecureRandom instances do not  any new entropy from
the Linux RNG when used.

This CL makes OpenSSL-backed SecureRandom instances  new entropy
from /dev/urandom into OpenSSL's RAND engine during the self-seeding
of the SecureRandom instances.

This is similar to how new entropy is ed into OpenSSL's RAND
engine from /dev/urandom by OpenSSLSocketImpl.

     benchmark     us linear runtime
 Before Change   21.3 ============================
  After Change  537.8 ==============================

Change-Id: I1d7467eac99b3627b64fbdb3e98be644581171bb/"
,,Conscrypt,"Conscrypt: add SHA-224 with tests

SHA-224 has made a comeback in the latest StandardNames documentation.
This change adds tests for SHA-224 and also Conscrypt providers for
things we have code paths to support.

Change-Id: I8c200082ff76ee4ae38b6efaa16e6741b33b7f5b/"
,,Conscrypt,"Do not throw exception on Mac#reset

OpenSSLMac was not checking whether it was initialized before
dereferencing its macKey field. This caused callers to Mac#reset()
before Mac#init() to get a NullPointerException.

Bug: https://code.google.com/p/android/issues/detail?id=58179
Change-Id: I8523983fec578079a66a04d585c1ddbe7732575e/"
,,Conscrypt,"Remove unsupported Cipher modes

OpenSSL silently ignores the padding modes when specified for stream
ciphers, but apparently Java does not.

Change-Id: Icd92122d63b3b8e99d704e8193414dda5057146d/Return IvParameters in OpenSSLCipher#getParameters

The getParameters() call was unimplemented in the OpenSSLCipher as an
oversight. Add it so code relying on it will continue to work.

Additionally add tests for getIV() and getParameters() to make sure they
work correctly.

(cherry picked from commit 8d59a14a150738b8b3a2a8c31d1a48b8ae0a3d0c)

Bug: 10423926
Change-Id: I6bc7fc540509242dff9e5411f66f82be54691cb4/"
Network Management,Network Management,Conscrypt,"Tidy up locking in OpenSSLSocketImpl.

We guard all state with a single lock ""stateLock"", which
replaces usages of ""this"" and ""handshakeLock"". We do not
perform any blocking operations while holding this lock.
In particular, startHandshake is no longer synchronized.

We use a single integer to keep track of handshake state
instead of a pair of booleans.

Also fix a bug in getSession, the previous implementation
wouldn't work in cut-through mode.

This fixes a deadlock in SSLSocketTest_interrupt.

Change-Id: I9aef991e0579d4094e287dde8e521d09d6468c51/"
,,Conscrypt,"Some cleanup while investigating test_SSLSocket_interrupt

Bug: 10681815
Change-Id: If9a76f4c55b578c6f135befebcc443ab9aef3073/"
,,Conscrypt,"Remove unsupported Cipher modes

OpenSSL silently ignores the padding modes when specified for stream
ciphers, but apparently Java does not.

Change-Id: Icd92122d63b3b8e99d704e8193414dda5057146d/Conscrypt: add SHA-224 with tests

SHA-224 has made a comeback in the latest StandardNames documentation.
This change adds tests for SHA-224 and also Conscrypt providers for
things we have code paths to support.

Change-Id: I8c200082ff76ee4ae38b6efaa16e6741b33b7f5b/"
,,Conscrypt,"Register Conscrypt as the AlgNameMapper source

Conscrypt was moved out of libcore, so the call directly to NativeCrypto
was removed as well. To break the dependency, introduce an interface
that Conscrypt registers as to answer algorithm name to OID mapping
queries and vice versa.

(cherry picked from commit 6fcfb5a75dfb595ccbcf0a7a576ee7515fe2da32)

Bug: 10310296
Change-Id: Ia9c802f1102df7209749a90d0ed3ed2831480b04/"
,,Conscrypt,"Support user-installed CA certs for cert pinning.

Additionally expose new isUserAddedCertificate() so clients can set policy
for user-installed CA certs.

Bug: 11257762
Change-Id: If45cd452ab76f393660b34594dcae464af0c0696/"
Network Management,Network Management,Conscrypt,"BEAST attack mitigation for OpenSSL-backed SSLSockets.

This enables 1/n-1 record splitting for SSLSocket instances backed by
OpenSSL.

OpenSSL change: https://android-review.googlesource.com/#/c/69253/

Bug: 11514124
Change-Id: I3fef273edd417c51c5723d290656d2e03331d68a/"
Network Management,Network Management,Conscrypt,"AArch64: Use long for pointers in Java sources.

Fixing some mistakes in the JNI signatures: some pointers were passed
via jint rather than jlong.

Change-Id: I6120cc5742c8429a9e0fddda715b5169d820d31a
Signed-off-by: Marcus Oakland <marcus.oakland@arm.com>/Stop depending on CipherSuite in OpenSSL-backed sockets.

This is in preparation for removing Harmony-backed TLS/SSL
implementations.

Change-Id: Ic108e16d086fb99b69f0a4e4faeb816dc50a7643/"
,,Conscrypt,"Stop depending on SSLContextImpl in OpenSSLContextImpl.

SSLContextImpl is the HarmonyJSSE provider's SSLContext SPI.
OpenSSLContextImpl is the AndroidOpenSSL provider's SSLContext SPI.
This CL adjusts the class hierarchy to match.

This is achieved by:
1. copying all of the functionality from SSLContextImpl into
  OpenSSLContextImpl, and
2. removing from SSLContextImpl the functionality used only by the
  default instance of AndroidOpenSSL provider's SSLContext.

Change-Id: I9e380be04e6a9a1660c3e6c0738ca026c171f4bd/"
,,Conscrypt,"OpenSSLKey: tolerate null encoding during conversion

Since we could have a situation where we have an opaque key backed by
some hardware device that we don't know how to handle, just throw an
InvalidKeyException instead of NullPointerException.

Change-Id: I33588d1654b6b33f11640b2d65e7213c864e6e1a/"
Network Management,Network Management,Conscrypt,"Remove HarmonyJSSE SSLContext, SSLSocket and SSLServerSocket.

HarmonyJSSE SSLEngine implementation is still in use and thus cannot be
removed.

Change-Id: I3c939e9275ba8f1d00342d1f83c6fdaf110f2317/SSLEngineImpl: fix DHE with client certs

If DHE-based key exchanges were selected and there was no matching
client certificate selected from X509ExtendedKeyManager, the array would
be zero-length and crash.

If the client and server certificates did not have DH public keys, the
client key exchange would never be created and the server would get a
change cipher spec unexpectedly.

Change-Id: Ie23b43f4de65e650658c0fb2931e4c1396c136bf/"
Network Management,Network Management,Conscrypt,"Remove HarmonyJSSE SSLContext, SSLSocket and SSLServerSocket.

HarmonyJSSE SSLEngine implementation is still in use and thus cannot be
removed.

Change-Id: I3c939e9275ba8f1d00342d1f83c6fdaf110f2317/"
,,Conscrypt,"Random cleanups of old code style

Add @Override annotation, remove unused imports, and remove unnecessary
casts. Also make sure annotations are on a line by themselves.

Change-Id: I294b43353d7b1e77fd1c9d031af7b7062f024eee/"
,,Conscrypt,"X509Certificate: SignatureException for verify

Any verification error can throw random things like BadPaddingException.
Swallow it and catch Exception for all these cases and rethrow as a
SignatureException to avoid acting as any kind of oracle.

Change-Id: I6b515148f86529fbe0895c9fdb0954306724ae54/"
,,Conscrypt,"OpenSSLX509Certificate: negative serial numbers

The constructor BigInteger(byte[]) expects two's complement encoding,
but that's not what OpenSSL bn2bin returns.

Bug: 12761797
Change-Id: I6c71f6fb88c2b1df7c372bf697728dac26571634/"
Network Management,Network Management,Conscrypt,"Deprioritize HMAC-MD5 in default TLS/SSL cipher suites.

Although HMAC-MD5 is not yet broken, the foundations are now much
more shaky that those of HMAC-SHA.
See http://tools.ietf.org/html/rfc6151.

This CL also adds a comment about the key rules governing the
preference order of cipher suites used by default.

Bug: 11220570
Change-Id: I2a2fe4d427650081637efc14fd7c427a33cbea7e/Prefer Forward Secrecy TLS/SSL cipher suites by default.

This modifies the list of TLS/SSL cipher suites used by default to
prefer those offering Forward Secrecy (FS) -- ECDHE and DHE.

Bug: 11220570
Change-Id: I20f635d11e937d64de4f4e2fea34e1c5ea7a67ac/Deprioritize RC4-based TLS/SSL cipher suites.

Now that BEAST and Lucky13 mitigations are enabled, it is prudent to
prefer AES CBC cipher suites over RC4 ones
(see http://www.isg.rhul.ac.uk/tls/).

Bug: 11220570
Change-Id: I52b9724700fd8eaeebbadcfa518a96823a1410b8/"
,,Conscrypt,"Adjust the default list of ciphers suites of SSLEngine.

SSLEngine should use the same cipher suites by default as SSLSocket
whose list was recently cleaned up. One complication is that the
current SSLEngine implementation does not support EC.

This CL removes cipher suites with bulk encryption cipher key length
shorter than 80 bits and adds 256-bit versions of AES cipher suites.

Bug: 11220570
Change-Id: I81fb34d8067a8565c0ae11883bb5c1ee65ed6875/"
,,Conscrypt,"Remove unnecessary throws CertificateException from isUserAddedCertificate.

Change-Id: If825391c86f7b03fbea42dd6da7700c752d156d7/Support user-installed CA certs for cert pinning.

Additionally expose new isUserAddedCertificate() so clients can set policy
for user-installed CA certs.

Bug: 11257762
Change-Id: If45cd452ab76f393660b34594dcae464af0c0696/"
,,Conscrypt,"Proper check for PrivateKey algorithm.

The correct way to check for the type of a PrivateKey is to inspect
the result of its getAlgorithm, rather than to check that the key is
instance of [RSA/DSA/EC]PrivateKey. For example, opaque RSA private
keys are instances of PrivateKey but not instances of RSAPrivateKey.

See Java PKCS#11 Reference Guide section 3.2 ""Token Keys"".

Change-Id: I2502b264fa87ccad747cd6fe41da3a18c5d01864/"
,,Conscrypt,"SSLEngine: record splitting for BEAST mitigation

Split the initial encrypted packet sent into 1 byte and n-1 bytes to
mitigate BEAST attacks.

Bug: 11463939
Change-Id: Id81920fb2ede4172ae3565303d215b776091afc8/"
Feature migration,Feature migration,Conscrypt,"Track update to OpenSSL 1.0.1f

The constants for handshake cutthrough and CBC record splitting were changed
during the upgrade to OpenSSL 1.0.1f. This changes NativeCrypto.java to track them.

Change-Id: I9e385c323d5557c5d50cffe3ce797dcf89667ad9/AArch64: Use long for pointers in Java sources.

Fixing some mistakes in the JNI signatures: some pointers were passed
via jint rather than jlong.

Change-Id: I6120cc5742c8429a9e0fddda715b5169d820d31a
Signed-off-by: Marcus Oakland <marcus.oakland@arm.com>/"
,,Conscrypt,"Actually prefer Forward Secrecy cipher suites.

The documentation for the list of TLS/SSL cipher suites used by
default states that cipher suites offering Forward Secrecy are
preferred. This CL adjusts the list to conform: FS cipher suites
that use RC4_128 bulk encryption algorithm were not preferred
over non-FS cipher suites that use AES.

Bug: 11220570
Change-Id: Ic9019306898600086920874474764186b710c3ef/"
,,Conscrypt,"Enable AES-GCM cipher suites by default in SSLSocket.

AES-GCM is preferred to AES-CBC whose MAC-pad-then-encrypt approach
has issues (e.g., Lucky 13 attack).

Bug: 11220570
Change-Id: Ib007bc89ccf08358ed3f093f630350fa859e7c35/"
,,Conscrypt,"Enable support for TLSv1.2 cipher suites in SSLSocket.

This adds support for AES-GCM and AES-CBC with MACs based on SHA256
and SHA384.

Bug: 11220570
Change-Id: I56e7e25c5cd65a4c7662da6d4bbe5720f427e677/"
,,Conscrypt,"Enable TLSv1.1 and TLSv1.2 by default for SSLSocket.

TLSv1.1 and TLSv1.2 offer built-in protection against BEAST attack
and support for GCM cipher suites.

This change causes TLS/SSL handshake failures with a small fraction
of servers, load balancers and TLS/SSL accelerators with broken
TLS/SSL implementations.

Scans demonstrate that the number is around 0.6%. Breaking
connectivity (using platform default settings) to a tiny minority of
the ecosystem is acceptable because this inconvenience is outweighed
by the added safety for the overwheling majority of the ecosystem.

App developers affected by this issue should consider asking such
servers to be fixed or explicitly disabling TLSv1.1 and TLSv1.2 in
their apps.

Bug: 11220570
Change-Id: Ice9e8ce550401ba5e3385fd369c40f01c06ac7fd/"
,,Conscrypt,"Stop depending on CipherSuite in OpenSSL-backed sockets.

This is in preparation for removing Harmony-backed TLS/SSL
implementations.

Change-Id: Ic108e16d086fb99b69f0a4e4faeb816dc50a7643/"
,,Conscrypt,"Deprioritize HMAC-MD5 in default TLS/SSL cipher suites.

Although HMAC-MD5 is not yet broken, the foundations are now much
more shaky that those of HMAC-SHA.
See http://tools.ietf.org/html/rfc6151.

This CL also adds a comment about the key rules governing the
preference order of cipher suites used by default.

Bug: 11220570
Change-Id: I2a2fe4d427650081637efc14fd7c427a33cbea7e/"
,,Conscrypt,"Prefer Forward Secrecy TLS/SSL cipher suites by default.

This modifies the list of TLS/SSL cipher suites used by default to
prefer those offering Forward Secrecy (FS) -- ECDHE and DHE.

Bug: 11220570
Change-Id: I20f635d11e937d64de4f4e2fea34e1c5ea7a67ac/"
,,Conscrypt,"Deprioritize RC4-based TLS/SSL cipher suites.

Now that BEAST and Lucky13 mitigations are enabled, it is prudent to
prefer AES CBC cipher suites over RC4 ones
(see http://www.isg.rhul.ac.uk/tls/).

Bug: 11220570
Change-Id: I52b9724700fd8eaeebbadcfa518a96823a1410b8/"
Network Management,Network Management,Conscrypt,"AArch64: Use long for pointers in Java sources.

Fixing some mistakes in the JNI signatures: some pointers were passed
via jint rather than jlong.

Change-Id: I6120cc5742c8429a9e0fddda715b5169d820d31a
Signed-off-by: Marcus Oakland <marcus.oakland@arm.com>/"
,,Conscrypt,"Use SNI hostname for session caching

The session caching wasn't paying attention to the ed SNI
hostname when finding cached sessions. This checks the ed SNI
hostname in an attempt to get the correct hostname from the cache.

Change-Id: If3dbc64f11377a615389de9774c4061d1c92b997/"
,,Conscrypt,"Use the new endpointVerificationAlgorithm API

Use the new X509ExtendedTrustManager and use the new
getEndpointVerificationAlgorithm to check the hostname during the
handshake.

Bug: 13103812
Change-Id: Id0a74d4ef21a7d7c90357a111f99b09971e535d0/"
Network Management,Network Management,Conscrypt,"Add OpenSSLEngineImpl

Add support for SSLEngine via OpenSSL APIs. Currently this supports just
the basic SSLEngine functionality. It can be improved in efficiency and
performance, but it appears not to leak anything and be correct
according to our test suites.

Change-Id: Iea2dc3922e7c30e26daca38361877bd2f88ae668/"
,,Conscrypt,"Refactor OpenSSLSocketImpl

Move functionality that will be shared with OpenSSL's SSLEngine
implementation out of OpenSSLSocketImpl and into the (soon-to-be) shared
SSLParametersImpl.

The functionality should stay the same.

Change-Id: If8faa3ad2c9c73c0a0cd4b9716639b362b2b26a1/"
,,Conscrypt,"Convert calls to BIO_free to BIO_free_all

If we have a chain of BIO, we want to free the entire chain. Otherwise,
we might accidentally leave references sitting around. This shouldn't
matter for our current use-case, but might help in the future.

Change-Id: I586937629e1e4f2e80b5feefe2f49a85e8a31d31/"
,,Conscrypt,"ALPN: change socket calls to SSL_set_alpn_protos

Calling SSL_CTX_set_alpn_protos appears to be detrimental to thread
safety since the implementation of it resets the values. It's not
idempotent to call it multiple times like SSL_CTX_enable_npn.

Bug: https://code.google.com/p/android/issues/detail?id=67940
Change-Id: I09ed9e75d08528300b86201c3e847b26702d4284/"
,,Conscrypt,"Fix up concurrent use of APIs

Code that is incorrectly using MessageDigest, Signature, or Mac in
multiple threads simultaneously could cause a SEGV if OpenSSL is
clearing out the MD_CTX at the same time another thread is trying to
write to it. Make sure we initialize a new MD_CTX after each run to
avoid crashing. The program using the instances concurrently is still
wrong and will most likely get inconsistent results.

Switch to using a context object instance to make sure we can hold a
reference to the object during the native call.

Bug: 8787753
Change-Id: I2518613a47cf03c811a29d17040804fc708394dd/"
,,Conscrypt,"Throw instead of segfaulting when NULL EVP_PKEY encountered.

Change-Id: Idba6702dd43e541b51c990fc3440a17351e6def9/"
,,Conscrypt,"NativeCrypto: Handle 0-byte bignum arrays

Some DSA tests were calling with bignum arrays that had the high bit set
indicating a negative number.

Also an empty array was being passed as another part of the test. This
was working, but it was reading one byte past the end of the buffer.

Change-Id: Ibd5a0dce61703ea569fd483f8acf66fd149703f8/"
Network Management,Network Management,Conscrypt,"Make AppData creation symmetric

AppData was being created in SSL_do_handshake, but freed in SSL_free.
Make it symmetric by creating AppData in SSL_new instead.

The SSLEngine may call do_handshake multiple times to complete a
handshake, but this was creating an AppData each time it entered.
Creating in SSL_new avoids the problem of checking whether it was
already created on each entry into SSL_do_handshake calls.

Bug: 14247219
Change-Id: I825486798250998a4d4141201bda68a4dffe13a4/Add OpenSSLEngineImpl

Add support for SSLEngine via OpenSSL APIs. Currently this supports just
the basic SSLEngine functionality. It can be improved in efficiency and
performance, but it appears not to leak anything and be correct
according to our test suites.

Change-Id: Iea2dc3922e7c30e26daca38361877bd2f88ae668/ALPN: change socket calls to SSL_set_alpn_protos

Calling SSL_CTX_set_alpn_protos appears to be detrimental to thread
safety since the implementation of it resets the values. It's not
idempotent to call it multiple times like SSL_CTX_enable_npn.

Bug: https://code.google.com/p/android/issues/detail?id=67940
Change-Id: I09ed9e75d08528300b86201c3e847b26702d4284/Add JNI_TRACE_MD to cut down on noise

During start-up of vogar, it does thousands of digests on the input
class files which makes the output really noisy. Since debugging MD
stuff is uncommon, just hide it behind another debug flag.

Change-Id: I972a1b61c6ffe2d4cc345b089f0be10751ea32e4/"
,,Conscrypt,"Throw SSLHandshakeException for errors during handshake

This is a subclass of SSLHandshake, so it's not technically any
different, but more sophisticated clients use this to differentiate
between a failure during handshake and a general SSL failure.

Bug: 13130968
Change-Id: Ifad026c9af6748c1f7cb6a75f8f49aa3e75deea8/"
,,Conscrypt,"Return SSL_TLSEXT_ERR_NOACK with no NPN/ALPN

We were returning SSL_TLSEXT_ERR_OK even if we did not select any
NPN/ALPN support.

Bug: https://code.google.com/p/android/issues/detail?id=66562
Change-Id: I79ea821512f03f1391247d3bcfc7ac7d042ecb41/Fix up concurrent use of APIs

Code that is incorrectly using MessageDigest, Signature, or Mac in
multiple threads simultaneously could cause a SEGV if OpenSSL is
clearing out the MD_CTX at the same time another thread is trying to
write to it. Make sure we initialize a new MD_CTX after each run to
avoid crashing. The program using the instances concurrently is still
wrong and will most likely get inconsistent results.

Switch to using a context object instance to make sure we can hold a
reference to the object during the native call.

Bug: 8787753
Change-Id: I2518613a47cf03c811a29d17040804fc708394dd/"
,,Conscrypt,"Throw ArrayIndexOutOfBoundsException instead of generic

This exception is specifically for arrays which is what we're dealing
with here.

Change-Id: I11be2c75019844701b305240152815d7c610fbef/"
,,Conscrypt,"Harden (EC)DSA signatures against weak nonces.

Private key information is leaked by (EC)DSA signatures when nonces
are produced by a weak RNG. This CL enables a mitigation provided by
OpenSSL: mix in private key and message being signed into randomly
generated nonce. Provided private key was generated by strong RNG,
this should mitigate the weakness.

NOTE: This mitigation is not implemented for signatures which use
hardware-backed private keys (AndroidKeyStore).

Change-Id: I60dbf57bff3cfcdcbbeb18be5d9dfba523cc6bb8/"
,,Conscrypt,"BIGNUM convert to Java BigInteger

Java BigInteger is in two's complement, so it needs conversion for
negative numbers. We were mishandling it before and the previous change
just hacked around it. Actually convert to two's complement instead.

Change-Id: I6bfe9577f0936678476193b55433b7d7dbc04400/"
,,Conscrypt,"Remove SSLEngineImpl

This is replaced by OpenSSL-backed SSLEngineImpl.

Change-Id: I7b51f6fa772e431c6283008535bfec90821d0bef/"
,,Conscrypt,"OpenSSLSignature: refactor key checking

Use OpenSSLKey to do the conversions from different key types.

Change-Id: Ie89730bba983cb5f2917fed7194e8b08562f6e16/"
,,Conscrypt,"Leave SSLParametersImpl.getDefaultX509TrustManager public.

I renamed this method from getDefaultTrustManager to
getDefaultX509TrustManager and erroneously made it private in
8d63ff1384e46407a7618df2b79b2b455795c396. I missed the fact that
it's being used from framework's
android.net.http.CertificateChainValidator.

This CL reverts this method to being public again.

Bug: 13563574
Change-Id: I601c651d631f5a2e4a04d21941186553988e5286/"
,,Conscrypt,"Support TLS/SSL without X509TrustManager or X509KeyManager.

This makes TLS/SSL primitives operate as expected when no
X509TrustManager or X509KeyManager is provided. Instead of blowing up
with KeyManagementException or NullPointerException (or similar) when
X509TrustManager or X509KeyManager is not provided, this CL makes
SSLContext.init accept such setup, and makes SSLSocket and SSLEngine
reject certificate chains, select no private keys/aliases, and accept
no certificate issuers.

Bug: 13563574
Change-Id: I8de58377a09025258357dd4da9f6cb1b6f2dab80/"
,,Conscrypt,"Late binding: convert OpenSSLSignature to late binding

You must be a child of SignatureSpi to do late binding correctly.

Also remove useless test.

Change-Id: I4190ec919ad0eca9f344a2d7ac4c03216dccab55/OpenSSLSignature: refactor key checking

Use OpenSSLKey to do the conversions from different key types.

Change-Id: Ie89730bba983cb5f2917fed7194e8b08562f6e16/"
Network Management,Network Management,Conscrypt,"SSLEngine: fix some behaviors

* We were not checking buffer lengths.

* wrap/unwrap should start a handshake.

Change-Id: I35fbd8bf5eb699923f4712e7590bce7e7e13e529/"
,,Conscrypt,"ALPN: change socket calls to SSL_set_alpn_protos

Calling SSL_CTX_set_alpn_protos appears to be detrimental to thread
safety since the implementation of it resets the values. It's not
idempotent to call it multiple times like SSL_CTX_enable_npn.

Bug: https://code.google.com/p/android/issues/detail?id=67940
Change-Id: I09ed9e75d08528300b86201c3e847b26702d4284/Use the new endpointVerificationAlgorithm API

Use the new X509ExtendedTrustManager and use the new
getEndpointVerificationAlgorithm to check the hostname during the
handshake.

Bug: 13103812
Change-Id: Id0a74d4ef21a7d7c90357a111f99b09971e535d0/"
,,Conscrypt,"OpenSSLX509Certificate: only catch BadPaddingException

We only need to catch BadPaddingException right now. Let the other
non-RuntimeException exceptions pass.

Change-Id: I5b6878250d428b1ee953092967b7418003ee9216/"
Network Management,Network Management,Conscrypt,"Finish switching to android.system.Os.

Looks like I missed one last time...

Change-Id: Ib009e87493b36fc815166c44ce3c3a532aa5cd82/"
,,Conscrypt,"Track libcore.os' move towards the light.

Change-Id: Id41fb809eb764ce60f6d3cecf5715a57af432027/"
,,Conscrypt,"Add back missing sslSession

Accidentally removed during refactor.

Change-Id: I4295af935b269ec7ea91f1d1d140f32188e15e64/"
,,Conscrypt,"Use the new endpointVerificationAlgorithm API

Use the new X509ExtendedTrustManager and use the new
getEndpointVerificationAlgorithm to check the hostname during the
handshake.

Bug: 13103812
Change-Id: Id0a74d4ef21a7d7c90357a111f99b09971e535d0/Support TLS/SSL without X509TrustManager or X509KeyManager.

This makes TLS/SSL primitives operate as expected when no
X509TrustManager or X509KeyManager is provided. Instead of blowing up
with KeyManagementException or NullPointerException (or similar) when
X509TrustManager or X509KeyManager is not provided, this CL makes
SSLContext.init accept such setup, and makes SSLSocket and SSLEngine
reject certificate chains, select no private keys/aliases, and accept
no certificate issuers.

Bug: 13563574
Change-Id: I8de58377a09025258357dd4da9f6cb1b6f2dab80/"
,,Conscrypt,"Offer PKCS#7 padding for AES and DES.

This offers PKCS#7 padding for all Cipher transformations which
currently support PKCS#5 padding.

PKCS#5 padding is a special case of PKCS#7 padding. PKCS#5 padding
is defined specifically for 64 bit long blocks. However, lots of code
assumes that PKCS#5 for other block sizes works exactly like PKCS#7,
and thus uses PKCS#5 padding where PKCS#7 should actually be used
(e.g., with AES). The current implementation of PKCS#5 padding works
exactly like PKCS#7 padding. For backward-compatibility reasons, this
will remain unchanged by this CL and the same padding implementation
will simply be used regardless of whether PKCS#5 of PKCS#7 one is
ed.

As an added benefit, this change speeds up by an order of magnitude
AES encryption and decryption when PKCS#7 padding is ed on
Android. This is because prior to this change AES with PKCS#7 padding
was by default backed by Bouncy Castle, and with this change it is
backed by the much faster OpenSSL implementation.

Change-Id: I0ca8a952c67bc7aff172e22bd730378d41438067/"
Network Management,Network Management,Conscrypt,"Assert that the padding extension is enabled by default.

Change-Id: I1c8aa589e3274bfd3a5fc66c3e948828903c1966/"
,,Conscrypt,"Expose support for TLS-PSK.

TLS-PSK (Pre-Shared Key) is a set of TLS/SSL cipher suites that use
symmetric (pre-shared) keys for mutual authentication of peers. These
cipher suites are in some scenarios more suitable than those based on
public key cryptography and X.509. See RFC 4279 (Pre-Shared Key
Ciphersuites for Transport Layer Security (TLS)) for more information.

OpenSSL currently supports only the following PSK cipher suites:
* TLS_ECDHE_PSK_WITH_AES_128_CBC_SHA256
* TLS_ECDHE_PSK_WITH_AES_256_CBC_SHA384
* TLS_PSK_WITH_3DES_EDE_CBC_SHA
* TLS_PSK_WITH_AES_128_CBC_SHA
* TLS_PSK_WITH_AES_256_CBC_SHA
* TLS_PSK_WITH_RC4_128_SHA

The last four cipher suites mutually authenticate the peers and
secure the connection using a pre-shared symmetric key. These cipher
suites do not provide Forward Secrecy -- once the pre-shared key is
compromised, all previous communications secured with that key can be
decrypted. The first two cipher suites combine the pre-shared
symmetric key with an ephemeral key obtained from an ECDH key
exchange performed during the TLS/SSL handshake, thus providing
Forward Secrecy.

Users of TLS-PSK are expected to provide an implementation of
PSKKeyManager to SSLContext.init and then enable at least one PSK
cipher suite in SSLSocket/SSLEngine.

Bug: 15073623
Change-Id: I8e59264455f980f23a5e66099c27b5b4d932b9bb/"
,,Conscrypt,"SSL: also allow calls to read/write after cutthrough

Also add test to make sure this works.

Bug: 14832989
Change-Id: I046111cdcc4086a7104d462696078a767e86b12c/"
Network Management,Network Management,Conscrypt,"reconcile aosp (e79c25bf33e10da41e489c537823f678e1a1169c) after branching. Please do not merge.

Change-Id: I39ab275cd9744ba442fee7db9038107b4603526f/"
,,Conscrypt,"DHKeyPairGenerator: use provided params

If the prime is provided in the DHParameterSpec, then use it to generate
the key.

Bug: 16188130
Change-Id: I42de02c71a58d691ef7ba6e2252367105687b758/"
,,Conscrypt,"Add ability to wrap platform keys

This is mostly useful for unbundled Conscrypt currently when working
with KeyChain-based keys, but could be good for use with PKCS11-like
keys in other JSSE providers.

Bug: 15469749
Change-Id: I56bf2eaf3228bdf42d671437f4fffdafb8b47b12/"
,,Conscrypt,"Add more debugging for getting methods

When JNI registration fails, we should log it immediately to help
with debugging. Otherwise, it will tell you that you called a JNI
function with an exception pending.

Change-Id: I7cbba4d6639265a79a9d043d120f1a2bf72a85f7/"
,,Conscrypt,"Unbundle: hacks to let Conscrypt compile standalone

This is the first pass at getting Conscrypt to compile standalone. It
works fine in apps currently. There are a few TODOs to fix.

Change-Id: I9b43ba12c55e04c8897ccacf38979ca671a55a26/"
,,Conscrypt,"NativeCryptoTest: fix shutdown test

These weren't actually testing that the exceptions were thrown before.
Since we actually throw now, make sure we're throwing the expected
exception type.

Change-Id: I57b11492118dd7c04faa57c58de7b023294b179c/"
,,Conscrypt,"Fix of native crash in the evpUpdate method

The org.apache.harmony.security.tests.java.security.MessageDigest1Test
CTS test class's testSHAProvider method was causing a SIGSEGV when
""md.update(bytes, 1, -1);"" was called, as the evpUpdate method was not
checking for the inLength parameter being negative. This has been
rectified and the test now passes.

Bug: 14821275
Change-Id: I94489a518f7a2d4a6e84e58f91d8eee6f0ceb045
Signed-off-by: Marcus Oakland <marcus.oakland@arm.com>/"
,,Conscrypt,"DH keys: derive public key if not available

Also make the params mutex when we're inflating from a serial object
since it will be null otherwise.

Change-Id: I36641725161c0a708ba303500acca368b0511abe/"
,,Conscrypt,"Check for renegotiate_pending for tests

Tests call SSL_renegotiate to force a renegotiation, but was relying on
AppData being unset in this function. Instead we check that both
SSL_is_init_finished is false and SSL_renegotiation_pending is false.
Renegotiation is handled by SSL_write implicitly instead of explicitly
like the wrapper around SSL_do_handshake does.

Change-Id: I7e761afa718503933334cc19fbc696d714eca500/"
,,Conscrypt,"Add DH keys

Add the initial steps for DH keys to be generated and handled via
OpenSSL. Next steps will be hooking it up via other APIs that use DH
keys.

Change-Id: Ib159e60db73f82b75e0ba375a1d165c51286edac/"
,,Conscrypt,"SSLSocket: restore previous pre-handshake behavior

Before AppData was created in the initial handshake, calling SSL_read or
SSL_write would have a NULL appData field. This caused an exception to
be thrown. Now we have to check to make sure the handshake completed
before we continue on with SSL_read and SSL_write.

Change-Id: I969577cf56f61858450a7981a5196f58a6502968/"
,,Conscrypt,"DH keys: fix some errors the tests caught

* Returning ""DSA"" instead of ""DH"" for key algorithm

* Not having the key type defined as translatable in OpenSSLKey

Change-Id: I19db78ddb6d8697e758692bc4830fb32c8a0176a/"
Network Management,Network Management,Conscrypt,"Enable PSK cipher suites when PSKKeyManager is provided.

This enables TLS-PSK cipher suites by default iff SSLContext is
initialized with a PSKKeyManager. For consistency, X.509 based
cipher suites are no longer enabled by default at all times -- they
are now only enabled by default iff SSLContext is initialized with a
X509KeyManager or a X509TrustManager.

When both X.509 and PSK cipher suites need to be enabled, PSK cipher
suites are given higher priority in the resulting list of cipher
suites. This is based on the assumption that in most cases users of
TLS/SSL who enable TLS-PSK would prefer TLS-PSK to be used when the
peer supports TLS-PSK.

Bug: 15073623
Change-Id: I8e2bc3e7a1ea8a986e468973b6bad19dc6b7bc3c/Make setEnabledProtocols/"
,,Conscrypt,"CipherSuites copy their inputs.

SSLSocket, SSLServerSocket, and SSLEngine offer setEnabledProtocols
and setEnabledCipherSuites methods which take an array of protocols
or cipher suites as input. If these methods store references to the
input arrays, then the internal state (lists of enabled protocols and
cipher suites) of SSLSocket, SSLServerSocket, and SSLEngine could be
modified without going through the setter methods of these classes.

Bug: 15753142
Change-Id: Ia5248050d81320ed1da99892278bd60872605f52/"
,,Conscrypt,"SSLParametersImpl is the source of enabled cipher suites and protocols.

An instance of SSLParametersImpl is associated with SSLContext and is
then cloned into any SSLSocketFactory, SSLServerSocketFactory,
SSLSocket, SSLServerSocket, and SSLEngine. This CL ensures that all
these primitives obtain their list of enabled cipher suites and
protocols from their instance of SSLParametersImpl.

Bug: 15073623
Change-Id: I40bf32e8654b299518ec0e77c3218a0790d9c4fd/"
,,Conscrypt,"Expose support for TLS-PSK.

TLS-PSK (Pre-Shared Key) is a set of TLS/SSL cipher suites that use
symmetric (pre-shared) keys for mutual authentication of peers. These
cipher suites are in some scenarios more suitable than those based on
public key cryptography and X.509. See RFC 4279 (Pre-Shared Key
Ciphersuites for Transport Layer Security (TLS)) for more information.


The last four cipher suites mutually authenticate the peers and
secure the connection using a pre-shared symmetric key. These cipher
suites do not provide Forward Secrecy -- once the pre-shared key is
compromised, all previous communications secured with that key can be
decrypted. The first two cipher suites combine the pre-shared
symmetric key with an ephemeral key obtained from an ECDH key
exchange performed during the TLS/SSL handshake, thus providing
Forward Secrecy.

Users of TLS-PSK are expected to provide an implementation of
PSKKeyManager to SSLContext.init and then enable at least one PSK
cipher suite in SSLSocket/SSLEngine.

Bug: 15073623
Change-Id: I8e59264455f980f23a5e66099c27b5b4d932b9bb/"
,,Conscrypt,"Get rid of some warnings.

Change-Id: I87f3ad5374d89e8acfdd78fe5af4b02be483cd3d/"
,,Conscrypt,"Turn off verify peer for servers with no client auth

Since the default is now SSL_VERIFY_PEER, as a server we need to
explicitly set that we don't want a client certificate by setting
SSL_VERIFY_NONE.

Change-Id: I740389cc59ef8cb444a0e504838a1c0591df2bf9/"
,,Conscrypt,"Call SSL_set_alpn_protos with right native pointer

This change was missed during rebase of the OpenSSLEngine code since
this used to be SSL_CTX_set_alpn_protos.

Bug: 14273022
Change-Id: Ib72b27c8d5a4ddfde4e0c0ee2ab97bfb039c7f56/"
,,Conscrypt,"Use specific Charset for encoding

Without using a specific Charset, it will call System.getProperty to
find the current locale. We only care about getting the bytes for
US-ASCII so explicitly ask for that instead.

Change-Id: I6902b59ccb8a13a8977b828c099ad493e4f17e5c/"
,,Conscrypt,"OpenSSLMessageDigestJDK: support clone

Add the ability to clone the MessageDigest state.

Bug: 14821275
Change-Id: Ifa1b48db708448b971afe1e7360876f3fbe47588/"
Network Management,Network Management,Conscrypt,"Enable PSK cipher suites when PSKKeyManager is provided.

This enables TLS-PSK cipher suites by default iff SSLContext is
initialized with a PSKKeyManager. For consistency, X.509 based
cipher suites are no longer enabled by default at all times -- they
are now only enabled by default iff SSLContext is initialized with a
X509KeyManager or a X509TrustManager.

When both X.509 and PSK cipher suites need to be enabled, PSK cipher
suites are given higher priority in the resulting list of cipher
suites. This is based on the assumption that in most cases users of
TLS/SSL who enable TLS-PSK would prefer TLS-PSK to be used when the
peer supports TLS-PSK.

Bug: 15073623
Change-Id: I8e2bc3e7a1ea8a986e468973b6bad19dc6b7bc3c/"
,,Conscrypt,"Adjust the list of supported ECDHE-PSK cipher suites.

The SHA-2 based cipher suites cannot be used with SSLv3 but there is
no way to express that in OpenSSL's configuration. This CL thus
adjusts the list of supported cipher suites accordingly.

Bug: 15073623
Change-Id: I427c99f4c1c72690d95e5a3c63763631c41ddae2/"
,,Conscrypt,"Expose support for TLS-PSK.

TLS-PSK (Pre-Shared Key) is a set of TLS/SSL cipher suites that use
symmetric (pre-shared) keys for mutual authentication of peers. These
cipher suites are in some scenarios more suitable than those based on
public key cryptography and X.509. See RFC 4279 (Pre-Shared Key
Ciphersuites for Transport Layer Security (TLS)) for more information.

OpenSSL currently supports only the following PSK cipher suites:
* TLS_ECDHE_PSK_WITH_AES_128_CBC_SHA256
* TLS_ECDHE_PSK_WITH_AES_256_CBC_SHA384
* TLS_PSK_WITH_3DES_EDE_CBC_SHA
* TLS_PSK_WITH_AES_128_CBC_SHA
* TLS_PSK_WITH_AES_256_CBC_SHA
* TLS_PSK_WITH_RC4_128_SHA

The last four cipher suites mutually authenticate the peers and
secure the connection using a pre-shared symmetric key. These cipher
suites do not provide Forward Secrecy -- once the pre-shared key is
compromised, all previous communications secured with that key can be
decrypted. The first two cipher suites combine the pre-shared
symmetric key with an ephemeral key obtained from an ECDH key
exchange performed during the TLS/SSL handshake, thus providing
Forward Secrecy.

Users of TLS-PSK are expected to provide an implementation of
PSKKeyManager to SSLContext.init and then enable at least one PSK
cipher suite in SSLSocket/SSLEngine.

Bug: 15073623
Change-Id: I8e59264455f980f23a5e66099c27b5b4d932b9bb/Add DH keys

Add the initial steps for DH keys to be generated and handled via
OpenSSL. Next steps will be hooking it up via other APIs that use DH
keys.

Change-Id: Ib159e60db73f82b75e0ba375a1d165c51286edac/"
,,Conscrypt,"Keep enough state to completely reset cipher instances

OpenSSL's RC4 mutates the given key. AES/CTR mutates the IV. We must
store these values locally to enable ""doFinal"" to cause the Cipher
instance to be reset to what it was right after ""init"".

Note that resetting and encrypting with the same key or IV breaks
semantic security.

Bug: 16298401
Bug: https://code.google.com/p/android/issues/detail?id=73339
Change-Id: Ie7e4dcb6cf6cc33ddad31d6b47066dc1b34e6894/"
Network Management,Network Management,Conscrypt,"Added CLOSED_INBOUND and CLOSED_OUTBOUND states to OpenSSLEngineImpl#getHandshakeStatus()

Bug: https://code.google.com/p/android/issues/detail?id=73745
Change-Id: I5bcaf3ee8910ff75e785baed4c4604fee6c5e700/OpenSSLEngineImpl: fix unwrap behavior with array

The decrypted bytes should written sequentially into each buffer of
the destination array until it's full before moving to the next
buffer.

Change-Id: I2454249c167deafde6c12134d3c8cd658cd7c21b/Log CCS exceptions do not merge.

Unlike the previous CL, this uses reflection for android.os.Process and
android.util.EventLog throughout.

(cherry picked from commit 35b1f354ec2b647966a198ffed932d82eb8eeb5b)

Bug: 15452942
Change-Id: I34b9eaedf1f1e450b1f8004887bb0482601d789e/"
,,Conscrypt,"OpenSSLEngine Impl: Fix bug where SSL Handshake never completes when using NPN.

(cherry picked from commit e08f238580e8ee471012bef8240c8d3397c7b780)

Bug: 16352665
Change-Id: Idc78204b7077fb367b64e1867c807cd39f596f98/Various fixes in OpenSSLEngineImpl.

Fix ""Buffers were not large enough"" exception by directly using the
destination buffers.

Corrections around bytesProduced and bytesConsumed behavior.

Return BUFFER_OVERFLOW if a zero length destination is provided to
unwrap.

(cherry picked from commit bdfcc189efe41a3f812aeb55ea634bace67d159a)

Bug: 16352665
Change-Id: I1f1e9b72cd6968ed4f3c3c0edccbccebc33d6790/"
,,Conscrypt,"Various fixes in OpenSSLEngineImpl.

Fix ""Buffers were not large enough"" exception by directly using the
destination buffers.

Corrections around bytesProduced and bytesConsumed behavior.

Return BUFFER_OVERFLOW if a zero length destination is provided to
unwrap.

Change-Id: I1f1e9b72cd6968ed4f3c3c0edccbccebc33d6790/"
,,Conscrypt,"Log OpenSSL CCS errors

Bug: 15452942
Change-Id: I49e7bad6a65c70e113324c02fc23315cff168f5b/"
,,Conscrypt,"SSLEngine: handle EOF for our BIOs

If we reache EOF (really the end of our current bytes buffered for read)
during writing or reading, don't try to count the -1 returned as part of
the read bytes.

Change-Id: I76d42b00f14b121f1524e7c035efcf2c99627278/"
Exception Management,Exception Management,Conscrypt,"Log CCS exceptions do not merge.

Unlike the previous CL, this uses reflection for android.os.Process and
android.util.EventLog throughout.

(cherry picked from commit 35b1f354ec2b647966a198ffed932d82eb8eeb5b)

Bug: 15452942
Change-Id: I34b9eaedf1f1e450b1f8004887bb0482601d789e/"
,,Conscrypt,"Remove

(cherry picked from commit b860016f415dfc5655dcee45f70e8871a2e3edfe)

Change-Id: I4302ea4e0200ac80a0b9f3b953d58270b65b3d0c/"
,,Conscrypt,"Remove

Change-Id: Iea7c633eb68df576bf72314ff5ce31bc8094d9ce/"
,,Conscrypt,"Expose support for TLS-PSK.

TLS-PSK (Pre-Shared Key) is a set of TLS/SSL cipher suites that use
symmetric (pre-shared) keys for mutual authentication of peers. These
cipher suites are in some scenarios more suitable than those based on
public key cryptography and X.509. See RFC 4279 (Pre-Shared Key
Ciphersuites for Transport Layer Security (TLS)) for more information.

OpenSSL currently supports only the following PSK cipher suites:
* TLS_ECDHE_PSK_WITH_AES_128_CBC_SHA256
* TLS_ECDHE_PSK_WITH_AES_256_CBC_SHA384
* TLS_PSK_WITH_3DES_EDE_CBC_SHA
* TLS_PSK_WITH_AES_128_CBC_SHA
* TLS_PSK_WITH_AES_256_CBC_SHA
* TLS_PSK_WITH_RC4_128_SHA

The last four cipher suites mutually authenticate the peers and
secure the connection using a pre-shared symmetric key. These cipher
suites do not provide Forward Secrecy -- once the pre-shared key is
compromised, all previous communications secured with that key can be
decrypted. The first two cipher suites combine the pre-shared
symmetric key with an ephemeral key obtained from an ECDH key
exchange performed during the TLS/SSL handshake, thus providing
Forward Secrecy.

Users of TLS-PSK are expected to provide an implementation of
PSKKeyManager to SSLContext.init and then enable at least one PSK
cipher suite in SSLSocket/SSLEngine.

Bug: 15073623
Change-Id: I8e59264455f980f23a5e66099c27b5b4d932b9bb/Unbundle: hacks to let Conscrypt compile standalone

This is the first pass at getting Conscrypt to compile standalone. It
works fine in apps currently. There are a few TODOs to fix.

Change-Id: I9b43ba12c55e04c8897ccacf38979ca671a55a26/"
,,Conscrypt,"Validate hostname is usable for SNI

According to RFC 6066 section 3, the hostname listed in the Server Name
Indication (SNI) field is a fully qualified domain name and IP
addresses are not permitted.

Bug: 16658420
Bug: 17059757
Change-Id: I804e46b6e66599b2770f0f4f0534467987e51208/"
,,Conscrypt,"Read property to enable SNI

Read the system property ""jsse.enableSNIExtension"" on whether to enable
Server Name Indication (SNI) extension. For unbundled builds, this will
be enabled by default. For platform builds, this will be disabled by
default.

Bug: 16658420
Bug: 17059757
Change-Id: I774f5406bf3fe601a42c4ef5e708b31800147eb9/"
Network Management,Network Management,Conscrypt,"OpenSSLEngineImpl: reduce number of copies needed

When the ByteBuffer didn't line up exactly with the backing array, it
would allocate a new buffer to write into. Instead, add the ability for
OpenSSL to read at an offset in the given array so a copy isn't needed.

Change-Id: I149d3f94e4b5cbdc010df80439ae3300cbdc87a5/"
,,Conscrypt,"OpenSSLSocketImpl: Move state checks inside mutex

Checking the state of the connection is unreliable if SSL_read and
SSL_write are happening in another thread. Move the state checks inside
our application mutex so we don't run into another thread mutating the
state at the same time.

Bug: 15606096
Change-Id: I5ecdeb1551a13098d1b66c5e4009607c9951fa38/"
,,Conscrypt,"Revert ""Revert ""Automatic management of OpenSSL error stack""""

The ""else"" statement in OpenSslError::reset wasn't properly resetting
the error state which made a second call into sslRead jump into
sslSelect when it should have just returned immediately.

Change-Id: I22e8025c0497a04e78daa07cef78191a6ca1a70c/Fix debugging with unbundled conscrypt

When JNI_TRACE was enabled, there were missing defines for the debugging
code since no platform code is included.

Also clang complains about more of the debugging statement formats, so
we have to move some things around to get it to be happy.

Change-Id: I1a6695c2ef2639cc01cfc3d3a8603f010c659844/"
,,Conscrypt,"Revert ""Automatic management of OpenSSL error stack""

This reverts commit 35666e4cb0fcd063a21d17eebbb571b4e4e822b8.

Change-Id: I926d159c4c4b99250caef750732976c1e601e9ef/"
,,Conscrypt,"Automatic management of OpenSSL error stack

This removes some complexity in remembering to free the OpenSSL error
stack. If you forget, the error will stick around until you make another
call.

Change-Id: I245a525dcc93077b2bf9909a14a0ef469a2daca4/"
,,Conscrypt,"Fix some JNI_TRACE lines

During debugging these would be enabled, but they were copy-pasta'd to
with the wrong args.

Change-Id: I23f39ff4807e3fa71f3220912aec3c99db6b9454/"
,,Conscrypt,"Rename hostname fields and methods to reflect usage

The hostname that was supplied when the socket was created is stored as
the ""peerHostname""  This is the only one that should be used for Server
Name Indication (SNI) purposes.

The ""peerHostname"" or the resolved IP address may be used for
certificate validation, so keep the use of ""getHostname()"" for
cerificate validation.

Bug: 16658420
Bug: 17059757
Change-Id: Ifd87dead44fb2f00bbfd5eac7e69fb3fc98e94b4/"
,,Conscrypt,"Relax checks for key vs cert for wrapped keys

If a key is a wrapped platform key, we must relax the check. The reason
is that we may not have the public values we need to pass the
EVP_PKEY_cmp checks that this does.

Change-Id: I7ab2be51b0968a9cf771edea01d33fe2367c8185/"
,,Conscrypt,"Add support for TLS_FALLBACK_SCSV

(cherry picked from commit 8d7e23e117da591a8d48e6bcda9ed6f58ff1a375)

Bug: 17750026
Change-Id: Iaf437ce2bc2b0ae86bb90a67e6e5378b25ae0a81/"
,,Conscrypt,"Add support for TLS_FALLBACK_SCSV

Bug: 17750026
Change-Id: I1c2ecbeb914db645f440d58e7f7daa86d880ad6f/OpenSSLEngineImpl: reduce number of copies needed

When the ByteBuffer didn't line up exactly with the backing array, it
would allocate a new buffer to write into. Instead, add the ability for
OpenSSL to read at an offset in the given array so a copy isn't needed.

Change-Id: I149d3f94e4b5cbdc010df80439ae3300cbdc87a5/"
Network Management,Network Management,Conscrypt,"Implement write socket timeouts for unbundled apps

Change-Id: I4fd604f057ba4288d4f31bf6b3b93307376023d5/"
,,Conscrypt,"Tracking change from AOSP

Change-Id: I889af3f7c1de9ef34d9328339e1b421651055ad4/"
,,Conscrypt,"Use consistent naming for SSLSocket arguments

This changes all the 'host' to be 'hostname' and anything that takes an
'InetAddress' will have the name of 'address' to avoid confusing it with
a hostname.

Bug: 16658420
Bug: 17059757
Change-Id: Iac0628d2d156023dbb80c2e636af6bfe63f46650/"
,,Conscrypt,"Allow conscrypt to work with BoringSSL.

This is quite a substantial change because of the changes to ENGINEs in
BoringSSL.

For the most part, #ifs are used to allow the code to work with either
OpenSSL or BoringSSL. However, in several places, support for things
that BoringSSL is dropping have been removed, even when OpenSSL is used.
This includes DSA keys and tests for the ENGINE bits that are going away
because it's unclear how to skip compiling those tests.

Change-Id: I941a5ed232391f84b45e070c19d2ffb7ad162b7b/"
,,Conscrypt,"Fix null elements in X509KeyManager.chooseClientAlias keyTypes.

This fixes an issue where client certificate types ed by the
server from the client, but not known by the client, manifest
themselves as null elements in X509KeyManager.chooseClientAlias
keyTypes argument.

The root cause was that for each element in the
Certificate.certificate_types array an element was output into
the keyTypes array. For unknown values of certificate_type, a null
was output.

This CL fixes the issue by ignoring unknown values in
certificate_types array.

Bug: 18414726
Change-Id: I8565e19a610c0ecfb7cab1b7707c335e0eeb8d89/"
,,Conscrypt,"Switch EVP_CIPHER_CTX to new style

Bug: 16656908
Change-Id: Id519c20474a02c70e72d362bc84d26855a74fa33/"
,,Conscrypt,"Convert EVP_PKEY to new style

To avoid conflicts in the language spec and how Conscrypt does native
calls, we need to wrap all native references in a Java object reference.
Calling NativeCrypto's static native methods with a raw pointer doesn't
guarantee that the calling object won't be finalized during the method
running.

This pass fixes EVP_PKEY references, but more passes are needed.

Bug: 16656908
Change-Id: I5925da40cb37cd328b3a126404944f771732a43e/"
,,Conscrypt,"Remove support for DSS TLS/SSL cipher suites.

This is in preparation for migration from OpenSSL to BoringSSL.
BoringSSL does not support DSS.

DSS cipher suites are used by a vanishingly tiny fraction of the
Android ecosystem. In all cases, the server's SSL certificate is
self-signed (rather than CA issued), making it easy to switch to
a new self-signed certificate which is based on RSA or ECDSA.

Bug: 17409664
Change-Id: I91067ca9df764edd2b7820e5dec995f24f3910a1/"
Network Management,Network Management,Conscrypt,"Treat SSL_ERROR_ZERO_RETURN correctly.

According to ssl_lib.c, this is returned whenever the socket
is being closed (s->shutdown && SSL_RECEIVED_SHUTDOWN &&
s->s3->warn_alert == SSL_AD_CLOSE_NOTIFY).

Change-Id: Ied7b3e18f11786351d42a770f4cad11ddae29ff3/"
,,Conscrypt,"Go back to BIO_s_null instead of empty mem buf

If you pass NULL to BIO_new_mem_buf, it adds an error to the stack and
returns NULL instead of an actual BIO. Go back to BIO_s_null instead.

Bug: 18870062
Change-Id: Idba61a90907fbc2ea3528734b8cc9e27eccb1b50/"
,,Conscrypt,"external/conscrypt: sync to latest BoringSSL.

This change tweaks one thing because of changes to BoringSSL: RSA
methods now have a |supports_digest| member.

It also updates several bits of code in order to work with the recently
added -Wunused.

Change-Id: I1d1ad80b3471fbbe6fcc259e659e425d8129ace5/Clear SSL state safely

Since SSL_clear can fail, we should clear the OpenSSL ERR stack if it
does fail. However, to aid in spotting bugs, only clear the stack if the
SSL_clear itself fails.

(cherry picked from commit 86dd832ac26112890b3e815a144ff062ae9b3559)

Bug: 18570895
Change-Id: I053d2e2792e64923c1e128b4fcae23b2e660a992/Clear SSL state safely

Since SSL_clear can fail, we should clear the OpenSSL ERR stack if it
does fail. However, to aid in spotting bugs, only clear the stack if the
SSL_clear itself fails.

Bug: 18570895
Change-Id: I053d2e2792e64923c1e128b4fcae23b2e660a992/"
,,Conscrypt,"Convert EC_GROUP and EC_POINT to new style

Bug: 16656908
Change-Id: Ie912f376f69327ce634cac50763bf86b418049f5/"
,,Conscrypt,"Restore EVP_CIPHER_CTX_set_key_length

During the compatibility with BoringSSL change, this appears accidentally
removed without removing any of the references from NativeCrypto.java
or OpenSSLCipher.java

Change-Id: I7fe686b367994f127675b076ab49712767203f49/"
,,Conscrypt,"Fix JNI_TRACE

The update to BoringSSL broke some of the tracing messages, so fix their
formatting to compile correctly with warning on.

Change-Id: I6c7a1e0069b61a787d9e00b929a6c4fa4358a063/"
,,Conscrypt,"Convert EVP_MD_CTX to new style

To avoid conflicts in the language spec and how Conscrypt does native
calls, we need to wrap all native references in a Java object reference.
Calling NativeCrypto's static native methods with a raw pointer doesn't
guarantee that the calling object won't be finalized during the method
running.

Bug: 16656908
Change-Id: I165e041a8fe056770d6ce6d6cd064c411575b7c4/"
,,Conscrypt,"Fix mac build.

Change-Id: Ib7297bb0631caafed1ff04bcf2d73aea512c01c1/"
,,Conscrypt,"OpenSSLRandom: restore parts for OpenSSL

BoringSSL reads /dev/urandom directly, so these calls aren't needed.
However, OpenSSL needs these calls in some instances to protect against
other things going wrong elsewhere.

Restore the previous code until BoringSSL is in the tree.

Change-Id: I55624e0d98b04e9f5411f69e13a70a78fa0c0d7f/Allow conscrypt to work with BoringSSL.

This is quite a substantial change because of the changes to ENGINEs in
BoringSSL.

For the most part, #ifs are used to allow the code to work with either
OpenSSL or BoringSSL. However, in several places, support for things
that BoringSSL is dropping have been removed, even when OpenSSL is used.
This includes DSA keys and tests for the ENGINE bits that are going away
because it's unclear how to skip compiling those tests.

Change-Id: I941a5ed232391f84b45e070c19d2ffb7ad162b7b/"
,,Conscrypt,"Squashed commit of changes from lmp-ub-dev

        Add pre-Honeycomb literal IP matching

        This will allow us to run this code on Gingerbread devices and others
        that don't have the InetAddress#isNumeric API.


        Read property to enable SNI

        Read the system property ""jsse.enableSNIExtension"" on whether to enable
        Server Name Indication (SNI) extension. For unbundled builds, this will
        be enabled by default. For platform builds, this will be disabled by
        default.

        Validate hostname is usable for SNI

        According to RFC 6066 section 3, the hostname listed in the Server Name
        Indication (SNI) field is a fully qualified domain name and IP
        addresses are not permitted.

        Rename hostname fields and methods to reflect usage

        The hostname that was supplied when the socket was created is stored as
        the ""peerHostname""  This is the only one that should be used for Server
        Name Indication (SNI) purposes.

        The ""peerHostname"" or the resolved IP address may be used for
        certificate validation, so keep the use of ""getHostname()"" for
        cerificate validation.
		
        OpenSSLEngineImpl: reduce number of copies needed

        When the ByteBuffer didn't line up exactly with the backing array, it
        would allocate a new buffer to write into. Instead, add the ability for
        OpenSSL to read at an offset in the given array so a copy isn't needed.

        OpenSSLSocketImpl: Move state checks inside mutex

        Checking the state of the connection is unreliable if SSL_read and
        SSL_write are happening in another thread. Move the state checks inside
        our application mutex so we don't run into another thread mutating the
        state at the same time.

        Relax checks for key vs cert for wrapped keys

        If a key is a wrapped platform key, we must relax the check. The reason
        is that we may not have the public values we need to pass the
        EVP_PKEY_cmp checks that this does.

        Fix some JNI_TRACE lines

        During debugging these would be enabled, but they were copy-pasta'd to
        with the wrong args.

        Various fixes in OpenSSLEngineImpl.

        Fix ""Buffers were not large enough"" exception by directly using the
        destination buffers.

        Corrections around bytesProduced and bytesConsumed behavior.

        Return BUFFER_OVERFLOW if a zero length destination is provided to
        unwrap.

        Keep enough state to completely reset cipher instances

        OpenSSL's RC4 mutates the given key. AES/CTR mutates the IV. We must
        store these values locally to enable ""doFinal"" to cause the Cipher
        instance to be reset to what it was right after ""init"".

        Note that resetting and encrypting with the same key or IV breaks
        semantic security.

        Enable PSK cipher suites when PSKKeyManager is "
,,Conscrypt,"Remove SSLv3 from default protocols list for TLS

SSLv3 has some systemic problems demonstrated by the POODLE attack.
Disable it by default when ""TLS"" is ed since the documentation
in Java Standard Names allows us to not support SSL when TLS is
ed.

Bug: 17136008
Change-Id: Icad1639c7e33b6e495f452a5289b0d20b819d679/"
,,Conscrypt,"Add way to get preferred SSLContext config

This change is in preparation to re-arrange the way OpenSSLContextImpl
gets its configuration. Since frameworks/base directly creates an
OpenSSLContextImpl instance, give it a stable way of retrieving the
""preferred"" version of this.

Bug: 17136008
Change-Id: I8817b8f7816f9d6f0f9c30288904635e06b0c6a5/"
,"Memory Management, Upgrade Features",Conscrypt,"Call EVP_CIPHER_CTX_free instead of EVP_CIPHER_CTX_cleanup.

The latter doesn't OpenSSL_free memory allocated by EVP_CIPHER_CTX_new.

It's worth noting that EVP_CIPHER_CTX_free doesn't check the return
value of EVP_CIPHER_CTX_cleanup so we can't throw if cleanup failed, but
we were only ever calling this method from a finalizer anyway.

(cherry picked from commit c64652932d8e17ccf7e54c0c76c1b38a86841732)

bug: 18617384
Change-Id: Ida65e14ffbed41f56a59e2f5fe77289cac0f5947/"
,,Conscrypt,"Clean up unused variables

Change-Id: I9234e649a910408cff9f9d33008642e0c8334276/"
,,Conscrypt,"Track upgrade to OpenSSL 1.0.1j

Bug: 18018599
Change-Id: I2b8c62190a9dd5e5fdc6894334cf1d3edfce0a06/"
,,Conscrypt,"OpenSSLEngineImpl: return bytes consumed for unwrap

During a handshake, unwrap should return the number of bytes consumed by
the SSL implementation in addition to changing the source buffer
position so that the client can alter its state based on either.

Bug: 18921387
Bug: https://code.google.com/p/android/issues/detail?id=93740
Change-Id: Idf5a3b24c8ad053ef2970bfb66d142a7c2685c02/"
,,Conscrypt,"Return BUFFER_UNDERFLOW if no source bytes were consumed.

... either during the handshake or after. With this change, we're
backward compatible with older versions of android. Note that newer
versions of apache-http rely on this behaviour.

bug: 18554122

(cherry picked from commit 6a1b7a85dcdeb19305ad5153579bd11c1eb0bfad)

Change-Id: I741d2585548b3d72abae2b696eee2a186e58414c/"
,,Conscrypt,"Return BUFFER_UNDERFLOW if no source bytes were consumed.

... either during the handshake or after. With this change, we're
backward compatible with older versions of android. Note that newer
versions of apache-http rely on this behaviour.

bug: 18554122
Change-Id: I574c263e8df4a5f2396ac860608fe85cdbcdbb49/"
,,Conscrypt,"Fix SSLEngine to support session resumption.

(cherry picked from commit cd50afad1567b1311e6e979e94a7167b7bf69c94)

Bug: 17877118
Change-Id: I388b59cde58fdc506ecac9f536e4bbd9161df6ad/"
Network Management,Network Management,Conscrypt,"Fix OpenSSLSocketImpl.getPort when SNI is used.

We were using a non-null hostname as a hint that the socket was
constructed with an explicit host and port. This is no longer true
because the hostname can be non-null when SNI is used (i.e setHostname
is called with a non-null hostname).

"
,,Conscrypt,"Declare which keys Conscrypt accepts.

This declares constraints on which keys Cipher, KeyAgreement, Mac,
and Signature instances provided by Conscrypt accept. Constraints are
expressed using JCA's SupportedKeyClasses and SupportedKeyFormats
attributes.

Declaring these contraints will make JCA use other providers for keys
not supported by Conscrypt. This in turn removes the need of users
of JCA to explicitly specify which provider to use.

This looks messy mostly because of how the JCA's constraining
mechanism works. Some of the weirdness and messiness also comes from
the inconsistencies in how Conscrypt handles different key types in
different primitives. Once these inconsistencies are fixed, this
change will become smaller and a bit nicer.

See https://docs.oracle.com/javase/8/docs/technotes/guides/security/crypto/HowToImplAProvider.html

Bug: 19284418
Change-Id: I7e862a620d7279e4eaf6e42acd9072e7be665024/Remove SSLv3 from default protocols list for TLS

SSLv3 has some systemic problems demonstrated by the POODLE attack.
Disable it by default when ""TLS"" is ed since the documentation
in Java Standard Names allows us to not support SSL when TLS is
ed.

Bug: 17136008
Change-Id: Icad1639c7e33b6e495f452a5289b0d20b819d679/"
,,Conscrypt,"external/conscrypt: add NativeConstants.

NativeConstants.java is generated by a C program and thus the values
will automatically be kept in sync with the contents of the OpenSSL
headers.

Bug: 20521989
Change-Id: Ib5a97bf6ace05988e3eef4a9c8e02d0f707d46ad/conscrypt: throw exception for null references in NativeCrypto

Adapted tests to use ""null"" instead of an Object with a null
context, as null contexts are now rejected by constructors.

bug: 19657430

Change-Id: I47ebfde7170e1818afd64a75a8e4bc1e1d588aea/"
Data conversion,Data conversion,Conscrypt,"external/conscrypt: fix WITH_JNI_TRACE in light of BoringSSL update.

These values in BoringSSL are now uint32_t's, which upsets the compiler
when printing them as longs.

This change casts the values to longs so that it continues to work with
OpenSSL.

Change-Id: I35af51d765d67b3c8c30e55b80eac24dda420a88/"
,,Conscrypt,"OpenSSLCipher: add AEAD cipher

This allows us to provide an implementation of AES-GCM using the new
EVP_AEAD interface in BoringSSL. It simply buffers up the input until
doFinal(...) is called which makes it much safer than any streaming
interfaces, because the caller can't use the plaintext until it's
authenticated by the GHASH (or whatever other AEAD you happen to use).

Bug: 20636336
Change-Id: I6e4b063a8137a16102b1f6ac15687a38ddfe1691/"
,,Conscrypt,"external/conscrypt: ask OpenSSL for supported cipher suites.

Rather than enumerate the list of supported cipher suites in conscrypt,
ask OpenSSL for the list and just maintain a mapping from OpenSSL's
names to the standard, external name.

(The mapping could also be removed with BoringSSL since it can return
the standard name for an SSL_CIPHER*. But in order to keep OpenSSL
compat this change doesn't depend on that.)

Bug: 20531880
Change-Id: Ib541c9787093e7b900052fdf12dd2a2029b4b020/"
,,Conscrypt,"RI: AttachCurrentThread has different type

Change-Id: Ia74b5ecb1af69010d51f963f4f757339deda8b9b/"
,,Conscrypt,"RI: cast to char* for JNI registration

The RI has a different type that causing compilation errors if you don't
do this cast.

Change-Id: I5961d79c88bef6cba2dc0de9c81e310005e4712c/"
,,Conscrypt,"NativeCrypto: not finding a key is not fatal

If we don't find a key in the keystore, we should just return null
reference. The only time we should throw exceptions is when the key
decoding failed or something else like that.

Bug: 20488918

(cherry picked from commit 8098cbbc7fbf2d22402da487465a153734f9f9b6)

Change-Id: I621b39257bc98d888f7ad390fb8648326c67dfc4/"
,,Conscrypt,"NativeCrypto: not finding a key is not fatal

If we don't find a key in the keystore, we should just return null
reference. The only time we should throw exceptions is when the key
decoding failed or something else like that.

Bug: 20488918
Change-Id: I85408615a9c7a63242178908f309f93a2972033c/"
,,Conscrypt,"NativeCrypto: do not discard pending exceptions

The switch to native reference objects left some duplicate
NullPointerException creation that led to some JNI warnings. Simply get
rid of the redundant NullPointerException throws.

Bug: 19657430
Change-Id: I7e6bcb74154078cf019bfdea5d2721f6e6cb8524/"
,,Conscrypt,"external/conscrypt: recognise des-ede-cbc as an alias for des-cbc.

Bug: 20518919
Change-Id: I2b697529420a5c3fd9f96887a11977d261b3d1aa/"
,,Conscrypt,"OpenSSLEngine: do not try to load ENGINE for BoringSSL

Since BoringSSL doesn't use ENGINE instances, we should not fail when
the native code returns the equivalent of a NULL instance. This change
propagates the knowledge of whether we're using BoringSSL or OpenSSL up
to the Java layer.

Change-Id: Ib8c2224a909564ae6f0c6d5984020c44517f6c29/"
,,Conscrypt,"Add fallthrough intention markers

To help identify accidental fallthroughs, clang has an option to warn
when one is detected. Add the macro to make it compatible with earlier
versions of Clang or GCC.

Change-Id: I48add3e3e9c0cbfe9b6d812d3336062a4d971909/"
,,Conscrypt,"BoringSSL PKCS#7 PEM and CRL support.

Based on recent additions to BoringSSL itself, this change adds PKCS#7
PEM and CRL support for conscrypt with BoringSSL.

Change-Id: Icef9d017dce54c3070b605a70773c60bb1b8cfa2/"
,,Conscrypt,"Add back d2i_PKCS7_bio and PEM_read_bio_PKCS7.

For the moment, the BoringSSL version is going to be broken until I get
the needed changes into BoringSSL to support this.

Change-Id: Id2c3f179c6f9fc4f4385d2274884e69530fabff0/"
,,Conscrypt,"Use a serialization proxy for DH keys

Since DHPrivateKey and DHPublicKey uses a lock to determine when to
fetch the key parameters, we need to have some way for the field to be
final. Using a serialization proxy makes that safe.

Change-Id: Ifac7330fa35f0dc13313c806efacffbd293c6f85/"
Network Management,Network Management,Conscrypt,"external/conscrypt: add NativeConstants.

NativeConstants.java is generated by a C program and thus the values
will automatically be kept in sync with the contents of the OpenSSL
headers.

Bug: 20521989
Change-Id: Ib5a97bf6ace05988e3eef4a9c8e02d0f707d46ad/"
,,Conscrypt,"Enable any opaque private keys to be used with TLS/SSL stack.

Prior to this CL, opaque private keys -- those that do not
expose/export their key material -- were not supported by Conscrypt's
SSLSocket, SSLServerSocket and SSLEngine implementations if the keys
were backed by other providers.

This CL fixes this issue. Conscrypt's TLS/SSL stack now works with
arbitrary opaque private keys provided that:
* for EC private key: an installed implementation of NONEwithECDSA
  Signature accepts the key for signing; and
* for RSA private key: an installed implementation of NONEwithRSA
  Signature accepts the key for signing and an installed
  implementation of RSA/ECB/PKCS1Padding Cipher accepts the key for
  decryption.
This normally requires that the JCA Provider which produced the
PrivateKey instance expose the above Cipher transformation and
Signature algorithms.

HOW THIS WORKS

The underlying OpenSSL TLS/SSL stack uses the provided private keys
only to decrypt and sign. For opaque private keys these s are
delegated (same as before, via CryptoUpcalls) to corresponding Cipher
(RSA/ECB/PKCS1Padding) and Signature (NONEwithRSA or NONEwithECDSA)
implementations.

Even when signing and decryption is outsourced, OpenSSL still needs
the modulus (for RSA) and order (for EC), supposedly to estimate
output size of signing or decryption operations. This information is
not available via the PrivateKey interface. However, an opaque private
key may still implement the RSAKey or ECKey interface which provides
access to modulus or order but does not provide access to key
material. Moreover, in all use cases of private keys with Conscrypt's
TLS/SSL stack the modulus or order can be obtained and provided to
OpenSSL. In the case of private keys used for client or server
authentication, the public key of the certificate is used as the
source of the information. In the case of TLS Channel ID, the order is
currently fixed and known (only NIST P-256 is supported).

Bug: 19284418

Change-Id: I8fea2492f9cf48cfc29c3e7d2ee99a68e84e82ec/"
,,Conscrypt,"SSLParametersImpl: make some methods public

To help with testing, make some of the methods public so we can call
them from tests in a different ClassLoader.

Bug: 19657440
Change-Id: Ib5cb0629ffb52ac57ff24d9d5c4df1509897bd05/"
,,Conscrypt,"Make methods public for testing.

When run under CTS, the implementation will be in a different
classloader than the test.

Change-Id: I657fd2541ec6721e032581de14bf45c3efa40a6e/"
Network Management,Network Management,Conscrypt,"external/conscrypt: support arbitrary ECC groups.

The Java provider mechanism doesn't really let us fallback to another
provider based on whether certain ECC groups are supported or not. Since
I expect that some people will be trying to do Bitcoin on Android, this
should keep them happy.

Change-Id: I1db48b104e12a6e7dae21df9c31c21bff0d62a9b/"
,,Conscrypt,"Split up JNI library initialization

Different platforms require vastly different ways of loading the JNI
glue library, so split the loading job into different directories so
they can be more easily compiled.

Change-Id: I963c2e0d4667cbb655a0788f161eae74d7a2f037/"
,,Conscrypt,"external/conscrypt: switch NativeCrypto itself to use NativeConstants.

Now that other users of the constants in NativeCrypto have been switched
over, those constants can be removed.

Bug: 20521989
Change-Id: I276a1c8daeb3501b6924ff68cf9f1e9f6fbd63a9/"
,,Conscrypt,"external/conscrypt: add SSL_CIPHER_get_kx_name

This will be used by a future change to avoid needing to know the
OpenSSL-internal SSL_aRSA (etc) constants.

Bug: 20521989
Change-Id: I99d83005530f81956d102426fe28beeaed058cea/"
,,Conscrypt,"OpenSSLEngine: do not try to load ENGINE for BoringSSL

Since BoringSSL doesn't use ENGINE instances, we should not fail when
the native code returns the equivalent of a NULL instance. This change
propagates the knowledge of whether we're using BoringSSL or OpenSSL up
to the Java layer.

Change-Id: Ib8c2224a909564ae6f0c6d5984020c44517f6c29/Enable any opaque private keys to be used with TLS/SSL stack.

Prior to this CL, opaque private keys -- those that do not
expose/export their key material -- were not supported by Conscrypt's
SSLSocket, SSLServerSocket and SSLEngine implementations if the keys
were backed by other providers.

This CL fixes this issue. Conscrypt's TLS/SSL stack now works with
arbitrary opaque private keys provided that:
* for EC private key: an installed implementation of NONEwithECDSA
  Signature accepts the key for signing; and
* for RSA private key: an installed implementation of NONEwithRSA
  Signature accepts the key for signing and an installed
  implementation of RSA/ECB/PKCS1Padding Cipher accepts the key for
  decryption.
This normally requires that the JCA Provider which produced the
PrivateKey instance expose the above Cipher transformation and
Signature algorithms.

HOW THIS WORKS

The underlying OpenSSL TLS/SSL stack uses the provided private keys
only to decrypt and sign. For opaque private keys these s are
delegated (same as before, via CryptoUpcalls) to corresponding Cipher
(RSA/ECB/PKCS1Padding) and Signature (NONEwithRSA or NONEwithECDSA)
implementations.

Even when signing and decryption is outsourced, OpenSSL still needs
the modulus (for RSA) and order (for EC), supposedly to estimate
output size of signing or decryption operations. This information is
not available via the PrivateKey interface. However, an opaque private
key may still implement the RSAKey or ECKey interface which provides
access to modulus or order but does not provide access to key
material. Moreover, in all use cases of private keys with Conscrypt's
TLS/SSL stack the modulus or order can be obtained and provided to
OpenSSL. In the case of private keys used for client or server
authentication, the public key of the certificate is used as the
source of the information. In the case of TLS Channel ID, the order is
currently fixed and known (only NIST P-256 is supported).

Bug: 19284418

Change-Id: I8fea2492f9cf48cfc29c3e7d2ee99a68e84e82ec/"
,,Conscrypt,"Fix a NullPointerException in CryptoUpcalls.

When RSA/ECB/PKCS1Padding is not supported,
CryptoUpcalls.rawCipherWithPrivateKey throws a NullPointerException
instead of returning null. This CL fixes the issue.

Change-Id: I46a389f22e40084950b80b9825644f2e1ffcff90/"
,,Conscrypt,"Clean up CryptoUpcalls.

These are minor code structure clean ups based on comments from
https://android-review.googlesource.com/#/c/130311/.

Change-Id: I66c2cbdb489db47167ae6cfb4df7b82b3f621e2d/"
,,Conscrypt,"OpenSSLCipher: refactor in preparation for AEAD

BoringSSL uses a different interface for AEAD that is much simplier
called EVP_AEAD. Separate out the EVP_CIPHER usage so that we can have
another subclass with the EVP_AEAD usage.

Bug: 20636336
Change-Id: I661d92bd449f2fcc3c4a6e511155490917ecef0c/"
,,Conscrypt,"OpenSSLCipher: exception when IV not specified

If you're decrypting with a mode that requires an IV, init should throw
an exception indicating as much. Add the checks to make sure this
happens.

Bug: 19201819
Change-Id: I2d3481da4f63bffb340dc1197f6b5cb29360fbff/"
Network Management,Network Management,Conscrypt,"Rename Arrays to ArrayUtils

To avoid conflict with the java.util.Arrays class, rename our own
internal compatibility class to ArrayUtils.

Change-Id: Iae79a4d37749e16e62712f3bb5038d870b78d999/"
,,Conscrypt,"external/conscrypt: add NativeConstants.

NativeConstants.java is generated by a C program and thus the values
will automatically be kept in sync with the contents of the OpenSSL
headers.

Bug: 20521989
Change-Id: Ib5a97bf6ace05988e3eef4a9c8e02d0f707d46ad/Enable any opaque private keys to be used with TLS/SSL stack.

Prior to this CL, opaque private keys -- those that do not
expose/export their key material -- were not supported by Conscrypt's
SSLSocket, SSLServerSocket and SSLEngine implementations if the keys
were backed by other providers.

This CL fixes this issue. Conscrypt's TLS/SSL stack now works with
arbitrary opaque private keys provided that:
* for EC private key: an installed implementation of NONEwithECDSA
  Signature accepts the key for signing; and
* for RSA private key: an installed implementation of NONEwithRSA
  Signature accepts the key for signing and an installed
  implementation of RSA/ECB/PKCS1Padding Cipher accepts the key for
  decryption.
This normally requires that the JCA Provider which produced the
PrivateKey instance expose the above Cipher transformation and
Signature algorithms.

HOW THIS WORKS

The underlying OpenSSL TLS/SSL stack uses the provided private keys
only to decrypt and sign. For opaque private keys these s are
delegated (same as before, via CryptoUpcalls) to corresponding Cipher
(RSA/ECB/PKCS1Padding) and Signature (NONEwithRSA or NONEwithECDSA)
implementations.

Even when signing and decryption is outsourced, OpenSSL still needs
the modulus (for RSA) and order (for EC), supposedly to estimate
output size of signing or decryption operations. This information is
not available via the PrivateKey interface. However, an opaque private
key may still implement the RSAKey or ECKey interface which provides
access to modulus or order but does not provide access to key
material. Moreover, in all use cases of private keys with Conscrypt's
TLS/SSL stack the modulus or order can be obtained and provided to
OpenSSL. In the case of private keys used for client or server
authentication, the public key of the certificate is used as the
source of the information. In the case of TLS Channel ID, the order is
currently fixed and known (only NIST P-256 is supported).

Bug: 19284418

Change-Id: I8fea2492f9cf48cfc29c3e7d2ee99a68e84e82ec/"
,,Conscrypt,"OpenSSLCipher: add AEAD cipher

This allows us to provide an implementation of AES-GCM using the new
EVP_AEAD interface in BoringSSL. It simply buffers up the input until
doFinal(...) is called which makes it much safer than any streaming
interfaces, because the caller can't use the plaintext until it's
authenticated by the GHASH (or whatever other AEAD you happen to use).

Bug: 20636336
Change-Id: I6e4b063a8137a16102b1f6ac15687a38ddfe1691/OpenSSLCipher: refactor in preparation for AEAD

BoringSSL uses a different interface for AEAD that is much simplier
called EVP_AEAD. Separate out the EVP_CIPHER usage so that we can have
another subclass with the EVP_AEAD usage.

Bug: 20636336
Change-Id: I661d92bd449f2fcc3c4a6e511155490917ecef0c/"
,,Conscrypt,"GCM: return the correct AlgorithmParameters

Instead of the correct AlgorithmParameters of type ""GCM,"" we were
returning the generic ""AES"" version that basically only converts to an
IvParameterSpec.

Bug: 22319986
Change-Id: Ib42905c3ad31e44b72e8066192bd26981c8351ba/"
,,Conscrypt,"Switch OpenSSLMac from EVP_PKEY_HMAC to HMAC_CTX.

EVP_PKEY_HMAC is just a wrapper over HMAC_CTX, so this is slightly more
efficient. This is also the last consumer of BoringSSL's EVP_PKEY_HMAC,
so the API may be removed after this.

Change-Id: I545914b429b23631efd3cacaa22c6d2e7d165fab/Use SSL_CTX_set_tmp_ecdh instead of SSL_CTX_set_tmp_ecdh_callback.

Conscrypt is the only consumer of SSL_CTX_set_tmp_ecdh_callback for BoringSSL.
The callback variant is also bizarre. The key length parameter is legacy and
pointless. When used with SSL_OP_SINGLE_ECDH (which BoringSSL always enables),
there's no point in configuring the callback over a static group. The callback
also does not participate in supported_curves negotiation.

Change-Id: Ie588532a559f13d2b69b7278f9b8d4d41e31828d/"
,,Conscrypt,"Consistently use ARRAY_OFFSET_*INVALID macros.

Not all the ad-hoc ones check for integer overflow correctly. Consistently use
the same check everywhere.

Change-Id: I913b7de792406d9819a6830cc21ec500ddceff6e/"
,,Conscrypt,"Fix error conditions in certificate/PKCS#7 reading

When an error condition is encountered in BoringSSL, sometimes it
deliberately does not put something on the ERR stack to prevent abuse of
that knowledge. Instead we need to throw an exception explicitly when no
error is pushed onto the stack.

Bug: 21034231
Change-Id: Ia06347c5653672c982ecff2c26be9b091d03009f/"
,,Conscrypt,"Fix up JNI_TRACE for AEAD

Bug: 21762837
Change-Id: I11042be8fe1e046ac96759b4554ce9229e1cf6f3/"
,,Conscrypt,"NativeCrypto: special case for empty cipher list

For the Java language, setting an empty cipher list is not an error but
it's an error in OpenSSL. However, the underlying API actually updates
the cipher list to an empty string as intended. So we need to handle
this special case by clearing the error stack and making sure that our
expectation is satisfied.

Bug: 21195269
Change-Id: Id21792215513f4e0d6e051160f69e5f830d39015/"
,,Conscrypt,"external/conscrypt: tweaks for next BoringSSL import.

Upstream BoringSSL has dropped |SSL_ST_BEFORE| (which appears to have been
unused) and all the |*_LOCK_*| symbols. The latter are replaced with
|*_up_ref|, with #if's so that it continues to work with OpenSSL.

Change-Id: Ib609c83d428b7624e24e3b96c93afc2e482e6a6d/"
,,Conscrypt,"NativeCrypto: return of 0 is error for EVP_Sign/VerifyFinal

We need to check the ERR stack on a return code of 0. Previously there
was a comment indicating the weird behavior about DSA keys throwing
after a check for a return value of -1, but this API is never supposed
to return anything other than 1 for success or 0 for failure.

(cherry picked from commit 49854878b83114e3e15c7ad3ca030352b786b5df)

Bug: 18869265
Change-Id: Ic871c63b6d65949053819950ed8053f47501bd60/"
,,Conscrypt,"NativeCrypto: return of 0 is error for EVP_Sign/VerifyFinal

We need to check the ERR stack on a return code of 0. Previously there
was a comment indicating the weird behavior about DSA keys throwing
after a check for a return value of -1, but this API is never supposed
to return anything other than 1 for success or 0 for failure.

Bug: 18869265
Change-Id: Ic871c63b6d65949053819950ed8053f47501bd60/"
,,Conscrypt,"OpenSSLX509Certificate: use OID if alg name unavailable

If we cannot map the signature OID type to a canonical name, then we
should try to get an instance of the signature type using the OID.
Additionally, we should return the OID for the #getSigAlgName instead of
null.

Bug: 22365511
Change-Id: I1ebf48667cf720ee5c7751667601eec2f6f8ec91/"
,,Conscrypt,"Revert ""OpenSSLX509Certificate: mark mContext as transient""

This reverts commit 998fbfcd4729ee2e196ed17106f76de93f33d7f0. Missing the test class.

Change-Id: I426680f74c4f3ebeb42abd80ebfdba469247c348/"
,,Conscrypt,"Switch OpenSSLMac from EVP_PKEY_HMAC to HMAC_CTX.

EVP_PKEY_HMAC is just a wrapper over HMAC_CTX, so this is slightly more
efficient. This is also the last consumer of BoringSSL's EVP_PKEY_HMAC,
so the API may be removed after this.

Change-Id: I545914b429b23631efd3cacaa22c6d2e7d165fab/"
,,Conscrypt,"Fix RSA upcalls from TLS/SSL into JCA.

When BoringSSL/OpenSSL TLS/SSL stack operates on opaque private keys
(those that don't expose their key material) it upcalls (via
Conscrypt's NativeCrypto) into corresponding JCA Signature and Cipher
primitives.

This CL fixes two issues with RSA-related upcalls, which prevented
the use of opaque RSA private keys for TLS/SSL with Conscrypt backed
by BoringSSL:
* RSA sign was upcalled into RSA Cipher decrypt using private key.
  In JCA, the correct upcall is RSA Signature sign. This is now
  invoked instead of RSA Cipher decrypt.
* RSA decrypt was not implemented. It's now implemented.

As part of implementing RSA decrypt upcall from BoringSSL, it
transpired that BoringSSL s no padding as opposed to OpenSSL
which s PKCS#1 padding. As a result, this CL modifies the
decrypt upcall to take a padding parameter. The implementation of
the upcall (see CryptoUpcalls.java) now supports PKCS#1 padding
scheme, OAEP padding scheme, and no padding.

This CL also drops the encrypt/decrypt flag from the RSA
encrypt/decrypt upcall and simplies it into an RSA decrypt upcall. RSA
encrypt upcall is not needed at all.

(cherry-picked from commit 279e98451390d0a90c5fc04eac7ddd4045180465)

Bug: 21738458
Change-Id: I075aa74e4cd89dd3ceab99f728ce371c7bc89cf0/"
,,Conscrypt,"Try to get preferred external provider

When using an opaque key, try to honor the system's preferred provider
which is selected via late binding. If it's not found, try to find the
first provider that initializes correctly with the given key.

Bug: 21737886
Change-Id: I17483136aa5c1c5e474109525aefac9facaf7379/"
,,Conscrypt,"Fix RSA upcalls from TLS/SSL into JCA.

When BoringSSL/OpenSSL TLS/SSL stack operates on opaque private keys
(those that don't expose their key material) it upcalls (via
Conscrypt's NativeCrypto) into corresponding JCA Signature and Cipher
primitives.

This CL fixes two issues with RSA-related upcalls, which prevented
the use of opaque RSA private keys for TLS/SSL with Conscrypt backed
by BoringSSL:
* RSA sign was upcalled into RSA Cipher decrypt using private key.
  In JCA, the correct upcall is RSA Signature sign. This is now
  invoked instead of RSA Cipher decrypt.
* RSA decrypt was not implemented. It's now implemented.

As part of implementing RSA decrypt upcall from BoringSSL, it
transpired that BoringSSL s no padding as opposed to OpenSSL
which s PKCS#1 padding. As a result, this CL modifies the
decrypt upcall to take a padding parameter. The implementation of
the upcall (see CryptoUpcalls.java) now supports PKCS#1 padding
scheme, OAEP padding scheme, and no padding.

This CL also drops the encrypt/decrypt flag from the RSA
encrypt/decrypt upcall and simplies it into an RSA decrypt upcall. RSA
encrypt upcall is not needed at all.

Bug: 21738458
Change-Id: I2a4610890ea1ed1a2e99eb1d5c34348fbf406e54/"
,,Conscrypt,"GCM: set default tag size to 12 bytes

According to RFC 5084, the default value of the GCM tag should be 12
octets (bytes). Change the default tag length from 0 to 12 to honor
this.

Bug: 22855843
Change-Id: I1ed16df24d0cfa9fff2593a3402c97faf913e05e/"
,,Conscrypt,"OpenSSLCipher: adjust expected length with padding in decrypt mode

- Consider the |final| buffer when computing the expected length
- Should not expect an extra block when using padding in decrypting
mode

Bug: 19186852
Change-Id: I206442d45c4cf68363201738ba9d0b035f19c436/"
,,Conscrypt,"Revert ""OpenSSLCipher: adjust expected length with padding in decrypt mode""

This reverts commit eb3a7e31c78231a19cb76ce2a5974b03a0187b96.

Change-Id: I822a51cfb7c8a2a3785d8694f5ab9f9fef552111/"
,,Conscrypt,"NativeCrypto: Add TLS SCT extension support.

Change-Id: I438b23ecb86340f837f62359b342637966b81512/NativeCrypto: support OCSP stapling

This only provides access to the OCSP data. It does not use it to verify
the certificate.

Change-Id: Ib448cbf52a5c824655585afc62d1580404a44f2c/"
,,Conscrypt,"NativeCrypto: add method to extract extensions from an OCSP response.

This will be useful to extract signed timestamps to perform Certificate
Transparency verification.

Change-Id: I44db4435ce47d9c5562323c18d475be24b00bca7/"
,,Conscrypt,"Move BlockGuard and CloseGuard to Platform

This was causing issues on Gingerbread devices since CloseGuard was not
in that release yet. Move them out to Platform so we can filter on
release when we decide whether to instantiate or not.

Bug: 24607028
Change-Id: Iba0bbb0b878076319ace40f848aa5e307e2c3ad8/"
,,Conscrypt,"Clear BoringSSL error queue in NativeCrypto.EVP_DigestVerifyFinal.

This fixes a bug introduced in
b4345a619c1f34e2390210d11476a8619cebd695 where
NativeCrypto.EVP_DigestVerifyFinal left the BAD_SIGNATURE error in the
BoringSSL error queue when a signature did not verify. Some of the
following NativeCrypto operations would then fail because they assumed
that it was their BoringSSL calls that generated the BAD_SIGNATURE
error.

The fix is to unconditionally clear the BoringSSL error queue at the
end of NativeCrypto.EVP_DigestVerifyFinal, same as its predecessor
NativeCrypto.EVP_VerifyFinal did.

Change-Id: I0d092b1b39afa3c6d19a785cbf7dd311ffcd4c04/"
,,Conscrypt,"Switch from EVP_[Sign|Verify] to EVP_Digest[Sign|Verify].

This switches Conscrypt's Signature implementations from the older
EVP_Sign/EVP_Verify API to the newer EVP_DigestSign/EVP_DigestVerify
API. The main factor driving this switch is to expose RSASSA-PSS
which does not work via the old API.

In particular, this change:
* adds EVP_DigestSign* and EVP_DigestVerify* to NativeCrypto. Some of
  these NativeCrypto functions were already there but weren't used.
  This made it easier to adjust their signatures to best results.
* switches Signature implementation from EVP_Sign/EVP_Verify to
  EVP_DigestSign/EVP_DigestVerify.
* removes EVP_Sign* and EVP_Verify* from NativeCrypto because they are
  no longer used.
* inlines NativeCrypto's evpInit into its EVP_DigestInit_ex because
  the latter became the only user of evpInit after the cleanup.

Change-Id: Id29ea4fc2bc5b1cd81daaee8b475fd147616de51/"
,,Conscrypt,"Adjust names of digest-related NativeCrypto methods.

This adjusts the names of digest-related NativeCrypto methods to match
the names of underlying BoringSSL functions. This makes it easier to
reason about the functionality being invoked via NativeCrypto.

Change-Id: I04e2148ba818ae3e9ad60871b046052fcfffec4d/"
,,Conscrypt,"Zero-copy digesting for direct ByteBuffer input.

Prior to this change, Conscrypt's MessageDigest.update(ByteBuffer)
invoked for a direct ByteBuffer resulted in the creation of a new
byte[] of size ByteBuffer.remaining() and the copying of the
ByteBuffer's contents into that array.

This change implements an optimization which avoids the allocation
and copying, by making BoringSSL EVP_DigestUpdate read directly from
the memory region represented by the direct ByteBuffer.

Change-Id: I112d318128402d1d78e226df9dfe54af55955953/"
,,Conscrypt,"NativeCrypto: Add TLS SCT extension support.

Change-Id: I438b23ecb86340f837f62359b342637966b81512/"
,,Conscrypt,"NativeCrypto: support OCSP stapling

This only provides access to the OCSP data. It does not use it to verify
the certificate.

Change-Id: Ib448cbf52a5c824655585afc62d1580404a44f2c/"
,,Conscrypt,"Add method to delete extension from a certificate

The OpenSSLX509Certificate is still immutable. Instead a modified copy is returned.
The use case for this is recreating the TBS component of a Precertificate as
described by RFC6962 section 3.2.

Change-Id: I2a9305ae7464642910decaf5ab46121a6f15d722/"
,,Conscrypt,"Remove references to OpenSSL's |wbuf|.

The |wbuf| member is an internal field that disappears in the latest
BoringSSL revision. Also, it doesn't appear to be neccessary: SSL_write
won't report that bytes were written until the record has hit the
transport, so there's no need to be sensitive to an implementation
detail.

(See also cl/100529082.)

Change-Id: I036bb7ebf69649025967a2af467313d7676e62ca/"
,,Conscrypt,"Verify certificate transparency on domain for which it is enabled.

For the time being, CT is enabled by using Java security properties.
This makes it possible to deploy it without updating Android's frameworks
through GMSCore.

Change-Id: Iff9646b7d80f1386965b2b4f4f46a9d80c780a58/"
,,Conscrypt,"Speed up digesting by avoiding unnecessary operations.

Conscrypt's MessageDigest implementations at the end of computing a
digest create and initialize a new EVP_MD_CTX and then also intialize
the digest struct there. This is done because the MessageDigest
instance could be reused for a new digesting session.

This change implements three optimizations:
1. MessageDigestSpi now reuses its EVP_MD_CTX instead of creating a
   new one for each new digesting session.
2. MessageDigestSpi now defers the initialization of the digest struct
   in EVP_MD_CTX till the first invocation of
   engineUpdate/engineDigest.
3. MessagDigestSpi (and SignatureSpi) no longer invoke EVP_MD_CTX_init
   after EVP_MD_CTX_create because EVP_MD_CTX_create initializes the
   EVP_MD_CTX it creates.

libcore's MessageDigestBenchmark on Nexus 5 shows:
* 10-15% faster performance for a single digest of 8192 bytes.
* 15-20% faster performance for reusing a MessageDigest instance to
  compute a digest of 8192 bytes ten times.

Change-Id: I8a0697310ef7efcd4db6870e54eb46102fd4a941/"
,,Conscrypt,"Revert ""Speed up digesting by avoiding unnecessary operations.""

This reverts commit 5041dd13c9499e4154436ef1f105a3d5d46caa19.

Change-Id: Ib925bc0aadb633bbad4240f1d52bdb6676afc56f/"
,,Conscrypt,"Speed up digesting by avoiding unnecessary operations.

Conscrypt's MessageDigest implementations at the end of computing a
digest create and initialize a new EVP_MD_CTX and then also intialize
the digest struct there. This is done because the MessageDigest
instance could be reused for a new digesting session.

This change implements two optimizations:
1. MessageDigestImpl instance now reuses its EVP_MD_CTX instead of
   creating a new one for each new digesting session.
2. MessageDigestImpl instance now defers the initialization of the
   digest struct in EVP_MD_CTX till the first invocation of
   engineUpdate/engineDigest.

libcore's MessageDigestBenchmark on Nexus 5 shows:
* 10-15% faster performance for a single digest of 8192 bytes.
* 15-20% faster performance for reusing a MessageDigest instance to
  compute a digest of 8192 bytes ten times.

Change-Id: I0e476381321127642315355f848a1ba90114fe7d/"
,,Conscrypt,"Zero-copy HMAC and signing/verification for direct ByteBuffer.

Prior to this change, Conscrypt's Mac and Signature implementations
copied the contents of direct ByteBuffer inputs. This change
implements an optimization which avoids the allocation and copying of
contents of direct ByteBuffer inputs.

Bug: 24674857
Change-Id: I1436839182483fd42318d4b0af4d633283e3453d/"
Network Management,Network Management,Conscrypt,"Add ExtendedSSLSession, et al.

In order to support SNI certificate selection of the server-side and
enhanced certificate verification on the client side, we add
ExtendedSSLSession and the getHandshakeSession support.

This is just to set up for future implementations of SNI and
ExtendedX509TrustManager and doesn't actually implement the logic needed
to fully support the new features.

Change-Id: I300d3134d8ab9c184d6473183612dc53658a8221/"
,,Conscrypt,"Fix for OpenJdk SocketImpl.

OpenJdk sockets start their life with a null FileDescriptor.

b/25805791 tracks fixing the SocketImpl

Change-Id: Ia14afda04aa0a109f944c549719ad50bb3aeadab/"
,,Conscrypt,"Remove java.lang.IntegralToString usage.

java.lang.IntegralToString is going away, replaced
its usage by small helper class, Hex.
+
Fixes the ""Illegal class access"" exception from
TrustedCertificateStoreTest & TrustManagerImplTest.

(cherry-picked from 61e984f441b9194f0ae907e6fc28502858df6852 +
61e984f441b9194f0ae907e6fc28502858df6852)

Bug: 24932279

(cherry picked from commit e279a9854d15d20a0b3807fe96f0805b43cd4dae)

Change-Id: Id48cd9c2dfade328f01c669afa20fe2e7a630fc2/"
,,Conscrypt,"Track False Start change in tests

In BoringSSL, the SSL_MODE_ENABLE_FALSE_START (aka
SSL_MODE_HANDSHAKE_CUTTHROUGH) is unconditionally enabled because
BoringSSL does the appropriate checks internally. Make sure our tests
also reflect this fact by testing the appropriate settings.

Bug: 26139262
Bug: 26139500
Change-Id: I125aa440cdb76d2efbfee2be7387b47d22446950/"
,,Conscrypt,"Add handshake session and ExtendedX509TrustManager support

This enables the new API to specify when a host should be verified by
hostname. Before there was no public API that was capable of indicating
to the TrustManager which DNS hostname you were intending to connect
with.

Change-Id: Ic5845d1e93f02b54d971673a280d0a3571739fbf/"
,,Conscrypt,"Compare keys using encoded form as a fallback

PublicKey.equals is not required to return true on the same public key
but from different providers, this causes incorrect lookup failures when
the key comes from keystore.

Change-Id: Iaedaa91c64eeede1d5021430c015aac746afbc97/"
,,Conscrypt,"Fix compilation with JNI_TRACE_**

Change-Id: I8417daea4b10f8c02642fe6c9be170312461139c/"
,,Conscrypt,"Use some C++11 concepts

Run clang-modernizer over the native code and clang-format -style=file
for the changed lines.

Change-Id: I02211de90214567a128c4e3ca88aad26541a7629/"
,,Conscrypt,"Basic implementation of RSASSA-PSS Signature.

This makes Conscrypt provide RSASSA-PSS Signature implementations.
These implementations currently do not support changing their
parameters (e.g., via Signature.setParameter(PSSParameterSpec)) and
returning their current parameters (e.g., via
Signature.getParameters()). This will be added in a follow-up change.

Bug: 25794302
Change-Id: I1488e0e9592f92a9e15365131c76ce2902ad4607/"
,,Conscrypt,"Support for PSS Signature configuration via PSSParameterSpec.

This adds support for configuring the PSS Signature implementation
via java.security.spec.PSSParameterSpec. This also makes the
signature implementation return its current configuration as
AlgorithmParameters of algorithm ""PSS"" from which a PSSParameterSpec
can be obtained.

Bug: 25794302
Change-Id: Ib7e087cdc75a6b02898afafdfc4308802d6eb5d5/"
,,Conscrypt,"Add support for SNI API

This adds support for retrieving SNI name as a server and setting SNI name
as a client. It currently doesn't implement use of the SNIMatcher API.

Change-Id: I4f76fcbd96bd7c3398532f3858bbdd0d06103082/"
,,Conscrypt,"Add ChaCha20-Poly1305 as an enabled cipher suite

Change-Id: Idc143d37c63cf3436ccdddc22abcb11802fc6615/"
,,Conscrypt,"external/conscrypt: sort list of cipher suite strings.

This change sorts the list using sort(1).

Change-Id: Ief0c407969c92405464b9b2e9ebc694f98260263/"
,,Conscrypt,"Revert ""Revert ""Add ExtendedSSLSession, et al.""""

This reverts commit 132c311de656e7396b78b388c6351be8a84a159c.

Some stubs were neded to allow building on unbundled builds.

Change-Id: I713d00923eecac7e323d53e561cf509794cc4fd4/"
,,Conscrypt,"Add compat methods for using SNI and other new features

Newer revisions of Android have SSLParameters with SNI support, endpoint
identification algorithm support, and honor cipher suite order
preference support. Add these to the /compat/ subtree so we can use them
if available in unbundled releases.

Change-Id: Iab3a3e6863b025c64790b08952a8b43cf087e920/"
Network Management,Network Management,Conscrypt,"OpenSSLSocketImpl: Don't accidentally create a SocketImpl.

We don't call super.close() when we're wrapping a socket, so we'll
have to be careful not to call any superclass methods that might
end up creating a SocketImpl.

bug: 27250522

Change-Id: Ie98127d002cc3b3dd3dd419b62adcfec47817479/"
,,Conscrypt,"Update d2i_SSL_SESSION test expectations

Update d2i_SSL_SESSION to only throw IOException and change tests to
expect that to happen. Since IOException is declared as a thrown
exception, non-test code should already be expecting this.

Bug: 27526112
Change-Id: Ic8c1a47debce9cb76221150d050be86d010c6ec3/"
,,Conscrypt,"Improve path building

This CL changes certificate path building from building the first
possible chain only to building all possible chains until a valid chain
is found or all potential chains are exhausted. This will allow us to
more gracefully handle CA and intermediate changes.

This CL does _not_ change the verification step in any way, all chains
generated are still verified the same as they were before.

(cherry-picked from commit 381c900af12815e6f0c01519d8ebdd57297303e9)
Change-Id: Ia8c4cd4131eb6ddf299da144b963a24cd1b64605/"
,,Conscrypt,"UniqueMutex for explicit ordering with ScopedSslBio

The MUTEX_LOCK / MUTEX_UNLOCK semantics work if you also explicitly
clear out resources that were supposed to be cleared before the lock is
released. However, with wrapper classes that do it automatically, you
can't get the correct ordering. Instead of converting these all to
manual acquire and release, convert the mutex handling to use automatic
release via UniqueMutex so that ordering is correct with resources that
should be protected by the mutex.

Thanks to Zhen Song for finding these issues.

Bug: 28473706
Change-Id: I4b63ce674e0fc343fe156936df7e8f6e3130722f/"
,,Conscrypt,"Revert ""Switch Conscrypt to EC_GROUP_new_arbitrary.""

This reverts commit f695b9fa2d3b67f95cc40fb485db5ee73da60f25. Having a
different API for this case than upstream is more trouble than is worth it.  A
separate ""incomplete EC_GROUP"" state is a nuisance, but not much more of a
nuisance for future hopes than having separate ""static EC_GROUP"" and ""arbitrary
EC_GROUP"" buckets.

(BoringSSL will keep both APIs around until this is cycled everywhere so we
won't need more multi-sided changes.)

Change-Id: Iad4604a04d75b29b9aac9dfde0f9ae18964017e8/"
,,Conscrypt,"OpenSSLSessionImpl: add better errors when converting

Frequently an old SSLSession cache from a different version of OpenSSL
or BoringSSL will cause the de-serialization of the SSLSession
information to fail. This will spam the logs and happens Frequently
when GmsCore's ProviderInstaller is used. For now try to extract a bit
more useful information from the error thrown by native code and don't
bother to print the stack trace since it's not fatal.

(cherry picked from commit de8236f4bb9d70fa4e6a52679b4bf40b04c44f9b)

Bug: 25328662
Change-Id: I0a396a52418e7911b98133b45bbfafcc6651e863/"
,,Conscrypt,"OpenSSLSessionImpl: add better errors when converting

Frequently an old SSLSession cache from a different version of OpenSSL
or BoringSSL will cause the de-serialization of the SSLSession
information to fail. This will spam the logs and happens Frequently
when GmsCore's ProviderInstaller is used. For now try to extract a bit
more useful information from the error thrown by native code and don't
bother to print the stack trace since it's not fatal.

Bug: 25328662
Change-Id: I0a396a52418e7911b98133b45bbfafcc6651e863/"
,,Conscrypt,"Use SSL_session_reused to check when a session was reused

The returned session_id could be exactly the same in the case of TLS
session tickets, so use the SSL_session_reused API to determine exactly
when a session was reused.

Bug: 28751153
Change-Id: Ie82e4d1bb326d7e7deb7981a1e57df393f6c0e1f/"
,,Conscrypt,"Wrap cached sessions before returning

SSLSession should be wrapped so that cached sessions might have the
functionality that ExtendedSSLSession offers. This also made the
SSLSessionTest fail because the pre-cached instance would be
ExtendedSSLSession and the post-cached session would be a regular
SSLSession.

To keep compatibility with older versions of the platform, it was
impossible to directly switch OpenSSLSessionImpl over to
ExtendedSSLSession. So the use of a delegate in the case when the
platform does have ExtendedSSLSession was required. Since older platform
versions still use OpenSSLSessionImpl that extends SSLSession, we just
directly inflate the serialized sessions to that.

The SSLSessionTest was changed to accomodate the delegate scheme since
SSLSession does not have an equals method, the tests for SSLSessionTest
were directly comparing object instance equality which fails when the
sessions are wrapped in a delegate like this.

(cherry picked from commit 710c0817a2a13135b35f14faaef5ca069daf7b6c)

Bug: 27123298
Change-Id: Iefbea03a72dbcc76ae0b439cfdcecd817926b7d0/"
,,Conscrypt,"Fix static analysis findings

Add annotations where we intentionally left out @Override and a brief
explanation.

Add synchronized keyword where needed by overriding methods so they
match the parent class.

Change-Id: I55591a5902530f1c2fb8cc89260c3df09648ec8e/"
,,Conscrypt,"Allow SSLSession to return IP address

In an effort to not use reverse DNS, we no longer return hostnames from
sockets created via IP addresses. However, this also made the SSLSession
return null when a Socket is created to an IP address instead of an
FQDN.

While being careful not to trigger another DNS lookup, simply return a
textual representation of the IP address connected when the SSLSocket has
no knowledge of what the actual FQDN is supposed to be.

(cherry picked from commit ee1a154153a1b20d55fc4b0dd9752277f0cd6451)

Bug: 27123298
Change-Id: Ie37e214f91e4f005f90da0d4a2aba1cd604d60b7/"
,,Conscrypt,"Try to get peer hostname from SocketAddress

Java 7 added a new method to InetSocketAddress called getHostString()
which returns the unresolved host for a given address. This should be
suitable for use with SNI as long as it isn't an IP address.

This also helps with testing because we can use serialization tricks to
rewrite the ""hostname"" field of an already-serialized loopback address.

Bug: 27271561
Change-Id: I9845e57d505712cdfee87d18246a1a3b021deea3/"
,,Conscrypt,"Return an empty list when no OCSP reponses received

Change OpenSSLSessionImpl#getStatusResponses() to return an empty list
instead of null. This matches the assumption of the serializing code in
AbstractSessionContext.

Add a test to make sure that serializing a trivial OpenSSLSessionImpl
instance completes without throwing an exception.

Test: cts-tradefed run cts -d -p android.core.tests.libcore.package.conscrypt
Bug: 30751283
Change-Id: If4c3e6a99c080fb3a0fd527c86a5ee8972475718/"
,,Conscrypt,"Use OCSP-stapled responses in TrustManagerImpl


Change-Id: I45762ab463be7aebea848387677d0fa8f92424bd/"
,,Conscrypt,"Move CertBlacklist to conscrypt

CertBlacklist is mostly unchanged from bouncycastle except removing the
bouncycastle Digest and Hex dependencies in isPublicKeyBlackListed.

(cherry picked from commit ce5bdd0391d93d9a4b1fe7005041271341eb69b2)
Bug: 29397721
Change-Id: Icccdcc0e108e8b0c60c47522114749518247a598/"
,,Conscrypt,"Expose CT through libcore NetworkSecurityPolicy

Bug: 28746284
Change-Id: I6549a997823b38dc256911a66ac558c90bf6f762/"
,,Conscrypt,"Use built-in key debugging mechanism

When debugging a network flow it's useful to log the negotiated keys to
see what is happening inside the session. Previously this was
implemented in Conscrypt, but BoringSSL has this capability built-in
now.



Change-Id: I50a5b315d302492667a28926161836e34b9dd357/"
,,Conscrypt,"JNI_TRACE_KEYS for non-RSA connections

Wireshark uses a different format for connections that are non-RSA such
as those negotiated with ECDHE or DHE. This change prints out the keys
in the format that Wireshark expects.

Change-Id: If81e172091c29fe7e65068091be382677f23e425/"
,,Conscrypt,"Return an empty list when no OCSP reponses received

Change OpenSSLSessionImpl#getStatusResponses() to return an empty list
instead of null. This matches the assumption of the serializing code in
AbstractSessionContext.

Add a test to make sure that serializing a trivial OpenSSLSessionImpl
instance completes without throwing an exception.

Change-Id: I45762ab463be7aebea848387677d0fa8f92424bd/"
Network Management,Network Management,Conscrypt,"Add implementation of OpenSSLSocket that uses OpenSSLEngineImpl
Use above class of wrapped socket does not have a file descriptor
OpenSSLEngine now exposes ALPN state
Enable use of PrivilegedAction to load .so
-------------
Created by MOE: https://github.com/google/moe
MOE_MIGRATED_REVID=123899267/"
Network Management,Network Management,Conscrypt,"Improve performance of OpenSSLEngine

Test: cts-tradefed run cts -m CtsLibcoreTestCases -a arm64-v8a
Test: cts-tradefed run cts -m CtsLibcoreOkHttpTestCases -a arm64-v8a
Change-Id: I947bd3701e90bd65104f8f5c07ba218c4e051944/"
,,Conscrypt,"Use fewer deprecated BoringSSL APIs.

- ""Handshake cutthrough"" was renamed to False Start at some point. Use
  the newer APIs which match what everyone refers to it as.

- SSL_set_reject_peer_renegotiations needed to be generalized at some
  point to take an enum. In doing so, it no longer has this weird
  double-negative spelling.

- The non-_long variants of SSL_alert_type_string and
  SSL_alert_desc_string don't do anything useful. We thought the
  one- and two-character codes were nuts, so they just return '!' and
  '!!' now.

Change-Id: I83d1fa26b0ea05284b0d73f1e2a58df07887aefe
Test: mma in external/conscrypt/Convert NativeCryptoTest to JUnit4

This is basically a regex substitution change with minimnal renames just
to convert to JUnit4. Further JUnit4-isms will come in subsequent
changes.

Test: cts-tradefed run cts -m CtsLibcoreTestCases -a arm64-v8a
Change-Id: Icb6aedc3acee31d62750132bbe8eeaf9150bd3c0/"
,,Conscrypt,"Remove NPN support

NPN is deprecated and clients should use ALPN instead for now. In the
interest of removing support for it in BoringSSL, we will remove NPN
support from Conscrypt as well.

For now keep around the NPN setters and getters in OpenSSLSocketImpl and
OpenSSLEngineImpl to help with backward compatibility for users that may
be using reflection.

Test: make -j32 && make -j32 cts && cts-tradefed run cts -d -m CtsLibcoreTestCases
Change-Id: Ia4edb21412d9c4b2440291ae0a8a97d2217bf5b5/"
,,Conscrypt,"Drop RC4 cipher suite support from TLS

Bug: 30977793
Test: libcore/run-libcore-tests libcore/luni/src/test/java/libcore/javax/net/ssl/* and running NativeCryptoTest.
Change-Id: I04b91a6d3bf75a757d2c74bd1a39aea2709a9199/"
Network Management,Network Management,Conscrypt,"Fix missing CTPolicy argument to TrustManagerImpl()

Incorrect merge resolution on my part dropped the constructor argument.

Test: run OpenSSLSocketImplTest
Change-Id: I9b71782050a59558d49223d398c3ad22847e89a5/"
,,Conscrypt,"ct: Allow configurable policy.

The configuration will be hooked up in a following CL

Test: ran OpenSSLSocketImplTest
Change-Id: Ia9af1564cbe7b37746e2cbfe8c5e9c08eab55c76/"
Network Management,Network Management,Conscrypt,"Fix some leaks on sk_push error.

sk_push only takes ownership of the pointer on success, so the pattern
needs to be slightly different.

Test: cts-tradefed run cts -m CtsLibcoreTestCases -a arm64-v8a
Change-Id: Ic1b10b9aae5addf20bf770c334ada9bc461c97b8/"
,,Conscrypt,"Fix reference counting.

SSL_use_certificate and friends were leaking and client_cert_cb was
failing to give refcounts for objects it returns. The two cancelled each
other out.

Test: cts-tradefed run cts -m CtsLibcoreTestCases -a arm64-v8a
Test: cts-tradefed run cts -m CtsLibcoreOkHttpTestCases -a arm64-v8a
Change-Id: I9e9e75902054f59be12f68fb14cf9f3f75a7a46e/"
,,Conscrypt,"Restrict TLS signing to non-RSA-PSS algorithms

A recent change in BoringSSL allowed connections to use RSA-PSS as the
signing algorithm for TLSv1.2 connections. However, the CryptoUpcalls
interface is not ready for this and it cannot currently make the
upcall correctly to have these signed.

Temporarily disable RSA-PSS signatures with TLS by explicitly setting
the list of signature algorithms.

Test: cts-tradefed run cts -m CtsLibcoreTestCases -a arm64-v8a
Test: cts-tradefed run cts -m CtsLibcoreOkHttpTestCases -a arm64-v8a
Bug: 31714503
Change-Id: Ie1c23b7231b5673816946a6c06030e1e25752415/"
,,Conscrypt,"OpenSSLCipherRSA: add support for OAEP labels

The OAEP label L can be specified by using a custom
PSource.PSpecified with OAEPParameterSpec. This is not commonly used,
but is an option that's easily supported.
Change-Id: Ide7a087b5bc4dde31d55e38a8efdd16e3dd44ba5/NativeCrypto: add debugging format checking when debug off

Before if you enabled WITH_JNI_TRACE you might get some formatting
errors because format is not checked when debugging is enabled. Switch
to constexpr to enable debugging and rely on Dead Code Elimination pass
in the compiler to remove all the debug code when it's not in use. This
allows the compiler to properly check printf-style formatting for debug
statements instead of the preprocessor removing the code.

Test: compile with kWithJniTrace = true and run vogar tests
Change-Id: Ief3fe1c099a38d802db32deb7ffa91e4c8d4a572/"
,,Conscrypt,"No need to call ERR_remove_thread_state.

In BoringSSL, error data is maintained in thread-locals and
automatically released via thread-local destructors.
ERR_remove_thread_state just calls ERR_clear_error now anyway.

https://commondatastorage.googleapis.com/chromium-boringssl-docs/err.h.html#ERR_remove_thread_state

Change-Id: Ie4b54ec0573f58076eba3102079f773425debcdc
Test: mma/Convert EVP_AEAD_CTX to stack-only

The EVP_AEAD_CTX object is only valid for the period of time in which it
is being used, so there is no need to have separate calls to
EVP_AEAD_CTX_init and EVP_AEAD_CTX_cleanup. Sweep the arguments to these
into the EVP_AEAD_CTX_seal and EVP_AEAD_CTX_open calls.
Change-Id: I02b2c5759efdd8f29ec354ee9c10053dd1f9c53c/"
,,Conscrypt,"Switch to BoringSSL scopers

The latest version of BoringSSL includes scopers for various types.
Switch to those to reduce redundancy.

There are a few that are missing and EVP_AEAD_CTX can be rewritten to
not need anything beyond the supplied ScopedEVP_AEAD_CTX in BoringSSL.

Change-Id: I814f10936e6d7eb479da99391723fc9bce4e6096/"
,,Conscrypt,"Substitute NULL for nullptr

Since we don't actually rely on a STL, we don't have <cstdlib> to get
NULL from, but since we're compiling C++11 we get nullptr for free.

This also fixes builds against MacOS SDK since it doesn't have <cstdlib>
available when you explicitly opt out of an STL in the Android.mk module.

Test: mmma -j32 external/conscrypt; make -j29 PRODUCT-sdk_phone_armv7-sdk
Change-Id: I54929c7e5c05ec271925f5f3d1896df1661e9b59/"
,,Conscrypt,"Strip out SSLv3 from enabled protocols for app compat

HttpsURLConnection on Android before Marshmallow tried to
setEnabledProtocols with just ""SSLv3"" without checking if it was a
supported protocol. Instead of throwing IllegalArgumentException when
the unsupported protocol is encountered, strip it out and later throw an
SSLHandshakeException if no protocols are enabled.

Test: cts-tradefed run cts -m CtsLibcoreTestCases -a arm64-v8a
Test: cts-tradefed run cts -m CtsLibcoreOkHttpTestCases -a arm64-v8a
Bug: 32053327
Bug: 30977793
Change-Id: I2f2008d85fcc5b5fbdc71722a3d6e0a9c22bfbc2/"
,,Conscrypt,"Move CT to platform/

This moves the CT code from main to platform and the checking from
OpenSSLSocketImpl to TrustManagerImpl. There's still some plumbing that
needs to be done in main to store the TLS extension data so
TrustManagerImpl can get it.

Test: Run OpenSSLSocketImplTest, verified network connections still work
Change-Id: I643db4668cbec2d1bb221156c5844667ae8701c8/"
,,Conscrypt,"Fix imports globally

Ran script to fix imports globally on all .java files. Committing
results.

Test: mmma -j32 external/conscrypt
Change-Id: I6cd19c0f3dc66c8dcaec3c92a6019b4c0154e9e4/"
,,Conscrypt,"Consolidate EVP_MD references to one place

There were several places where EVP_get_digestbyname was being called
for the same data. Consolidate these all down to one place so there is
no need to call it several times in the same program.

Test: cts-tradefed run cts -m CtsLibcoreTestCases -a arm64-v8a
Change-Id: Ib3f8b678c775e74eb5edaabde42f042d7b4eac95/"
,,Conscrypt,"Track changes to the CT log format

Logs are now stored in /data/misc/keychain/ct_trusted_logs/current/ and
use \n as a delimiter instead of ,.

Bug: 28746284

(cherry picked from commit 788bf530ca321a5d41d546b6888b62c5dc6ad193)

Change-Id: Ib4037cee75fbf96ae6b1c6a1b50952221c22c7be/"
,,Conscrypt,"Move MGF1 algorithm name and OID to EvpMdRef

Test: cts-tradefed run cts -m CtsLibcoreTestCases -a arm64-v8a
Change-Id: Ia3bbceb2a6022ebfbbd7ce1b4c2bb8d8c5ca956b/"
,,Conscrypt,"Move JCA names and OIDs to constants

Test: cts-tradefed run cts -m CtsLibcoreTestCases -a arm64-v8a
Change-Id: I2fb67d6e9aa812b3b2ea26e14d18fbe752c70fc3/Use EvpMdRef for size calculation

Test: cts-tradefed run cts -m CtsLibcoreTestCases -a arm64-v8a
Change-Id: I9c909e401d6224f626b69dae3ac21e16a7a9b03c/"
,,Conscrypt,"OpenSSLSignature: always throw on setting context

Most of the EVP_PKEY_CTX_ctrl error codes are not handled by Conscrypt
so make sure we have a default exception in case something goes awry
during setup.

Change-Id: I1f5a753242b6bc31cca9feb96486bbc86ad8af54/"
,,Conscrypt,"OpenSSLCipherRSA: add support for OAEP labels

The OAEP label L can be specified by using a custom
PSource.PSpecified with OAEPParameterSpec. This is not commonly used,
but is an option that's easily supported.

Change-Id: Ide7a087b5bc4dde31d55e38a8efdd16e3dd44ba5/"
,,Conscrypt,"Add NativeCrypto.EVP_PKEY_CTX_set_rsa_oaep_md

This will allow us to set the OAEP message digest function later on in
addition to the MGF1 message digest function.

Test:  mmma -j32 external/conscrypt && vogar --mode host --classpath out/host/common/obj/JAVA_LIBRARIES/core-tests-support-hostdex_intermediates/classes.jack --classpath out/host/common/obj/JAVA_LIBRARIES/conscrypt-hostdex_intermediates/classes.jack --classpath out/host/common/obj/JAVA_LIBRARIES/conscrypt-tests-hostdex_intermediates/classes.jack com.android.org.conscrypt.NativeCryptoTest
Change-Id: Iee45d973e253f3b5c60919d70571abb96d97bb08/"
,,Conscrypt,"Add EVP_PKEY_{encrypt,decrypt} calls

This adds the structure and tests for the EVP_PKEY_encrypt and
EVP_PKEY_decrypt functions to be called from Java.

Test: mmma -j32 external/conscrypt && vogar --mode host --classpath out/host/common/obj/JAVA_LIBRARIES/core-tests-support-hostdex_intermediates/classes.jack --classpath out/host/common/obj/JAVA_LIBRARIES/conscrypt-hostdex_intermediates/classes.jack --classpath out/host/common/obj/JAVA_LIBRARIES/conscrypt-tests-hostdex_intermediates/classes.jack com.android.org.conscrypt.NativeCryptoTest
Change-Id: I36dfbd6c6f76c5cbda8490375e8579ca32a4babb/"
,,Conscrypt,"Use fewer deprecated BoringSSL APIs.

- ""Handshake cutthrough"" was renamed to False Start at some point. Use
  the newer APIs which match what everyone refers to it as.

- SSL_set_reject_peer_renegotiations needed to be generalized at some
  point to take an enum. In doing so, it no longer has this weird
  double-negative spelling.

- The non-_long variants of SSL_alert_type_string and
  SSL_alert_desc_string don't do anything useful. We thought the
  one- and two-character codes were nuts, so they just return '!' and
  '!!' now.

Change-Id: I83d1fa26b0ea05284b0d73f1e2a58df07887aefe
Test: mma in external/conscrypt/Blacklisting TLS 1.3 ciphersuites from Android

TLS 1.3 adds a new set of AEAD-only ciphers, which will be exposed by
BoringSSL's draft TLS 1.3 implementation. We're not ready to ship TLS 1.3
in Conscrypt yet, but get_cipher_names returns the new ciphers by default
(cipher/version filtering happens much later). Suppress those ciphers for
now.

Test: cts-tradefed run cts -m CtsLibcoreTestCases -m
CtsLibcoreOkHttpTestCases -a arm64-v8a
Change-Id: I14421aec8dceb4b0eb7347b8ebf88a87a10ba856/"
,,Conscrypt,"Drop SSLv3 support

Change-Id: Ic88ff61bb16017e213a017ecdb16a1ac5b9baa48/"
,,Conscrypt,"Convert EVP_AEAD_CTX to stack-only

The EVP_AEAD_CTX object is only valid for the period of time in which it
is being used, so there is no need to have separate calls to
EVP_AEAD_CTX_init and EVP_AEAD_CTX_cleanup. Sweep the arguments to these
into the EVP_AEAD_CTX_seal and EVP_AEAD_CTX_open calls.

Change-Id: I02b2c5759efdd8f29ec354ee9c10053dd1f9c53c/"
Network Management,Network Management,Conscrypt,"Improve performance of OpenSSLEngine

Test: cts-tradefed run cts -m CtsLibcoreTestCases -a arm64-v8a
Test: cts-tradefed run cts -m CtsLibcoreOkHttpTestCases -a arm64-v8a
Change-Id: I947bd3701e90bd65104f8f5c07ba218c4e051944/Move CT to platform/

This moves the CT code from main to platform and the checking from
OpenSSLSocketImpl to TrustManagerImpl. There's still some plumbing that
needs to be done in main to store the TLS extension data so
TrustManagerImpl can get it.

Test: Run OpenSSLSocketImplTest, verified network connections still work
Change-Id: I643db4668cbec2d1bb221156c5844667ae8701c8/"
,,Conscrypt,"OpenSSLEngineImpl: throw ISE if client/server mode not set

According to SSLEngine documentation, IllegalStateException will be
thrown if #wrap or #unwrap is called before setting the client/server
mode. When OpenSSLEngineImpl was written, it was written against the
existing tests which did not fail in this scenario due to a missing
fail() call in the try/catch.

The existing test calls #wrap with a 10 byte buffer which immediatly
hits the BUFFER_OVERFLOW condition. To avoid this the ByteBuffer check
was moved below the state check which means calling #wrap with a
too-small buffer without starting the handshake first will fail on the
buffer size check only after the first call. This should not affect
callers as they have to handle this condition during the normal
operation of the SSLEngine anyway.

Bug: 31301555
Change-Id: I6f4c36d5abcc71e020ce40c7f61df2ed01b1a53e/"
Network Management,Network Management,Conscrypt,"Improve performance of OpenSSLEngine

Change-Id: I947bd3701e90bd65104f8f5c07ba218c4e051944/"
,,Conscrypt,"Use fewer deprecated BoringSSL APIs.

- ""Handshake cutthrough"" was renamed to False Start at some point. Use
  the newer APIs which match what everyone refers to it as.

- SSL_set_reject_peer_renegotiations needed to be generalized at some
  point to take an enum. In doing so, it no longer has this weird
  double-negative spelling.

- The non-_long variants of SSL_alert_type_string and
  SSL_alert_desc_string don't do anything useful. We thought the
  one- and two-character codes were nuts, so they just return '!' and
  '!!' now.

This moves the CT code from main to platform and the checking from
OpenSSLSocketImpl to TrustManagerImpl. There's still some plumbing that
needs to be done in main to store the TLS extension data so
TrustManagerImpl can get it.

This causes deadlocks for wrapped sockets.

Bug: 31449904
Change-Id: Iedf2244ae4ed8bd79dae996406c7668ebadc832e/"
,,Conscrypt,Locking down NativeConstants (#165)/
Network Management,Network Management,Conscrypt,"Locking down public APIs (#157)

Tried to be as aggressive as I could, so this probably deserves a fairly thorough review.  I left most of OpenSSLSocketImpl public, because I think it's needed by a few external projects.

I also did some cleanup work to get rid of a bunch of compiler warnings that we seem to have accumulated.

Fixes #142/"
Network Management,Network Management,Conscrypt,"Locking down public APIs (#157)

Tried to be as aggressive as I could, so this probably deserves a fairly thorough review.  I left most of OpenSSLSocketImpl public, because I think it's needed by a few external projects.

I also did some cleanup work to get rid of a bunch of compiler warnings that we seem to have accumulated.

Fixes #142/Add Java 8 style SNI hostname to OpenSSLEngineImpl (#155)

The SNIHostName, et al., support was lacking from OpenSSLEngineImpl
causing endpoint protocol identification to fail in Netty tests./"
Network Management,Network Management,Conscrypt,"Adding all factory methods for engine socket. (#192)

Also properly throwing SSLHandshakeException in some cases.

Fixes #191/Refactor OpenSSLSocketImplTest to cover both socket types (#182)/"
Network Management,Network Management,Conscrypt,"Locking down public APIs (#157)

Tried to be as aggressive as I could, so this probably deserves a fairly thorough review.  I left most of OpenSSLSocketImpl public, because I think it's needed by a few external projects.

I also did some cleanup work to get rid of a bunch of compiler warnings that we seem to have accumulated.

Fixes #142/Configure OCSP and SCTs on the SSL, not SSL_CTX.

As Conscrypt is currently set up, one SSL_CTX (owned, ultimately, by the
SSLContext) may correspond to multiple SSLParameters which, in the Java
API, are configured on the SSLSocket or SSLEngine directly. Thus we
should use the SSL versions of the APIs which now exist. This avoids
mutating an SSL_CTX which may be shared by multiple SSLs with different
configurations.

Change-Id: I19485c316087004c6050d85520b0169f2ca0d493/"
,,Conscrypt,Benchmark fixes and various cleanup. (#188)/
,,Conscrypt,Make openjdk target support Java 7/Adding platform to the build (#158)/
,,Conscrypt,"Add Java 8 style SNI hostname to OpenSSLEngineImpl (#155)

The SNIHostName, et al., support was lacking from OpenSSLEngineImpl
causing endpoint protocol identification to fail in Netty tests./"
,,Conscrypt,"Fix unwrap bug for large messages. (#189)

If you write a record and don't have enough destination buffer space to read all the plaintext, the plaintext gets left in the plaintext buffer and the next record you write ends up in the ciphertext buffer (and you read the leftover plaintext from the last record), and you continue to have a record sitting in the ciphertext buffer until you get two records that don't fit in the buffer together, at which point you get the short write and subsequent exception.

Also added a test to verify the bug./"
Network Management,Network Management,Conscrypt,"Locking down public APIs (#157)

Tried to be as aggressive as I could, so this probably deserves a fairly thorough review.  I left most of OpenSSLSocketImpl public, because I think it's needed by a few external projects.

I also did some cleanup work to get rid of a bunch of compiler warnings that we seem to have accumulated.

Fixes #142/Introducing top-level Conscrypt class (#152)

This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./Add error-prone and fix all the errors (#146)/"
,,Conscrypt,"Introducing top-level Conscrypt class (#152)

This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./"
Network Management,Network Management,Conscrypt,"Locking down public APIs (#157)

Tried to be as aggressive as I could, so this probably deserves a fairly thorough review.  I left most of OpenSSLSocketImpl public, because I think it's needed by a few external projects.

I also did some cleanup work to get rid of a bunch of compiler warnings that we seem to have accumulated.

Fixes #142/Introducing top-level Conscrypt class (#152)

This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./Configure OCSP and SCTs on the SSL, not SSL_CTX.

As Conscrypt is currently set up, one SSL_CTX (owned, ultimately, by the
SSLContext) may correspond to multiple SSLParameters which, in the Java
API, are configured on the SSLSocket or SSLEngine directly. Thus we
should use the SSL versions of the APIs which now exist. This avoids
mutating an SSL_CTX which may be shared by multiple SSLs with different
configurations.

Change-Id: I19485c316087004c6050d85520b0169f2ca0d493/Adding conversion utility ALPN protocols (#140)

Exposing additional set methods in OpenSSLEngineImpl and OpenSSLSocketImpl to allow the caller to set the ALPN protocols without having to manually encode.

Also simplifying the exposure of the maxSealOverhead value./"
Network Management,Network Management,Conscrypt,"Locking down public APIs (#157)

Tried to be as aggressive as I could, so this probably deserves a fairly thorough review.  I left most of OpenSSLSocketImpl public, because I think it's needed by a few external projects.

I also did some cleanup work to get rid of a bunch of compiler warnings that we seem to have accumulated.

Fixes #142/Introducing top-level Conscrypt class (#152)

This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./"
,,Conscrypt,"Fixing various javadoc issues. (#124)

Also locking down access to a couple utility classes./"
,,Conscrypt,"Implement X509Certificate.verify(PublicKey, Provider).

Removed NoSuchProvider from verifyOpenSSL(OpenSSLKey pkey),
native code underneath doesn't throw it.

Changed unblundled version to current to
fix the breakage due to missing verify method
in earlier API.

(cherry picked from commit f881571464b8b989d000fcd33846ae2a653fb2cf)

Test: CtsLibcoreTestCases
Bug: 31294527
Change-Id: Ide755ccc0ad1163ac8dafc9bce762f680671a488/"
,,Conscrypt,"Add handshake listener to engine. (#136)

Fixes #60/"
,,Conscrypt,"Make SSLUtils public for testing but @Internal

On Android the class-under-test is going to be in the bootclasspath
ClassLoader, but the test-class will be in the ""app"" ClassLoader. This
means that you can't directly test package-scoped classes or methods.

Make this public so we can test it on Android, but marked @Internal so
it doesn't end up in the Javadocs.

Test: cts-tradefed run cts -m CtsLibcoreTestCases -a arm64-v8a
Change-Id: I127693e1092fa9016c4a5d89218cc8de0c3421a5/"
,,Conscrypt,"Adding conversion utility ALPN protocols (#140)

Exposing additional set methods in OpenSSLEngineImpl and OpenSSLSocketImpl to allow the caller to set the ALPN protocols without having to manually encode.

Also simplifying the exposure of the maxSealOverhead value./"
,,Conscrypt,"Less restrictive output buffer size in wrap() (#114)

We currently require that the output buffer be >= MAX_PACKET_SIZE. This is needlessly strict and causes the Netty tests to fail, since they only use 2k buffers.

This PR copies over some of the recent changes from Netty to handle this properly./"
Network Management,Network Management,Conscrypt,Importing more Android integ tests. (#184)/
,,Conscrypt,"Configure OCSP and SCTs on the SSL, not SSL_CTX.

As Conscrypt is currently set up, one SSL_CTX (owned, ultimately, by the
SSLContext) may correspond to multiple SSLParameters which, in the Java
API, are configured on the SSLSocket or SSLEngine directly. Thus we
should use the SSL versions of the APIs which now exist. This avoids
mutating an SSL_CTX which may be shared by multiple SSLs with different
configurations.

Change-Id: I19485c316087004c6050d85520b0169f2ca0d493/"
,,Conscrypt,"Exposing SSL_max_seal_overhead (#135)

Also adding a method to calculate the maximum buffer size required for a wrap operation./Remove DHE/"
,,Conscrypt,"Create @hide Doclet for public API docs

This allows us to use @hide to prevent a class from showing up in the
public API documentation./"
,,Conscrypt,"Extract AEADBadTagException throwing to method

This is not available on all Android versions, so we access it via
reflection here. Extract it to its own method so we can suppress the
Error-Prone warning more precisely./"
Network Management,Network Management,Conscrypt,"Adding all factory methods for engine socket. (#192)

Also properly throwing SSLHandshakeException in some cases.

Fixes #191/Introducing top-level Conscrypt class (#152)

This is a one-stop-shop for creating and configuring Conscrypt types. It allows a standard way for configuring extended settings that are not currently supported by the standard Java APIs./"
Network Management,Network Management,Conscrypt,"Adding all factory methods for engine socket. (#192)

Also properly throwing SSLHandshakeException in some cases.

Fixes #191/"
,,Conscrypt,"Suppress Error-Prone warnings

These warnings are not useful here, so suppress them./"
,,Conscrypt,Allow handshakeListener to be set when engineState is MODE_SET (#137)/
Network Management,Network Management,Conscrypt,Refactor OpenSSLSocketImplTest to cover both socket types (#182)/
,,Conscrypt,"Importing Android integration tests (#178)/Locking down public APIs (#157)

Tried to be as aggressive as I could, so this probably deserves a fairly thorough review.  I left most of OpenSSLSocketImpl public, because I think it's needed by a few external projects.

I also did some cleanup work to get rid of a bunch of compiler warnings that we seem to have accumulated.

Fixes #142/"
Network Management,Network Management,Conscrypt,"Add additional aliases.

These aliases are provided by Bouncy Castle for these algorithms, so we're
adding them as well so that anyone using them can get the Conscrypt
versions without having to change their code./"
,,Conscrypt,"Some parsing and serializing fixes. (#219)

This fixes a memory leak in NativeCrypto_i2d_PKCS7. It never frees
derBytes. Also removing a dependency on the legacy ASN.1 stack./"
,,Conscrypt,"Refactoring session management (#172)

This change breaks session management into two distinct types:

- SslSessionWrapper: These are created as BoringSSL calls back the new session handler, allowing the application to cache sessions. Clients will also offer these to BoringSSL for reuse if a compatible session was found. BoringSSL is free to use it or not, but the Conscrypt code no longer makes assumptions here. Instead, it always uses the ActiveSession.

- ActiveSession: This is a session that wraps the SSL instance (rather than the SSL_SESSION wherever possible). That way no concern has to be paid to what BoringSSL is doing with sessions under the covers.

Fixes #98/"
,,Conscrypt,"Use InetAddress originalHostName (if present) for getHostnameOrIP (#303)

* Use InetAddress originalHostName (is present) for getHostnameOrIP

Test: CtsLibcoreTestCases
Bug: 35942385
Bug: 31028374
Change-Id: Ie1acc2dd23fadbbc48de1a5845146e9a5953e9cf

* Fix lint problems and remove class introduced in API 19./"
,,Conscrypt,"Support Java 6 Runtime (#299)

Various fixes to support Java 6, 7, and 8. Separating out
utility classes (for openjdk) to be explicit as to which methods are
supported by particular Java version.

Adding the ability to specify the test JVM to use on the command-line.

For example, the following will build with the default Java
installation, but will run the openjdk and integ-tests with Java 6:

./gradlew build -DjavaExecutable64=${JAVA6_HOME}/bin/java

Fixes #298/"
,,Conscrypt,Various fixes to the Conscrypt engine. (#201)/
,,Conscrypt,"Switch to SSL_get0_peer_certificates.

This works towards issue #258. So the exception can be routed out
properly, this moves the SSL_get0_peer_certificates call to after
doHandshake completes in ConscryptFileDescriptorSocket.

Although, due to False Start (the ""cut-through"" logic in that class),
the handshake may not be fully complete at the time, BoringSSL's API is
such that the certificates and other properties will be available once
SSL_do_handshake first completes./Pass encoded local certs to BoringSSL (#253)

Current code was encoding and then decoding the certs before finally
passing them to the native code. We were also separately setting
and then verifying the private key.  All of this can be replaced
with a single JNI call SSL_set_chain_and_key, which accepts the
encoded certs (we don't have to decode them again).

See #247

This shows a perf bump for the handshake (from ~750 to 800 ops/sec)./"
,,Conscrypt,"Cleaning up JNI exceptions (#252)

There were a bunch of exceptions that are being thrown from JNI methods that aren't currently declared.

Also removed a few unused JNI methods and duplicate constants, preferring those from NativeConstants./"
,,Conscrypt,"Remove Java <-> OpenSSL name mapping. (#227)

As of [0], BoringSSL supports the standard cipher suite names. The Java
names are the same, with the exception of
TLS_RSA_WITH_3DES_EDE_CBC_SHA/SSL_RSA_WITH_3DES_EDE_CBC_SHA for
historical reasons. Add code to map between that exception but otherwise
rely on the native support.

[0] https://boringssl.googlesource.com/boringssl/+/6fff386492d9f316f5f780ff9d0ddaf1700f98a9,/"
Network Management,Network Management,Conscrypt,"Support Java 6 Runtime (#299)

Various fixes to support Java 6, 7, and 8. Separating out
utility classes (for openjdk) to be explicit as to which methods are
supported by particular Java version.

Adding the ability to specify the test JVM to use on the command-line.

For example, the following will build with the default Java
installation, but will run the openjdk and integ-tests with Java 6:

./gradlew build -DjavaExecutable64=${JAVA6_HOME}/bin/java

Fixes #298/Conformance fixes for the engine-based socket. (#202)

This allows the SSLSocketTest to pass with the engine-based socket
enabled.

Also restructuring the inheritance hierarchy so that the FD and engine
sockets both behave the same way (both either wrappers or not). The
restructure involves the following:

- AbstractConscryptSocket: New base class for both sockets. It handles
the wrap/no-wrap logic.

- OpenSSLSocketImplWrapper: deleted and replaced by
AbstractConscryptSocket.

- OpenSSLSocketImpl: reduced to a public shim class between
AbstractConscryptSocket and the implementations. For backward-compat
only.

- ConscryptFileDescriptorSocket: Renamed from OpenSSLSocketImpl. The
old FD socket./"
,,Conscrypt,"Remove some dead code from the loader. (#291)

The Fedora-specific code is for system OpenSSL, which doesn't apply to
Conscrypt./"
,,Conscrypt,"User-friendly errors when native library fails to load (#295)

Currently, we throw in static initialization of NativeCryptoJni, which
leads to ClassNotFoundException whenever we try to access the native
code at runtime. The user has no way of knowing that the library failed
to load, or why. This PR attempts to make it more clear as to what
went wrong, by doing the following:

- Log each load error, in the order attempted.
- Select the ""best"" load error to be thrown
- Don't throw from the NativeCrypto static initializer. Rather, save
the best error and throw when attempting to access top-level conscrypt
classes./"
,,Conscrypt,"Support Java 6 Runtime (#299)

Various fixes to support Java 6, 7, and 8. Separating out
utility classes (for openjdk) to be explicit as to which methods are
supported by particular Java version.

Adding the ability to specify the test JVM to use on the command-line.

For example, the following will build with the default Java
installation, but will run the openjdk and integ-tests with Java 6:

./gradlew build -DjavaExecutable64=${JAVA6_HOME}/bin/java

Fixes #298/Encode certs in verify callback (#248)

This code was copied from Netty/netty-tcnative and seems to
significantly increases performance of the verify callback. Before
we call back to Java, we first encode all of the certs and then
decode them in Java into X509Certificate instances. Previous code
was calling into JNI for each method in the certificate.

This helps in addressing #247./Refactoring session management (#172)

This change breaks session management into two distinct types:

- SslSessionWrapper: These are created as BoringSSL calls back the new session handler, allowing the application to cache sessions. Clients will also offer these to BoringSSL for reuse if a compatible session was found. BoringSSL is free to use it or not, but the Conscrypt code no longer makes assumptions here. Instead, it always uses the ActiveSession.

- ActiveSession: This is a session that wraps the SSL instance (rather than the SSL_SESSION wherever possible). That way no concern has to be paid to what BoringSSL is doing with sessions under the covers.

Fixes #98/Various fixes to the Conscrypt engine. (#201)/"
,,Conscrypt,"Support Java 6 Runtime (#299)

Various fixes to support Java 6, 7, and 8. Separating out
utility classes (for openjdk) to be explicit as to which methods are
supported by particular Java version.

Adding the ability to specify the test JVM to use on the command-line.

For example, the following will build with the default Java
installation, but will run the openjdk and integ-tests with Java 6:

./gradlew build -DjavaExecutable64=${JAVA6_HOME}/bin/java

Fixes #298/Implement Caliper benchmarks for running on Android devices (#10). (#278)

Netty tcnative doesn't work on ARM, so we split out the Netty-dependent
benchmark portions into benchmark-openjdk and put non-Netty versions
in benchmark-android.

The gradle operations required to execute the benchmarks on devices are ugly,
but they appear to work.  This might get better some time in the future
with improved Caliper support for Android.

This only includes the time-based benchmarks, not the throughput-based ones,
since those will require some more modifications to work in Caliper and these
at least get us started./"
,,Conscrypt,"Expanding benchmarks. (#239)

Including benchmarks for:

- handshake
- Netty's OPENSSL_REFCNT provider

Partial fix for: #156

Handshake benchmarks show conscrypt may have some work to do, but
it's not clear yet how much of the cost is creation of the engines.
Refactoring benchmarks. (#233)

Adding a set of base benchmarks that are framework agnostic. This
will allow us to extend them to support caliper or JMH. We don't
want to mix/match caliper and JMH in the same code because Android
can now support caliper, but not JMH./"
,"Memory Management, Upgrade Features",Conscrypt,"Switch to CRYPTO_BUFFER client CA APIs.

This gets us closer to TLS_with_buffers_method (issue #258). It also,
along the way, much more thoroughly checks errors of all the various
operations involved./Fixing leak introduced in #253 (#264)/"
,,Conscrypt,Change unsigned to int to avoid signed-unsigned comparison. (#261)/
,,Conscrypt,"Encode certs in verify callback (#248)

This code was copied from Netty/netty-tcnative and seems to
significantly increases performance of the verify callback. Before
we call back to Java, we first encode all of the certs and then
decode them in Java into X509Certificate instances. Previous code
was calling into JNI for each method in the certificate.

This helps in addressing #247./Pass encoded local certs to BoringSSL (#253)

Current code was encoding and then decoding the certs before finally
passing them to the native code. We were also separately setting
and then verifying the private key.  All of this can be replaced
with a single JNI call SSL_set_chain_and_key, which accepts the
encoded certs (we don't have to decode them again).

See #247

This shows a perf bump for the handshake (from ~750 to 800 ops/sec)./"
,,Conscrypt,"Avoid util class pattern. (#282)

It's unusual in C++ to use classes for utility methods (common in Java).
Reworking the utility classes to be just namespaces with global variables
and methods./"
,,Conscrypt,"Fix a bunch of warnings. (#254)

Removes unused variables, fixes cases of non-final ALL_CAPS_VARS, removes
wildcard import./Various fixes to the Conscrypt engine. (#201)/"
,,Conscrypt,"Support Java 6 Runtime (#299)

Various fixes to support Java 6, 7, and 8. Separating out
utility classes (for openjdk) to be explicit as to which methods are
supported by particular Java version.

Adding the ability to specify the test JVM to use on the command-line.

For example, the following will build with the default Java
installation, but will run the openjdk and integ-tests with Java 6:

./gradlew build -DjavaExecutable64=${JAVA6_HOME}/bin/java

Fixes #298/Refactoring session management (#172)

This change breaks session management into two distinct types:

- SslSessionWrapper: These are created as BoringSSL calls back the new session handler, allowing the application to cache sessions. Clients will also offer these to BoringSSL for reuse if a compatible session was found. BoringSSL is free to use it or not, but the Conscrypt code no longer makes assumptions here. Instead, it always uses the ActiveSession.

- ActiveSession: This is a session that wraps the SSL instance (rather than the SSL_SESSION wherever possible). That way no concern has to be paid to what BoringSSL is doing with sessions under the covers.

Fixes #98/"
,,Conscrypt,"Add availability checks (#216)

Fixes #211/Various fixes to the Conscrypt engine. (#201)/"
,,Conscrypt,"Remove Java <-> OpenSSL name mapping. (#227)

As of [0], BoringSSL supports the standard cipher suite names. The Java
names are the same, with the exception of
TLS_RSA_WITH_3DES_EDE_CBC_SHA/SSL_RSA_WITH_3DES_EDE_CBC_SHA for
historical reasons. Add code to map between that exception but otherwise
rely on the native support.

* Implement AlgorithmParameters.GCM in Conscrypt.

In order to handle the ASN.1 encoding, exposes a subset of the ASN.1
encoding API from BoringSSL in NativeCrypto.

* Rename {write,read}_integer to {write,read}_uint64.

Add a UniquePtr to ensure exceptions don't cause a memory leak./"
,,Conscrypt,"Add availability checks (#216)

Fixes #211/"
,,Conscrypt,"Support Java 6 Runtime (#299)

Various fixes to support Java 6, 7, and 8. Separating out
utility classes (for openjdk) to be explicit as to which methods are
supported by particular Java version.

Adding the ability to specify the test JVM to use on the command-line.

For example, the following will build with the default Java
installation, but will run the openjdk and integ-tests with Java 6:

./gradlew build -DjavaExecutable64=${JAVA6_HOME}/bin/java

Fixes #298/Use Java logging consistently (#294)

There are many places in the code that print to System.out/err. We
should use the standard Java logging facilities.

Fixes #212/"
,,Conscrypt,"Fix a bunch of warnings. (#254)

Removes unused variables, fixes cases of non-final ALL_CAPS_VARS, removes
wildcard import./Refactoring session management (#172)

This change breaks session management into two distinct types:

- SslSessionWrapper: These are created as BoringSSL calls back the new session handler, allowing the application to cache sessions. Clients will also offer these to BoringSSL for reuse if a compatible session was found. BoringSSL is free to use it or not, but the Conscrypt code no longer makes assumptions here. Instead, it always uses the ActiveSession.

- ActiveSession: This is a session that wraps the SSL instance (rather than the SSL_SESSION wherever possible). That way no concern has to be paid to what BoringSSL is doing with sessions under the covers.

Fixes #98/"
Network Management,Network Management,Conscrypt,"Conformance fixes for the engine-based socket. (#202)

This allows the SSLSocketTest to pass with the engine-based socket
enabled.

Also restructuring the inheritance hierarchy so that the FD and engine
sockets both behave the same way (both either wrappers or not). The
restructure involves the following:

- AbstractConscryptSocket: New base class for both sockets. It handles
the wrap/no-wrap logic.

- OpenSSLSocketImplWrapper: deleted and replaced by
AbstractConscryptSocket.

- OpenSSLSocketImpl: reduced to a public shim class between
AbstractConscryptSocket and the implementations. For backward-compat
only.

- ConscryptFileDescriptorSocket: Renamed from OpenSSLSocketImpl. The
old FD socket./"
Network Management,Network Management,Conscrypt,"Conformance fixes for the engine-based socket. (#202)

This allows the SSLSocketTest to pass with the engine-based socket
enabled.

Also restructuring the inheritance hierarchy so that the FD and engine
sockets both behave the same way (both either wrappers or not). The
restructure involves the following:

- AbstractConscryptSocket: New base class for both sockets. It handles
the wrap/no-wrap logic.

- OpenSSLSocketImplWrapper: deleted and replaced by
AbstractConscryptSocket.

- OpenSSLSocketImpl: reduced to a public shim class between
AbstractConscryptSocket and the implementations. For backward-compat
only.

- ConscryptFileDescriptorSocket: Renamed from OpenSSLSocketImpl. The
old FD socket./Various fixes to the Conscrypt engine. (#201)/"
,,Conscrypt,"Allow beginHandshake() when a handshake is in process. (#293)

The documentation for beginHandshake() says that IllegalStateException is
thrown if the client/server mode hasn't been set, and it's reported that
the OpenJDK implementation and previous Android releases both allowed
calling beginHandshake() when a handshake is in progress.  Since
beginHandshake() is documented as being non-blocking, this should be safe."
Network Management,Network Management,Conscrypt,"Move closer to the old test version
The original test never called shutdownInput() since the Javadoc
indicates it will make the socket behave in a different manner than
expected in this test.
Also assert that we're not using the type of socket that we expect to
return -1 if we get a SocketException. This will indicate when this
difference is fixed and the test can be changed to reflect the"
,,Conscrypt,"Conformance fixes for the engine-based socket. (#202)

This allows the SSLSocketTest to pass with the engine-based socket
enabled.

Also restructuring the inheritance hierarchy so that the FD and engine
sockets both behave the same way (both either wrappers or not). The
restructure involves the following:

- AbstractConscryptSocket: New base class for both sockets. It handles
the wrap/no-wrap logic.

- OpenSSLSocketImplWrapper: deleted and replaced by
AbstractConscryptSocket.

- OpenSSLSocketImpl: reduced to a public shim class between
AbstractConscryptSocket and the implementations. For backward-compat
only.

- ConscryptFileDescriptorSocket: Renamed from OpenSSLSocketImpl. The
old FD socket./Updates to SSLSocketTest to support engine-based socket. (#199)/"
,,Conscrypt,"Fix SSLSessionTest flakiness.

connect never waited for the futures to resolve./"
,,Conscrypt,"Fetch the peer certificates if necessary when handshake completes. (#256)

When BoringSSL resumes a session, it doesn't call cert_verify_callback
because it presumes the certs were verified on previous calls, so we need
to fetch them explicitly in that case.  Also adds a test that a resumed
session has proper certs./"
,,Conscrypt,"Customize default provider name based on platform (#314)

Fixes #312/"
Network Management,Network Management,Conscrypt,"Support renegotiation with sockets (#321)

Fixes #228
Fixes #310/"
,,Conscrypt,"Add utility method for identifying a Conscrypt Provider. (#313)

Also flattening the Conscrypt class. The current structure with
subclasses helps to organize, but it makes API stability harder.

Partially addresses #312/"
Network Management,Network Management,Conscrypt,"Adding support for Java 9 server-side ALPN protocol selection. (#319)

Java 9 adds a setHandshakeApplicationProtocolSelector method to both
SSLEngine and SSLSocket that allows the application to provide a
BiFunction to choose the protocol. This PR attempts to provide
support for this method while still maintaining backward compatibility
with ealier versions of Java.

Fixes #316/"
Network Management,Network Management,Conscrypt,"Add missing Java 9 methods for ALPN (#339)

Fixes #316/"
,,Conscrypt,"Adding support for Java 9 server-side ALPN protocol selection. (#319)

Java 9 adds a setHandshakeApplicationProtocolSelector method to both
SSLEngine and SSLSocket that allows the application to provide a
BiFunction to choose the protocol. This PR attempts to provide
support for this method while still maintaining backward compatibility
with ealier versions of Java.

Fixes #316/"
Network Management,Network Management,Conscrypt,"Adding support for Java 9 server-side ALPN protocol selection. (#319)

Java 9 adds a setHandshakeApplicationProtocolSelector method to both
SSLEngine and SSLSocket that allows the application to provide a
BiFunction to choose the protocol. This PR attempts to provide
support for this method while still maintaining backward compatibility
with ealier versions of Java.

Fixes #316/"
,,Conscrypt,"Support renegotiation with sockets (#321)

Fixes #228
Fixes #310/"
,,Conscrypt,"Add utility method for identifying a Conscrypt Provider. (#313)

Also flattening the Conscrypt class. The current structure with
subclasses helps to organize, but it makes API stability harder.

Partially addresses #312/"
,,Conscrypt,"Move the OS-identification code into NativeLibraryLoader (#332)

This makes it easier to replace in scenarios where the user wants
to load libraries differently, since it doesn't require any changes
to Platform, it just requires replacing NativeCryptoJni and
NativeLibraryLoader wholesale./"
Network Management,Network Management,Conscrypt,"Specify all TLS versions 1.0-1.2. (#360)

If you specify a set of TLS versions with a gap in them, all versions
above the gap are ignored, because the TLS protocol actually only
includes a single number (the highest protocol version that's allowable).
So the current version was specifying TLS 1.0 only.  This fixes it to use
whatever version is the highest available./Support renegotiation with sockets (#321)

Fixes #228
Fixes #310/"
Network Management,Network Management,Conscrypt,"Add missing Java 9 methods for ALPN (#339)

Fixes #316/"
,,Conscrypt,"Reorder app_data.h field init ordering. (#337)

The ordering needs to be in the same order as the fields to comply with
strict warnings./"
,,Conscrypt,"Adding support for Java 9 server-side ALPN protocol selection. (#319)

Java 9 adds a setHandshakeApplicationProtocolSelector method to both
SSLEngine and SSLSocket that allows the application to provide a
BiFunction to choose the protocol. This PR attempts to provide
support for this method while still maintaining backward compatibility
with ealier versions of Java.

Fixes #316/"
Network Management,Network Management,Conscrypt,"Implement unadorned ChaCha20. (#367)

ChaCha20 is a stream cipher in its own right, and it was pointed
out that it was weird that ing ""ChaCha20"" alone from Conscrypt
returned an implementation of ChaCha20+Poly1305.  This implements
plain ChaCha20 and makes it the default implementation, so to access
ChaCha20+Poly1305 the caller must ask for ChaCha20/Poly1305/NoPadding
explicitly.

We haven't made a release with ChaCha20 in it yet, so it should be fine
to change the meaning of ing ""ChaCha20""./"
,,Conscrypt,"Implement ChaCha20 support. (#356)

Adds Cipher, KeyGenerator, and AlgorithmParameters for ChaCha20.

Main changes are in OpenSSLCipher$EVP_AEAD.  Refactored so the GCM-
specific portions are all in the GCM subclass and the generic AEAD
portions (such as not allowing reuse of key/IV combinations) are in
the superclass.

Also updates so that calling Cipher.init() on an instance of
AES/GCM/NoPadding with a GCM AlgorithmParameters will pick up the
values from the AlgorithmParameters (previously, it would only work
with GCMParameterSpec or something convertable to IvParameterSpec)./Use public API to get session ID. (#348)/"
,,Conscrypt,"Add missing Java 9 methods for ALPN (#339)

Fixes #316/"
,,Conscrypt,"Adding support for Java 9 server-side ALPN protocol selection. (#319)

Java 9 adds a setHandshakeApplicationProtocolSelector method to both
SSLEngine and SSLSocket that allows the application to provide a
BiFunction to choose the protocol. This PR attempts to provide
support for this method while still maintaining backward compatibility
with ealier versions of Java.

Fixes #316/"
Network Management,Network Management,Conscrypt,"Switch from wrapping to subclassing for Java 9 methods. (#359)

Older Android apps rely on OpenSSLSocketImpl being in the superclass
chain of sockets, which means wrapping them can cause problems.  Instead,
use a subclass to add the appropriate methods.  This also makes the
subclass implementation quite clean, but it pollutes the Platform classes
with a bunch of methods and means we have to remember not to use the
constructors directly./"
,,Conscrypt,"Provide the issuers when calling chooseClientAlias. (#344)

I can't see any reason why the issuers shouldn't be provided, and this
method hasn't been changed in at least 4 years, so I expect it was
just an oversight.

Fixes #323./"
,,Conscrypt,"Add missing Java 9 methods for ALPN (#339)

Fixes #316/"
,,Conscrypt,"Adding support for Java 9 server-side ALPN protocol selection. (#319)

Java 9 adds a setHandshakeApplicationProtocolSelector method to both
SSLEngine and SSLSocket that allows the application to provide a
BiFunction to choose the protocol. This PR attempts to provide
support for this method while still maintaining backward compatibility
with ealier versions of Java.

Fixes #316/"
Network Management,Network Management,Conscrypt,"Adding support for Java 9 server-side ALPN protocol selection. (#319)

Java 9 adds a setHandshakeApplicationProtocolSelector method to both
SSLEngine and SSLSocket that allows the application to provide a
BiFunction to choose the protocol. This PR attempts to provide
support for this method while still maintaining backward compatibility
with ealier versions of Java.

Fixes #316/Support renegotiation with sockets (#321)

Fixes #228
Fixes #310/Cleanly send and receive close_notify alerts (#325)

As part:

* Don't call SSL_clear() when shutting down an SSL, because we need
  to be able to hand it close_notify messages still.  Since both SSLEngine
  and SSLSocket aren't reusable, this is safe.

* Return SSL_ERROR_ZERO_RETURN explicitly when it happens, so that we
  can properly adjust our state in response to a close_notify being received.

* Check if there are pending bytes to be sent when wrap() is called, even if
  the SSLEngine is closed./"
,,Conscrypt,Cleaning up various warnings. (#365)/
,,Conscrypt,"Allow modes (ECB and NONE) and padding (NOPADDING) for ARC4. (#361)

This allows callers to  ARC4/NONE/NOPADDING as an equivalent
name to ing just ARC4.  Both NONE and ECB are supported as
modes for RSA and for other providers' implementations of stream
ciphers like ARC4 that don't use modes, so we accept both here as
well./Implement ChaCha20 support. (#356)

Adds Cipher, KeyGenerator, and AlgorithmParameters for ChaCha20.

Main changes are in OpenSSLCipher$EVP_AEAD.  Refactored so the GCM-
specific portions are all in the GCM subclass and the generic AEAD
portions (such as not allowing reuse of key/IV combinations) are in
the superclass.

Also updates so that calling Cipher.init() on an instance of
AES/GCM/NoPadding with a GCM AlgorithmParameters will pick up the
values from the AlgorithmParameters (previously, it would only work
with GCMParameterSpec or something convertable to IvParameterSpec)./"
,,Conscrypt,"Add missing Java 9 methods for ALPN (#339)

Fixes #316/"
,,Conscrypt,"Implement ChaCha20 support. (#356)

Adds Cipher, KeyGenerator, and AlgorithmParameters for ChaCha20.

Main changes are in OpenSSLCipher$EVP_AEAD.  Refactored so the GCM-
specific portions are all in the GCM subclass and the generic AEAD
portions (such as not allowing reuse of key/IV combinations) are in
the superclass.

Also updates so that calling Cipher.init() on an instance of
AES/GCM/NoPadding with a GCM AlgorithmParameters will pick up the
values from the AlgorithmParameters (previously, it would only work
with GCMParameterSpec or something convertable to IvParameterSpec)./"
Network Management,Network Management,Conscrypt,"Throw SocketException if the FD socket has had its SSL freed (#343)

Under some circumstances (which are repeatable, but I haven't been able
to lock down), the HTTP connection reuse system in the JDK will attempt to
reuse a finalized ConscryptFileDescriptorSocket.  When this happens, the
socket throws a NullPointerException from the native code when methods
are called, because the native SSL object has been freed, which causes
a lot of problems.  Instead, have the socket throw SocketException instead,
which everything understands and responds properly to.

As best as I can tell, this is happening because the finalizer of some
object that's not ours but has a strong reference to our socket adds
a new strong reference, but our finalizer has already been enqueued
and thus is run even though the object then can later be reused.  I haven't
been able to find this finalizer, but everything tolerates the code as
written and responds properly to the SocketException, so it seems like a
good solution until we get rid of the FD socket entirely.

Fixes #331./"
,,Conscrypt,"Adding support for Java 9 server-side ALPN protocol selection. (#319)

Java 9 adds a setHandshakeApplicationProtocolSelector method to both
SSLEngine and SSLSocket that allows the application to provide a
BiFunction to choose the protocol. This PR attempts to provide
support for this method while still maintaining backward compatibility
with ealier versions of Java.

Fixes #316/"
Network Management,Network Management,Conscrypt,"Implement ChaCha20 support. (#356)

Adds Cipher, KeyGenerator, and AlgorithmParameters for ChaCha20.

Main changes are in OpenSSLCipher$EVP_AEAD.  Refactored so the GCM-
specific portions are all in the GCM subclass and the generic AEAD
portions (such as not allowing reuse of key/IV combinations) are in
the superclass.

Also updates so that calling Cipher.init() on an instance of
AES/GCM/NoPadding with a GCM AlgorithmParameters will pick up the
values from the AlgorithmParameters (previously, it would only work
with GCMParameterSpec or something convertable to IvParameterSpec)./Adding support for Java 9 server-side ALPN protocol selection. (#319)

Java 9 adds a setHandshakeApplicationProtocolSelector method to both
SSLEngine and SSLSocket that allows the application to provide a
BiFunction to choose the protocol. This PR attempts to provide
support for this method while still maintaining backward compatibility
with ealier versions of Java.

Fixes #316/"
Network Management,Network Management,Conscrypt,"Adding support for Java 9 server-side ALPN protocol selection. (#319)

Java 9 adds a setHandshakeApplicationProtocolSelector method to both
SSLEngine and SSLSocket that allows the application to provide a
BiFunction to choose the protocol. This PR attempts to provide
support for this method while still maintaining backward compatibility
with ealier versions of Java.

Fixes #316/Support renegotiation with sockets (#321)

Fixes #228
Fixes #310/"
,,Conscrypt,"More finalization safety. (#410)

This updates OpenSSLX509Certificate and OpenSSLX509CRL in the same way
as NativeSsl was done previously./"
Network Management,Network Management,Conscrypt,"Refactoring externalization of SSLSessions (#383)

This is an implementation to #381. This change attempts to provide more
consistency to the session that is returned to the caller by `ConscryptEngine`/`ConscryptFileDescriptorSocket`.

Main changes:

- New interface ConscryptSession adds a few methods currently only defined by ActiveSession
- New interface SessionDecorator that defines getDelegate()
- New class ProvidedSessionDecorator delegates to an external provider of the ""current"" session. The provider implementations are in ConscryptEngine and ConscryptFileDescriptorSocket.
- New class SessionSnapshot that takes a snapshot of any ConscryptSession.
- Changed ActiveSession and SSLNullSession to implement ConscryptSession.
- Updated ConscryptEngine/ConscryptFileDescriptorSocket to create a SessionSnapshot when closing.

Additional cleanup:

- Split out Java7SessionWrapper into two classes: Java7ExtendedSSLSession and Java8ExtendedSSLSession. The Java 8 version no longer requires reflection and is more consistent with platform-specific code elsewhere. Both classes implement SessionDecorator.
- Renamed SslWrapper->NativeSsl and SslSessionWrapper->NativeSslSession for clarity, since the term ""wrapper"" was being overloaded.

Fixes #379/"
Network Management,Network Management,Conscrypt,"Pass NativeSsl references to NativeCrypto (#408)

* Pass NativeSsl references to NativeCrypto

The existing implementation of passing raw addresses to NativeCrypto
can cause issues where the native code may still be executing when the
finalizer runs and frees the underlying native resources.  A call to
NativeSsl.read(), for instance, is not enough to keep the NativeSsl or
its owning socket alive, so if it's waiting for input the finalizer
can run.  Switching to passing the Java object to native code keeps
the Java object alive for GC purposes, preventing its finalizer from
running.

As part of this, also move the freeing of NativeSsl instances into a
finalizer on NativeSsl instead of on the sockets.  The sockets can
still become garbage even if the NativeSsl is kept alive, so we only
want to free it when the NativeSsl itself is garbage.

We will also want to do this for other native objects, but SSL*
instances are by far the most-used native objects and the most likely
to be used in a long-running I/O operation, so starting here gives us
a lot of benefit.

* Reliably close objects in tests.

* Pass both pointer and Java reference.

This allows us to access the SSL* pointer without having to indirect
through the Java object's fields, but still prevents the NativeSsl
from being GCed while the method is being run.

* Explain unsafe finalization fix in NativeCrypto Javadoc./"
,,Conscrypt,"Update testing infrastructure. (#393)

This is part one of migrating the source of truth for tests from
Conscrypt to AOSP.  This sets up the basic layout we want to use:
tests under org.conscrypt.** that test the installed provider (or all
installed providers, for future crypto-related tests), and a separate
test suite class used by the Gradle build that installs the provider.
That way these tests can easily be used in any environment that can
properly install the provider to be tested, including AOSP.

This also updates the test classes with a few changes that had been
made to AOSP, mostly additional test cases./"
,,Conscrypt,"Add compile option for checking error queue. (#416)

We need to ensure that the BoringSSL error stack is clear when
returning from native functions, otherwise a later function might
inspect the error stack and interpret it incorrectly.

Adds the compile option CONSCRYPT_CHECK_ERROR_QUEUE which enables the
macro CHECK_ERROR_QUEUE_ON_RETURN.  That macro, when enabled, creates
a class that checks the error queue is empty in its destructor.  The
macro has been added to almost every native method called from Java,
enforcing that the error queue is empty when we return from native
code back to Java code.

Adds the gradle property checkErrorQueue to enable the code and adds
it to the Travis config.

Also fixes a couple places found by this checking that we were failing
to clear the error queue after handling errors./"
,,Conscrypt,"Clean up exception throwing in native code. (#417)

Change throwExceptionIfNecessary to throwExceptionFromBoringSSLError.
The definition changes from throwing an exception if there's an error
on the stack to having a precondition of having an error on the stack.
This makes its behavior and the expected usage more clear (it always
results in an exception pending).  This also should let us know if
we're encountering return-failure-but-don't-stack-an-error situations
that we didn't know about.

Normalize function names to throwFooException.

Ensure that we always return immediately after throwing an exception.
Some call sites allowed the exception-throwing branch to fall through
to the return statement from a non-throwing branch, which is unclear,
since that return statement is useless, and runs the risk of
additional code being inserted after the exception.

Fixes #97./"
Network Management,Network Management,Conscrypt,"Throw SocketException on ERR_SSL_SYSCALL. (#430)

SSLSocketTest#test_SSLSocket_interrupt_readWrapperAndCloseUnderlying
is failing periodically on our internal continuous builds, and it
appears to be happening due to a race condition.

The test is testing what happens when an SSLSocket that's wrapping an
underlying Socket is blocked on a read and then underlying socket is
closed by another thread.  There appears to be a race condition
between the OS waking up the reading thread and the write of -1 to
java.io.FileDescriptor's private field.  If the reading thread wakes
up and proceeds past the check of the file descriptor's validity
before the field write is visible, then it will attempt to call
SSL_read() and get ERR_SSL_SYSCALL, and it responds by returning -1,
whereas the test expects SocketException to be thrown (which it does
if the file descriptor is invalid).

This changes the code to always throw SocketException when
ERR_SSL_SYSCALL is reported with a return value of 0, which the
BoringSSL docs say happens ""if the transport returned EOF"", which
should mean the file descriptor is closed./Finalization safety for SSL_CTX objects. (#427)/"
,,Conscrypt,"Add compile option for checking error queue. (#416)

We need to ensure that the BoringSSL error stack is clear when
returning from native functions, otherwise a later function might
inspect the error stack and interpret it incorrectly.

Adds the compile option CONSCRYPT_CHECK_ERROR_QUEUE which enables the
macro CHECK_ERROR_QUEUE_ON_RETURN.  That macro, when enabled, creates
a class that checks the error queue is empty in its destructor.  The
macro has been added to almost every native method called from Java,
enforcing that the error queue is empty when we return from native
code back to Java code.

Adds the gradle property checkErrorQueue to enable the code and adds
it to the Travis config.

Also fixes a couple places found by this checking that we were failing
to clear the error queue after handling errors./"
,,Conscrypt,"Clean up exception throwing in native code. (#417)

Change throwExceptionIfNecessary to throwExceptionFromBoringSSLError.
The definition changes from throwing an exception if there's an error
on the stack to having a precondition of having an error on the stack.
This makes its behavior and the expected usage more clear (it always
results in an exception pending).  This also should let us know if
we're encountering return-failure-but-don't-stack-an-error situations
that we didn't know about.

Normalize function names to throwFooException.

Ensure that we always return immediately after throwing an exception.
Some call sites allowed the exception-throwing branch to fall through
to the return statement from a non-throwing branch, which is unclear,
since that return statement is useless, and runs the risk of
additional code being inserted after the exception.

Fixes #97./Mark unused parameters in native_crypto.cc (#412)

Our Android build rules generate errors for unused parameters.  We
can't enable the warnings in the external build rules because
BoringSSL has many unused parameters and we build the two together in
the external build./More finalization safety. (#410)

This updates OpenSSLX509Certificate and OpenSSLX509CRL in the same way
as NativeSsl was done previously./"
,,Conscrypt,"Pass NativeSsl references to NativeCrypto (#408)

* Pass NativeSsl references to NativeCrypto

The existing implementation of passing raw addresses to NativeCrypto
can cause issues where the native code may still be executing when the
finalizer runs and frees the underlying native resources.  A call to
NativeSsl.read(), for instance, is not enough to keep the NativeSsl or
its owning socket alive, so if it's waiting for input the finalizer
can run.  Switching to passing the Java object to native code keeps
the Java object alive for GC purposes, preventing its finalizer from
running.

As part of this, also move the freeing of NativeSsl instances into a
finalizer on NativeSsl instead of on the sockets.  The sockets can
still become garbage even if the NativeSsl is kept alive, so we only
want to free it when the NativeSsl itself is garbage.

We will also want to do this for other native objects, but SSL*
instances are by far the most-used native objects and the most likely
to be used in a long-running I/O operation, so starting here gives us
a lot of benefit.

* Reliably close objects in tests.

* Pass both pointer and Java reference.

This allows us to access the SSL* pointer without having to indirect
through the Java object's fields, but still prevents the NativeSsl
from being GCed while the method is being run.

* Explain unsafe finalization fix in NativeCrypto Javadoc./"
,,Conscrypt,"Fix error detection in RSA_generate_key_ex. (#398)

RSA_generate_key_ex returns 1 on success and 0 on failure, so we could
never detect failures that happened.  Also update an allocation
failure to throw OutOfMemoryError instead of RuntimeException./Add support for accessing tls-unique channel binding value. (#388)/"
,,Conscrypt,"Check an X509-like structure submember for nullness. (#380)

We've seen very sporadic crashes due to null pointer dereferencing
somewhere inside X509_get_ext_by_critical, and this is the only way I
can see that that can happen.  X509_get_ext_by_critical passes
x->cert_info->extensions to X509v3_get_ext_by_critical, and that's the
only pointer that isn't explicitly checked for nullness.  These
crashes are incredibly rare, so it's not out of the realm of
possibility for them to be memory corruption or something, but better
safe than sorry./"
Network Management,Network Management,Conscrypt,"Clean up exception throwing in native code. (#417)

Change throwExceptionIfNecessary to throwExceptionFromBoringSSLError.
The definition changes from throwing an exception if there's an error
on the stack to having a precondition of having an error on the stack.
This makes its behavior and the expected usage more clear (it always
results in an exception pending).  This also should let us know if
we're encountering return-failure-but-don't-stack-an-error situations
that we didn't know about.

Normalize function names to throwFooException.

Ensure that we always return immediately after throwing an exception.
Some call sites allowed the exception-throwing branch to fall through
to the return statement from a non-throwing branch, which is unclear,
since that return statement is useless, and runs the risk of
additional code being inserted after the exception.

Fixes #97./Check if an exception is pending before throwing another one. (#386)

This can occur if a BoringSSL call results in a socket read from a
Java socket which throws an exception.  Since throwing an exception
when another exception is pending causes the process to crash, just
let the other exception propagate out of the function./"
,,Conscrypt,"Move session value API into ProvidedSessionDecorator. (#389)

* Move session value API into ProvidedSessionDecorator.

The application-level value API on SSLSession objects makes them
mutable, which means that using singleton objects (like we do with
SSLNullSession) or swapping out implementations (like we do with
ActiveSession/SessionSnapshot) doesn't work properly if you actually
want to use this API.  By moving it into ProvidedSessionDecorator, it
can be used without any of these problems.

* Rename ProvidedSessionDecorator to ExternalSession./"
Network Management,Network Management,Conscrypt,"Pass NativeSsl references to NativeCrypto (#408)

* Pass NativeSsl references to NativeCrypto

The existing implementation of passing raw addresses to NativeCrypto
can cause issues where the native code may still be executing when the
finalizer runs and frees the underlying native resources.  A call to
NativeSsl.read(), for instance, is not enough to keep the NativeSsl or
its owning socket alive, so if it's waiting for input the finalizer
can run.  Switching to passing the Java object to native code keeps
the Java object alive for GC purposes, preventing its finalizer from
running.

As part of this, also move the freeing of NativeSsl instances into a
finalizer on NativeSsl instead of on the sockets.  The sockets can
still become garbage even if the NativeSsl is kept alive, so we only
want to free it when the NativeSsl itself is garbage.

We will also want to do this for other native objects, but SSL*
instances are by far the most-used native objects and the most likely
to be used in a long-running I/O operation, so starting here gives us
a lot of benefit.

* Reliably close objects in tests.

* Pass both pointer and Java reference.

This allows us to access the SSL* pointer without having to indirect
through the Java object's fields, but still prevents the NativeSsl
from being GCed while the method is being run.

* Explain unsafe finalization fix in NativeCrypto Javadoc./"
,,Conscrypt,Add support for accessing tls-unique channel binding value. (#388)/
Network Management,Network Management,Conscrypt,"Move session value API into ProvidedSessionDecorator. (#389)

* Move session value API into ProvidedSessionDecorator.

The application-level value API on SSLSession objects makes them
mutable, which means that using singleton objects (like we do with
SSLNullSession) or swapping out implementations (like we do with
ActiveSession/SessionSnapshot) doesn't work properly if you actually
want to use this API.  By moving it into ProvidedSessionDecorator, it
can be used without any of these problems.

* Rename ProvidedSessionDecorator to ExternalSession./"
,,Conscrypt,"Refactoring externalization of SSLSessions (#383)

This is an implementation to #381. This change attempts to provide more
consistency to the session that is returned to the caller by `ConscryptEngine`/`ConscryptFileDescriptorSocket`.

Main changes:

- New interface ConscryptSession adds a few methods currently only defined by ActiveSession
- New interface SessionDecorator that defines getDelegate()
- New class ProvidedSessionDecorator delegates to an external provider of the ""current"" session. The provider implementations are in ConscryptEngine and ConscryptFileDescriptorSocket.
- New class SessionSnapshot that takes a snapshot of any ConscryptSession.
- Changed ActiveSession and SSLNullSession to implement ConscryptSession.
- Updated ConscryptEngine/ConscryptFileDescriptorSocket to create a SessionSnapshot when closing.

Additional cleanup:

- Split out Java7SessionWrapper into two classes: Java7ExtendedSSLSession and Java8ExtendedSSLSession. The Java 8 version no longer requires reflection and is more consistent with platform-specific code elsewhere. Both classes implement SessionDecorator.
- Renamed SslWrapper->NativeSsl and SslSessionWrapper->NativeSslSession for clarity, since the term ""wrapper"" was being overloaded.

Fixes #379/"
,,Conscrypt,"Add tests for classes in java.security. (#434)

These tests are ed from AOSP with minor modifications.  They test
the behavior of the CertificateFactory, MessageDigest, and Signature
classes, as well as a bunch of varieties of
AlgorithmParameterGenerator, AlgorithmParameters, KeyFactory, and
KeyPairGenerator classes for all installed providers (which includes
Conscrypt when run under the ConscryptSuite).

As usual, I've adapted them to JUnit4, removed the tests for the
classes themselves and the provider framework, and made them work
under OpenJDK as well as Android.

The quality of these tests is again variable, but that can be improved
once they're in and running./"
,,Conscrypt,"Use printf when tracing on non-Android platforms. (#437)

If the ALOG macro isn't defined, we define it in macros.h to just do
nothing, which means that enabling tracing on OpenJDK or other
platforms doesn't actually result in any tracing output.  On
non-Android platforms, we can make the tracing macros use printf()
instead of ALOG./"
,,Conscrypt,"Add a function to force an engine read. (#453)

The SSLEngine implementation sometimes need to prompt BoringSSL
to process any incoming TLS data in order to determine whether
there is enough data to produce plaintext.  We previously were
reading into a zero-byte buffer to do this, but that causes
ambiguity in return values because a return of 0 from SSL_read()
could mean a failure (generally EOF) or could mean a successful
read of 0 bytes (which would be expected when reading into a
zero-byte array).  Instead, use SSL_peek.

Fixes #452./"
,,Conscrypt,"Force struct timeval's tv_usec to print as long. (#474)
Apparently some Darwin environments have 32-bit suseconds_t, which
means we get a warning trying to print it via %ld.  Just force
whatever we have to be a long, so it always gets printed properly./"""
,,Conscrypt,"Add logging macros that work on all platforms. (#462)

This adds CONSCRYPT_LOG_X macros that redirect to either ALOG on
Android or fprintf(stderr) on non-Android.  In the future, we could
use these to allow users to register a logging callback and send the
logs to a destination of their choice (via java.util.Logger or log4j
or what have you), but for now we'll keep it simple.

Fixes #460./"
,,Conscrypt,"Add support for token binding and EKM. (#445)
Token binding allows higher-level protocols to bind their higher-level
authentication tokens to their TLS sessions, making it more difficult
for attackers to present those higher-level tokens in a future
session.  At the TLS level, all that needs to occur is a parameter
negotiation, the rest of the token binding protocol is left up to the
caller.

Parameter options are specified as integers, and I decided not to
supply constants for the currently-defined values.  This is a niche
enough use case that any user of it should be able to decide what,
values they want to support (and will want to share constants with
whatever higher-level protocol they're using with token binding).
BoringSSL also doesn't supply constants for these values, so we're in
good company there.
Keying material exporting (aka EKM, for exported keying material) is
specified in RFC 5705, and is necessary for implementing token binding"
,,Conscrypt,"Parse ASN1_TIME structures in constructors. (#446)

The legacy OpenSSL APIs in BoringSSL don't parse ASN1_TIME values
until they're used, which means that the existing code could explode
in the middle of X509Certificate.getNotAfter() and similar
Date-returning calls, and those calls aren't declared to throw
anything.  Instead, read and cache the values in the constructor
where we can throw a relevant exception if necessary.

We have to clone the Date values when returning them because Date is"
,,Conscrypt,"Add logging macros that work on all platforms. (#462)

This adds CONSCRYPT_LOG_X macros that redirect to either ALOG on
Android or fprintf(stderr) on non-Android.  In the future, we could
use these to allow users to register a logging callback and send the
logs to a destination of their choice (via java.util.Logger or log4j
or what have you), but for now we'll keep it simple.

Fixes #460./"
,,Conscrypt,"Disallow invalid SNI hostnames in setHostname(). (#470)

The code that sets the SNI value on the connection checks for

impl.getUseSni() && AddressUtils.isValidSniHostname(hostname)

which is good for hostnames that were supplied as the hostname to
connect to, since they may or may not be valid for SNI, but means that
if you set an invalid hostname with setHostname() it will just
silently be omitted from the connection and no SNI extension will be
included in the handshake.  Better to reject the hostname immediately.

Also disallow hostnames with trailing dots, which aren't legal SNI
hostnames per RFC 6066.

Also disallow null bytes./"
,,Conscrypt,"Update short buffer handling. (#440)

This fixes a number of problems in Conscrypt's ciphers when they are
given an output buffer that is too small for the output.

We didn't specifically handle CIPHER_R_BUFFER_TOO_SMALL errors from
BoringSSL, so we threw RuntimeException on encountering them.  We
should be throwing ShortBufferException.

Raw ChaCha20 didn't check for a short buffer at all, so it just passed
the arrays down to native code, which could cause crashes or other
weird behavior.

EVP_AEAD ciphers used update() to only record data that needed to be
encrypted and then did the actual encrypting in doFinal().  This
combined with our implementation that implements doFinal() as a
combination of updateInternal() + doFinalInternal() led to a situation
where if the buffer passed to doFinal() was too small, the data would
get added to the buffer to be encrypted in updateInternal() and then
doFinalInternal() call would fail, which would mean a future call to
doFinal() with the same data would end up encrypting that data twice.
This call pattern is used by some internal CipherSpi methods from
OpenJDK, so you could see it in practice even if the caller did
nothing wrong.

Also add tests around short buffer handling./"
,,Conscrypt,"Calculate output size ourselves for AES/GCM and ChaCha20. (#438)

Since both AES/GCM and ChaCha20 don't have any variance in the length
of their AEAD tags, we can just calculate the output length directly
rather than making a JNI call.  This gives better bounds for the
necessary buffer size; in particular, we previously weren't factoring
in the removal of the tag during decryption, so we always demanded an
unnecessarily-long buffer./"
,,Conscrypt,"Add a function to force an engine read. (#453)

The SSLEngine implementation sometimes need to prompt BoringSSL
to process any incoming TLS data in order to determine whether
there is enough data to produce plaintext.  We previously were
reading into a zero-byte buffer to do this, but that causes
ambiguity in return values because a return of 0 from SSL_read()
could mean a failure (generally EOF) or could mean a successful
read of 0 bytes (which would be expected when reading into a
zero-byte array).  Instead, use SSL_peek.

Fixes #452./"
,,Conscrypt,"Allow localhost as an SNI hostname. (#475)

As an exception to the normal rule that you can't have an SNI hostname
without dots, localhost is a fine hostname for SNI./"
,,Conscrypt,"Disallow invalid SNI hostnames in setHostname(). (#470)

The code that sets the SNI value on the connection checks for

impl.getUseSni() && AddressUtils.isValidSniHostname(hostname)

which is good for hostnames that were supplied as the hostname to
connect to, since they may or may not be valid for SNI, but means that
if you set an invalid hostname with setHostname() it will just
silently be omitted from the connection and no SNI extension will be
included in the handshake.  Better to reject the hostname immediately.

Also disallow hostnames with trailing dots, which aren't legal SNI
hostnames per RFC 6066.

Also disallow null bytes./"
,,Conscrypt,"Use the Conscrypt provider name in StandardNames (#435)

This makes it easier to run the tests in environments where the
provider name is different from the default, since only the one
reference in StandardNames needs to be updated./"
,,Conscrypt,"Fix NPE from closed SSLEngine. (#499)

When calling wrap() on a closed SSLEngine instance, we check to see if
anything is already pending to be written, but the native BIO will
already have been freed, so the native code threw
NullPointerException.  Check for it being already freed when looking
for pending data and just return zero in that case.

Also sets the bio pointer to 0 before freeing it and and marks it as
volatile to reduce the chances of any weird race conditions, though
the locking in ConscryptEngine should already make these impossible.

Fixes netty/netty#7988./Add support for token binding and EKM. (#445)

Token binding allows higher-level protocols to bind their higher-level
authentication tokens to their TLS sessions, making it more difficult
for attackers to present those higher-level tokens in a future
session.  At the TLS level, all that needs to occur is a parameter
negotiation, the rest of the token binding protocol is left up to the
caller.

Parameter options are specified as integers, and I decided not to
supply constants for the currently-defined values.  This is a niche
enough use case that any user of it should be able to decide what
values they want to support (and will want to share constants with
whatever higher-level protocol they're using with token binding).
BoringSSL also doesn't supply constants for these values, so we're in
good company there.

Keying material exporting (aka EKM, for exported keying material) is
specified in RFC 5705, and is necessary for implementing token binding
as well as other protocols./"
,,Conscrypt,"Disallow invalid SNI hostnames in setHostname(). (#470)

The code that sets the SNI value on the connection checks for

impl.getUseSni() && AddressUtils.isValidSniHostname(hostname)

which is good for hostnames that were supplied as the hostname to
connect to, since they may or may not be valid for SNI, but means that
if you set an invalid hostname with setHostname() it will just
silently be omitted from the connection and no SNI extension will be
included in the handshake.  Better to reject the hostname immediately.

Also disallow hostnames with trailing dots, which aren't legal SNI
hostnames per RFC 6066.

Also disallow null bytes./"
,,Conscrypt,"Add support for token binding and EKM. (#445)

Token binding allows higher-level protocols to bind their higher-level
authentication tokens to their TLS sessions, making it more difficult
for attackers to present those higher-level tokens in a future
session.  At the TLS level, all that needs to occur is a parameter
negotiation, the rest of the token binding protocol is left up to the
caller.

Parameter options are specified as integers, and I decided not to
supply constants for the currently-defined values.  This is a niche
enough use case that any user of it should be able to decide what
values they want to support (and will want to share constants with
whatever higher-level protocol they're using with token binding).
BoringSSL also doesn't supply constants for these values, so we're in
good company there.

Keying material exporting (aka EKM, for exported keying material) is
specified in RFC 5705, and is necessary for implementing token binding
as well as other protocols./"
,,Conscrypt,"Remove SHA-2 CBC cipher suites. (#484)

We automatically  our supported set of cipher suites from
BoringSSL, and they removed support for these in
https://boringssl.googlesource.com/boringssl/+/6e678eeb./"
Network Management,Network Management,Conscrypt,"Support TLS 1.3 sessions. (#515)

TLS 1.3 sessions are designed to only be used once, preventing
correlation across connections.  This implements support for those
sessions by removing such sessions from the cache whenever they're
retrieved from the session cache and not sending them to the
persistent (filesystem-based) cache, which we currently cannot delete
individual items from.

Also switch our per-host-port caches to support a list of cached
sessions rather than a single one, so that if we get multiple sessions
from the peer we can cache them all and use them when establishing
multiple connections in parallel or such things./"
,,Conscrypt,"Obey supported_signature_algorithms when present (#521)

In TLS 1.2, servers can supply a list of supported signature,,
algorithms, and we should base our key choice on what is supported by
the server.  In TLS 1.3, this is the only mechanism for supplying the
server's requirements, so this is necessary for TLS 1.3 support./"""
,,Conscrypt,"Support opaque keys with RSA-PSS signatures. (#513)

TLS 1.3 only uses RSA-PSS signatures (rather than RSA-PKCS#1), so we
need to support these.  Implement it by switching to
Cipher.RSA/ECB/NoPadding instead of using Signature.RSA.  Fix the
opaque key tests so they actually work properly./"
,,Conscrypt,"Stop using set_options to set protocol support. (#500)

The *_set_min_protocol_version and *_set_max_protocol_version
functions were introduced to be explicit about setting supported
versions.  Use those functions instead of SSL_set_options with
disabling constants.

There aren't any higher-level tests because this is already covered by
tests like SSLSocket.test_SSLSocket_setEnabledProtocols
SSLSocket.test_SSLSocket_noncontiguousProtocols_useLower, etc./"""
Network Management,Network Management,Conscrypt,"Provide TrustManagerFactory (#516)

This is necessary for users who want to enable TLS 1.3, since the
TrustManagerFactory implementation shipped with OpenJDK throws an
exception if it encounters an SSLSocket or SSLEngine that's negotiated
TLS 1.3.  Use our TrustManager in tests.

Also adds a HostnameVerifier in the tests that does the simplest
thing, because our TrustManager verifies hostnames by default, whereas
the OpenJDK one doesn't, and the bundled HostnameVerifier on OpenJDK
just fails to verify anything./"
Network Management,Network Management,Conscrypt,"Merge ConscryptSocketBase and AbstractConscryptSocket (#529)

We used to need this separation because we had delegating sockets that
inherited from AbstractConscryptSocket but not ConscryptSocketBase,
but those are gone now, so we no longer need to have two classes here.

While the remaining file is named AbstractConscryptSocket (to parallel
AbstractConscryptEngine), what really happened is the abstract methods
from AbstractConscryptSocket got merged into ConscryptSocketBase, and
then the resulting class was renamed./"
,,Conscrypt,"fix #491 ConscryptEngine.closeInbound should free resources (#511)

If closeOutbound is invoked prior to closeInbound, resources should
be freed./"
Network Management,Network Management,Conscrypt,"Don't init cipher suites if native code didn't load (#517)

The try/catch and setting of loadError prevents the first static
initializer in NativeCrypto from rendering the class unloadable, but
the second static initializer still does, which means that users tend
to get NoClassDefFoundErrors when they try to use Conscrypt instead of
a more useful UnsatisfiedLinkError that points to the root cause./"
,,Conscrypt,"Stop using set_options to set protocol support. (#500)

The *_set_min_protocol_version and *_set_max_protocol_version
functions were introduced to be explicit about setting supported
versions.  Use those functions instead of SSL_set_options with
disabling constants.

There aren't any higher-level tests because this is already covered by
tests like SSLSocket.test_SSLSocket_setEnabledProtocols,
SSLSocket.test_SSLSocket_noncontiguousProtocols_useLower, etc./"
Network Management,Network Management,Conscrypt,"Provide TrustManagerFactory (#516)

This is necessary for users who want to enable TLS 1.3, since the
TrustManagerFactory implementation shipped with OpenJDK throws an
exception if it encounters an SSLSocket or SSLEngine that's negotiated
TLS 1.3.  Use our TrustManager in tests.

Also adds a HostnameVerifier in the tests that does the simplest
thing, because our TrustManager verifies hostnames by default, whereas
the OpenJDK one doesn't, and the bundled HostnameVerifier on OpenJDK
just fails to verify anything./"
,,Conscrypt,"Support opaque keys with RSA-PSS signatures. (#513)

TLS 1.3 only uses RSA-PSS signatures (rather than RSA-PKCS#1), so we
need to support these.  Implement it by switching to
Cipher.RSA/ECB/NoPadding instead of using Signature.RSA.  Fix the
opaque key tests so they actually work properly./"
Network Management,Network Management,Conscrypt,"Support TLS 1.3 (#524)

Enables support for negotiating TLS 1.3.  TLS 1.3 is not enabled
unless SSLContext.TLSv1.3 is ed or setEnabledProtocols() is
called with a set of values that includes TLSv1.3.

Detailed changes:

Adds protocol constants for TLS 1.3, and provides SSLContext.TLSv1.3,
which has TLS 1.3 enabled by default.

Adjusts cipher suite code for TLS 1.3 suites.  When enabled, all TLS
1.3 cipher suites are always returned from supportedCipherSuites() and
enabledCipherSuites().  Attempts to customize TLS 1.3 cipher suites in
setEnabledCipherSuites() are ignored.

Splits {SSLEngine,SSLSocket}Test into version-dependent and
version-independent tests.  The latter remain in
{SSLEngine,SSLSocket}Test and the former move into new files
{SSLEngine,SSLSocket}VersionCompatibilityTest, which are parameterized
to test all combinations of client and server on TLS 1.2 and TLS 1.3.

Remove a pile of RI-specific TLS-related expectation declarations from
StandardNames.  We don't actually verify the behavior of the RI at any
point, so it was just making the code more confusing.

Fixes #479./"
,,Conscrypt,"Allow d2i_X509_bio and friends to not set an error (#552)

BoringSSL recently changed so that d2i_X509_bio doesn't set an error
in the error queue when it receives certain garbage inputs.  We don't
actually care what error is set in the queue (we just end up catching
whatever is thrown and throwing CertificateException), so change to
tolerate having no error in the queue./"
,,Conscrypt,"Add API to get Conscrypt version (#541)

Libraries like OkHttp want to be able to check what version of
Conscrypt is available before attempting to enable new features (like
TLS 1.3).  Add a simple API to check what version of Conscrypt is
being used.

Fixes #538./"
,,Conscrypt,"Add explicit no-arg constructors (#563)

We're somewhat inconsistent on whether we include these or not, but we
now want these to be present so we can add annotations to them to
support Android features.  This also makes it more obvious that these
classes are publicly constructible./"
Network Management,Network Management,Conscrypt,"Make OpenSSLBIOInputStream read as fully as possible (#587)

Some of the callers of BIO APIs in BoringSSL (particularly the DER
decoder) depend on the BIO read returning either a full buffer or an
EOF, but InputStream allows returning less than the ed amount
of data without hitting EOF.  This causes the DER decoder to fail in
some cases (specifically, we've seen it fail with overly large OCSP
responses where the socket hasn't buffered the entire response before
we try to decode it).

Fix this by repeatedly calling read() until our output buffer is full
or EOF is reached.

Also add a test for decoding DER-encoded certs, we previously only
tested PEM-encoded ones./"
,,Conscrypt,"Allow d2i_X509_bio and friends to not set an error (#552)

BoringSSL recently changed so that d2i_X509_bio doesn't set an error
in the error queue when it receives certain garbage inputs.  We don't
actually care what error is set in the queue (we just end up catching
whatever is thrown and throwing CertificateException), so change to,,
tolerate having no error in the queue./"""
Network Management,Network Management,Conscrypt,"Add SSLEngine ClientHello tests (#570)

SSLSocket has had these tests for a while but they've been missing
from SSLEngine./Support TLS 1.3 (#524)

Enables support for negotiating TLS 1.3.  TLS 1.3 is not enabled
unless SSLContext.TLSv1.3 is ed or setEnabledProtocols() is
called with a set of values that includes TLSv1.3.

Detailed changes:

Adds protocol constants for TLS 1.3, and provides SSLContext.TLSv1.3,
which has TLS 1.3 enabled by default.

Adjusts cipher suite code for TLS 1.3 suites.  When enabled, all TLS
1.3 cipher suites are always returned from supportedCipherSuites() and
enabledCipherSuites().  Attempts to customize TLS 1.3 cipher suites in
setEnabledCipherSuites() are ignored.

Splits {SSLEngine,SSLSocket}Test into version-dependent and
version-independent tests.  The latter remain in
{SSLEngine,SSLSocket}Test and the former move into new files
{SSLEngine,SSLSocket}VersionCompatibilityTest, which are parameterized
to test all combinations of client and server on TLS 1.2 and TLS 1.3.

Remove a pile of RI-specific TLS-related expectation declarations from
StandardNames.  We don't actually verify the behavior of the RI at any
point, so it was just making the code more confusing.

Fixes #479./"
,,Conscrypt,Switch preferred SSLContext to TLSv13/
,,Conscrypt,"Update Provider installation in tests (#629)

Change CertPinManagerTest and CTVerifierTest to only install the
provider if it's not installed already and check if the provider was
installed before uninstalling.  Otherwise, when running on the Android
platform they will uninstall the platform copy of Conscrypt and break
every following test./"
,,Conscrypt,"Update Provider installation in tests (#629)

Change CertPinManagerTest and CTVerifierTest to only install the
provider if it's not installed already and check if the provider was
installed before uninstalling.  Otherwise, when running on the Android
platform they will uninstall the platform copy of Conscrypt and break
every following test./Drop support for Java 6 (#606)/"
,,Conscrypt,"Restore C++ API to use to CompatibilityCloseMonitor.

Still needed for unbundled Conscrypt on Android versions which
do not have a C API to AsynchronousCloseMonitor.

Test: ./gradlew check/"
,,Conscrypt,Drop support for Java 6 (#606)/
Network Management,Network Management,Conscrypt,"Make ConscryptEngineSocket call correct methods (#643)

ConscryptEngineSocket uses an SSLEngine internally to function, and
that naturally calls the SSLEngine-accepting methods on
X509ExtendedTrustManager to do trust checks.  We were passing our
given trust manager directly to the SSLEngine, which resulted in an
SSLSocket implementation that ended up calling SSLEngine-based
methods, which isn't right.

Instead, create a delegating trust manager that maps the calls to the
correct objects.  On platforms where X509ExtendedTrustManager isn't
available, we can pass in the provided trust manager directly, since
it doesn't receive a reference to the calling object.

Also adds tests for getHandshakeSession() that ensure it functions in
the middle of the handshake and provides properties that should be
set./"
,,Conscrypt,"Support updateAAD(ByteBuffer) (#647)/
,,Conscrypt,Add server certificate SNI support (#712)/"
,,Conscrypt,",Run tests on Java 11 as well as 7 and 8 (#668)/
"
